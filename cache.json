{"2024-06-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.16858v1","updated":"2024-06-24T17:59:11Z","published":"2024-06-24T17:59:11Z","title":"EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees","summary":"  Inference with modern Large Language Models (LLMs) is expensive and\ntime-consuming, and speculative sampling has proven to be an effective\nsolution. Most speculative sampling methods such as EAGLE use a static draft\ntree, implicitly assuming that the acceptance rate of draft tokens depends only\non their position. Interestingly, we found that the acceptance rate of draft\ntokens is also context-dependent. In this paper, building upon EAGLE, we\npropose EAGLE-2, which introduces a new technique of context-aware dynamic\ndraft tree into drafting modeling. This improvement leverages the fact that the\ndraft model of EAGLE is well-calibrated: the confidence scores from the draft\nmodel approximate acceptance rates with small errors. We conducted extensive\nevaluations on three series of LLMs and six tasks, with EAGLE-2 achieving\nspeedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also\nensures that the distribution of the generated text remains unchanged, making\nit a lossless acceleration algorithm.\n","authors":["Yuhui Li","Fangyun Wei","Chao Zhang","Hongyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16851v1","updated":"2024-06-24T17:58:03Z","published":"2024-06-24T17:58:03Z","title":"Losing Visual Needles in Image Haystacks: Vision Language Models are\n  Easily Distracted in Short and Long Contexts","summary":"  We present LoCoVQA, a dynamic benchmark generator for evaluating long-context\nextractive reasoning in vision language models (VLMs). LoCoVQA augments test\nexamples for mathematical reasoning, VQA, and character recognition tasks with\nincreasingly long visual contexts composed of both in-distribution and\nout-of-distribution distractor images.\n  Across these tasks, a diverse set of VLMs rapidly lose performance as the\nvisual context length grows, often exhibiting a striking exponential decay\ntrend. This test assesses how well VLMs can ignore irrelevant information when\nanswering queries -- a task that is quite easy for language models (LMs) in the\ntext domain -- demonstrating that current state-of-the-art VLMs lack this\nessential capability for many long-context applications.\n","authors":["Aditya Sharma","Michael Saxon","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.16851v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2406.16845v1","updated":"2024-06-24T17:49:28Z","published":"2024-06-24T17:49:28Z","title":"RaTEScore: A Metric for Radiology Report Generation","summary":"  This paper introduces a novel, entity-aware metric, termed as Radiological\nReport (Text) Evaluation (RaTEScore), to assess the quality of medical reports\ngenerated by AI models. RaTEScore emphasizes crucial medical entities such as\ndiagnostic outcomes and anatomical details, and is robust against complex\nmedical synonyms and sensitive to negation expressions. Technically, we\ndeveloped a comprehensive medical NER dataset, RaTE-NER, and trained an NER\nmodel specifically for this purpose. This model enables the decomposition of\ncomplex radiological reports into constituent medical entities. The metric\nitself is derived by comparing the similarity of entity embeddings, obtained\nfrom a language model, based on their types and relevance to clinical\nsignificance. Our evaluations demonstrate that RaTEScore aligns more closely\nwith human preference than existing metrics, validated both on established\npublic benchmarks and our newly proposed RaTE-Eval benchmark.\n","authors":["Weike Zhao","Chaoyi Wu","Xiaoman Zhang","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2406.16845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16842v1","updated":"2024-06-24T17:47:55Z","published":"2024-06-24T17:47:55Z","title":"Exploring Factual Entailment with NLI: A News Media Study","summary":"  We explore the relationship between factuality and Natural Language Inference\n(NLI) by introducing FactRel -- a novel annotation scheme that models\n\\textit{factual} rather than \\textit{textual} entailment, and use it to\nannotate a dataset of naturally occurring sentences from news articles. Our\nanalysis shows that 84\\% of factually supporting pairs and 63\\% of factually\nundermining pairs do not amount to NLI entailment or contradiction,\nrespectively, suggesting that factual relationships are more apt for analyzing\nmedia discourse. We experiment with models for pairwise classification on the\nnew dataset, and find that in some cases, generating synthetic data with GPT-4\non the basis of the annotated dataset can improve performance. Surprisingly,\nfew-shot learning with GPT-4 yields strong results on par with medium LMs\n(DeBERTa) trained on the labelled dataset. We hypothesize that these results\nindicate the fundamental dependence of this task on both world knowledge and\nadvanced reasoning abilities.\n","authors":["Guy Mor-Lan","Effi Levi"],"pdf_url":"https://arxiv.org/pdf/2406.16842v1.pdf","comment":"Presented at *SEM 2024"},{"id":"http://arxiv.org/abs/2406.16838v1","updated":"2024-06-24T17:45:59Z","published":"2024-06-24T17:45:59Z","title":"From Decoding to Meta-Generation: Inference-time Algorithms for Large\n  Language Models","summary":"  One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.\n","authors":["Sean Welleck","Amanda Bertsch","Matthew Finlayson","Hailey Schoelkopf","Alex Xie","Graham Neubig","Ilia Kulikov","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2406.16838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16833v1","updated":"2024-06-24T17:41:53Z","published":"2024-06-24T17:41:53Z","title":"USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and\n  $\\underline{D}$ogmatism in Long $\\underline{C}$onversations","summary":"  Identifying user's opinions and stances in long conversation threads on\nvarious topics can be extremely critical for enhanced personalization, market\nresearch, political campaigns, customer service, conflict resolution, targeted\nadvertising, and content moderation. Hence, training language models to\nautomate this task is critical. However, to train such models, gathering manual\nannotations has multiple challenges: 1) It is time-consuming and costly; 2)\nConversation threads could be very long, increasing chances of noisy\nannotations; and 3) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Inspired by the recent success of large language\nmodels (LLMs) for complex natural language processing (NLP) tasks, we leverage\nMistral Large and GPT-4 to automate the human annotation process on the\nfollowing two tasks while also providing reasoning: i) User Stance\nclassification, which involves labeling a user's stance of a post in a\nconversation on a five-point scale; ii) User Dogmatism classification, which\ndeals with labeling a user's overall opinion in the conversation on a\nfour-point scale. The majority voting on zero-shot, one-shot, and few-shot\nannotations from these two LLMs on 764 multi-user Reddit conversations helps us\ncurate the USDC dataset. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models for the 5-class stance and 4-class\ndogmatism classification tasks. We make the code and dataset publicly available\n[https://anonymous.4open.science/r/USDC-0F7F].\n","authors":["Mounika Marreddy","Subba Reddy Oota","Venkata Charan Chinni","Manish Gupta","Lucie Flek"],"pdf_url":"https://arxiv.org/pdf/2406.16833v1.pdf","comment":"32 pages, 18 figures"},{"id":"http://arxiv.org/abs/2406.16829v1","updated":"2024-06-24T17:38:02Z","published":"2024-06-24T17:38:02Z","title":"Understanding and Mitigating Tokenization Bias in Language Models","summary":"  State-of-the-art language models are autoregressive and operate on subword\nunits known as tokens. Specifically, one must encode the conditioning string\ninto a list of tokens before passing to the language models for next-token\nprediction. We show that, for encoding schemes such as maximum prefix matching,\ntokenization induces a sampling bias that cannot be mitigated with more\ntraining or data. To counter this universal problem, we propose a novel\nalgorithm to obtain unbiased estimates from a model that was trained on\ntokenized data. Our method does not require finetuning the model, and its\ncomplexity, defined as the number of model runs, scales linearly with the\nsequence length. As a consequence, we show that one can simulate token-free\nbehavior from a tokenized language model. We empirically verify the correctness\nof our method through a Markov-chain setup, where it accurately recovers the\ntransition probabilities, as opposed to the conventional method of directly\nprompting tokens into the language model.\n","authors":["Buu Phan","Marton Havasi","Matthew Muckley","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2406.16829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16828v1","updated":"2024-06-24T17:37:52Z","published":"2024-06-24T17:37:52Z","title":"Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024\n  Retrieval-Augmented Generation Track","summary":"  Did you try out the new Bing Search? Or maybe you fiddled around with Google\nAI~Overviews? These might sound familiar because the modern-day search stack\nhas recently evolved to include retrieval-augmented generation (RAG) systems.\nThey allow searching and incorporating real-time data into large language\nmodels (LLMs) to provide a well-informed, attributed, concise summary in\ncontrast to the traditional search paradigm that relies on displaying a ranked\nlist of documents. Therefore, given these recent advancements, it is crucial to\nhave an arena to build, test, visualize, and systematically evaluate RAG-based\nsearch systems. With this in mind, we propose the TREC 2024 RAG Track to foster\ninnovation in evaluating RAG systems. In our work, we lay out the steps we've\nmade towards making this track a reality -- we describe the details of our\nreusable framework, Ragnar\\\"ok, explain the curation of the new MS MARCO V2.1\ncollection choice, release the development topics for the track, and\nstandardize the I/O definitions which assist the end user. Next, using\nRagnar\\\"ok, we identify and provide key industrial baselines such as OpenAI's\nGPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface\nfor an interactive arena allowing benchmarking pairwise RAG systems by\ncrowdsourcing. We open-source our Ragnar\\\"ok framework and baselines to achieve\na unified standard for future RAG systems.\n","authors":["Ronak Pradeep","Nandan Thakur","Sahel Sharifymoghaddam","Eric Zhang","Ryan Nguyen","Daniel Campos","Nick Craswell","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2406.16828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15193v2","updated":"2024-06-24T17:36:11Z","published":"2024-06-21T14:35:16Z","title":"Reward Steering with Evolutionary Heuristics for Decoding-time Alignment","summary":"  The widespread applicability and increasing omnipresence of LLMs have\ninstigated a need to align LLM responses to user and stakeholder preferences.\nMany preference optimization approaches have been proposed that fine-tune LLM\nparameters to achieve good alignment. However, such parameter tuning is known\nto interfere with model performance on many tasks. Moreover, keeping up with\nshifting user preferences is tricky in such a situation. Decoding-time\nalignment with reward model guidance solves these issues at the cost of\nincreased inference time. However, most of such methods fail to strike the\nright balance between exploration and exploitation of reward -- often due to\nthe conflated formulation of these two aspects - to give well-aligned\nresponses. To remedy this we decouple these two aspects and implement them in\nan evolutionary fashion: exploration is enforced by decoding from mutated\ninstructions and exploitation is represented as the periodic replacement of\npoorly-rewarded generations with well-rewarded ones. Empirical evidences\nindicate that this strategy outperforms many preference optimization and\ndecode-time alignment approaches on two widely accepted alignment benchmarks\nAlpacaEval 2 and MT-Bench. Our implementation will be available at:\nhttps://darwin-alignment.github.io.\n","authors":["Chia-Yu Hung","Navonil Majumder","Ambuj Mehrish","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2406.15193v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16810v1","updated":"2024-06-24T17:22:36Z","published":"2024-06-24T17:22:36Z","title":"PISTOL: Dataset Compilation Pipeline for Structural Unlearning of LLMs","summary":"  Recently, machine unlearning, which seeks to erase specific data stored in\nthe pre-trained or fine-tuned models, has emerged as a crucial protective\nmeasure for LLMs. However, unlearning approaches for LLMs that have been\nconsidered thus far have focused on the removal of independent data points and\nhave not taken into account that the stored facts are logically connected to\none another and form an implicit knowledge graph. To facilitate the development\nof structural unlearning methods, which are essential for the practical\napplication of unlearning, we propose PISTOL, a pipeline for compiling\nmulti-scenario datasets for benchmarking structural LLM unlearning.\nAdditionally, leveraging sample datasets synthesized using PISTOL, we conducted\nbenchmarks with four distinct unlearning methods on both Llama2-7B and\nMistral-7B models. This analysis helps to illustrate the prevailing challenges\nin effectively and robustly removing highly inter-connected data, batched data,\nor data skewed towards a specific domain. It also highlights the choice of\npre-trained model can impact unlearning performance. This work not only\nadvances our understandings on the limitation of current LLMs unlearning\nmethods and proposes future research directions, but also provides a replicable\nframework for ongoing exploration and validation in the field.\n","authors":["Xinchi Qiu","William F. Shen","Yihong Chen","Nicola Cancedda","Pontus Stenetorp","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2406.16810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16807v1","updated":"2024-06-24T17:19:34Z","published":"2024-06-24T17:19:34Z","title":"Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback\n  for Text-to-Image Generation","summary":"  Human feedback plays a critical role in learning and refining reward models\nfor text-to-image generation, but the optimal form the feedback should take for\nlearning an accurate reward function has not been conclusively established.\nThis paper investigates the effectiveness of fine-grained feedback which\ncaptures nuanced distinctions in image quality and prompt-alignment, compared\nto traditional coarse-grained feedback (for example, thumbs up/down or ranking\nbetween a set of options). While fine-grained feedback holds promise,\nparticularly for systems catering to diverse societal preferences, we show that\ndemonstrating its superiority to coarse-grained feedback is not automatic.\nThrough experiments on real and synthetic preference data, we surface the\ncomplexities of building effective models due to the interplay of model choice,\nfeedback type, and the alignment between human judgment and computational\ninterpretation. We identify key challenges in eliciting and utilizing\nfine-grained feedback, prompting a reassessment of its assumed benefits and\npracticality. Our findings -- e.g., that fine-grained feedback can lead to\nworse models for a fixed budget, in some settings; however, in controlled\nsettings with known attributes, fine grained rewards can indeed be more helpful\n-- call for careful consideration of feedback attributes and potentially beckon\nnovel modeling approaches to appropriately unlock the potential value of\nfine-grained feedback in-the-wild.\n","authors":["Katherine M. Collins","Najoung Kim","Yonatan Bitton","Verena Rieser","Shayegan Omidshafiei","Yushi Hu","Sherol Chen","Senjuti Dutta","Minsuk Chang","Kimin Lee","Youwei Liang","Georgina Evans","Sahil Singla","Gang Li","Adrian Weller","Junfeng He","Deepak Ramachandran","Krishnamurthy Dj Dvijotham"],"pdf_url":"https://arxiv.org/pdf/2406.16807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16801v1","updated":"2024-06-24T17:08:17Z","published":"2024-06-24T17:08:17Z","title":"RES-Q: Evaluating Code-Editing Large Language Model Systems at the\n  Repository Scale","summary":"  The instruction-following ability of Large Language Models (LLMs) has\ncultivated a class of LLM-based systems capable of approaching complex tasks\nsuch as making edits to large code repositories. Due to the high sensitivity\nand unpredictability of LLM behavior in response to changes in prompting,\nrobust evaluation tools are needed to drive future iteration of these systems.\nWe propose RES-Q, a natural language instruction-based benchmark for evaluating\n$\\textbf{R}$epository $\\textbf{E}$diting $\\textbf{S}$ystems, which consists of\n100 repository editing tasks derived from real GitHub commits. Given an edit\ninstruction and a code repository, RES-Q evaluates an LLM system's ability to\ngather information and construct an edit that satisfies the criteria set by the\ninstruction. We argue that evaluating LLMs in this way addresses issues with\ntraditional benchmarks and provides a more holistic assessment of a model's\nabilities. We evaluate various state-of-the-art LLMs as language agents in a\nrepository-editing system built on Qurrent OS, our language agent development\nsoftware. Despite their 1% pass@1 performance difference on HumanEval, we find\nClaude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q, indicating RES-Q's\ncapacity to differentiate model capability as traditional benchmarks approach\nsaturation. We further investigate token efficiency, performance relationships\nwith existing benchmarks, and interesting disparities between closed and\nopen-source LLMs. Code and dataset are available at\nhttps://github.com/Qurrent-AI/RES-Q.\n","authors":["Beck LaBash","August Rosedale","Alex Reents","Colin Wiel"],"pdf_url":"https://arxiv.org/pdf/2406.16801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03287v2","updated":"2024-06-24T17:00:43Z","published":"2023-05-05T05:32:50Z","title":"Low-Resource Multi-Granularity Academic Function Recognition Based on\n  Multiple Prompt Knowledge","summary":"  Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally\nrequires large numbers of annotated data to achieve state-of-the-art\nperformance on a range of NLP tasks in the scientific domain. However,\nobtaining the fine-tune data for scientific NLP task is still challenging and\nexpensive. Inspired by recent advancement in prompt learning, in this paper, we\npropose the Mix Prompt Tuning (MPT), which is a semi-supervised method to\nalleviate the dependence on annotated data and improve the performance of\nmulti-granularity academic function recognition tasks with a small number of\nlabeled examples. Specifically, the proposed method provides multi-perspective\nrepresentations by combining manual prompt templates with automatically learned\ncontinuous prompt templates to help the given academic function recognition\ntask take full advantage of knowledge in PLMs. Based on these prompt templates\nand the fine-tuned PLM, a large number of pseudo labels are assigned to the\nunlabeled examples. Finally, we fine-tune the PLM using the pseudo training\nset. We evaluate our method on three academic function recognition tasks of\ndifferent granularity including the citation function, the abstract sentence\nfunction, and the keyword function, with datasets from computer science domain\nand biomedical domain. Extensive experiments demonstrate the effectiveness of\nour method and statistically significant improvements against strong baselines.\nIn particular, it achieves an average increase of 5% in Macro-F1 score compared\nwith fine-tuning, and 6% in Macro-F1 score compared with other semi-supervised\nmethod under low-resource settings. In addition, MPT is a general method that\ncan be easily applied to other low-resource scientific classification tasks.\n","authors":["Jiawei Liu","Zi Xiong","Yi Jiang","Yongqiang Ma","Wei Lu","Yong Huang","Qikai Cheng"],"pdf_url":"https://arxiv.org/pdf/2305.03287v2.pdf","comment":"This article has been accepted by The Electronic Library and the full\n  article is now available on Emerald Insight"},{"id":"http://arxiv.org/abs/2406.16797v1","updated":"2024-06-24T16:58:23Z","published":"2024-06-24T16:58:23Z","title":"Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs","summary":"  Existing methods for adapting large language models (LLMs) to new tasks are\nnot suited to multi-task adaptation because they modify all the model weights\n-- causing destructive interference between tasks. The resulting effects, such\nas catastrophic forgetting of earlier tasks, make it challenging to obtain good\nperformance on multiple tasks at the same time. To mitigate this, we propose\nLottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies\nand optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide\nrange of challenging tasks such as instruction following, reasoning, math, and\nsummarization. LoTA obtains better performance than full fine-tuning and\nlow-rank adaptation (LoRA), and maintains good performance even after training\non other tasks -- thus, avoiding catastrophic forgetting. By extracting and\nfine-tuning over \\emph{lottery tickets} (or \\emph{sparse task vectors}), LoTA\nalso enables model merging over highly dissimilar tasks.\n","authors":["Ashwinee Panda","Berivan Isik","Xiangyu Qi","Sanmi Koyejo","Tsachy Weissman","Prateek Mittal"],"pdf_url":"https://arxiv.org/pdf/2406.16797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16783v1","updated":"2024-06-24T16:45:13Z","published":"2024-06-24T16:45:13Z","title":"M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in\n  Large Language Models","summary":"  Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. Numerous effective IFT datasets have been\nproposed in the recent past, but most focus on high resource languages such as\nEnglish. In this work, we propose a fully synthetic, novel taxonomy (Evol)\nguided Multilingual, Multi-turn instruction finetuning dataset, called\nM2Lingual, to better align LLMs on a diverse set of languages and tasks.\nM2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds,\ncovering 70 languages, 17 NLP tasks and general instruction-response pairs.\nLLMs finetuned with M2Lingual substantially outperform the majority of existing\nmultilingual IFT datasets. Importantly, LLMs trained with M2Lingual\nconsistently achieve competitive results across a wide variety of evaluation\nbenchmarks compared to existing multilingual IFT datasets. Specifically, LLMs\nfinetuned with M2Lingual achieve strong performance on our translated\nmultilingual, multi-turn evaluation benchmark as well as a wide variety of\nmultilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for\nits creation. M2Lingual repository -\nhttps://huggingface.co/datasets/ServiceNow-AI/M2Lingual\n","authors":["Rishabh Maheshwary","Vikas Yadav","Hoang Nguyen","Khyati Mahajan","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2406.16783v1.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2406.16779v1","updated":"2024-06-24T16:43:11Z","published":"2024-06-24T16:43:11Z","title":"It Is Not About What You Say, It Is About How You Say It: A Surprisingly\n  Simple Approach for Improving Reading Comprehension","summary":"  Natural language processing has seen rapid progress over the past decade. Due\nto the speed of developments, some practices get established without proper\nevaluation. Considering one such case and focusing on reading comprehension, we\nask our first research question: 1) How does the order of inputs -- i.e.,\nquestion and context -- affect model performance? Additionally, given recent\nadvancements in input emphasis, we ask a second research question: 2) Does\nemphasizing either the question, the context, or both enhance performance?\nExperimenting with 9 large language models across 3 datasets, we find that\npresenting the context before the question improves model performance, with an\naccuracy increase of up to $31\\%$. Furthermore, emphasizing the context yields\nsuperior results compared to question emphasis, and in general, emphasizing\nparts of the input is particularly effective for addressing questions that\nmodels lack the parametric knowledge to answer. Experimenting with both\nprompt-based and attention-based emphasis methods, we additionally find that\nthe best method is surprisingly simple: it only requires concatenating a few\ntokens to the input and results in an accuracy improvement of up to $36\\%$,\nallowing smaller models to outperform their significantly larger counterparts.\n","authors":["Sagi Shaier","Lawrence E Hunter","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2406.16779v1.pdf","comment":"Accepted to ACL Findings"},{"id":"http://arxiv.org/abs/2406.16778v1","updated":"2024-06-24T16:40:54Z","published":"2024-06-24T16:40:54Z","title":"Finding Transformer Circuits with Edge Pruning","summary":"  The path to interpreting a language model often proceeds via analysis of\ncircuits -- sparse computational subgraphs of the model that capture specific\naspects of its behavior. Recent work has automated the task of discovering\ncircuits. Yet, these methods have practical limitations, as they rely either on\ninefficient search algorithms or inaccurate approximations. In this paper, we\nframe automated circuit discovery as an optimization problem and propose *Edge\nPruning* as an effective and scalable solution. Edge Pruning leverages\ngradient-based pruning techniques, but instead of removing neurons or\ncomponents, it prunes the \\emph{edges} between components. Our method finds\ncircuits in GPT-2 that use less than half the number of edges compared to\ncircuits found by previous methods while being equally faithful to the full\nmodel predictions on standard circuit-finding tasks. Edge Pruning is efficient\neven with as many as 100K examples, outperforming previous methods in speed and\nproducing substantially better circuits. It also perfectly recovers the\nground-truth circuits in two models compiled with Tracr. Thanks to its\nefficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale\nthat prior methods operate on. We use this setting for a case study comparing\nthe mechanisms behind instruction prompting and in-context learning. We find\ntwo circuits with more than 99.96% sparsity that match the performance of the\nfull model and reveal that the mechanisms in the two settings overlap\nsubstantially. Our case study shows that Edge Pruning is a practical and\nscalable tool for interpretability and sheds light on behaviors that only\nemerge in large models.\n","authors":["Adithya Bhaskar","Alexander Wettig","Dan Friedman","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16778v1.pdf","comment":"We release our code and data publicly at\n  https://github.com/princeton-nlp/Edge-Pruning"},{"id":"http://arxiv.org/abs/2406.16777v1","updated":"2024-06-24T16:38:17Z","published":"2024-06-24T16:38:17Z","title":"Blending LLMs into Cascaded Speech Translation: KIT's Offline Speech\n  Translation System for IWSLT 2024","summary":"  Large Language Models (LLMs) are currently under exploration for various\ntasks, including Automatic Speech Recognition (ASR), Machine Translation (MT),\nand even End-to-End Speech Translation (ST). In this paper, we present KIT's\noffline submission in the constrained + LLM track by incorporating recently\nproposed techniques that can be added to any cascaded speech translation.\nSpecifically, we integrate\nMistral-7B\\footnote{mistralai/Mistral-7B-Instruct-v0.1} into our system to\nenhance it in two ways. Firstly, we refine the ASR outputs by utilizing the\nN-best lists generated by our system and fine-tuning the LLM to predict the\ntranscript accurately. Secondly, we refine the MT outputs at the document level\nby fine-tuning the LLM, leveraging both ASR and MT predictions to improve\ntranslation quality. We find that integrating the LLM into the ASR and MT\nsystems results in an absolute improvement of $0.3\\%$ in Word Error Rate and\n$0.65\\%$ in COMET for tst2019 test set. In challenging test sets with\noverlapping speakers and background noise, we find that integrating LLM is not\nbeneficial due to poor ASR performance. Here, we use ASR with chunked long-form\ndecoding to improve context usage that may be unavailable when transcribing\nwith Voice Activity Detection segmentation alone.\n","authors":["Sai Koneru","Thai-Binh Nguyen","Ngoc-Quan Pham","Danni Liu","Zhaolin Li","Alexander Waibel","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2406.16777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16772v1","updated":"2024-06-24T16:31:12Z","published":"2024-06-24T16:31:12Z","title":"OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?","summary":"  In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic\nmedal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and\nClaude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to\ncontinuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).\n","authors":["Zhen Huang","Zengzhi Wang","Shijie Xia","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2406.16772v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2406.16767v1","updated":"2024-06-24T16:24:18Z","published":"2024-06-24T16:24:18Z","title":"The GPT-WritingPrompts Dataset: A Comparative Analysis of Character\n  Portrayal in Short Stories","summary":"  The improved generative capabilities of large language models have made them\na powerful tool for creative writing and storytelling. It is therefore\nimportant to quantitatively understand the nature of generated stories, and how\nthey differ from human storytelling. We augment the Reddit WritingPrompts\ndataset with short stories generated by GPT-3.5, given the same prompts. We\nquantify and compare the emotional and descriptive features of storytelling\nfrom both generative processes, human and machine, along a set of six\ndimensions. We find that generated stories differ significantly from human\nstories along all six dimensions, and that human and machine generations\ndisplay similar biases when grouped according to the narrative point-of-view\nand gender of the main protagonist. We release our dataset and code at\nhttps://github.com/KristinHuangg/gpt-writing-prompts.\n","authors":["Xi Yu Huang","Krishnapriya Vishnubhotla","Frank Rudzicz"],"pdf_url":"https://arxiv.org/pdf/2406.16767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17296v7","updated":"2024-06-24T16:10:18Z","published":"2023-12-28T16:25:52Z","title":"Structured Packing in LLM Training Improves Long Context Utilization","summary":"  Recent advancements in long-context large language models have attracted\nsignificant attention, yet their practical applications often suffer from\nsuboptimal context utilization. This study investigates structuring training\ndata to enhance semantic interdependence, demonstrating that this approach\neffectively improves context utilization. To this end, we introduce the\nStructured Packing for Long Context (SPLiCe) method, which utilizes retrieval\nto collate mutually relevant documents into long and coherent training\nexamples. We validate SPLiCe empirically across models of varying sizes -- 3B,\n7B, and 13B -- achieving improved performance in long-context tasks, such as\nQasper and HotpotQA. Remarkably, even brief fine-tuning with SPLiCe is\nsufficient to realize these benefits. Additionally, SPLiCe effectively\nmitigates the lost-in-middle phenomenon often observed in large models. Our\ncomprehensive analysis of SPLiCe explores its design choices and reveals\nintriguing transfer effects; for instance, training on programming code\nenhances performance on natural language tasks.\n","authors":["Konrad Staniszewski","Szymon Tworkowski","Sebastian Jaszczur","Yu Zhao","Henryk Michalewski","Łukasz Kuciński","Piotr Miłoś"],"pdf_url":"https://arxiv.org/pdf/2312.17296v7.pdf","comment":"new experiments with a 13B model"},{"id":"http://arxiv.org/abs/2406.16758v1","updated":"2024-06-24T16:06:50Z","published":"2024-06-24T16:06:50Z","title":"Towards Fast Multilingual LLM Inference: Speculative Decoding and\n  Specialized Drafters","summary":"  Large language models (LLMs) have revolutionized natural language processing\nand broadened their applicability across diverse commercial applications.\nHowever, the deployment of these models is constrained by high inference time\nin multilingual settings. To mitigate this challenge, this paper explores a\ntraining recipe of an assistant model in speculative decoding, which are\nleveraged to draft and-then its future tokens are verified by the target LLM.\nWe show that language-specific draft models, optimized through a targeted\npretrain-and-finetune strategy, substantially brings a speedup of inference\ntime compared to the previous methods. We validate these models across various\nlanguages in inference time, out-of-domain speedup, and GPT-4o evaluation.\n","authors":["Euiin Yi","Taehyeon Kim","Hongseok Jeung","Du-Seong Chang","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2406.16758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11629v2","updated":"2024-06-24T16:02:21Z","published":"2024-06-17T15:11:58Z","title":"Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See\n  More, Judge Better!","summary":"  Leveraging Large Language Models (LLMs) as judges for evaluating the\nperformance of LLMs has recently garnered attention. Nonetheless, this type of\napproach concurrently introduces potential biases from LLMs, raising concerns\nabout the reliability of the evaluation results. To mitigate this issue, we\npropose and study two versions of many-shot in-context prompts, Reinforced and\nUnsupervised ICL, for helping GPT-4o-as-a-Judge in single answer grading. The\nformer uses in-context examples with model-generated rationales, and the latter\nwithout. Based on the designed prompts, we investigate the impact of scaling\nthe number of in-context examples on the agreement and quality of the\nevaluation. Furthermore, we first reveal the symbol bias in GPT-4o-as-a-Judge\nfor pairwise comparison and then propose a simple yet effective approach to\nmitigate it. Experimental results show that advanced long-context LLMs, such as\nGPT-4o, perform better in the many-shot regime than in the zero-shot regime.\nMeanwhile, the experimental results further verify the effectiveness of the\nsymbol bias mitigation approach.\n","authors":["Mingyang Song","Mao Zheng","Xuan Luo"],"pdf_url":"https://arxiv.org/pdf/2406.11629v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2406.16751v1","updated":"2024-06-24T15:58:15Z","published":"2024-06-24T15:58:15Z","title":"Towards Zero-Shot Text-To-Speech for Arabic Dialects","summary":"  Zero-shot multi-speaker text-to-speech (ZS-TTS) systems have advanced for\nEnglish, however, it still lags behind due to insufficient resources. We\naddress this gap for Arabic, a language of more than 450 million native\nspeakers, by first adapting a sizeable existing dataset to suit the needs of\nspeech synthesis. Additionally, we employ a set of Arabic dialect\nidentification models to explore the impact of pre-defined dialect labels on\nimproving the ZS-TTS model in a multi-dialect setting. Subsequently, we\nfine-tune the\nXTTS\\footnote{https://docs.coqui.ai/en/latest/models/xtts.html}\\footnote{https://medium.com/machine-learns/xtts-v2-new-version-of-the-open-source-text-to-speech-model-af73914db81f}\\footnote{https://medium.com/@erogol/xtts-v1-techincal-notes-eb83ff05bdc}\nmodel, an open-source architecture. We then evaluate our models on a dataset\ncomprising 31 unseen speakers and an in-house dialectal dataset. Our automated\nand human evaluation results show convincing performance while capable of\ngenerating dialectal speech. Our study highlights significant potential for\nimprovements in this emerging area of research in Arabic.\n","authors":["Khai Duy Doan","Abdul Waheed","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2406.16751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16748v1","updated":"2024-06-24T15:57:48Z","published":"2024-06-24T15:57:48Z","title":"OCALM: Object-Centric Assessment with Language Models","summary":"  Properly defining a reward signal to efficiently train a reinforcement\nlearning (RL) agent is a challenging task. Designing balanced objective\nfunctions from which a desired behavior can emerge requires expert knowledge,\nespecially for complex environments. Learning rewards from human feedback or\nusing large language models (LLMs) to directly provide rewards are promising\nalternatives, allowing non-experts to specify goals for the agent. However,\nblack-box reward models make it difficult to debug the reward. In this work, we\npropose Object-Centric Assessment with Language Models (OCALM) to derive\ninherently interpretable reward functions for RL agents from natural language\ntask descriptions. OCALM uses the extensive world-knowledge of LLMs while\nleveraging the object-centric nature common to many environments to derive\nreward functions focused on relational concepts, providing RL agents with the\nability to derive policies from task descriptions.\n","authors":["Timo Kaufmann","Jannis Blüml","Antonia Wüst","Quentin Delfosse","Kristian Kersting","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2406.16748v1.pdf","comment":"Accepted at the RLBRew Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2406.16747v1","updated":"2024-06-24T15:55:59Z","published":"2024-06-24T15:55:59Z","title":"Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers","summary":"  Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.\n","authors":["Chao Lou","Zixia Jia","Zilong Zheng","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2406.16747v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2406.16746v1","updated":"2024-06-24T15:55:49Z","published":"2024-06-24T15:55:49Z","title":"The Responsible Foundation Model Development Cheatsheet: A Review of\n  Tools & Resources","summary":"  Foundation model development attracts a rapidly expanding body of\ncontributors, scientists, and applications. To help shape responsible\ndevelopment practices, we introduce the Foundation Model Development\nCheatsheet: a growing collection of 250+ tools and resources spanning text,\nvision, and speech modalities. We draw on a large body of prior work to survey\nresources (e.g. software, documentation, frameworks, guides, and practical\ntools) that support informed data selection, processing, and understanding,\nprecise and limitation-aware artifact documentation, efficient model training,\nadvance awareness of the environmental impact from training, careful model\nevaluation of capabilities, risks, and claims, as well as responsible model\nrelease, licensing and deployment practices. We hope this curated collection of\nresources helps guide more responsible development. The process of curating\nthis list, enabled us to review the AI development ecosystem, revealing what\ntools are critically missing, misused, or over-used in existing practices. We\nfind that (i) tools for data sourcing, model evaluation, and monitoring are\ncritically under-serving ethical and real-world needs, (ii) evaluations for\nmodel safety, capabilities, and environmental impact all lack reproducibility\nand transparency, (iii) text and particularly English-centric analyses continue\nto dominate over multilingual and multi-modal analyses, and (iv) evaluation of\nsystems, rather than just models, is needed so that capabilities and impact are\nassessed in context.\n","authors":["Shayne Longpre","Stella Biderman","Alon Albalak","Hailey Schoelkopf","Daniel McDuff","Sayash Kapoor","Kevin Klyman","Kyle Lo","Gabriel Ilharco","Nay San","Maribeth Rauh","Aviya Skowron","Bertie Vidgen","Laura Weidinger","Arvind Narayanan","Victor Sanh","David Adelani","Percy Liang","Rishi Bommasani","Peter Henderson","Sasha Luccioni","Yacine Jernite","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2406.16746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16743v1","updated":"2024-06-24T15:51:30Z","published":"2024-06-24T15:51:30Z","title":"Adversarial Contrastive Decoding: Boosting Safety Alignment of Large\n  Language Models via Opposite Prompt Optimization","summary":"  With the widespread application of Large Language Models (LLMs), it has\nbecome a significant concern to ensure their safety and prevent harmful\nresponses. While current safe-alignment methods based on instruction\nfine-tuning and Reinforcement Learning from Human Feedback (RLHF) can\neffectively reduce harmful responses from LLMs, they often require high-quality\ndatasets and heavy computational overhead during model training. Another way to\nalign language models is to modify the logit of tokens in model outputs without\nheavy training. Recent studies have shown that contrastive decoding can enhance\nthe performance of language models by reducing the likelihood of confused\ntokens. However, these methods require the manual selection of contrastive\nmodels or instruction templates. To this end, we propose Adversarial\nContrastive Decoding (ACD), an optimization-based framework to generate two\nopposite system prompts for prompt-based contrastive decoding. ACD only needs\nto apply a lightweight prompt tuning on a rather small anchor dataset (< 3 min\nfor each model) without training the target model. Experiments conducted on\nextensive models and benchmarks demonstrate that the proposed method achieves\nmuch better safety performance than previous model training-free decoding\nmethods without sacrificing its original generation ability.\n","authors":["Zhengyue Zhao","Xiaoyun Zhang","Kaidi Xu","Xing Hu","Rui Zhang","Zidong Du","Qi Guo","Yunji Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08695v2","updated":"2024-06-24T15:51:13Z","published":"2023-11-15T04:50:30Z","title":"Attribute Diversity Determines the Systematicity Gap in VQA","summary":"  The degree to which neural networks can generalize to new combinations of\nfamiliar concepts, and the conditions under which they are able to do so, has\nlong been an open question. In this work, we study the systematicity gap in\nvisual question answering: the performance difference between reasoning on\npreviously seen and unseen combinations of object attributes. To test, we\nintroduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased\nquantity of training data does not reduce the systematicity gap, increased\ntraining data diversity of the attributes in the unseen combination does. In\nall, our experiments suggest that the more distinct attribute type combinations\nare seen during training, the more systematic we can expect the resulting model\nto be.\n","authors":["Ian Berlot-Attwell","Kumar Krishna Agrawal","A. Michael Carrell","Yash Sharma","Naomi Saphra"],"pdf_url":"https://arxiv.org/pdf/2311.08695v2.pdf","comment":"33 pages, 20 figures"},{"id":"http://arxiv.org/abs/2406.05673v2","updated":"2024-06-24T15:49:09Z","published":"2024-06-09T07:06:58Z","title":"Flow of Reasoning: Efficient Training of LLM Policy with Divergent\n  Thinking","summary":"  Divergent thinking, the cognitive process of generating diverse solutions, is\na hallmark of human creativity and problem-solving. For machines, sampling\ndiverse solution trajectories in complex reasoning problems is crucial for\nrobust outcomes, data augmentation, and enhanced model generalization. Large\nlanguage models (LLMs) often struggle with generating high-quality, diverse\nreasoning. While supervised fine-tuning helps with quality, it requires\nextensive supervision data to capture the full diversity of solutions.\nAlternatively, reinforcement learning methods like PPO aim to find limited\nhighest-reward solutions while neglecting the solution diversity, akin to\nconvergent thinking. To address these limitations, we propose Flow of Reasoning\n(FoR) -- an efficient LLM training approach enabling diverse reasoning with\nminimal data. FoR formulates multi-step LLM reasoning as a Markovian flow from\nan initial state to terminal states. The formulation allows to adapt principled\nGFlowNet approaches to train the LLM as a policy, which is able to sample\nmultiple reasoning paths with probabilities proportional to the unnormalized\nreward. Empirical results show that, with limited training data (e.g., 15\nexamples), FoR can discover diverse high-quality solutions that excel greatly\nbeyond current state-of-the-art methods across three tasks, including embodied\nreasoning (BlocksWorld), math puzzle solving (Game24), and logical reasoning\n(PrOntoQA). Code is available at https://github.com/Yu-Fangxu/FoR.\n","authors":["Fangxu Yu","Lai Jiang","Haoqiang Kang","Shibo Hao","Lianhui Qin"],"pdf_url":"https://arxiv.org/pdf/2406.05673v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04233v2","updated":"2024-06-24T15:39:17Z","published":"2024-06-06T16:31:47Z","title":"FairytaleQA Translated: Enabling Educational Question and Answer\n  Generation in Less-Resourced Languages","summary":"  Question Answering (QA) datasets are crucial in assessing reading\ncomprehension skills for both machines and humans. While numerous datasets have\nbeen developed in English for this purpose, a noticeable void exists in\nless-resourced languages. To alleviate this gap, our paper introduces\nmachine-translated versions of FairytaleQA, a renowned QA dataset designed to\nassess and enhance narrative comprehension skills in young children. By\nemploying fine-tuned, modest-scale models, we establish benchmarks for both\nQuestion Generation (QG) and QA tasks within the translated datasets. In\naddition, we present a case study proposing a model for generating\nquestion-answer pairs, with an evaluation incorporating quality metrics such as\nquestion well-formedness, answerability, relevance, and children suitability.\nOur evaluation prioritizes quantifying and describing error cases, along with\nproviding directions for future work. This paper contributes to the advancement\nof QA and QG research in less-resourced languages, promoting accessibility and\ninclusivity in the development of these models for reading comprehension. The\ncode and data is publicly available at\ngithub.com/bernardoleite/fairytaleqa-translated.\n","authors":["Bernardo Leite","Tomás Freitas Osório","Henrique Lopes Cardoso"],"pdf_url":"https://arxiv.org/pdf/2406.04233v2.pdf","comment":"Preprint - Accepted for publication at ECTEL 2024"},{"id":"http://arxiv.org/abs/2406.16732v1","updated":"2024-06-24T15:36:00Z","published":"2024-06-24T15:36:00Z","title":"CLIMATELI: Evaluating Entity Linking on Climate Change Data","summary":"  Climate Change (CC) is a pressing topic of global importance, attracting\nincreasing attention across research fields, from social sciences to Natural\nLanguage Processing (NLP). CC is also discussed in various settings and\ncommunication platforms, from academic publications to social media forums.\nUnderstanding who and what is mentioned in such data is a first critical step\nto gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),\nthe first manually annotated CC dataset that links 3,087 entity spans to\nWikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing\nentity linking (EL) systems on the CC topic across various genres and propose\nautomated filtering methods for CC entities. We find that the performance of EL\nmodels notably lags behind humans at both token and entity levels. Testing\nwithin the scope of retaining or excluding non-nominal and/or non-CC entities\nparticularly impacts the models' performances.\n","authors":["Shijia Zhou","Siyao Peng","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2406.16732v1.pdf","comment":"7 pages, ClimateNLP 2024"},{"id":"http://arxiv.org/abs/2406.13431v2","updated":"2024-06-24T15:31:59Z","published":"2024-06-19T10:45:12Z","title":"Children's Speech Recognition through Discrete Token Enhancement","summary":"  Children's speech recognition is considered a low-resource task mainly due to\nthe lack of publicly available data. There are several reasons for such data\nscarcity, including expensive data collection and annotation processes, and\ndata privacy, among others. Transforming speech signals into discrete tokens\nthat do not carry sensitive information but capture both linguistic and\nacoustic information could be a solution for privacy concerns. In this study,\nwe investigate the integration of discrete speech tokens into children's speech\nrecognition systems as input without significantly degrading the ASR\nperformance. Additionally, we explored single-view and multi-view strategies\nfor creating these discrete labels. Furthermore, we tested the models for\ngeneralization capabilities with unseen domain and nativity dataset. Results\nreveal that the discrete token ASR for children achieves nearly equivalent\nperformance with an approximate 83% reduction in parameters.\n","authors":["Vrunda N. Sukhadia","Shammur Absar Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2406.13431v2.pdf","comment":"Accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.14862v2","updated":"2024-06-24T15:30:34Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16722v1","updated":"2024-06-24T15:27:21Z","published":"2024-06-24T15:27:21Z","title":"Venturing into Uncharted Waters: The Navigation Compass from Transformer\n  to Mamba","summary":"  Transformer, a deep neural network architecture, has long dominated the field\nof natural language processing and beyond. Nevertheless, the recent\nintroduction of Mamba challenges its supremacy, sparks considerable interest\namong researchers, and gives rise to a series of Mamba-based models that have\nexhibited notable potential. This survey paper orchestrates a comprehensive\ndiscussion, diving into essential research dimensions, covering: (i) the\nfunctioning of the Mamba mechanism and its foundation on the principles of\nstructured state space models; (ii) the proposed improvements and the\nintegration of Mamba with various networks, exploring its potential as a\nsubstitute for Transformers; (iii) the combination of Transformers and Mamba to\ncompensate for each other's shortcomings. We have also made efforts to\ninterpret Mamba and Transformer in the framework of kernel functions, allowing\nfor a comparison of their mathematical nature within a unified context. Our\npaper encompasses the vast majority of improvements related to Mamba to date.\n","authors":["Yuchen Zou","Yineng Chen","Zuchao Li","Lefei Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16714v1","updated":"2024-06-24T15:16:45Z","published":"2024-06-24T15:16:45Z","title":"AutoDetect: Towards a Unified Framework for Automated Weakness Detection\n  in Large Language Models","summary":"  Although Large Language Models (LLMs) are becoming increasingly powerful,\nthey still exhibit significant but subtle weaknesses, such as mistakes in\ninstruction-following or coding tasks. As these unexpected errors could lead to\nsevere consequences in practical deployments, it is crucial to investigate the\nlimitations within LLMs systematically. Traditional benchmarking approaches\ncannot thoroughly pinpoint specific model deficiencies, while manual\ninspections are costly and not scalable. In this paper, we introduce a unified\nframework, AutoDetect, to automatically expose weaknesses in LLMs across\nvarious tasks. Inspired by the educational assessment process that measures\nstudents' learning outcomes, AutoDetect consists of three LLM-powered agents:\nExaminer, Questioner, and Assessor. The collaboration among these three agents\nis designed to realize comprehensive and in-depth weakness identification. Our\nframework demonstrates significant success in uncovering flaws, with an\nidentification success rate exceeding 30% in prominent models such as ChatGPT\nand Claude. More importantly, these identified weaknesses can guide specific\nmodel improvements, proving more effective than untargeted data augmentation\nmethods like Self-Instruct. Our approach has led to substantial enhancements in\npopular LLMs, including the Llama series and Mistral-7b, boosting their\nperformance by over 10% across several benchmarks. Code and data are publicly\navailable at https://github.com/thu-coai/AutoDetect.\n","authors":["Jiale Cheng","Yida Lu","Xiaotao Gu","Pei Ke","Xiao Liu","Yuxiao Dong","Hongning Wang","Jie Tang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16714v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13131v2","updated":"2024-06-24T15:13:13Z","published":"2024-06-19T00:48:44Z","title":"When Parts are Greater Than Sums: Individual LLM Components Can\n  Outperform Full Models","summary":"  This paper studies in-context learning (ICL) by decomposing the output of\nlarge language models into the individual contributions of attention heads and\nMLPs (components). We observe curious components: good-performing ones that\nindividually do well on a classification task, even when the model performs\npoorly; bad-performing ones that do much worse than chance; and label-biased\ncomponents that always predict the same label. We find that component\naccuracies are well-correlated across different demonstration sets and\nperturbations of prompt templates, even when the full-model accuracy varies\ngreatly. Based on our findings, we propose component reweighting, which learns\nto linearly re-scale the component activations from a few labeled examples.\nGiven 24 labeled examples, our method improves by an average of 6.0% accuracy\npoints over 24-shot ICL across 8 tasks on Llama-2-7B. Overall, this paper both\nenriches our understanding of ICL and provides a practical method for\nimprovement by examining model internals.\n","authors":["Ting-Yun Chang","Jesse Thomason","Robin Jia"],"pdf_url":"https://arxiv.org/pdf/2406.13131v2.pdf","comment":"fix typos and citations; appendix"},{"id":"http://arxiv.org/abs/2406.07393v2","updated":"2024-06-24T14:59:54Z","published":"2024-06-11T15:58:59Z","title":"Limited Out-of-Context Knowledge Reasoning in Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated strong capabilities as\nknowledge bases and significant in-context reasoning capabilities. However,\nprevious work challenges their out-of-context reasoning ability, i.e., the\nability to infer information from their training data, instead of from the\ncontext or prompt. This paper focuses on a significant facet of out-of-context\nreasoning: Out-of-Context Knowledge Reasoning (OCKR), which is to combine\nmultiple knowledge to infer new knowledge. We designed a synthetic dataset with\nseven representative OCKR tasks to systematically assess the OCKR capabilities\nof LLMs. Using this dataset, we evaluated the LLaMA2-13B-chat model and\ndiscovered that its proficiency in this aspect is limited, regardless of\nwhether the knowledge is trained in a separate or adjacent training settings.\nMoreover, training the model to reason with complete reasoning data did not\nresult in significant improvement. Training the model to perform explicit\nknowledge retrieval helps in only one of the tasks, indicating that the model's\nlimited OCKR capabilities are due to difficulties in retrieving relevant\nknowledge. Furthermore, we treat cross-lingual knowledge transfer as a distinct\nform of OCKR, and evaluate this ability. Our results show that the evaluated\nmodel also exhibits limited ability in transferring knowledge across languages.\nThe dataset used in this study is available at\nhttps://github.com/NJUNLP/ID-OCKR.\n","authors":["Peng Hu","Changjiang Gao","Ruiqi Gao","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2406.07393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16694v1","updated":"2024-06-24T14:58:11Z","published":"2024-06-24T14:58:11Z","title":"Task Oriented In-Domain Data Augmentation","summary":"  Large Language Models (LLMs) have shown superior performance in various\napplications and fields. To achieve better performance on specialized domains\nsuch as law and advertisement, LLMs are often continue pre-trained on in-domain\ndata. However, existing approaches suffer from two major issues. First,\nin-domain data are scarce compared with general domain-agnostic data. Second,\ndata used for continual pre-training are not task-aware, such that they may not\nbe helpful to downstream applications. We propose TRAIT, a task-oriented\nin-domain data augmentation framework. Our framework is divided into two parts:\nin-domain data selection and task-oriented synthetic passage generation. The\ndata selection strategy identifies and selects a large amount of in-domain data\nfrom general corpora, and thus significantly enriches domain knowledge in the\ncontinual pre-training data. The synthetic passages contain guidance on how to\nuse domain knowledge to answer questions about downstream tasks. By training on\nsuch passages, the model aligns with the need of downstream applications. We\nadapt LLMs to two domains: advertisement and math. On average, TRAIT improves\nLLM performance by 8% in the advertisement domain and 7.5% in the math domain.\n","authors":["Xiao Liang","Xinyu Hu","Simiao Zuo","Yeyun Gong","Qiang Lou","Yi Liu","Shao-Lun Huang","Jian Jiao"],"pdf_url":"https://arxiv.org/pdf/2406.16694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16690v1","updated":"2024-06-24T14:51:31Z","published":"2024-06-24T14:51:31Z","title":"Scaling Laws for Linear Complexity Language Models","summary":"  The interest in linear complexity models for large language models is on the\nrise, although their scaling capacity remains uncertain. In this study, we\npresent the scaling laws for linear complexity language models to establish a\nfoundation for their scalability. Specifically, we examine the scaling\nbehaviors of three efficient linear architectures. These include TNL, a linear\nattention model with data-independent decay; HGRN2, a linear RNN with\ndata-dependent decay; and cosFormer2, a linear attention model without decay.\nWe also include LLaMA as a baseline architecture for softmax attention for\ncomparison. These models were trained with six variants, ranging from 70M to 7B\nparameters on a 300B-token corpus, and evaluated with a total of 1,376\nintermediate checkpoints on various downstream tasks. These tasks include\nvalidation loss, commonsense reasoning, and information retrieval and\ngeneration. The study reveals that existing linear complexity language models\nexhibit similar scaling capabilities as conventional transformer-based models\nwhile also demonstrating superior linguistic proficiency and knowledge\nretention.\n","authors":["Xuyang Shen","Dong Li","Ruitao Leng","Zhen Qin","Weigao Sun","Yiran Zhong"],"pdf_url":"https://arxiv.org/pdf/2406.16690v1.pdf","comment":"Technical report. Yiran Zhong is the corresponding author"},{"id":"http://arxiv.org/abs/2406.16678v1","updated":"2024-06-24T14:36:11Z","published":"2024-06-24T14:36:11Z","title":"Segment Any Text: A Universal Approach for Robust, Efficient and\n  Adaptable Sentence Segmentation","summary":"  Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.\n","authors":["Markus Frohmann","Igor Sterner","Ivan Vulić","Benjamin Minixhofer","Markus Schedl"],"pdf_url":"https://arxiv.org/pdf/2406.16678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16674v1","updated":"2024-06-24T14:31:34Z","published":"2024-06-24T14:31:34Z","title":"Computational Approaches to the Detection of Lesser-Known Rhetorical\n  Figures: A Systematic Survey and Research Challenges","summary":"  Rhetorical figures play a major role in our everyday communication as they\nmake text more interesting, more memorable, or more persuasive. Therefore, it\nis important to computationally detect rhetorical figures to fully understand\nthe meaning of a text. We provide a comprehensive overview of computational\napproaches to lesser-known rhetorical figures. We explore the linguistic and\ncomputational perspectives on rhetorical figures, emphasizing their\nsignificance for the domain of Natural Language Processing. We present\ndifferent figures in detail, delving into datasets, definitions, rhetorical\nfunctions, and detection approaches. We identified challenges such as dataset\nscarcity, language limitations, and reliance on rule-based methods.\n","authors":["Ramona Kühn","Jelena Mitrović","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2406.16674v1.pdf","comment":"Submitted to ACM Computing Surveys. 35 pages"},{"id":"http://arxiv.org/abs/2406.16672v1","updated":"2024-06-24T14:27:54Z","published":"2024-06-24T14:27:54Z","title":"CAVE: Controllable Authorship Verification Explanations","summary":"  Authorship Verification (AV) (do two documents have the same author?) is\nessential for many sensitive real-life applications. AV is often used in\nproprietary domains that require a private, offline model, making SOTA online\nmodels like ChatGPT undesirable. Other SOTA systems use methods, e.g. Siamese\nNetworks, that are uninterpretable, and hence cannot be trusted in high-stakes\napplications. In this work, we take the first step to address the above\nchallenges with our model CAVE (Controllable Authorship Verification\nExplanations): CAVE generates free-text AV explanations that are controlled to\nbe 1) structured (can be decomposed into sub-explanations with respect to\nrelevant linguistic features), and 2) easily verified for explanation-label\nconsistency (via intermediate labels in sub-explanations). In this work, we\ntrain a Llama-3-8B as CAVE; since there are no human-written corpora for AV\nexplanations, we sample silver-standard explanations from GPT-4-TURBO and\ndistill them into a pretrained Llama-3-8B. Results on three difficult AV\ndatasets IMdB2, Blog-Auth, and FanFiction show that CAVE generates high quality\nexplanations (as measured by automatic and human evaluation) as well as\ncompetitive task accuracies.\n","authors":["Sahana Ramnath","Kartik Pandey","Elizabeth Boschee","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2406.16672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16655v1","updated":"2024-06-24T14:03:04Z","published":"2024-06-24T14:03:04Z","title":"Large Language Models Are Cross-Lingual Knowledge-Free Reasoners","summary":"  Large Language Models have demonstrated impressive reasoning capabilities\nacross multiple languages. However, the relationship between capabilities in\ndifferent languages is less explored. In this work, we decompose the process of\nreasoning tasks into two separated parts: knowledge retrieval and\nknowledge-free reasoning, and analyze the cross-lingual transferability of\nthem. With adapted and constructed knowledge-free reasoning datasets, we show\nthat the knowledge-free reasoning capability can be nearly perfectly\ntransferred across various source-target language directions despite the\nsecondary impact of resource in some specific target languages, while\ncross-lingual knowledge retrieval significantly hinders the transfer. Moreover,\nby analyzing the hidden states and feed-forward network neuron activation\nduring the reasoning tasks, we show that higher similarity of hidden\nrepresentations and larger overlap of activated neurons could explain the\nbetter cross-lingual transferability of knowledge-free reasoning than knowledge\nretrieval. Thus, we hypothesize that knowledge-free reasoning embeds in some\nlanguage-shared mechanism, while knowledge is stored separately in different\nlanguages.\n","authors":["Peng Hu","Sizhe Liu","Changjiang Gao","Xin Huang","Xue Han","Junlan Feng","Chao Deng","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10670v2","updated":"2024-06-24T13:52:37Z","published":"2024-06-15T15:28:02Z","title":"CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language\n  Model Pre-training","summary":"  Selecting high-quality data for pre-training is crucial in shaping the\ndownstream task performance of language models. A major challenge lies in\nidentifying this optimal subset, a problem generally considered intractable,\nthus necessitating scalable and effective heuristics. In this work, we propose\na data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering),\nwhich leverages an empirical Bayes-inspired approach to derive a simple and\ncomputationally efficient selection criterion based on the relative loss values\nof two auxiliary models.\n  In addition to the modeling rationale, we evaluate CoLoR-Filter empirically\non two language modeling tasks: (1) selecting data from C4 for domain\nadaptation to evaluation on Books and (2) selecting data from C4 for a suite of\ndownstream multiple-choice question answering tasks. We demonstrate favorable\nscaling both as we subselect more aggressively and using small auxiliary models\nto select data for large target models. As one headline result, CoLoR-Filter\ndata selected using a pair of 150m parameter auxiliary models can train a 1.2b\nparameter target model to match a 1.2b parameter model trained on 25b randomly\nselected tokens with 25x less data for Books and 11x less data for the\ndownstream tasks.\n  Code: https://github.com/davidbrandfonbrener/color-filter-olmo\n  Filtered data:\nhttps://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4\n","authors":["David Brandfonbrener","Hanlin Zhang","Andreas Kirsch","Jonathan Richard Schwarz","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2406.10670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16635v1","updated":"2024-06-24T13:41:08Z","published":"2024-06-24T13:41:08Z","title":"ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models","summary":"  The high power consumption and latency-sensitive deployments of large\nlanguage models (LLMs) have motivated techniques like quantization and\nsparsity. Contextual sparsity, where the sparsity pattern is input-dependent,\nis crucial in LLMs because the permanent removal of attention heads or neurons\nfrom LLMs can significantly degrade accuracy. Prior work has attempted to model\ncontextual sparsity using neural networks trained to predict activation\nmagnitudes, which can be used to dynamically prune structures with low\npredicted activation magnitude. In this paper, we look beyond magnitude-based\npruning criteria to assess attention head and neuron importance in LLMs. We\ndeveloped a novel predictor called ShadowLLM, which can shadow the LLM behavior\nand enforce better sparsity patterns, resulting in over 15% improvement in\nend-to-end accuracy without increasing latency compared to previous methods.\nShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu\nframework. These enhancements are validated on models with up to 30 billion\nparameters. Our code is available at\n\\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.\n","authors":["Yash Akhauri","Ahmed F AbouElhamayed","Jordan Dotzel","Zhiru Zhang","Alexander M Rush","Safeen Huda","Mohamed S Abdelfattah"],"pdf_url":"https://arxiv.org/pdf/2406.16635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02051v3","updated":"2024-06-24T13:26:47Z","published":"2023-06-03T08:39:25Z","title":"A Comprehensive Survey on Relation Extraction: Recent Advances and New\n  Frontiers","summary":"  Relation extraction (RE) involves identifying the relations between entities\nfrom underlying content. RE serves as the foundation for many natural language\nprocessing (NLP) and information retrieval applications, such as knowledge\ngraph completion and question answering. In recent years, deep neural networks\nhave dominated the field of RE and made noticeable progress. Subsequently, the\nlarge pre-trained language models have taken the state-of-the-art RE to a new\nlevel. This survey provides a comprehensive review of existing deep learning\ntechniques for RE. First, we introduce RE resources, including datasets and\nevaluation metrics. Second, we propose a new taxonomy to categorize existing\nworks from three perspectives, i.e., text representation, context encoding, and\ntriplet prediction. Third, we discuss several important challenges faced by RE\nand summarize potential techniques to tackle these challenges. Finally, we\noutline some promising future directions and prospects in this field. This\nsurvey is expected to facilitate researchers' collaborative efforts to address\nthe challenges of real-world RE systems.\n","authors":["Xiaoyan Zhao","Yang Deng","Min Yang","Lingzhi Wang","Rui Zhang","Hong Cheng","Wai Lam","Ying Shen","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2306.02051v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19181v2","updated":"2024-06-24T13:22:22Z","published":"2024-03-28T07:22:16Z","title":"Make Large Language Model a Better Ranker","summary":"  Large Language Models (LLMs) demonstrate robust capabilities across various\nfields, leading to a paradigm shift in LLM-enhanced Recommender System (RS).\nResearch to date focuses on point-wise and pair-wise recommendation paradigms,\nwhich are inefficient for LLM-based recommenders due to high computational\ncosts. However, existing list-wise approaches also fall short in ranking tasks\ndue to misalignment between ranking objectives and next-token prediction.\nMoreover, these LLM-based methods struggle to effectively address the order\nrelation among candidates, particularly given the scale of ratings. To address\nthese challenges, this paper introduces the large language model framework with\nAligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap\nbetween the capabilities of LLMs and the nuanced requirements of ranking tasks.\nSpecifically, ALRO employs explicit feedback in a listwise manner by\nintroducing soft lambda loss, a customized adaptation of lambda loss designed\nfor optimizing order relations. This mechanism provides more accurate\noptimization goals, enhancing the ranking process. Additionally, ALRO\nincorporates a permutation-sensitive learning mechanism that addresses position\nbias, a prevalent issue in generative models, without imposing additional\ncomputational burdens during inference. Our evaluative studies reveal that ALRO\noutperforms both existing embedding-based recommendation methods and LLM-based\nrecommendation baselines.\n","authors":["Wenshuo Chao","Zhi Zheng","Hengshu Zhu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19181v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.05268v2","updated":"2024-06-24T13:15:33Z","published":"2024-03-08T12:45:53Z","title":"Deep Prompt Multi-task Network for Abuse Language Detection","summary":"  The detection of abusive language remains a long-standing challenge with the\nextensive use of social networks. The detection task of abusive language\nsuffers from limited accuracy. We argue that the existing detection methods\nutilize the fine-tuning technique of the pre-trained language models (PLMs) to\nhandle downstream tasks. Hence, these methods fail to stimulate the general\nknowledge of the PLMs. To address the problem, we propose a novel Deep Prompt\nMulti-task Network (DPMN) for abuse language detection. Specifically, DPMN\nfirst attempts to design two forms of deep prompt tuning and light prompt\ntuning for the PLMs. The effects of different prompt lengths, tuning\nstrategies, and prompt initialization methods on detecting abusive language are\nstudied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which\ncan be used as a short text classifier. Eventually, DPMN utilizes multi-task\nlearning to improve detection metrics further. The multi-task network has the\nfunction of transferring effective knowledge. The proposed DPMN is evaluated\nagainst eight typical methods on three public datasets: OLID, SOLID, and\nAbuseAnalyzer. The experimental results show that our DPMN outperforms the\nstate-of-the-art methods.\n","authors":["Jian Zhu","Yuping Ruan","Jingfei Chang","Wenhui Sun","Hui Wan","Jian Long","Cheng Luo"],"pdf_url":"https://arxiv.org/pdf/2403.05268v2.pdf","comment":"Accepted by the International Conference on Pattern Recognition\n  (ICPR) 2024"},{"id":"http://arxiv.org/abs/2406.16611v1","updated":"2024-06-24T12:52:02Z","published":"2024-06-24T12:52:02Z","title":"Evaluation of Language Models in the Medical Context Under\n  Resource-Constrained Settings","summary":"  Since the emergence of the Transformer architecture, language model\ndevelopment has increased, driven by their promising potential. However,\nreleasing these models into production requires properly understanding their\nbehavior, particularly in sensitive domains such as medicine. Despite this\nneed, the medical literature still lacks technical assessments of pre-trained\nlanguage models, which are especially valuable in resource-constrained settings\nin terms of computational power or limited budget. To address this gap, we\nprovide a comprehensive survey of language models in the medical domain. In\naddition, we selected a subset of these models for thorough evaluation,\nfocusing on classification and text generation tasks. Our subset encompasses 53\nmodels, ranging from 110 million to 13 billion parameters, spanning the three\nfamilies of Transformer-based models and from diverse knowledge domains. This\nstudy employs a series of approaches for text classification together with\nzero-shot prompting instead of model training or fine-tuning, which closely\nresembles the limited resource setting in which many users of language models\nfind themselves. Encouragingly, our findings reveal remarkable performance\nacross various tasks and datasets, underscoring the latent potential of certain\nmodels to contain medical knowledge, even without domain specialization.\nConsequently, our study advocates for further exploration of model applications\nin medical contexts, particularly in resource-constrained settings. The code is\navailable on https://github.com/anpoc/Language-models-in-medicine.\n","authors":["Andrea Posada","Daniel Rueckert","Felix Meissen","Philip Müller"],"pdf_url":"https://arxiv.org/pdf/2406.16611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16605v1","updated":"2024-06-24T12:46:15Z","published":"2024-06-24T12:46:15Z","title":"CLEAR: Can Language Models Really Understand Causal Graphs?","summary":"  Causal reasoning is a cornerstone of how humans interpret the world. To model\nand reason about causality, causal graphs offer a concise yet effective\nsolution. Given the impressive advancements in language models, a crucial\nquestion arises: can they really understand causal graphs? To this end, we\npioneer an investigation into language models' understanding of causal graphs.\nSpecifically, we develop a framework to define causal graph understanding, by\nassessing language models' behaviors through four practical criteria derived\nfrom diverse disciplines (e.g., philosophy and psychology). We then develop\nCLEAR, a novel benchmark that defines three complexity levels and encompasses\n20 causal graph-based tasks across these levels. Finally, based on our\nframework and benchmark, we conduct extensive experiments on six leading\nlanguage models and summarize five empirical findings. Our results indicate\nthat while language models demonstrate a preliminary understanding of causal\ngraphs, significant potential for improvement remains. Our project website is\nat https://github.com/OpenCausaLab/CLEAR.\n","authors":["Sirui Chen","Mengying Xu","Kun Wang","Xingyu Zeng","Rui Zhao","Shengjie Zhao","Chaochao Lu"],"pdf_url":"https://arxiv.org/pdf/2406.16605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17263v2","updated":"2024-06-24T12:45:27Z","published":"2024-02-27T07:14:12Z","title":"MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient\n  Fine-Tuning","summary":"  Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring\npre-trained large language models (LLMs), especially as the models' scale and\nthe diversity of tasks increase. Low-rank adaptation (LoRA) is based on the\nidea that the adaptation process is intrinsically low-dimensional, i.e.,\nsignificant model changes can be represented with relatively few parameters.\nHowever, decreasing the rank encounters challenges with generalization errors\nfor specific tasks when compared to full-parameter fine-tuning. We present\nMELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters\nwhile maintaining a higher rank, thereby offering improved performance\npotential. The core idea is to freeze original pretrained weights and train a\ngroup of mini LoRAs with only a small number of parameters. This can capture a\nsignificant degree of diversity among mini LoRAs, thus promoting better\ngeneralization ability. We conduct a theoretical analysis and empirical studies\non various NLP tasks. Our experimental results show that, compared to LoRA,\nMELoRA achieves better performance with 8 times fewer trainable parameters on\nnatural language understanding tasks and 36 times fewer trainable parameters on\ninstruction following tasks, which demonstrates the effectiveness of MELoRA.\n","authors":["Pengjie Ren","Chengshun Shi","Shiguang Wu","Mengqi Zhang","Zhaochun Ren","Maarten de Rijke","Zhumin Chen","Jiahuan Pei"],"pdf_url":"https://arxiv.org/pdf/2402.17263v2.pdf","comment":"ACL2024"},{"id":"http://arxiv.org/abs/2403.16512v4","updated":"2024-06-24T12:41:52Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages. Our code is publicly\nreleased at https://github.com/SamuelCahyawijaya/in-context-alignment\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04067v3","updated":"2024-06-24T12:32:41Z","published":"2024-04-05T12:51:37Z","title":"CLUE: A Clinical Language Understanding Evaluation for LLMs","summary":"  Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, evaluation has\nprimarily been limited to non-clinical tasks, which do not reflect the\ncomplexity of practical clinical applications. To fill this gap, we present the\nClinical Language Understanding Evaluation (CLUE), a benchmark tailored to\nevaluate LLMs on clinical tasks. CLUE includes six tasks to test the practical\napplicability of LLMs in complex healthcare settings. Our evaluation includes a\ntotal of $25$ LLMs. In contrast to previous evaluations, CLUE shows a decrease\nin performance for nine out of twelve biomedical models. Our benchmark\nrepresents a step towards a standardized approach to evaluating and developing\nLLMs in healthcare to align future model development with the real-world needs\nof clinical application. We open-source all evaluation scripts and datasets for\nfuture research at https://github.com/TIO-IKIM/CLUE.\n","authors":["Amin Dada","Marie Bauer","Amanda Butler Contreras","Osman Alperen Koraş","Constantin Marc Seibold","Kaleb E Smith","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2404.04067v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14952v2","updated":"2024-06-24T12:24:52Z","published":"2024-06-21T08:03:33Z","title":"ESC-Eval: Evaluating Emotion Support Conversations in Large Language\n  Models","summary":"  Emotion Support Conversation (ESC) is a crucial application, which aims to\nreduce human stress, offer emotional guidance, and ultimately enhance human\nmental and physical well-being. With the advancement of Large Language Models\n(LLMs), many researchers have employed LLMs as the ESC models. However, the\nevaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome\ndevelopment of role-playing agents, we propose an ESC Evaluation framework\n(ESC-Eval), which uses a role-playing agent to interact with ESC models,\nfollowed by a manual evaluation of the interactive dialogues. In detail, we\nfirst re-organize 2,801 role-playing cards from seven existing datasets to\ndefine the roles of the role-playing agent. Second, we train a specific\nrole-playing model called ESC-Role which behaves more like a confused person\nthan GPT-4. Third, through ESC-Role and organized role cards, we systematically\nconduct experiments using 14 LLMs as the ESC models, including general\nAI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct\ncomprehensive human annotations on interactive multi-turn dialogues of\ndifferent ESC models. The results show that ESC-oriented LLMs exhibit superior\nESC abilities compared to general AI-assistant LLMs, but there is still a gap\nbehind human performance. Moreover, to automate the scoring process for future\nESC models, we developed ESC-RANK, which trained on the annotated data,\nachieving a scoring performance surpassing 35 points of GPT-4. Our data and\ncode are available at https://github.com/haidequanbu/ESC-Eval.\n","authors":["Haiquan Zhao","Lingyu Li","Shisong Chen","Shuqi Kong","Jiaan Wang","Kexin Huang","Tianle Gu","Yixu Wang","Dandan Liang","Zhixu Li","Yan Teng","Yanghua Xiao","Yingchun Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14952v2.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2401.15351v2","updated":"2024-06-24T12:08:06Z","published":"2024-01-27T08:52:19Z","title":"A Survey on Neural Topic Models: Methods, Applications, and Challenges","summary":"  Topic models have been prevalent for decades to discover latent topics and\ninfer topic proportions of documents in an unsupervised fashion. They have been\nwidely used in various applications like text analysis and context\nrecommendation. Recently, the rise of neural networks has facilitated the\nemergence of a new research field -- Neural Topic Models (NTMs). Different from\nconventional topic models, NTMs directly optimize parameters without requiring\nmodel-specific derivations. This endows NTMs with better scalability and\nflexibility, resulting in significant research attention and plentiful new\nmethods and applications. In this paper, we present a comprehensive survey on\nneural topic models concerning methods, applications, and challenges.\nSpecifically, we systematically organize current NTM methods according to their\nnetwork structures and introduce the NTMs for various scenarios like short\ntexts and bilingual documents. We also discuss a wide range of popular\napplications built on NTMs. Finally, we highlight the challenges confronted by\nNTMs to inspire future research. We accompany this survey with a repository for\neasier access to the mentioned paper resources:\nhttps://github.com/bobxwu/Paper-Neural-Topic-Models.\n","authors":["Xiaobao Wu","Thong Nguyen","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2401.15351v2.pdf","comment":"Accepted to Artificial Intelligence Review. See\n  https://doi.org/10.1007/s10462-023-10661-7 and a paper list at\n  https://github.com/BobXWu/Paper-Neural-Topic-Models"},{"id":"http://arxiv.org/abs/2406.16567v1","updated":"2024-06-24T12:02:56Z","published":"2024-06-24T12:02:56Z","title":"Data Augmentation of Multi-turn Psychological Dialogue via\n  Knowledge-driven Progressive Thought Prompting","summary":"  Existing dialogue data augmentation (DA) techniques predominantly focus on\naugmenting utterance-level dialogues, which makes it difficult to take dialogue\ncontextual information into account. The advent of large language models (LLMs)\nhas simplified the implementation of multi-turn dialogues. Due to absence of\nprofessional understanding and knowledge, it remains challenging to deliver\nsatisfactory performance in low-resource domain, like psychological dialogue\ndialogue. DA involves creating new training or prompting data based on the\nexisting data, which help the model better understand and generate\npsychology-related responses. In this paper, we aim to address the issue of\nmulti-turn dialogue data augmentation for boosted performance in the psychology\ndomain. We propose a knowledge-driven progressive thought prompting method to\nguide LLM to generate multi-turn psychology-related dialogue. This method\nintegrates a progressive thought generator, a psychology knowledge generator,\nand a multi-turn dialogue generator. The thought generated by the progressive\nthought generator serves as a prompt to prevent the generated dialogue from\nhaving significant semantic deviations, while the psychology knowledge\ngenerator produces psychological knowledge to serve as the dialogue history for\nthe LLM, guiding the dialogue generator to create multi-turn psychological\ndialogue. To ensure the precision of multi-turn psychological dialogue\ngeneration by LLM, a meticulous professional evaluation is required. Extensive\nexperiments conducted on three datasets related to psychological dialogue\nverify the effectiveness of the proposed method.\n","authors":["Jiyue Jiang","Liheng Chen","Sheng Wang","Lingpeng Kong","Yu Li","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2406.16567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16563v1","updated":"2024-06-24T11:58:33Z","published":"2024-06-24T11:58:33Z","title":"Are there identifiable structural parts in the sentence embedding whole?","summary":"  Sentence embeddings from transformer models encode in a fixed length vector\nmuch linguistic information. We explore the hypothesis that these embeddings\nconsist of overlapping layers of information that can be separated, and on\nwhich specific types of information -- such as information about chunks and\ntheir structural and semantic properties -- can be detected. We show that this\nis the case using a dataset consisting of sentences with known chunk structure,\nand two linguistic intelligence datasets, solving which relies on detecting\nchunks and their grammatical number, and respectively, their semantic roles,\nand through analyses of the performance on the tasks and of the internal\nrepresentations built during learning.\n","authors":["Vivi Nastase","Paola Merlo"],"pdf_url":"https://arxiv.org/pdf/2406.16563v1.pdf","comment":"17 pages, 14 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.16562v1","updated":"2024-06-24T11:56:15Z","published":"2024-06-24T11:56:15Z","title":"EvalAlign: Evaluating Text-to-Image Models through Precision Alignment\n  of Multimodal Large Models with Supervised Fine-Tuning to Human Annotations","summary":"  The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive datasets. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We Supervised Fine-Tune (SFT) the MLLM to\nalign closely with human evaluative judgments, resulting in a robust evaluation\nmodel. Our comprehensive tests across 24 text-to-image generation models\ndemonstrate that EvalAlign not only provides superior metric stability but also\naligns more closely with human preferences than existing metrics, confirming\nits effectiveness and utility in model assessment.\n","authors":["Zhiyu Tan","Xiaomeng Yang","Luozheng Qin","Mengping Yang","Cheng Zhang","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2406.16562v1.pdf","comment":"Github Repository: https://github.com/SAIS-FUXI/EvalAlign"},{"id":"http://arxiv.org/abs/2406.16554v1","updated":"2024-06-24T11:43:07Z","published":"2024-06-24T11:43:07Z","title":"LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual\n  Pre-training","summary":"  Mixture-of-Experts (MoE) has gained increasing popularity as a promising\nframework for scaling up large language models (LLMs). However, training MoE\nfrom scratch in a large-scale setting still suffers from data-hungry and\ninstability problems. Motivated by this limit, we investigate building MoE\nmodels from existing dense large language models. Specifically, based on the\nwell-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert\nConstruction, which partitions the parameters of original Feed-Forward Networks\n(FFNs) into multiple experts; (2) Continual Pre-training, which further trains\nthe transformed MoE model and additional gate networks. In this paper, we\ncomprehensively explore different methods for expert construction and various\ndata sampling strategies for continual pre-training. After these stages, our\nLLaMA-MoE models could maintain language abilities and route the input tokens\nto specific experts with part of the parameters activated. Empirically, by\ntraining 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense\nmodels that contain similar activation parameters. The source codes and models\nare available at https://github.com/pjlab-sys4nlp/llama-moe .\n","authors":["Tong Zhu","Xiaoye Qu","Daize Dong","Jiacheng Ruan","Jingqi Tong","Conghui He","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.16554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16536v1","updated":"2024-06-24T11:16:31Z","published":"2024-06-24T11:16:31Z","title":"C-LLM: Learn to Check Chinese Spelling Errors Character by Character","summary":"  Chinese Spell Checking (CSC) aims to detect and correct spelling errors in\nsentences. Despite Large Language Models (LLMs) exhibit robust capabilities and\nare widely applied in various tasks, their performance on CSC is often\nunsatisfactory. We find that LLMs fail to meet the Chinese character-level\nconstraints of the CSC task, namely equal length and phonetic similarity,\nleading to a performance bottleneck. Further analysis reveal that this issue\nstems from the granularity of tokenization, as current mixed character-word\ntokenization struggles to satisfy these character-level constraints. To address\nthis issue, we propose C-LLM, a Large Language Model-based Chinese Spell\nChecking method that learns to check errors Character by Character.\nCharacter-level tokenization enables the model to learn character-level\nalignment, effectively mitigating issues related to character-level\nconstraints. Furthermore, CSC is simplified to replication-dominated and\nsubstitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate\nthat C-LLM achieves an average improvement of 10% over existing methods.\nSpecifically, it shows a 2.1% improvement in general scenarios and a\nsignificant 12% improvement in vertical domain scenarios, establishing\nstate-of-the-art performance. The source code can be accessed at\nhttps://github.com/ktlKTL/C-LLM.\n","authors":["Kunting Li","Yong Hu","Liang He","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.16536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16535v1","updated":"2024-06-24T11:16:26Z","published":"2024-06-24T11:16:26Z","title":"Token-based Decision Criteria Are Suboptimal in In-context Learning","summary":"  In-Context Learning (ICL) typically utilizes classification criteria from\nprobabilities of manually selected label tokens. However, we argue that such\ntoken-based classification criteria lead to suboptimal decision boundaries,\ndespite delicate calibrations through translation and constrained rotation. To\naddress this problem, we propose Hidden Calibration, which renounces token\nprobabilities and uses the nearest centroid classifier on the LM's last hidden\nstates. In detail, we use the nearest centroid classification on the hidden\nstates, assigning the category of the nearest centroid previously observed from\na few-shot calibration set to the test sample as the predicted label. Our\nexperiments on 3 models and 10 classification datasets indicate that Hidden\nCalibration consistently outperforms current token-based calibrations by about\n20%. Our further analysis demonstrates that Hidden Calibration finds better\nclassification criteria with less inter-categories overlap, and LMs provide\nlinearly separable intra-category clusters with the help of demonstrations,\nwhich supports Hidden Calibration and gives new insights into the conventional\nICL.\n","authors":["Hakaze Cho","Yoshihiro Sakai","Mariko Kato","Kenshiro Tanaka","Akira Ishii","Naoya Inoue"],"pdf_url":"https://arxiv.org/pdf/2406.16535v1.pdf","comment":"21 pages, 14 figures, 8 tables"},{"id":"http://arxiv.org/abs/2406.16529v1","updated":"2024-06-24T11:08:28Z","published":"2024-06-24T11:08:28Z","title":"Towards Better Graph-based Cross-document Relation Extraction via\n  Non-bridge Entity Enhancement and Prediction Debiasing","summary":"  Cross-document Relation Extraction aims to predict the relation between\ntarget entities located in different documents. In this regard, the dominant\nmodels commonly retain useful information for relation prediction via bridge\nentities, which allows the model to elaborately capture the intrinsic\ninterdependence between target entities. However, these studies ignore the\nnon-bridge entities, each of which co-occurs with only one target entity and\noffers the semantic association between target entities for relation\nprediction. Besides, the commonly-used dataset--CodRED contains substantial NA\ninstances, leading to the prediction bias during inference. To address these\nissues, in this paper, we propose a novel graph-based cross-document RE model\nwith non-bridge entity enhancement and prediction debiasing. Specifically, we\nuse a unified entity graph to integrate numerous non-bridge entities with\ntarget entities and bridge entities, modeling various associations between\nthem, and then use a graph recurrent network to encode this graph. Finally, we\nintroduce a novel debiasing strategy to calibrate the original prediction\ndistribution. Experimental results on the closed and open settings show that\nour model significantly outperforms all baselines, including the GPT-3.5-turbo\nand InstructUIE, achieving state-of-the-art performance. Particularly, our\nmodel obtains 66.23% and 55.87% AUC points in the official\nleaderboard\\footnote{\\url{https://codalab.lisn.upsaclay.fr/competitions/3770#results}}\nunder the two settings, respectively, ranking the first place in all\nsubmissions since December 2023. Our code is available at\nhttps://github.com/DeepLearnXMU/CoRE-NEPD.\n","authors":["Hao Yue","Shaopeng Lai","Chengyi Yang","Liang Zhang","Junfeng Yao","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2406.16529v1.pdf","comment":"Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2406.16528v1","updated":"2024-06-24T11:07:01Z","published":"2024-06-24T11:07:01Z","title":"Evaluating the Ability of Large Language Models to Reason about Cardinal\n  Directions","summary":"  We investigate the abilities of a representative set of Large language Models\n(LLMs) to reason about cardinal directions (CDs). To do so, we create two\ndatasets: the first, co-created with ChatGPT, focuses largely on recall of\nworld knowledge about CDs; the second is generated from a set of templates,\ncomprehensively testing an LLM's ability to determine the correct CD given a\nparticular scenario. The templates allow for a number of degrees of variation\nsuch as means of locomotion of the agent involved, and whether set in the first\n, second or third person. Even with a temperature setting of zero, Our\nexperiments show that although LLMs are able to perform well in the simpler\ndataset, in the second more complex dataset no LLM is able to reliably\ndetermine the correct CD, even with a temperature setting of zero.\n","authors":["Anthony G Cohn","Robert E Blackwell"],"pdf_url":"https://arxiv.org/pdf/2406.16528v1.pdf","comment":"9 pages, 3 figures, 1 table. Short paper accepted by COSIT 24, The\n  16th Conference on Spatial Information Theory"},{"id":"http://arxiv.org/abs/2406.16527v1","updated":"2024-06-24T11:04:43Z","published":"2024-06-24T11:04:43Z","title":"SyROCCo: Enhancing Systematic Reviews using Machine Learning","summary":"  The sheer number of research outputs published every year makes systematic\nreviewing increasingly time- and resource-intensive. This paper explores the\nuse of machine learning techniques to help navigate the systematic review\nprocess. ML has previously been used to reliably 'screen' articles for review -\nthat is, identify relevant articles based on reviewers' inclusion criteria. The\napplication of ML techniques to subsequent stages of a review, however, such as\ndata extraction and evidence mapping, is in its infancy. We therefore set out\nto develop a series of tools that would assist in the profiling and analysis of\n1,952 publications on the theme of 'outcomes-based contracting'. Tools were\ndeveloped for the following tasks: assign publications into 'policy area'\ncategories; identify and extract key information for evidence mapping, such as\norganisations, laws, and geographical information; connect the evidence base to\nan existing dataset on the same topic; and identify subgroups of articles that\nmay share thematic content. An interactive tool using these techniques and a\npublic dataset with their outputs have been released. Our results demonstrate\nthe utility of ML techniques to enhance evidence accessibility and analysis\nwithin the systematic review processes. These efforts show promise in\npotentially yielding substantial efficiencies for future systematic reviewing\nand for broadening their analytical scope. Our work suggests that there may be\nimplications for the ease with which policymakers and practitioners can access\nevidence. While ML techniques seem poised to play a significant role in\nbridging the gap between research and policy by offering innovative ways of\ngathering, accessing, and analysing data from systematic reviews, we also\nhighlight their current limitations and the need to exercise caution in their\napplication, particularly given the potential for errors and biases.\n","authors":["Zheng Fang","Miguel Arana-Catania","Felix-Anselm van Lier","Juliana Outes Velarde","Harry Bregazzi","Mara Airoldi","Eleanor Carter","Rob Procter"],"pdf_url":"https://arxiv.org/pdf/2406.16527v1.pdf","comment":"28 pages, 5 figures. To appear in Data & Policy journal"},{"id":"http://arxiv.org/abs/2406.16524v1","updated":"2024-06-24T10:59:26Z","published":"2024-06-24T10:59:26Z","title":"The Privileged Students: On the Value of Initialization in Multilingual\n  Knowledge Distillation","summary":"  Knowledge distillation (KD) has proven to be a successful strategy to improve\nthe performance of a smaller model in many NLP tasks. However, most of the work\nin KD only explores monolingual scenarios. In this paper, we investigate the\nvalue of KD in multilingual settings. We find the significance of KD and model\ninitialization by analyzing how well the student model acquires multilingual\nknowledge from the teacher model. Our proposed method emphasizes copying the\nteacher model's weights directly to the student model to enhance\ninitialization. Our finding shows that model initialization using copy-weight\nfrom the fine-tuned teacher contributes the most compared to the distillation\nprocess itself across various multilingual settings. Furthermore, we\ndemonstrate that efficient weight initialization preserves multilingual\ncapabilities even in low-resource scenarios.\n","authors":["Haryo Akbarianto Wibowo","Thamar Solorio","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2406.16524v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2406.16521v1","updated":"2024-06-24T10:55:31Z","published":"2024-06-24T10:55:31Z","title":"Carrot and Stick: Inducing Self-Motivation with Positive & Negative\n  Feedback","summary":"  Positive thinking is thought to be an important component of self-motivation\nin various practical fields such as education and the workplace. Previous work,\nincluding sentiment transfer and positive reframing, has focused on the\npositive side of language. However, self-motivation that drives people to reach\ntheir goals has not yet been studied from a computational perspective.\nMoreover, negative feedback has not yet been explored, even though positive and\nnegative feedback are both necessary to grow self-motivation. To facilitate\nself-motivation, we propose CArrot and STICk (CASTIC) dataset, consisting of\n12,590 sentences with 5 different strategies for enhancing self-motivation. Our\ndata and code are publicly available at here.\n","authors":["Jimin Sohn","Jeihee Cho","Junyong Lee","Songmu Heo","Ji-Eun Han","David R. Mortensen"],"pdf_url":"https://arxiv.org/pdf/2406.16521v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.16508v1","updated":"2024-06-24T10:27:07Z","published":"2024-06-24T10:27:07Z","title":"Large Vocabulary Size Improves Large Language Models","summary":"  This paper empirically investigates the relationship between subword\nvocabulary size and the performance of large language models (LLMs) to provide\ninsights on how to define the vocabulary size. Experimental results show that\nlarger vocabulary sizes lead to better performance in LLMs. Moreover, we\nconsider a continual training scenario where a pre-trained language model is\ntrained on a different target language. We introduce a simple method to use a\nnew vocabulary instead of the pre-defined one. We show that using the new\nvocabulary outperforms the model with the vocabulary used in pre-training.\n","authors":["Sho Takase","Ryokan Ri","Shun Kiyono","Takuya Kato"],"pdf_url":"https://arxiv.org/pdf/2406.16508v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2402.14963v2","updated":"2024-06-24T10:05:24Z","published":"2024-02-22T20:57:17Z","title":"Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich\n  Reasoning","summary":"  While Large language models (LLMs) have the capability to iteratively reflect\non their own outputs, recent studies have observed their struggles with\nknowledge-rich problems without access to external resources. In addition to\nthe inefficiency of LLMs in self-assessment, we also observe that LLMs struggle\nto revisit their predictions despite receiving explicit negative feedback.\nTherefore, We propose Mirror, a Multiple-perspective self-reflection method for\nknowledge-rich reasoning, to avoid getting stuck at a particular reflection\niteration. Mirror enables LLMs to reflect from multiple-perspective clues,\nachieved through a heuristic interaction between a Navigator and a Reasoner. It\nguides agents toward diverse yet plausibly reliable reasoning trajectory\nwithout access to ground truth by encouraging (1) diversity of directions\ngenerated by Navigator and (2) agreement among strategically induced\nperturbations in responses generated by the Reasoner. The experiments on five\nreasoning datasets demonstrate that Mirror's superiority over several\ncontemporary self-reflection approaches. Additionally, the ablation study\nstudies clearly indicate that our strategies alleviate the aforementioned\nchallenges.\n","authors":["Hanqi Yan","Qinglin Zhu","Xinyu Wang","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2402.14963v2.pdf","comment":"ACL24, Main Conference, long paper. Code is available at\n  https://github.com/hanqi-qi/Mirror.git"},{"id":"http://arxiv.org/abs/2406.16495v1","updated":"2024-06-24T10:05:23Z","published":"2024-06-24T10:05:23Z","title":"OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to\n  construct Observer-Thinker-Conceiver-Expresser","summary":"  Recent research has shown that combining Mamba with Transformer architecture,\nwhich has selective state space and quadratic self-attention mechanism,\noutperforms using Mamba or Transformer architecture alone in language modeling\ntasks. The quadratic self-attention mechanism effectively alleviates the\nshortcomings of selective state space in handling long-term dependencies of any\nelement in the sequence. We propose a position information injection method\nthat connects the selective state space model with the quadratic attention, and\nintegrates these two architectures with hybrid experts with cross-sharing\ndomains, so that we can enjoy the advantages of both. We design a new\narchitecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser\n(OTCE), which can compete with well-known medium-scale open-source language\nmodels on a small scale in language modeling tasks.\n","authors":["Jingze Shi","Ting Xie","Bingheng Wu","Chunjun Zheng","Kai Wang"],"pdf_url":"https://arxiv.org/pdf/2406.16495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16490v1","updated":"2024-06-24T09:57:44Z","published":"2024-06-24T09:57:44Z","title":"eagerlearners at SemEval2024 Task 5: The Legal Argument Reasoning Task\n  in Civil Procedure","summary":"  This study investigates the performance of the zero-shot method in\nclassifying data using three large language models, alongside two models with\nlarge input token sizes and the two pre-trained models on legal data. Our main\ndataset comes from the domain of U.S. civil procedure. It includes summaries of\nlegal cases, specific questions, potential answers, and detailed explanations\nfor why each solution is relevant, all sourced from a book aimed at law\nstudents. By comparing different methods, we aimed to understand how\neffectively they handle the complexities found in legal datasets. Our findings\nshow how well the zero-shot method of large language models can understand\ncomplicated data. We achieved our highest F1 score of 64% in these experiments.\n","authors":["Hoorieh Sabzevari","Mohammadmostafa Rostamkhani","Sauleh Eetemadi"],"pdf_url":"https://arxiv.org/pdf/2406.16490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16489v1","updated":"2024-06-24T09:55:31Z","published":"2024-06-24T09:55:31Z","title":"Deepfake tweets automatic detection","summary":"  This study addresses the critical challenge of detecting DeepFake tweets by\nleveraging advanced natural language processing (NLP) techniques to distinguish\nbetween genuine and AI-generated texts. Given the increasing prevalence of\nmisinformation, our research utilizes the TweepFake dataset to train and\nevaluate various machine learning models. The objective is to identify\neffective strategies for recognizing DeepFake content, thereby enhancing the\nintegrity of digital communications. By developing reliable methods for\ndetecting AI-generated misinformation, this work contributes to a more\ntrustworthy online information environment.\n","authors":["Adam Frej","Adrian Kaminski","Piotr Marciniak","Szymon Szmajdzinski","Soveatin Kuntur","Anna Wroblewska"],"pdf_url":"https://arxiv.org/pdf/2406.16489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16478v1","updated":"2024-06-24T09:32:28Z","published":"2024-06-24T09:32:28Z","title":"EMMI -- Empathic Multimodal Motivational Interviews Dataset: Analyses\n  and Annotations","summary":"  The study of multimodal interaction in therapy can yield a comprehensive\nunderstanding of therapist and patient behavior that can be used to develop a\nmultimodal virtual agent supporting therapy. This investigation aims to uncover\nhow therapists skillfully blend therapy's task goal (employing classical steps\nof Motivational Interviewing) with the social goal (building a trusting\nrelationship and expressing empathy). Furthermore, we seek to categorize\npatients into various ``types'' requiring tailored therapeutic approaches. To\nthis intent, we present multimodal annotations of a corpus consisting of\nsimulated motivational interviewing conversations, wherein actors portray the\nroles of patients and therapists. We introduce EMMI, composed of two publicly\navailable MI corpora, AnnoMI and the Motivational Interviewing Dataset, for\nwhich we add multimodal annotations. We analyze these annotations to\ncharacterize functional behavior for developing a virtual agent performing\nmotivational interviews emphasizing social and empathic behaviors. Our analysis\nfound three clusters of patients expressing significant differences in behavior\nand adaptation of the therapist's behavior to those types. This shows the\nimportance of a therapist being able to adapt their behavior depending on the\ncurrent situation within the dialog and the type of user.\n","authors":["Lucie Galland","Catherine Pelachaud","Florian Pecune"],"pdf_url":"https://arxiv.org/pdf/2406.16478v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.16477v1","updated":"2024-06-24T09:30:36Z","published":"2024-06-24T09:30:36Z","title":"DaLPSR: Leverage Degradation-Aligned Language Prompt for Real-World\n  Image Super-Resolution","summary":"  Image super-resolution pursuits reconstructing high-fidelity high-resolution\ncounterpart for low-resolution image. In recent years, diffusion-based models\nhave garnered significant attention due to their capabilities with rich prior\nknowledge. The success of diffusion models based on general text prompts has\nvalidated the effectiveness of textual control in the field of text2image.\nHowever, given the severe degradation commonly presented in low-resolution\nimages, coupled with the randomness characteristics of diffusion models,\ncurrent models struggle to adequately discern semantic and degradation\ninformation within severely degraded images. This often leads to obstacles such\nas semantic loss, visual artifacts, and visual hallucinations, which pose\nsubstantial challenges for practical use. To address these challenges, this\npaper proposes to leverage degradation-aligned language prompt for accurate,\nfine-grained, and high-fidelity image restoration. Complementary priors\nincluding semantic content descriptions and degradation prompts are explored.\nSpecifically, on one hand, image-restoration prompt alignment decoder is\nproposed to automatically discern the degradation degree of LR images, thereby\ngenerating beneficial degradation priors for image restoration. On the other\nhand, much richly tailored descriptions from pretrained multimodal large\nlanguage model elicit high-level semantic priors closely aligned with human\nperception, ensuring fidelity control for image restoration. Comprehensive\ncomparisons with state-of-the-art methods have been done on several popular\nsynthetic and real-world benchmark datasets. The quantitative and qualitative\nanalysis have demonstrated that the proposed method achieves a new\nstate-of-the-art perceptual quality level, especially in real-world cases based\non reference-free metrics.\n","authors":["Aiwen Jiang","Zhi Wei","Long Peng","Feiqiang Liu","Wenbo Li","Mingwen Wang"],"pdf_url":"https://arxiv.org/pdf/2406.16477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15279v2","updated":"2024-06-24T09:18:48Z","published":"2024-03-22T15:22:06Z","title":"Fundus: A Simple-to-Use News Scraper Optimized for High Quality\n  Extractions","summary":"  This paper introduces Fundus, a user-friendly news scraper that enables users\nto obtain millions of high-quality news articles with just a few lines of code.\nUnlike existing news scrapers, we use manually crafted, bespoke content\nextractors that are specifically tailored to the formatting guidelines of each\nsupported online newspaper. This allows us to optimize our scraping for quality\nsuch that retrieved news articles are textually complete and without HTML\nartifacts. Further, our framework combines both crawling (retrieving HTML from\nthe web or large web archives) and content extraction into a single pipeline.\nBy providing a unified interface for a predefined collection of newspapers, we\naim to make Fundus broadly usable even for non-technical users. This paper\ngives an overview of the framework, discusses our design choices, and presents\na comparative evaluation against other popular news scrapers. Our evaluation\nshows that Fundus yields significantly higher quality extractions (complete and\nartifact-free news articles) than prior work. The framework is available on\nGitHub under https://github.com/flairNLP/fundus and can be simply installed\nusing pip.\n","authors":["Max Dallabetta","Conrad Dobberstein","Adrian Breiding","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2403.15279v2.pdf","comment":"10 pages, 4 figures, ACL 2024, for a screencast see\n  https://www.youtube.com/watch?v=9GJExMelhdI"},{"id":"http://arxiv.org/abs/2406.16469v1","updated":"2024-06-24T09:18:15Z","published":"2024-06-24T09:18:15Z","title":"Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark\n  with Human-VLM Collaboration","summary":"  To create culturally inclusive vision-language models (VLMs), the foremost\nrequirement is developing a test benchmark that can diagnose the models'\nability to respond to questions reflecting cultural elements. This paper\naddresses the necessity for such benchmarks, noting that existing research has\nrelied on human annotators' manual efforts, which impedes diversity and\nefficiency. We propose a semi-automated pipeline for constructing cultural VLM\nbenchmarks to enhance diversity and efficiency. This pipeline leverages\nhuman-VLM collaboration, where VLMs generate questions based on guidelines,\nhuman-annotated examples, and image-wise relevant knowledge, which are then\nreviewed by native speakers for quality and cultural relevance. The\neffectiveness of our adaptable pipeline is demonstrated through a specific\napplication: creating a dataset tailored to Korean culture, dubbed K-Viscuit.\nThe resulting benchmark features two types of questions: Type 1 questions\nmeasure visual recognition abilities, while Type 2 assess fine-grained visual\nreasoning skills. This ensures a thorough diagnosis of VLM models across\nvarious aspects. Our evaluation using K-Viscuit revealed that open-source\nmodels notably lag behind proprietary models in understanding Korean culture,\nhighlighting areas for improvement. We provided diverse analyses of VLM\nperformance across different cultural aspects. Besides, we explored the\npotential of incorporating external knowledge retrieval to enhance the\ngeneration process, suggesting future directions for improving cultural\ninterpretation ability of VLMs. Our dataset and code will be made publicly\navailable.\n","authors":["Yujin Baek","ChaeHun Park","Jaeseok Kim","Yu-Jung Heo","Du-Seong Chang","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2406.16469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16464v1","updated":"2024-06-24T09:13:42Z","published":"2024-06-24T09:13:42Z","title":"InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection","summary":"  The prevalence of sarcasm in social media, conveyed through text-image\ncombinations, presents significant challenges for sentiment analysis and\nintention mining. Current multi-modal sarcasm detection methods have been\nproven to struggle with biases from spurious cues, leading to a superficial\nunderstanding of the complex interactions between text and image. To address\nthese issues, we propose InterCLIP-MEP, a robust framework for multi-modal\nsarcasm detection. InterCLIP-MEP introduces a refined variant of CLIP,\nInteractive CLIP (InterCLIP), as the backbone, enhancing sample representations\nby embedding cross-modality information in each encoder. Furthermore, a novel\ntraining strategy is designed to adapt InterCLIP for a Memory-Enhanced\nPredictor (MEP). MEP uses dynamic dual-channel memory to store valuable\nhistorical knowledge of test samples and then leverages this memory as a\nnon-parametric classifier to derive the final prediction. By using InterCLIP to\nencode text-image interactions more effectively and incorporating MEP,\nInterCLIP-MEP offers a more robust recognition of multi-modal sarcasm.\nExperiments demonstrate that InterCLIP-MEP achieves state-of-the-art\nperformance on the MMSD2.0 benchmark. Code and data are available at\n[https://github.com/CoderChen01/InterCLIP-MEP](https://github.com/CoderChen01/InterCLIP-MEP).\n","authors":["Junjie Chen","Subin Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16464v1.pdf","comment":"8 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2404.08676v3","updated":"2024-06-24T08:50:22Z","published":"2024-04-06T15:01:47Z","title":"ALERT: A Comprehensive Benchmark for Assessing Large Language Models'\n  Safety through Red Teaming","summary":"  When building Large Language Models (LLMs), it is paramount to bear safety in\nmind and protect them with guardrails. Indeed, LLMs should never generate\ncontent promoting or normalizing harmful, illegal, or unethical behavior that\nmay contribute to harm to individuals or society. This principle applies to\nboth normal and adversarial use. In response, we introduce ALERT, a large-scale\nbenchmark to assess safety based on a novel fine-grained risk taxonomy. It is\ndesigned to evaluate the safety of LLMs through red teaming methodologies and\nconsists of more than 45k instructions categorized using our novel taxonomy. By\nsubjecting LLMs to adversarial testing scenarios, ALERT aims to identify\nvulnerabilities, inform improvements, and enhance the overall safety of the\nlanguage models. Furthermore, the fine-grained taxonomy enables researchers to\nperform an in-depth evaluation that also helps one to assess the alignment with\nvarious policies. In our experiments, we extensively evaluate 10 popular open-\nand closed-source LLMs and demonstrate that many of them still struggle to\nattain reasonable levels of safety.\n","authors":["Simone Tedeschi","Felix Friedrich","Patrick Schramowski","Kristian Kersting","Roberto Navigli","Huu Nguyen","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2404.08676v3.pdf","comment":"17 pages, preprint"},{"id":"http://arxiv.org/abs/2402.05359v5","updated":"2024-06-24T08:49:29Z","published":"2024-02-08T02:37:30Z","title":"Prompting with Divide-and-Conquer Program Makes Large Language Models\n  Discerning to Hallucination and Deception","summary":"  Foundation models, such as Large language Models (LLMs), have attracted\nsignificant amount of interest due to their large number of applications.\nHowever, when handling tasks involving repetitive sub-tasks and/or deceptive\ncontents, such as arithmetic calculation and article-level fake news detection,\nsimple instructional prompts suffer from inaccurate responses. Existing works\nshow that more complicated prompting strategies, such as Chain-of-Thoughts and\nLeast-to-Most, can unlock LLM's powerful capacity in diverse areas. Recent\nresearches reveal that simple divide-and-conquer prompting strategy, i.e.\nsimply dividing the input sequence to multiple sub-inputs, can also\nsubstantially improve LLM's performance in some specific tasks such as\nmisinformation detection. In this paper, we aim at examining the utility of\ndivide-and-conquer prompting strategy and answer on which kind of tasks this\nstrategy gets advantages. Specifically, we provide a theoretic analysis to\ndivide-and-conquer prompting strategy and help us identify the specific tasks\nwhere DaC prompting can bring performance boost with theoretic guarantee. We\nthen present two cases (large integer arithmetic and fact verification) where\nexperimental results aligns with our theoretic analysis.\n","authors":["Yizhou Zhang","Lun Du","Defu Cao","Qiang Fu","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2402.05359v5.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.16450v1","updated":"2024-06-24T08:43:21Z","published":"2024-06-24T08:43:21Z","title":"Building on Efficient Foundations: Effectively Training LLMs with\n  Structured Feedforward Layers","summary":"  State-of-the-art results in large language models (LLMs) often rely on scale,\nwhich becomes computationally expensive. This has sparked a research agenda to\nreduce these models' parameter count and computational costs without\nsignificantly impacting their performance. Our study focuses on\ntransformer-based LLMs, specifically targeting the computationally intensive\nfeedforward networks (FFN), which are less studied than attention blocks. We\nconsider three candidate linear layer approximations in the FFN by combining\nefficient low-rank and block-diagonal matrices. In contrast to many previous\nworks that examined these approximations, our study i) explores these\nstructures from the training-from-scratch perspective, ii) scales up to 1.3B\nparameters, and iii) is conducted within recent Transformer-based LLMs rather\nthan convolutional architectures. We first demonstrate they can lead to actual\ncomputational gains in various scenarios, including online decoding when using\na pre-merge technique. Additionally, we propose a novel training regime, called\n\\textit{self-guided training}, aimed at improving the poor training dynamics\nthat these approximations exhibit when used from initialization. Experiments on\nthe large RefinedWeb dataset show that our methods are both efficient and\neffective for training and inference. Interestingly, these structured FFNs\nexhibit steeper scaling curves than the original models. Further applying\nself-guided training to the structured matrices with 32\\% FFN parameters and\n2.5$\\times$ speed-up enables only a 0.4 perplexity increase under the same\ntraining FLOPs. Finally, we develop the wide and structured networks surpassing\nthe current medium-sized and large-sized Transformer in perplexity and\nthroughput performance. Our code is available at\n\\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.\n","authors":["Xiuying Wei","Skander Moalla","Razvan Pascanu","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2406.16450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11254v2","updated":"2024-06-24T08:34:56Z","published":"2024-02-17T11:28:08Z","title":"C-ICL: Contrastive In-context Learning for Information Extraction","summary":"  There has been increasing interest in exploring the capabilities of advanced\nlarge language models (LLMs) in the field of information extraction (IE),\nspecifically focusing on tasks related to named entity recognition (NER) and\nrelation extraction (RE). Although researchers are exploring the use of\nfew-shot information extraction through in-context learning with LLMs, they\ntend to focus only on using correct or positive examples for demonstration,\nneglecting the potential value of incorporating incorrect or negative examples\ninto the learning process. In this paper, we present c-ICL, a novel few-shot\ntechnique that leverages both correct and incorrect sample constructions to\ncreate in-context learning demonstrations. This approach enhances the ability\nof LLMs to extract entities and relations by utilizing prompts that incorporate\nnot only the positive samples but also the reasoning behind them. This method\nallows for the identification and correction of potential interface errors.\nSpecifically, our proposed method taps into the inherent contextual information\nand valuable information in hard negative samples and the nearest positive\nneighbors to the test and then applies the in-context learning demonstrations\nbased on LLMs. Our experiments on various datasets indicate that c-ICL\noutperforms previous few-shot in-context learning methods, delivering\nsubstantial enhancements in performance across a broad spectrum of related\ntasks. These improvements are noteworthy, showcasing the versatility of our\napproach in miscellaneous scenarios.\n","authors":["Ying Mo","Jiahao Liu","Jian Yang","Qifan Wang","Shun Zhang","Jingang Wang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2402.11254v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2312.17055v2","updated":"2024-06-24T08:34:18Z","published":"2023-12-28T15:02:03Z","title":"Improving In-context Learning via Bidirectional Alignment","summary":"  Large language models (LLMs) have shown impressive few-shot generalization on\nmany tasks via in-context learning (ICL). Despite their success in showing such\nemergent abilities, the scale and complexity of larger models also lead to\nunprecedentedly high computational demands and deployment challenges. In\nreaction, researchers explore transferring the powerful capabilities of larger\nmodels to more efficient and compact models by typically aligning the output of\nsmaller (student) models with that of larger (teacher) models. Existing methods\neither train student models on the generated outputs of teacher models or\nimitate their token-level probability distributions. However, these\ndistillation methods pay little to no attention to the input, which also plays\na crucial role in ICL. Based on the finding that the performance of ICL is\nhighly sensitive to the selection of demonstration examples, we propose\nBidirectional Alignment (BiAlign) to fully leverage the models' preferences for\nICL examples to improve the ICL abilities of student models. Specifically, we\nintroduce the alignment of input preferences between student and teacher models\nby incorporating a novel ranking loss, in addition to aligning the token-level\noutput distribution. With extensive experiments and analysis, we demonstrate\nthat BiAlign can consistently outperform existing baselines on a variety of\ntasks involving language understanding, reasoning, and coding.\n","authors":["Chengwei Qin","Wenhan Xia","Fangkai Jiao","Chen Chen","Yuchen Hu","Bosheng Ding","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2312.17055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16441v1","updated":"2024-06-24T08:32:48Z","published":"2024-06-24T08:32:48Z","title":"UniCoder: Scaling Code Large Language Model via Universal Code","summary":"  Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code.\n","authors":["Tao Sun","Linzheng Chai","Jian Yang","Yuwei Yin","Hongcheng Guo","Jiaheng Liu","Bing Wang","Liqun Yang","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2406.16441v1.pdf","comment":"Accepted by ACL 2024 (Main)"},{"id":"http://arxiv.org/abs/2403.00393v2","updated":"2024-06-24T08:28:18Z","published":"2024-03-01T09:28:38Z","title":"TRUCE: Private Benchmarking to Prevent Contamination and Improve\n  Comparative Evaluation of LLMs","summary":"  Benchmarking is the de-facto standard for evaluating LLMs, due to its speed,\nreplicability and low cost. However, recent work has pointed out that the\nmajority of the open source benchmarks available today have been contaminated\nor leaked into LLMs, meaning that LLMs have access to test data during\npretraining and/or fine-tuning. This raises serious concerns about the validity\nof benchmarking studies conducted so far and the future of evaluation using\nbenchmarks. To solve this problem, we propose Private Benchmarking, a solution\nwhere test datasets are kept private and models are evaluated without revealing\nthe test data to the model. We describe various scenarios (depending on the\ntrust placed on model owners or dataset owners), and present solutions to avoid\ndata contamination using private benchmarking. For scenarios where the model\nweights need to be kept private, we describe solutions from confidential\ncomputing and cryptography that can aid in private benchmarking. We build an\nend-to-end system, TRUCE, that enables such private benchmarking showing that\nthe overheads introduced to protect models and benchmark are negligible (in the\ncase of confidential computing) and tractable (when cryptographic security is\nrequired). Finally, we also discuss solutions to the problem of benchmark\ndataset auditing, to ensure that private benchmarks are of sufficiently high\nquality.\n","authors":["Tanmay Rajore","Nishanth Chandran","Sunayana Sitaram","Divya Gupta","Rahul Sharma","Kashish Mittal","Manohar Swaminathan"],"pdf_url":"https://arxiv.org/pdf/2403.00393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13372v3","updated":"2024-06-24T08:20:04Z","published":"2024-03-20T08:08:54Z","title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models","summary":"  Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It provides a\nsolution for flexibly customizing the fine-tuning of 100+ LLMs without the need\nfor coding through the built-in web UI LlamaBoard. We empirically validate the\nefficiency and effectiveness of our framework on language modeling and text\ngeneration tasks. It has been released at\nhttps://github.com/hiyouga/LLaMA-Factory and received over 24,000 stars and\n3,000 forks.\n","authors":["Yaowei Zheng","Richong Zhang","Junhao Zhang","Yanhan Ye","Zheyan Luo","Zhangchi Feng","Yongqiang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13372v3.pdf","comment":"13 pages, accepted to ACL 2024 System Demonstration Track"},{"id":"http://arxiv.org/abs/2406.15111v2","updated":"2024-06-24T08:19:00Z","published":"2024-06-21T12:59:20Z","title":"Investigating the impact of 2D gesture representation on co-speech\n  gesture generation","summary":"  Co-speech gestures play a crucial role in the interactions between humans and\nembodied conversational agents (ECA). Recent deep learning methods enable the\ngeneration of realistic, natural co-speech gestures synchronized with speech,\nbut such approaches require large amounts of training data. \"In-the-wild\"\ndatasets, which compile videos from sources such as YouTube through human pose\ndetection models, offer a solution by providing 2D skeleton sequences that are\npaired with speech. Concurrently, innovative lifting models have emerged,\ncapable of transforming these 2D pose sequences into their 3D counterparts,\nleading to large and diverse datasets of 3D gestures. However, the derived 3D\npose estimation is essentially a pseudo-ground truth, with the actual ground\ntruth being the 2D motion data. This distinction raises questions about the\nimpact of gesture representation dimensionality on the quality of generated\nmotions, a topic that, to our knowledge, remains largely unexplored. In this\nwork, we evaluate the impact of the dimensionality of the training data, 2D or\n3D joint coordinates, on the performance of a multimodal speech-to-gesture deep\ngenerative model. We use a lifting model to convert 2D-generated sequences of\nbody pose to 3D. Then, we compare the sequence of gestures generated directly\nin 3D to the gestures generated in 2D and lifted to 3D as post-processing.\n","authors":["Teo Guichoux","Laure Soulier","Nicolas Obin","Catherine Pelachaud"],"pdf_url":"https://arxiv.org/pdf/2406.15111v2.pdf","comment":"8 pages. Paper accepted at WACAI 2024"},{"id":"http://arxiv.org/abs/2406.16416v1","updated":"2024-06-24T08:06:56Z","published":"2024-06-24T08:06:56Z","title":"Multilingual Knowledge Editing with Language-Agnostic Factual Neurons","summary":"  Multilingual knowledge editing (MKE) aims to simultaneously revise factual\nknowledge across multilingual languages within large language models (LLMs).\nHowever, most existing MKE methods just adapt existing monolingual editing\nmethods to multilingual scenarios, overlooking the deep semantic connections of\nthe same factual knowledge between different languages, thereby limiting edit\nperformance. To address this issue, we first investigate how LLMs represent\nmultilingual factual knowledge and discover that the same factual knowledge in\ndifferent languages generally activates a shared set of neurons, which we call\nlanguage-agnostic factual neurons. These neurons represent the semantic\nconnections between multilingual knowledge and are mainly located in certain\nlayers. Inspired by this finding, we propose a new MKE method by locating and\nmodifying Language-Agnostic Factual Neurons (LAFN) to simultaneously edit\nmultilingual knowledge. Specifically, we first generate a set of paraphrases\nfor each multilingual knowledge to be edited to precisely locate the\ncorresponding language-agnostic factual neurons. Then we optimize the update\nvalues for modifying these located neurons to achieve simultaneous modification\nof the same factual knowledge in multiple languages. Experimental results on\nBi-ZsRE and MzsRE benchmarks demonstrate that our method outperforms existing\nMKE methods and achieves remarkable edit performance, indicating the importance\nof considering the semantic connections among multilingual knowledge.\n","authors":["Xue zhang","Yunlong Liang","Fandong Meng","Songming Zhang","Yufeng Chen","Jinan Xu","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.16416v1.pdf","comment":"12 pages, 4 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.16408v1","updated":"2024-06-24T08:05:21Z","published":"2024-06-24T08:05:21Z","title":"A Symmetry Property of Christoffel Words","summary":"  Motivated by the theory of trapezoidal words, whose sequences of cardinality\nof factors by length are symmetric, we introduce a bivariate variant of this\nsymmetry. We show that this symmetry characterizes Christoffel words, and\nestablish other related results.\n","authors":["Yan Lanciault","Christophe Reutenauer"],"pdf_url":"https://arxiv.org/pdf/2406.16408v1.pdf","comment":"In Proceedings GASCom 2024, arXiv:2406.14588"},{"id":"http://arxiv.org/abs/2403.15250v2","updated":"2024-06-24T07:49:25Z","published":"2024-03-22T14:47:35Z","title":"Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A\n  Multifaceted Statistical Approach","summary":"  Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.\n","authors":["Kun Sun","Rong Wang","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2403.15250v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16382v1","updated":"2024-06-24T07:47:34Z","published":"2024-06-24T07:47:34Z","title":"UNO Arena for Evaluating Sequential Decision-Making Capability of Large\n  Language Models","summary":"  Sequential decision-making refers to algorithms that take into account the\ndynamics of the environment, where early decisions affect subsequent decisions.\nWith large language models (LLMs) demonstrating powerful capabilities between\ntasks, we can't help but ask: Can Current LLMs Effectively Make Sequential\nDecisions? In order to answer this question, we propose the UNO Arena based on\nthe card game UNO to evaluate the sequential decision-making capability of LLMs\nand explain in detail why we choose UNO. In UNO Arena, We evaluate the\nsequential decision-making capability of LLMs dynamically with novel metrics\nbased Monte Carlo methods. We set up random players, DQN-based reinforcement\nlearning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison\ntesting. Furthermore, in order to improve the sequential decision-making\ncapability of LLMs, we propose the TUTRI player, which can involves having LLMs\nreflect their own actions wtih the summary of game history and the game\nstrategy. Numerous experiments demonstrate that the TUTRI player achieves a\nnotable breakthrough in the performance of sequential decision-making compared\nto the vanilla LLM player.\n","authors":["Zhanyue Qin","Haochuan Wang","Deyuan Liu","Ziyang Song","Cunhang Fan","Zhao Lv","Jinlin Wu","Zhen Lei","Zhiying Tu","Dianhui Chu","Xiaoyan Yu","Dianbo Sui"],"pdf_url":"https://arxiv.org/pdf/2406.16382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16377v1","updated":"2024-06-24T07:42:32Z","published":"2024-06-24T07:42:32Z","title":"On the Transformations across Reward Model, Parameter Update, and\n  In-Context Prompt","summary":"  Despite the general capabilities of pre-trained large language models (LLMs),\nthey still need further adaptation to better serve practical applications. In\nthis paper, we demonstrate the interchangeability of three popular and distinct\nadaptation tools: parameter updating, reward modeling, and in-context\nprompting. This interchangeability establishes a triangular framework with six\ntransformation directions, each of which facilitates a variety of applications.\nOur work offers a holistic view that unifies numerous existing studies and\nsuggests potential research directions. We envision our work as a useful\nroadmap for future research on LLMs.\n","authors":["Deng Cai","Huayang Li","Tingchen Fu","Siheng Li","Weiwen Xu","Shuaiyi Li","Bowen Cao","Zhisong Zhang","Xinting Huang","Leyang Cui","Yan Wang","Lemao Liu","Taro Watanabe","Shuming Shi"],"pdf_url":"https://arxiv.org/pdf/2406.16377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05365v2","updated":"2024-06-24T07:39:26Z","published":"2024-06-08T06:04:55Z","title":"CaLM: Contrasting Large and Small Language Models to Verify Grounded\n  Generation","summary":"  Grounded generation aims to equip language models (LMs) with the ability to\nproduce more credible and accountable responses by accurately citing verifiable\nsources. However, existing methods, by either feeding LMs with raw or\npreprocessed materials, remain prone to errors. To address this, we introduce\nCaLM, a novel verification framework. CaLM leverages the insight that a robust\ngrounded response should be consistent with information derived solely from its\ncited sources. Our framework empowers smaller LMs, which rely less on\nparametric memory and excel at processing relevant information given a query,\nto validate the output of larger LMs. Larger LM responses that closely align\nwith the smaller LMs' output, which relies exclusively on cited documents, are\nverified. Responses showing discrepancies are iteratively refined through a\nfeedback loop. Experiments on three open-domain question-answering datasets\ndemonstrate significant performance gains of 1.5% to 7% absolute average\nwithout any required model fine-tuning.\n","authors":["I-Hung Hsu","Zifeng Wang","Long T. Le","Lesly Miculicich","Nanyun Peng","Chen-Yu Lee","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2406.05365v2.pdf","comment":"ACL 2024 Camera Ready Version"},{"id":"http://arxiv.org/abs/2406.16374v1","updated":"2024-06-24T07:32:35Z","published":"2024-06-24T07:32:35Z","title":"KEHRL: Learning Knowledge-Enhanced Language Representations with\n  Hierarchical Reinforcement Learning","summary":"  Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation\ntriples from knowledge graphs (KGs) and integrate these external data sources\ninto language models via self-supervised learning. Previous works treat\nknowledge enhancement as two independent operations, i.e., knowledge injection\nand knowledge integration. In this paper, we propose to learn\nKnowledge-Enhanced language representations with Hierarchical Reinforcement\nLearning (KEHRL), which jointly addresses the problems of detecting positions\nfor knowledge injection and integrating external knowledge into the model in\norder to avoid injecting inaccurate or irrelevant knowledge. Specifically, a\nhigh-level reinforcement learning (RL) agent utilizes both internal and prior\nknowledge to iteratively detect essential positions in texts for knowledge\ninjection, which filters out less meaningful entities to avoid diverting the\nknowledge learning direction. Once the entity positions are selected, a\nrelevant triple filtration module is triggered to perform low-level RL to\ndynamically refine the triples associated with polysemic entities through\nbinary-valued actions. Experiments validate KEHRL's effectiveness in probing\nfactual knowledge and enhancing the model's performance on various natural\nlanguage understanding tasks.\n","authors":["Dongyang Li","Taolin Zhang","Longtao Huang","Chengyu Wang","Xiaofeng He","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2406.16374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09086v3","updated":"2024-06-24T07:31:19Z","published":"2023-11-15T16:30:44Z","title":"The Uli Dataset: An Exercise in Experience Led Annotation of oGBV","summary":"  Online gender based violence has grown concomitantly with adoption of the\ninternet and social media. Its effects are worse in the Global majority where\nmany users use social media in languages other than English. The scale and\nvolume of conversations on the internet has necessitated the need for automated\ndetection of hate speech, and more specifically gendered abuse. There is,\nhowever, a lack of language specific and contextual data to build such\nautomated tools. In this paper we present a dataset on gendered abuse in three\nlanguages- Hindi, Tamil and Indian English. The dataset comprises of tweets\nannotated along three questions pertaining to the experience of gender abuse,\nby experts who identify as women or a member of the LGBTQIA community in South\nAsia. Through this dataset we demonstrate a participatory approach to creating\ndatasets that drive AI systems.\n","authors":["Arnav Arora","Maha Jinadoss","Cheshta Arora","Denny George"," Brindaalakshmi","Haseena Dawood Khan","Kirti Rawat"," Div"," Ritash","Seema Mathur","Shivani Yadav","Shehla Rashid Shora","Rie Raut","Sumit Pawar","Apurva Paithane"," Sonia"," Vivek","Dharini Priscilla"," Khairunnisha","Grace Banu","Ambika Tandon","Rishav Thakker","Rahul Dev Korra","Aatman Vaidya","Tarunima Prabhakar"],"pdf_url":"https://arxiv.org/pdf/2311.09086v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16372v1","updated":"2024-06-24T07:27:01Z","published":"2024-06-24T07:27:01Z","title":"UniPSDA: Unsupervised Pseudo Semantic Data Augmentation for Zero-Shot\n  Cross-Lingual Natural Language Understanding","summary":"  Cross-lingual representation learning transfers knowledge from resource-rich\ndata to resource-scarce ones to improve the semantic understanding abilities of\ndifferent languages. However, previous works rely on shallow unsupervised data\ngenerated by token surface matching, regardless of the global context-aware\nsemantics of the surrounding text tokens. In this paper, we propose an\nUnsupervised Pseudo Semantic Data Augmentation (UniPSDA) mechanism for\ncross-lingual natural language understanding to enrich the training data\nwithout human interventions. Specifically, to retrieve the tokens with similar\nmeanings for the semantic data augmentation across different languages, we\npropose a sequential clustering process in 3 stages: within a single language,\nacross multiple languages of a language family, and across languages from\nmultiple language families. Meanwhile, considering the multi-lingual knowledge\ninfusion with context-aware semantics while alleviating computation burden, we\ndirectly replace the key constituents of the sentences with the above-learned\nmulti-lingual family knowledge, viewed as pseudo-semantic. The infusion process\nis further optimized via three de-biasing techniques without introducing any\nneural parameters. Extensive experiments demonstrate that our model\nconsistently improves the performance on general zero-shot cross-lingual\nnatural language understanding tasks, including sequence classification,\ninformation extraction, and question answering.\n","authors":["Dongyang Li","Taolin Zhang","Jiali Deng","Longtao Huang","Chengyu Wang","Xiaofeng He","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2406.16372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14745v2","updated":"2024-06-24T06:57:05Z","published":"2024-06-20T21:27:57Z","title":"Relation Extraction with Fine-Tuned Large Language Models in Retrieval\n  Augmented Generation Frameworks","summary":"  Information Extraction (IE) is crucial for converting unstructured data into\nstructured formats like Knowledge Graphs (KGs). A key task within IE is\nRelation Extraction (RE), which identifies relationships between entities in\ntext. Various RE methods exist, including supervised, unsupervised, weakly\nsupervised, and rule-based approaches. Recent studies leveraging pre-trained\nlanguage models (PLMs) have shown significant success in this area. In the\ncurrent era dominated by Large Language Models (LLMs), fine-tuning these models\ncan overcome limitations associated with zero-shot LLM prompting-based RE\nmethods, especially regarding domain adaptation challenges and identifying\nimplicit relations between entities in sentences. These implicit relations,\nwhich cannot be easily extracted from a sentence's dependency tree, require\nlogical inference for accurate identification. This work explores the\nperformance of fine-tuned LLMs and their integration into the Retrieval\nAugmented-based (RAG) RE approach to address the challenges of identifying\nimplicit relations at the sentence level, particularly when LLMs act as\ngenerators within the RAG framework. Empirical evaluations on the TACRED,\nTACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant\nperformance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B,\nand T5 (Large). Notably, our approach achieves substantial gains on SemEVAL,\nwhere implicit relations are common, surpassing previous results on this\ndataset. Additionally, our method outperforms previous works on TACRED, TACREV,\nand Re-TACRED, demonstrating exceptional performance across diverse evaluation\nscenarios.\n","authors":["Sefika Efeoglu","Adrian Paschke"],"pdf_url":"https://arxiv.org/pdf/2406.14745v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2406.16356v1","updated":"2024-06-24T06:53:36Z","published":"2024-06-24T06:53:36Z","title":"Evaluation of Instruction-Following Ability for Large Language Models on\n  Story-Ending Generation","summary":"  Instruction-tuned Large Language Models (LLMs) have achieved remarkable\nperformance across various benchmark tasks. While providing instructions to\nLLMs for guiding their generations is user-friendly, assessing their\ninstruction-following capabilities is still unclarified due to a lack of\nevaluation metrics. In this paper, we focus on evaluating the\ninstruction-following ability of LLMs in the context of story-ending\ngeneration, which requires diverse and context-specific instructions. We\npropose an automatic evaluation pipeline that utilizes a machine reading\ncomprehension (MRC) model to determine whether the generated story-ending\nreflects instruction. Our findings demonstrate that our proposed metric aligns\nwith human evaluation. Furthermore, our experiments confirm that recent\nopen-source LLMs can achieve instruction-following performance close to\nGPT-3.5, as assessed through automatic evaluation.\n","authors":["Rem Hida","Junki Ohmura","Toshiyuki Sekiya"],"pdf_url":"https://arxiv.org/pdf/2406.16356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12620v2","updated":"2024-06-24T06:34:35Z","published":"2024-06-18T13:45:50Z","title":"What Makes Two Language Models Think Alike?","summary":"  Do architectural differences significantly affect the way models represent\nand process language? We propose a new approach, based on metric-learning\nencoding models (MLEMs), as a first step to answer this question. The approach\nprovides a feature-based comparison of how any two layers of any two models\nrepresent linguistic information. We apply the method to BERT, GPT-2 and Mamba.\nUnlike previous methods, MLEMs offer a transparent comparison, by identifying\nthe specific linguistic features responsible for similarities and differences.\nMore generally, the method uses formal, symbolic descriptions of a domain, and\nuse these to compare neural representations. As such, the approach can\nstraightforwardly be extended to other domains, such as speech and vision, and\nto other neural systems, including human brains.\n","authors":["Jeanne Salle","Louis Jalouzot","Nur Lan","Emmanuel Chemla","Yair Lakretz"],"pdf_url":"https://arxiv.org/pdf/2406.12620v2.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.10638v6","updated":"2024-06-24T06:28:42Z","published":"2023-10-16T17:57:12Z","title":"In-context Pretraining: Language Modeling Beyond Document Boundaries","summary":"  Large language models (LMs) are currently trained to predict tokens given\ndocument prefixes, enabling them to directly perform long-form generation and\nprompting-style tasks which can be reduced to document completion. Existing\npretraining pipelines train LMs by concatenating random sets of short documents\nto create input contexts but the prior documents provide no signal for\npredicting the next document. We instead present In-Context Pretraining, a new\napproach where language models are pretrained on a sequence of related\ndocuments, thereby explicitly encouraging them to read and reason across\ndocument boundaries. We can do In-Context Pretraining by simply changing the\ndocument ordering so that each context contains related documents, and directly\napplying existing pretraining pipelines. However, this document sorting problem\nis challenging. There are billions of documents and we would like the sort to\nmaximize contextual similarity for every document without repeating any data.\nTo do this, we introduce approximate algorithms for finding related documents\nwith efficient nearest neighbor search and constructing coherent input contexts\nwith a graph traversal algorithm. Our experiments show In-Context Pretraining\noffers a simple and scalable approach to significantly enhance LMs'performance:\nwe see notable improvements in tasks that require more complex contextual\nreasoning, including in-context learning (+8%), reading comprehension (+15%),\nfaithfulness to previous contexts (+16%), long-context reasoning (+5%), and\nretrieval augmentation (+9%).\n","authors":["Weijia Shi","Sewon Min","Maria Lomeli","Chunting Zhou","Margaret Li","Gergely Szilvasy","Rich James","Xi Victoria Lin","Noah A. Smith","Luke Zettlemoyer","Scott Yih","Mike Lewis"],"pdf_url":"https://arxiv.org/pdf/2310.10638v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16342v1","updated":"2024-06-24T06:27:47Z","published":"2024-06-24T06:27:47Z","title":"ADVSCORE: A Metric for the Evaluation and Creation of Adversarial\n  Benchmarks","summary":"  Adversarial benchmarks validate model abilities by providing samples that\nfool models but not humans. However, despite the proliferation of datasets that\nclaim to be adversarial, there does not exist an established metric to evaluate\nhow adversarial these datasets are. To address this lacuna, we introduce\nADVSCORE, a metric which quantifies how adversarial and discriminative an\nadversarial dataset is and exposes the features that make data adversarial. We\nthen use ADVSCORE to underpin a dataset creation pipeline that incentivizes\nwriting a high-quality adversarial dataset. As a proof of concept, we use\nADVSCORE to collect an adversarial question answering (QA) dataset, ADVQA, from\nour pipeline. The high-quality questions in ADVQA surpasses three adversarial\nbenchmarks across domains at fooling several models but not humans. We validate\nour result based on difficulty estimates from 9,347 human responses on four\ndatasets and predictions from three models. Moreover, ADVSCORE uncovers which\nadversarial tactics used by human writers fool models (e.g., GPT-4) but not\nhumans. Through ADVSCORE and its analyses, we offer guidance on revealing\nlanguage model vulnerabilities and producing reliable adversarial examples.\n","authors":["Yoo Yeon Sung","Eve Fleisig","Ishani Mondal","Jordan Lee Boyd-Graber"],"pdf_url":"https://arxiv.org/pdf/2406.16342v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2401.11185"},{"id":"http://arxiv.org/abs/2406.16341v1","updated":"2024-06-24T06:26:50Z","published":"2024-06-24T06:26:50Z","title":"EHRCon: Dataset for Checking Consistency between Unstructured Notes and\n  Structured Tables in Electronic Health Records","summary":"  Electronic Health Records (EHRs) are integral for storing comprehensive\npatient medical records, combining structured data (e.g., medications) with\ndetailed clinical notes (e.g., physician notes). These elements are essential\nfor straightforward data retrieval and provide deep, contextual insights into\npatient care. However, they often suffer from discrepancies due to unintuitive\nEHR system designs and human errors, posing serious risks to patient safety. To\naddress this, we developed EHRCon, a new dataset and task specifically designed\nto ensure data consistency between structured tables and unstructured notes in\nEHRs. EHRCon was crafted in collaboration with healthcare professionals using\nthe MIMIC-III EHR dataset, and includes manual annotations of 3,943 entities\nacross 105 clinical notes checked against database entries for consistency.\nEHRCon has two versions, one using the original MIMIC-III schema, and another\nusing the OMOP CDM schema, in order to increase its applicability and\ngeneralizability. Furthermore, leveraging the capabilities of large language\nmodels, we introduce CheckEHR, a novel framework for verifying the consistency\nbetween clinical notes and database tables. CheckEHR utilizes an eight-stage\nprocess and shows promising results in both few-shot and zero-shot settings.\nThe code is available at https://github.com/dustn1259/EHRCon.\n","authors":["Yeonsu Kwon","Jiho Kim","Gyubok Lee","Seongsu Bae","Daeun Kyung","Wonchul Cha","Tom Pollard","Alistair Johnson","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2406.16341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16332v1","updated":"2024-06-24T06:10:13Z","published":"2024-06-24T06:10:13Z","title":"DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task","summary":"  Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly apply a\ndemonstration retriever to retrieve demonstrations and use top-$k$\ndemonstrations for in-context learning (ICL). Although effective, this approach\noverlooks the dependencies between demonstrations, leading to inferior\nperformance of few-shot ICL in the passage ranking task. In this paper, we\nformulate the demonstration selection as a \\textit{retrieve-then-rerank}\nprocess and introduce the DemoRank framework. In this framework, we first use\nLLM feedback to train a demonstration retriever and construct a novel\ndependency-aware training samples to train a demonstration reranker to improve\nfew-shot ICL. The construction of such training samples not only considers\ndemonstration dependencies but also performs in an efficient way. Extensive\nexperiments demonstrate DemoRank's effectiveness in in-domain scenarios and\nstrong generalization to out-of-domain scenarios. Our codes are available\nat~\\url{https://github.com/8421BCD/DemoRank}.\n","authors":["Wenhan Liu","Yutao Zhu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2406.16332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16330v1","updated":"2024-06-24T05:57:55Z","published":"2024-06-24T05:57:55Z","title":"Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer\n  Merging","summary":"  While large language models (LLMs) excel in many domains, their complexity\nand scale challenge deployment in resource-limited environments. Current\ncompression techniques, such as parameter pruning, often fail to effectively\nutilize the knowledge from pruned parameters. To address these challenges, we\npropose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA),\na novel approach that uses manifold learning and the Normalized Pairwise\nInformation Bottleneck (NPIB) measure to merge similar layers, reducing model\nsize while preserving essential performance. We evaluate MKA on multiple\nbenchmark datasets and various LLMs. Our findings show that MKA not only\npreserves model performance but also achieves substantial compression ratios,\noutperforming traditional pruning methods. Moreover, when coupled with\nquantization, MKA delivers even greater compression. Specifically, on the MMLU\ndataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75%\nwith a minimal performance decrease of only 2.82\\%. The proposed MKA method\noffers a resource-efficient and performance-preserving model compression\ntechnique for LLMs.\n","authors":["Deyuan Liu","Zhanyue Qin","Hairu Wang","Zhao Yang","Zecheng Wang","Fangying Rong","Qingbin Liu","Yanchao Hao","Xi Chen","Cunhang Fan","Zhao Lv","Zhiying Tu","Dianhui Chu","Bo Li","Dianbo Sui"],"pdf_url":"https://arxiv.org/pdf/2406.16330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12842v2","updated":"2024-06-24T05:40:38Z","published":"2024-02-20T09:10:08Z","title":"PromptKD: Distilling Student-Friendly Knowledge for Generative Language\n  Models via Prompt Tuning","summary":"  Recent advancements in large language models (LLMs) have raised concerns\nabout inference costs, increasing the need for research into model compression.\nWhile knowledge distillation (KD) is a prominent method for this, research on\nKD for generative language models like LLMs is relatively sparse, and the\napproach of distilling student-friendly knowledge, which has shown promising\nperformance in KD for classification models, remains unexplored in generative\nlanguage models. To explore this approach, we propose PromptKD, a simple yet\neffective method that utilizes prompt tuning - for the first time in KD - to\nenable generative language models to transfer student-friendly knowledge.\nUnlike previous works in classification that require fine-tuning the entire\nteacher model for extracting student-friendly knowledge, PromptKD achieves\nsimilar effects by adding a small number of prompt tokens and tuning only the\nprompt with student guidance. Extensive experiments on instruction-following\ndatasets show that PromptKD achieves state-of-the-art performance while adding\nonly 0.0007% of the teacher's parameters as prompts. Further analysis suggests\nthat distilling student-friendly knowledge alleviates exposure bias effectively\nthroughout the entire training process, leading to performance enhancements.\n","authors":["Gyeongman Kim","Doohyuk Jang","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2402.12842v2.pdf","comment":"Code: https://github.com/gmkim-ai/PromptKD"},{"id":"http://arxiv.org/abs/2403.19260v3","updated":"2024-06-24T05:31:35Z","published":"2024-03-28T09:34:31Z","title":"NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using\n  Representative Data","summary":"  To address the global issue of online hate, hate speech detection (HSD)\nsystems are typically developed on datasets from the United States, thereby\nfailing to generalize to English dialects from the Majority World. Furthermore,\nHSD models are often evaluated on non-representative samples, raising concerns\nabout overestimating model performance in real-world settings. In this work, we\nintroduce NaijaHate, the first dataset annotated for HSD which contains a\nrepresentative sample of Nigerian tweets. We demonstrate that HSD evaluated on\nbiased datasets traditionally used in the literature consistently overestimates\nreal-world performance by at least two-fold. We then propose NaijaXLM-T, a\npretrained model tailored to the Nigerian Twitter context, and establish the\nkey role played by domain-adaptive pretraining and finetuning in maximizing HSD\nperformance. Finally, owing to the modest performance of HSD systems in\nreal-world conditions, we find that content moderators would need to review\nabout ten thousand Nigerian tweets flagged as hateful daily to moderate 60% of\nall hateful content, highlighting the challenges of moderating hate speech at\nscale as social media usage continues to grow globally. Taken together, these\nresults pave the way towards robust HSD systems and a better protection of\nsocial media users from hateful content in low-resource settings.\n","authors":["Manuel Tonneau","Pedro Vitor Quinta de Castro","Karim Lasri","Ibrahim Farouq","Lakshminarayanan Subramanian","Victor Orozco-Olvera","Samuel P. Fraiberger"],"pdf_url":"https://arxiv.org/pdf/2403.19260v3.pdf","comment":"ACL 2024 main conference. Data and models available at\n  https://github.com/worldbank/NaijaHate"},{"id":"http://arxiv.org/abs/2406.16320v1","updated":"2024-06-24T05:13:19Z","published":"2024-06-24T05:13:19Z","title":"What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for\n  Noise-free Text-Image Corruption and Evaluation","summary":"  Vision-Language Models (VLMs) have gained community-spanning prominence due\nto their ability to integrate visual and textual inputs to perform complex\ntasks. Despite their success, the internal decision-making processes of these\nmodels remain opaque, posing challenges in high-stakes applications. To address\nthis, we introduce NOTICE, the first Noise-free Text-Image Corruption and\nEvaluation pipeline for mechanistic interpretability in VLMs. NOTICE\nincorporates a Semantic Minimal Pairs (SMP) framework for image corruption and\nSymmetric Token Replacement (STR) for text. This approach enables semantically\nmeaningful causal mediation analysis for both modalities, providing a robust\nmethod for analyzing multimodal integration within models like BLIP. Our\nexperiments on the SVO-Probes, MIT-States, and Facial Expression Recognition\ndatasets reveal crucial insights into VLM decision-making, identifying the\nsignificant role of middle-layer cross-attention heads. Further, we uncover a\nset of ``universal cross-attention heads'' that consistently contribute across\ntasks and modalities, each performing distinct functions such as implicit image\nsegmentation, object inhibition, and outlier inhibition. This work paves the\nway for more transparent and interpretable multimodal systems.\n","authors":["Michal Golovanevsky","William Rudman","Vedant Palit","Ritambhara Singh","Carsten Eickhoff"],"pdf_url":"https://arxiv.org/pdf/2406.16320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14725v2","updated":"2024-06-24T05:01:06Z","published":"2024-03-20T21:53:56Z","title":"Testing the Limits of Jailbreaking Defenses with the Purple Problem","summary":"  The rise of \"jailbreak\" attacks on language models has led to a flurry of\ndefenses aimed at preventing undesirable responses. We critically examine the\ntwo stages of the defense pipeline: (i) defining what constitutes unsafe\noutputs, and (ii) enforcing the definition via methods such as input processing\nor fine-tuning. To test the efficacy of existing enforcement mechanisms, we\nconsider a simple and well-specified definition of unsafe outputs--outputs that\ncontain the word \"purple\". Surprisingly, existing fine-tuning and input\ndefenses fail on this simple problem, casting doubt on whether enforcement\nalgorithms can be robust for more complicated definitions. We find that real\nsafety benchmarks similarly test enforcement for a fixed definition. We hope\nthat future research can lead to effective/fast enforcement as well as high\nquality definitions used for enforcement and evaluation.\n","authors":["Taeyoun Kim","Suhas Kotha","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2403.14725v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16319v1","updated":"2024-06-24T04:56:26Z","published":"2024-06-24T04:56:26Z","title":"Modelled Multivariate Overlap: A method for measuring vowel merger","summary":"  This paper introduces a novel method for quantifying vowel overlap. There is\na tension in previous work between using multivariate measures, such as those\nderived from empirical distributions, and the ability to control for unbalanced\ndata and extraneous factors, as is possible when using fitted model parameters.\nThe method presented here resolves this tension by jointly modelling all\nacoustic dimensions of interest and by simulating distributions from the model\nto compute a measure of vowel overlap. An additional benefit of this method is\nthat computation of uncertainty becomes straightforward. We evaluate this\nmethod on corpus speech data targeting the PIN-PEN merger in four dialects of\nEnglish and find that using modelled distributions to calculate Bhattacharyya\naffinity substantially improves results compared to empirical distributions,\nwhile the difference between multivariate and univariate modelling is subtle.\n","authors":["Irene Smith","Morgan Sonderegger","The Spade Consortium"],"pdf_url":"https://arxiv.org/pdf/2406.16319v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2403.07379v2","updated":"2024-06-24T04:53:34Z","published":"2024-03-12T07:32:47Z","title":"Hallmarks of Optimization Trajectories in Neural Networks: Directional\n  Exploration and Redundancy","summary":"  We propose a fresh take on understanding the mechanisms of neural networks by\nanalyzing the rich directional structure of optimization trajectories,\nrepresented by their pointwise parameters. Towards this end, we introduce some\nnatural notions of the complexity of optimization trajectories, both\nqualitative and quantitative, which hallmark the directional nature of\noptimization in neural networks: when is there redundancy, and when\nexploration. We use them to reveal the inherent nuance and interplay involved\nbetween various optimization choices, such as momentum and weight decay.\nFurther, the trajectory perspective helps us see the effect of scale on\nregularizing the directional nature of trajectories, and as a by-product, we\nalso observe an intriguing heterogeneity of Q,K,V dynamics in the middle\nattention layers in LLMs and which is homogenized by scale. Importantly, we put\nthe significant directional redundancy observed to the test by demonstrating\nthat training only scalar batchnorm parameters some while into training matches\nthe performance of training the entire network, which thus exhibits the\npotential of hybrid optimization schemes that are geared towards efficiency.\n","authors":["Sidak Pal Singh","Bobby He","Thomas Hofmann","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2403.07379v2.pdf","comment":"Preprint, 57 pages"},{"id":"http://arxiv.org/abs/2406.16316v1","updated":"2024-06-24T04:50:12Z","published":"2024-06-24T04:50:12Z","title":"Does Cross-Cultural Alignment Change the Commonsense Morality of\n  Language Models?","summary":"  Alignment of the language model with human preferences is a common approach\nto making a language model useful to end users. However, most alignment work is\ndone in English, and human preference datasets are dominated by English,\nreflecting only the preferences of English-speaking annotators. Nevertheless,\nit is common practice to use the English preference data, either directly or by\ntranslating it into the target language, when aligning a multilingual language\nmodel. The question is whether such an alignment strategy marginalizes the\npreference of non-English speaking users. To this end, we investigate the\neffect of aligning Japanese language models with (mostly) English resources. In\nparticular, we focus on evaluating whether the commonsense morality of the\nresulting fine-tuned models is aligned with Japanese culture using the\nJCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show\nthat the fine-tuned model outperforms the SFT model. However, it does not\ndemonstrate the same level of improvement as a model fine-tuned using the JCM,\nsuggesting that while some aspects of commonsense morality are transferable,\nothers may not be.\n","authors":["Yuu Jinnai"],"pdf_url":"https://arxiv.org/pdf/2406.16316v1.pdf","comment":"The 2nd Workshop on Cross-Cultural Considerations in NLP (C3NLP) at\n  ACL 2024"},{"id":"http://arxiv.org/abs/2402.14208v3","updated":"2024-06-24T04:49:16Z","published":"2024-02-22T01:20:51Z","title":"LLM-Assisted Content Conditional Debiasing for Fair Text Embedding","summary":"  Mitigating biases in machine learning models has become an increasing concern\nin Natural Language Processing (NLP), particularly in developing fair text\nembeddings, which are crucial yet challenging for real-world applications like\nsearch engines. In response, this paper proposes a novel method for learning\nfair text embeddings. First, we define a novel content-conditional equal\ndistance (CCED) fairness for text embeddings, ensuring content-conditional\nindependence between sensitive attributes and text embeddings. Building on\nCCED, we introduce a content-conditional debiasing (CCD) loss to ensure that\nembeddings of texts with different sensitive attributes but identical content\nmaintain the same distance from the embedding of their corresponding neutral\ntext. Additionally, we tackle the issue of insufficient training data by using\nLarge Language Models (LLMs) with instructions to fairly augment texts into\ndifferent sensitive groups. Our extensive evaluations show that our approach\neffectively enhances fairness while maintaining the utility of embeddings.\nFurthermore, our augmented dataset, combined with the CCED metric, serves as an\nnew benchmark for evaluating fairness.\n","authors":["Wenlong Deng","Blair Chen","Beidi Zhao","Chiyu Zhang","Xiaoxiao Li","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2402.14208v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16315v1","updated":"2024-06-24T04:48:29Z","published":"2024-06-24T04:48:29Z","title":"Song Data Cleansing for End-to-End Neural Singer Diarization Using\n  Neural Analysis and Synthesis Framework","summary":"  We propose a data cleansing method that utilizes a neural analysis and\nsynthesis (NANSY++) framework to train an end-to-end neural diarization model\n(EEND) for singer diarization. Our proposed model converts song data with\nchoral singing which is commonly contained in popular music and unsuitable for\ngenerating a simulated dataset to the solo singing data. This cleansing is\nbased on NANSY++, which is a framework trained to reconstruct an input\nnon-overlapped audio signal. We exploit the pre-trained NANSY++ to convert\nchoral singing into clean, non-overlapped audio. This cleansing process\nmitigates the mislabeling of choral singing to solo singing and helps the\neffective training of EEND models even when the majority of available song data\ncontains choral singing sections. We experimentally evaluated the EEND model\ntrained with a dataset using our proposed method using annotated popular duet\nsongs. As a result, our proposed method improved 14.8 points in diarization\nerror rate.\n","authors":["Hokuto Munakata","Ryo Terashima","Yusuke Fujita"],"pdf_url":"https://arxiv.org/pdf/2406.16315v1.pdf","comment":"INTERSPEECH 2024 accepted"},{"id":"http://arxiv.org/abs/2406.16308v1","updated":"2024-06-24T04:17:03Z","published":"2024-06-24T04:17:03Z","title":"Anomaly Detection of Tabular Data Using LLMs","summary":"  Large language models (LLMs) have shown their potential in long-context\nunderstanding and mathematical reasoning. In this paper, we study the problem\nof using LLMs to detect tabular anomalies and show that pre-trained LLMs are\nzero-shot batch-level anomaly detectors. That is, without extra\ndistribution-specific model fitting, they can discover hidden outliers in a\nbatch of data, demonstrating their ability to identify low-density data\nregions. For LLMs that are not well aligned with anomaly detection and\nfrequently output factual errors, we apply simple yet effective data-generating\nprocesses to simulate synthetic batch-level anomaly detection datasets and\npropose an end-to-end fine-tuning strategy to bring out the potential of LLMs\nin detecting real anomalies. Experiments on a large anomaly detection benchmark\n(ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art\ntransductive learning-based anomaly detection methods and ii) the efficacy of\nour synthetic dataset and fine-tuning strategy in aligning LLMs to this task.\n","authors":["Aodong Li","Yunhan Zhao","Chen Qiu","Marius Kloft","Padhraic Smyth","Maja Rudolph","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2406.16308v1.pdf","comment":"accepted at the Anomaly Detection with Foundation Models workshop"},{"id":"http://arxiv.org/abs/2406.16306v1","updated":"2024-06-24T04:08:35Z","published":"2024-06-24T04:08:35Z","title":"Cascade Reward Sampling for Efficient Decoding-Time Alignment","summary":"  Aligning large language models (LLMs) with human preferences is critical for\ntheir deployment. Recently, decoding-time alignment has emerged as an effective\nplug-and-play technique that requires no fine-tuning of model parameters.\nHowever, generating text that achieves both high reward and high likelihood\nremains a significant challenge. Existing methods often fail to generate\nhigh-reward text or incur substantial computational costs. In this paper, we\npropose Cascade Reward Sampling (CARDS) to address both issues, guaranteeing\nthe generation of high-reward and high-likelihood text with significantly low\ncosts. Based on our analysis of reward models (RMs) on incomplete text and our\nobservation that high-reward prefixes induce high-reward complete text, we use\nrejection sampling to iteratively generate small semantic segments to form such\nprefixes. The segment length is dynamically determined by the predictive\nuncertainty of LLMs. This strategy guarantees desirable prefixes for subsequent\ngenerations and significantly reduces wasteful token re-generations and the\nnumber of reward model scoring. Our experiments demonstrate substantial gains\nin both generation efficiency and alignment ratings compared to the baselines,\nachieving five times faster text generation and 99\\% win-ties in GPT-4/Claude-3\nhelpfulness evaluation.\n","authors":["Bolian Li","Yifan Wang","Ananth Grama","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07045v2","updated":"2024-06-24T04:04:21Z","published":"2023-09-13T15:56:50Z","title":"SafetyBench: Evaluating the Safety of Large Language Models","summary":"  With the rapid development of Large Language Models (LLMs), increasing\nattention has been paid to their safety concerns. Consequently, evaluating the\nsafety of LLMs has become an essential task for facilitating the broad\napplications of LLMs. Nevertheless, the absence of comprehensive safety\nevaluation benchmarks poses a significant impediment to effectively assess and\nenhance the safety of LLMs. In this work, we present SafetyBench, a\ncomprehensive benchmark for evaluating the safety of LLMs, which comprises\n11,435 diverse multiple choice questions spanning across 7 distinct categories\nof safety concerns. Notably, SafetyBench also incorporates both Chinese and\nEnglish data, facilitating the evaluation in both languages. Our extensive\ntests over 25 popular Chinese and English LLMs in both zero-shot and few-shot\nsettings reveal a substantial performance advantage for GPT-4 over its\ncounterparts, and there is still significant room for improving the safety of\ncurrent LLMs. We also demonstrate that the measured safety understanding\nabilities in SafetyBench are correlated with safety generation abilities. Data\nand evaluation guidelines are available at\n\\url{https://github.com/thu-coai/SafetyBench}{https://github.com/thu-coai/SafetyBench}.\nSubmission entrance and leaderboard are available at\n\\url{https://llmbench.ai/safety}{https://llmbench.ai/safety}.\n","authors":["Zhexin Zhang","Leqi Lei","Lindong Wu","Rui Sun","Yongkang Huang","Chong Long","Xiao Liu","Xuanyu Lei","Jie Tang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2309.07045v2.pdf","comment":"ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2406.11217v2","updated":"2024-06-24T03:55:30Z","published":"2024-06-17T05:23:18Z","title":"WeatherQA: Can Multimodal Language Models Reason about Severe Weather?","summary":"  Severe convective weather events, such as hail, tornadoes, and thunderstorms,\noften occur quickly yet cause significant damage, costing billions of dollars\nevery year. This highlights the importance of forecasting severe weather\nthreats hours in advance to better prepare meteorologists and residents in\nat-risk areas. Can modern large foundation models perform such forecasting?\nExisting weather benchmarks typically focus only on predicting time-series\nchanges in certain weather parameters (e.g., temperature, moisture) with\ntext-only features. In this work, we introduce WeatherQA, the first multimodal\ndataset designed for machines to reason about complex combinations of weather\nparameters (a.k.a., ingredients) and predict severe weather in real-world\nscenarios. The dataset includes over 8,000 (multi-images, text) pairs for\ndiverse severe weather events. Each pair contains rich information crucial for\nforecasting -- the images describe the ingredients capturing environmental\ninstability, surface observations, and radar reflectivity, and the text\ncontains forecast analyses written by human experts. With WeatherQA, we\nevaluate state-of-the-art vision language models, including GPT4, Claude3.5,\nGemini-1.5, and a fine-tuned Llama3-based VLM, by designing two challenging\ntasks: (1) multi-choice QA for predicting affected area and (2) classification\nof the development potential of severe convection. These tasks require deep\nunderstanding of domain knowledge (e.g., atmospheric dynamics) and complex\nreasoning over multimodal data (e.g., interactions between weather parameters).\nWe show a substantial gap between the strongest VLM, GPT4o, and human\nreasoning. Our comprehensive case study with meteorologists further reveals the\nweaknesses of the models, suggesting that better training and data integration\nare necessary to bridge this gap. WeatherQA link:\nhttps://github.com/chengqianma/WeatherQA.\n","authors":["Chengqian Ma","Zhanxiang Hua","Alexandra Anderson-Frey","Vikram Iyer","Xin Liu","Lianhui Qin"],"pdf_url":"https://arxiv.org/pdf/2406.11217v2.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.16299v1","updated":"2024-06-24T03:52:52Z","published":"2024-06-24T03:52:52Z","title":"Compensate Quantization Errors: Make Weights Hierarchical to Compensate\n  Each Other","summary":"  Emergent Large Language Models (LLMs) use their extraordinary performance and\npowerful deduction capacity to discern from traditional language models.\nHowever, the expenses of computational resources and storage for these LLMs are\nstunning, quantization then arises as a trending conversation. To address\naccuracy decay caused by quantization, two streams of works in post-training\nquantization methods stand out. One uses other weights to compensate existing\nquantization error, while the other transfers the quantization difficulty to\nother parts in the model. Combining both merits, we introduce Learnable\nSingular value Increment (LSI) as an advanced solution. LSI uses Singular Value\nDecomposition to extract singular values of the weights and make them learnable\nto help weights compensate each other conditioned on activation. Incorporating\nLSI with existing techniques, we achieve state-of-the-art performance in\ndiverse quantization settings, no matter in weight-only, weight-activation or\nextremely low bit scenarios. By unleashing the potential of LSI, efficient\nfinetuning on quantized model is no longer a prohibitive problem.\n","authors":["Yifei Gao","Jie Ou","Lei Wang","Yuting Xiao","Zhiyuan Xiang","Ruiting Dai","Jun Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.16299v1.pdf","comment":"Efficient quantization method"},{"id":"http://arxiv.org/abs/2406.16294v1","updated":"2024-06-24T03:36:29Z","published":"2024-06-24T03:36:29Z","title":"LangSuitE: Planning, Controlling and Interacting with Large Language\n  Models in Embodied Text Environments","summary":"  Recent advances in Large Language Models (LLMs) have shown inspiring\nachievements in constructing autonomous agents that rely on language\ndescriptions as inputs. However, it remains unclear how well LLMs can function\nas few-shot or zero-shot embodied agents in dynamic interactive environments.\nTo address this gap, we introduce LangSuitE, a versatile and simulation-free\ntestbed featuring 6 representative embodied tasks in textual embodied worlds.\nCompared with previous LLM-based testbeds, LangSuitE (i) offers adaptability to\ndiverse environments without multiple simulation engines, (ii) evaluates\nagents' capacity to develop ``internalized world knowledge'' with embodied\nobservations, and (iii) allows easy customization of communication and action\nstrategies. To address the embodiment challenge, we devise a novel\nchain-of-thought (CoT) schema, EmMem, which summarizes embodied states w.r.t.\nhistory information. Comprehensive benchmark results illustrate challenges and\ninsights of embodied planning. LangSuitE represents a significant step toward\nbuilding embodied generalists in the context of language models.\n","authors":["Zixia Jia","Mengmeng Wang","Baichen Tong","Song-Chun Zhu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.16294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16293v1","updated":"2024-06-24T03:36:19Z","published":"2024-06-24T03:36:19Z","title":"Combining Supervised Learning and Reinforcement Learning for Multi-Label\n  Classification Tasks with Partial Labels","summary":"  Traditional supervised learning heavily relies on human-annotated datasets,\nespecially in data-hungry neural approaches. However, various tasks, especially\nmulti-label tasks like document-level relation extraction, pose challenges in\nfully manual annotation due to the specific domain knowledge and large class\nsets. Therefore, we address the multi-label positive-unlabelled learning\n(MLPUL) problem, where only a subset of positive classes is annotated. We\npropose Mixture Learner for Partially Annotated Classification (MLPAC), an\nRL-based framework combining the exploration ability of reinforcement learning\nand the exploitation ability of supervised learning. Experimental results\nacross various tasks, including document-level relation extraction, multi-label\nimage classification, and binary PU learning, demonstrate the generalization\nand effectiveness of our framework.\n","authors":["Zixia Jia","Junpeng Li","Shichuan Zhang","Anji Liu","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.16293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16288v1","updated":"2024-06-24T03:29:53Z","published":"2024-06-24T03:29:53Z","title":"PlagBench: Exploring the Duality of Large Language Models in Plagiarism\n  Generation and Detection","summary":"  Recent literature has highlighted potential risks to academic integrity\nassociated with large language models (LLMs), as they can memorize parts of\ntraining instances and reproduce them in the generated texts without proper\nattribution. In addition, given their capabilities in generating high-quality\ntexts, plagiarists can exploit LLMs to generate realistic paraphrases or\nsummaries indistinguishable from original work. In response to possible\nmalicious use of LLMs in plagiarism, we introduce PlagBench, a comprehensive\ndataset consisting of 46.5K synthetic plagiarism cases generated using three\ninstruction-tuned LLMs across three writing domains. The quality of PlagBench\nis ensured through fine-grained automatic evaluation for each type of\nplagiarism, complemented by human annotation. We then leverage our proposed\ndataset to evaluate the plagiarism detection performance of five modern LLMs\nand three specialized plagiarism checkers. Our findings reveal that GPT-3.5\ntends to generates paraphrases and summaries of higher quality compared to\nLlama2 and GPT-4. Despite LLMs' weak performance in summary plagiarism\nidentification, they can surpass current commercial plagiarism detectors.\nOverall, our results highlight the potential of LLMs to serve as robust\nplagiarism detection tools.\n","authors":["Jooyoung Lee","Toshini Agrawal","Adaku Uchendu","Thai Le","Jinghui Chen","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2406.16288v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.16275v1","updated":"2024-06-24T02:50:09Z","published":"2024-06-24T02:50:09Z","title":"Investigating the Influence of Prompt-Specific Shortcuts in AI Generated\n  Text Detection","summary":"  AI Generated Text (AIGT) detectors are developed with texts from humans and\nLLMs of common tasks. Despite the diversity of plausible prompt choices, these\ndatasets are generally constructed with a limited number of prompts. The lack\nof prompt variation can introduce prompt-specific shortcut features that exist\nin data collected with the chosen prompt, but do not generalize to others. In\nthis paper, we analyze the impact of such shortcuts in AIGT detection. We\npropose Feedback-based Adversarial Instruction List Optimization (FAILOpt), an\nattack that searches for instructions deceptive to AIGT detectors exploiting\nprompt-specific shortcuts. FAILOpt effectively drops the detection performance\nof the target detector, comparable to other attacks based on adversarial\nin-context examples. We also utilize our method to enhance the robustness of\nthe detector by mitigating the shortcuts. Based on the findings, we further\ntrain the classifier with the dataset augmented by FAILOpt prompt. The\naugmented classifier exhibits improvements across generation models, tasks, and\nattacks. Our code will be available at https://github.com/zxcvvxcz/FAILOpt.\n","authors":["Choonghyun Park","Hyuhng Joon Kim","Junyeob Kim","Youna Kim","Taeuk Kim","Hyunsoo Cho","Hwiyeol Jo","Sang-goo Lee","Kang Min Yoo"],"pdf_url":"https://arxiv.org/pdf/2406.16275v1.pdf","comment":"19 pages, 3 figures, 13 tables, under review"},{"id":"http://arxiv.org/abs/2405.05955v3","updated":"2024-06-24T02:44:57Z","published":"2024-05-09T17:49:04Z","title":"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency\n  for Tool Planning","summary":"  The emergence of large language models (LLMs) has opened up unprecedented\npossibilities for automating complex tasks that are often comparable to human\nperformance. Despite their capabilities, LLMs still encounter difficulties in\ncompleting tasks that require high levels of accuracy and complexity due to\ntheir inherent limitations in handling multifaceted problems single-handedly.\nThis paper introduces `Smurfs', a cutting-edge multi-agent framework designed\nto revolutionize the application of LLMs. By seamlessly transforming a\nconventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance\nthe model's ability to solve complex tasks at no additional cost. This is\nachieved through innovative prompting strategies that allocate distinct roles\nwithin the model, thereby facilitating collaboration among specialized agents\nand forming an intelligent multi-agent system. Our empirical investigation on\nboth open-ended task of StableToolBench and closed-ended task on HotpotQA\nshowcases Smurfs' superior capability in intricate tool utilization scenarios.\nNotably, Smurfs outmatches all the baseline methods in both experiments,\nsetting new state-of-the-art performance. Furthermore, through comprehensive\nablation studies, we dissect the contribution of the core components of the\nmulti-agent framework to its overall efficacy. This not only verifies the\neffectiveness of the framework, but also sets a route for future exploration of\nmulti-agent LLM systems.\n","authors":["Junzhi Chen","Juhao Liang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2405.05955v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01054v3","updated":"2024-06-24T02:31:06Z","published":"2024-04-01T11:26:50Z","title":"Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language\n  Model Alignment","summary":"  Best-of-N (BoN) sampling with a reward model has been shown to be an\neffective strategy for aligning Large Language Models (LLMs) to human\npreferences at the time of decoding. BoN sampling is susceptible to a problem\nknown as reward hacking. Because the reward model is an imperfect proxy for the\ntrue objective, over-optimizing its value can compromise its performance on the\ntrue objective. A common solution to prevent reward hacking in preference\nlearning techniques is to optimize a reward using proximity regularization\n(e.g., KL regularization), which ensures that the language model remains close\nto the reference model. In this research, we propose Regularized Best-of-N\n(RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating\na proximity term in response selection, similar to preference learning\ntechniques. We evaluate RBoN on the AlpacaFarm and Anthropic's hh-rlhf datasets\nand find that it outperforms BoN. As an application of RBoN, we use RBoN to\ngenerate a pairwise preference learning dataset. Experimental results show that\na DPO model trained on a dataset generated with RBoN outperforms a DPO model\ngenerated with vanilla BoN. Our code is available at\nhttps://github.com/CyberAgentAILab/regularized-bon\n","authors":["Yuu Jinnai","Tetsuro Morimura","Kaito Ariu","Kenshi Abe"],"pdf_url":"https://arxiv.org/pdf/2404.01054v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07269v3","updated":"2024-06-24T02:17:57Z","published":"2023-08-14T16:52:42Z","title":"EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models","summary":"  Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners from applying knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub,\nalong with Google Colab tutorials and comprehensive documentation for beginners\nto get started. Besides, we present an online system for real-time knowledge\nediting, and a demo video.\n","authors":["Peng Wang","Ningyu Zhang","Bozhong Tian","Zekun Xi","Yunzhi Yao","Ziwen Xu","Mengru Wang","Shengyu Mao","Xiaohan Wang","Siyuan Cheng","Kangwei Liu","Yuansheng Ni","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2308.07269v3.pdf","comment":"ACL 2024 System Demonstrations; Code:\n  https://github.com/zjunlp/EasyEdit HF Demo:\n  https://huggingface.co/spaces/zjunlp/EasyEdit Video:\n  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit"},{"id":"http://arxiv.org/abs/2311.09538v3","updated":"2024-06-24T02:11:44Z","published":"2023-11-16T03:28:43Z","title":"Reducing Privacy Risks in Online Self-Disclosures with Language Models","summary":"  Self-disclosure, while being common and rewarding in social media\ninteraction, also poses privacy risks. In this paper, we take the initiative to\nprotect the user-side privacy associated with online self-disclosure through\ndetection and abstraction. We develop a taxonomy of 19 self-disclosure\ncategories and curate a large corpus consisting of 4.8K annotated disclosure\nspans. We then fine-tune a language model for detection, achieving over 65%\npartial span F$_1$. We further conduct an HCI user study, with 82% of\nparticipants viewing the model positively, highlighting its real-world\napplicability. Motivated by the user feedback, we introduce the task of\nself-disclosure abstraction, which is rephrasing disclosures into less specific\nterms while preserving their utility, e.g., \"Im 16F\" to \"I'm a teenage girl\".\nWe explore various fine-tuning strategies, and our best model can generate\ndiverse abstractions that moderately reduce privacy risks while maintaining\nhigh utility according to human evaluation. To help users in deciding which\ndisclosures to abstract, we present a task of rating their importance for\ncontext understanding. Our fine-tuned model achieves 80% accuracy, on-par with\nGPT-3.5. Given safety and privacy considerations, we will only release our\ncorpus and models to researcher who agree to the ethical guidelines outlined in\nEthics Statement.\n","authors":["Yao Dou","Isadora Krsek","Tarek Naous","Anubha Kabra","Sauvik Das","Alan Ritter","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2311.09538v3.pdf","comment":"Accepted at ACL 2024"},{"id":"http://arxiv.org/abs/2402.03049v4","updated":"2024-06-24T02:10:23Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v4.pdf","comment":"ACL 2024 System Demonstrations; Project website:\n  https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2406.16264v1","updated":"2024-06-24T02:03:57Z","published":"2024-06-24T02:03:57Z","title":"One Thousand and One Pairs: A \"novel\" challenge for long-context\n  language models","summary":"  Synthetic long-context LLM benchmarks (e.g., \"needle-in-the-haystack\") test\nonly surface-level retrieval capabilities, but how well can long-context LLMs\nretrieve, synthesize, and reason over information across book-length inputs? We\naddress this question by creating NoCha, a dataset of 1,001 minimally different\npairs of true and false claims about 67 recently-published English fictional\nbooks, written by human readers of those books. In contrast to existing\nlong-context benchmarks, our annotators confirm that the largest share of pairs\nin NoCha require global reasoning over the entire book to verify. Our\nexperiments show that while human readers easily perform this task, it is\nenormously challenging for all ten long-context LLMs that we evaluate: no\nopen-weight model performs above random chance (despite their strong\nperformance on synthetic benchmarks), while GPT-4o achieves the highest\naccuracy at 55.8%. Further analysis reveals that (1) on average, models perform\nmuch better on pairs that require only sentence-level retrieval vs. global\nreasoning; (2) model-generated explanations for their decisions are often\ninaccurate even for correctly-labeled claims; and (3) models perform\nsubstantially worse on speculative fiction books that contain extensive\nworld-building. The methodology proposed in NoCha allows for the evolution of\nthe benchmark dataset and the easy analysis of future models.\n","authors":["Marzena Karpinska","Katherine Thai","Kyle Lo","Tanya Goyal","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2406.16264v1.pdf","comment":"preprint, 29 pages"},{"id":"http://arxiv.org/abs/2406.16254v1","updated":"2024-06-24T01:31:03Z","published":"2024-06-24T01:31:03Z","title":"Confidence Regulation Neurons in Language Models","summary":"  Despite their widespread use, the mechanisms by which large language models\n(LLMs) represent and regulate uncertainty in next-token predictions remain\nlargely unexplored. This study investigates two critical components believed to\ninfluence this uncertainty: the recently discovered entropy neurons and a new\nset of components that we term token frequency neurons. Entropy neurons are\ncharacterized by an unusually high weight norm and influence the final layer\nnormalization (LayerNorm) scale to effectively scale down the logits. Our work\nshows that entropy neurons operate by writing onto an unembedding null space,\nallowing them to impact the residual stream norm with minimal direct effect on\nthe logits themselves. We observe the presence of entropy neurons across a\nrange of models, up to 7 billion parameters. On the other hand, token frequency\nneurons, which we discover and describe here for the first time, boost or\nsuppress each token's logit proportionally to its log frequency, thereby\nshifting the output distribution towards or away from the unigram distribution.\nFinally, we present a detailed case study where entropy neurons actively manage\nconfidence in the setting of induction, i.e. detecting and continuing repeated\nsubsequences.\n","authors":["Alessandro Stolfo","Ben Wu","Wes Gurnee","Yonatan Belinkov","Xingyi Song","Mrinmaya Sachan","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2406.16254v1.pdf","comment":"25 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.16253v1","updated":"2024-06-24T01:30:22Z","published":"2024-06-24T01:30:22Z","title":"LLMs assist NLP Researchers: Critique Paper (Meta-)Reviewing","summary":"  This work is motivated by two key trends. On one hand, large language models\n(LLMs) have shown remarkable versatility in various generative tasks such as\nwriting, drawing, and question answering, significantly reducing the time\nrequired for many routine tasks. On the other hand, researchers, whose work is\nnot only time-consuming but also highly expertise-demanding, face increasing\nchallenges as they have to spend more time reading, writing, and reviewing\npapers. This raises the question: how can LLMs potentially assist researchers\nin alleviating their heavy workload?\n  This study focuses on the topic of LLMs assist NLP Researchers, particularly\nexamining the effectiveness of LLM in assisting paper (meta-)reviewing and its\nrecognizability. To address this, we constructed the ReviewCritique dataset,\nwhich includes two types of information: (i) NLP papers (initial submissions\nrather than camera-ready) with both human-written and LLM-generated reviews,\nand (ii) each review comes with \"deficiency\" labels and corresponding\nexplanations for individual segments, annotated by experts. Using\nReviewCritique, this study explores two threads of research questions: (i)\n\"LLMs as Reviewers\", how do reviews generated by LLMs compare with those\nwritten by humans in terms of quality and distinguishability? (ii) \"LLMs as\nMetareviewers\", how effectively can LLMs identify potential issues, such as\nDeficient or unprofessional review segments, within individual paper reviews?\nTo our knowledge, this is the first work to provide such a comprehensive\nanalysis.\n","authors":["Jiangshu Du","Yibo Wang","Wenting Zhao","Zhongfen Deng","Shuaiqi Liu","Renze Lou","Henry Peng Zou","Pranav Narayanan Venkit","Nan Zhang","Mukund Srinath","Haoran Ranran Zhang","Vipul Gupta","Yinghui Li","Tao Li","Fei Wang","Qin Liu","Tianlin Liu","Pengzhi Gao","Congying Xia","Chen Xing","Jiayang Cheng","Zhaowei Wang","Ying Su","Raj Sanjay Shah","Ruohao Guo","Jing Gu","Haoran Li","Kangda Wei","Zihao Wang","Lu Cheng","Surangika Ranathunga","Meng Fang","Jie Fu","Fei Liu","Ruihong Huang","Eduardo Blanco","Yixin Cao","Rui Zhang","Philip S. Yu","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2406.16253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09724v2","updated":"2024-06-24T01:08:24Z","published":"2024-03-12T17:07:53Z","title":"ClaimVer: Explainable Claim-Level Verification and Evidence Attribution\n  of Text Through Knowledge Graphs","summary":"  In the midst of widespread misinformation and disinformation through social\nmedia and the proliferation of AI-generated texts, it has become increasingly\ndifficult for people to validate and trust information they encounter. Many\nfact-checking approaches and tools have been developed, but they often lack\nappropriate explainability or granularity to be useful in various contexts. A\ntext validation method that is easy to use, accessible, and can perform\nfine-grained evidence attribution has become crucial. More importantly,\nbuilding user trust in such a method requires presenting the rationale behind\neach prediction, as research shows this significantly influences people's\nbelief in automated systems. Localizing and bringing users' attention to the\nspecific problematic content is also paramount, instead of providing simple\nblanket labels. In this paper, we present ClaimVer, a human-centric framework\ntailored to meet users' informational and verification needs by generating rich\nannotations and thereby reducing cognitive load. Designed to deliver\ncomprehensive evaluations of texts, it highlights each claim, verifies it\nagainst a trusted knowledge graph (KG), presents the evidence, and provides\nsuccinct, clear explanations for each claim prediction. Finally, our framework\nintroduces an attribution score, enhancing applicability across a wide range of\ndownstream tasks.\n","authors":["Preetam Prabhu Srikar Dammu","Himanshu Naidu","Mouly Dewan","YoungMin Kim","Tanya Roosta","Aman Chadha","Chirag Shah"],"pdf_url":"https://arxiv.org/pdf/2403.09724v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09801v2","updated":"2024-06-24T00:50:58Z","published":"2024-02-15T08:58:03Z","title":"EFUF: Efficient Fine-grained Unlearning Framework for Mitigating\n  Hallucinations in Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) have attracted increasing attention\nin the past few years, but they may still generate descriptions that include\nobjects not present in the corresponding images, a phenomenon known as object\nhallucination. To eliminate hallucinations, existing methods manually annotate\npaired responses with and without hallucinations, and then employ various\nalignment algorithms to improve the alignment capability between images and\ntext. However, they not only demand considerable computation resources during\nthe finetuning stage but also require expensive human annotation to construct\npaired data needed by the alignment algorithms. To address these issues, we\nborrow the idea of unlearning and propose an efficient fine-grained unlearning\nframework (EFUF), which can eliminate hallucinations without the need for\npaired data. Extensive experiments show that our method consistently reduces\nhallucinations while preserving the generation quality with modest\ncomputational overhead. Our code and datasets will be publicly available.\n","authors":["Shangyu Xing","Fei Zhao","Zhen Wu","Tuo An","Weihao Chen","Chunhui Li","Jianbing Zhang","Xinyu Dai"],"pdf_url":"https://arxiv.org/pdf/2402.09801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17186v1","updated":"2024-06-24T23:57:57Z","published":"2024-06-24T23:57:57Z","title":"CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented\n  Analysis Generation","summary":"  Legal professionals need to write analyses that rely on citations to relevant\nprecedents, i.e., previous case decisions. Intelligent systems assisting legal\nprofessionals in writing such documents provide great benefits but are\nchallenging to design. Such systems need to help locate, summarize, and reason\nover salient precedents in order to be useful. To enable systems for such\ntasks, we work with legal professionals to transform a large open-source legal\ncorpus into a dataset supporting two important backbone tasks: information\nretrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC\n(Case Law Evaluation Retrieval Corpus), is constructed for training and\nevaluating models on their ability to (1) find corresponding citations for a\ngiven piece of legal analysis and to (2) compile the text of these citations\n(as well as previous context) into a cogent analysis that supports a reasoning\ngoal. We benchmark state-of-the-art models on CLERC, showing that current\napproaches still struggle: GPT-4o generates analyses with the highest ROUGE\nF-scores but hallucinates the most, while zero-shot IR models only achieve\n48.3% recall@1000.\n","authors":["Abe Bohan Hou","Orion Weller","Guanghui Qin","Eugene Yang","Dawn Lawrie","Nils Holzenberger","Andrew Blair-Stanek","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2406.17186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10289v2","updated":"2024-06-24T23:53:05Z","published":"2024-06-12T21:23:48Z","title":"VeraCT Scan: Retrieval-Augmented Fake News Detection with Justifiable\n  Reasoning","summary":"  The proliferation of fake news poses a significant threat not only by\ndisseminating misleading information but also by undermining the very\nfoundations of democracy. The recent advance of generative artificial\nintelligence has further exacerbated the challenge of distinguishing genuine\nnews from fabricated stories. In response to this challenge, we introduce\nVeraCT Scan, a novel retrieval-augmented system for fake news detection. This\nsystem operates by extracting the core facts from a given piece of news and\nsubsequently conducting an internet-wide search to identify corroborating or\nconflicting reports. Then sources' credibility is leveraged for information\nverification. Besides determining the veracity of news, we also provide\ntransparent evidence and reasoning to support its conclusions, resulting in the\ninterpretability and trust in the results. In addition to GPT-4 Turbo, Llama-2\n13B is also fine-tuned for news content understanding, information\nverification, and reasoning. Both implementations have demonstrated\nstate-of-the-art accuracy in the realm of fake news detection.\n","authors":["Cheng Niu","Yang Guan","Yuanhao Wu","Juno Zhu","Juntong Song","Randy Zhong","Kaihua Zhu","Siliang Xu","Shizhe Diao","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.10289v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17185v1","updated":"2024-06-24T23:47:20Z","published":"2024-06-24T23:47:20Z","title":"Vaporetto: Efficient Japanese Tokenization Based on Improved Pointwise\n  Linear Classification","summary":"  This paper proposes an approach to improve the runtime efficiency of Japanese\ntokenization based on the pointwise linear classification (PLC) framework,\nwhich formulates the whole tokenization process as a sequence of linear\nclassification problems. Our approach optimizes tokenization by leveraging the\ncharacteristics of the PLC framework and the task definition. Our approach\ninvolves (1) composing multiple classifications into array-based operations,\n(2) efficient feature lookup with memory-optimized automata, and (3) three\northogonal pre-processing methods for reducing actual score calculation. Thus,\nour approach makes the tokenization speed 5.7 times faster than the current\napproach based on the same model without decreasing tokenization accuracy. Our\nimplementation is available at https://github.com/daac-tools/vaporetto under\nthe MIT or Apache-2.0 license.\n","authors":["Koichi Akabe","Shunsuke Kanda","Yusuke Oda","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2406.17185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19651v2","updated":"2024-06-24T23:41:29Z","published":"2024-03-28T17:59:20Z","title":"MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions","summary":"  Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent works leverage text\ninstructions to allow users to more freely express their search intents.\nHowever, they primarily focus on image pairs that are visually similar and/or\ncan be characterized by a small set of pre-defined relations. The core thesis\nof this paper is that text instructions can enable retrieving images with\nricher relations beyond visual similarity. To show this, we introduce\nMagicLens, a series of self-supervised image retrieval models that support\nopen-ended instructions. MagicLens is built on a key novel insight: image pairs\nthat naturally occur on the same web pages contain a wide range of implicit\nrelations (e.g., inside view of), and we can bring those implicit relations\nexplicit by synthesizing instructions via foundation models. Trained on 36.7M\n(query image, instruction, target image) triplets with rich semantic relations\nmined from the web, MagicLens achieves results comparable with or better than\nprior best on eight benchmarks of various image retrieval tasks, while\nmaintaining high parameter efficiency with a significantly smaller model size.\nAdditional human analyses on a 1.4M-image unseen corpus further demonstrate the\ndiversity of search intents supported by MagicLens. Code and models are\npublicly available at https://open-vision-language.github.io/MagicLens/.\n","authors":["Kai Zhang","Yi Luan","Hexiang Hu","Kenton Lee","Siyuan Qiao","Wenhu Chen","Yu Su","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2403.19651v2.pdf","comment":"ICML 2024 (Oral); Project Website:\n  https://open-vision-language.github.io/MagicLens/"},{"id":"http://arxiv.org/abs/2406.06369v3","updated":"2024-06-24T23:23:22Z","published":"2024-06-10T15:30:13Z","title":"Annotation alignment: Comparing LLM and human annotations of\n  conversational safety","summary":"  To what extent do LLMs align with human perceptions of safety? We study this\nquestion via *annotation alignment*, the extent to which LLMs and humans agree\nwhen annotating the safety of user-chatbot conversations. We leverage the\nrecent DICES dataset (Aroyo et al., 2023), in which 350 conversations are each\nrated for safety by 112 annotators spanning 10 race-gender groups. GPT-4\nachieves a Pearson correlation of $r = 0.59$ with the average annotator rating,\nhigher than the median annotator's correlation with the average ($r=0.51$). We\nshow that larger datasets are needed to resolve whether GPT-4 exhibits\ndisparities in how well it correlates with demographic groups. Also, there is\nsubstantial idiosyncratic variation in correlation *within* groups, suggesting\nthat race & gender do not fully capture differences in alignment. Finally, we\nfind that GPT-4 cannot predict when one demographic group finds a conversation\nmore unsafe than another.\n","authors":["Rajiv Movva","Pang Wei Koh","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2406.06369v3.pdf","comment":"Working draft, short paper. Main text is 5 pages, 1 figure. (v3\n  corrects minor typo)"},{"id":"http://arxiv.org/abs/2405.05506v2","updated":"2024-06-24T23:17:52Z","published":"2024-05-09T02:33:14Z","title":"Cross-Care: Assessing the Healthcare Implications of Pre-training Data\n  on Language Model Bias","summary":"  Large language models (LLMs) are increasingly essential in processing natural\nlanguages, yet their application is frequently compromised by biases and\ninaccuracies originating in their training data. In this study, we introduce\nCross-Care, the first benchmark framework dedicated to assessing biases and\nreal world knowledge in LLMs, specifically focusing on the representation of\ndisease prevalence across diverse demographic groups. We systematically\nevaluate how demographic biases embedded in pre-training corpora like $ThePile$\ninfluence the outputs of LLMs. We expose and quantify discrepancies by\njuxtaposing these biases against actual disease prevalences in various U.S.\ndemographic groups. Our results highlight substantial misalignment between LLM\nrepresentation of disease prevalence and real disease prevalence rates across\ndemographic subgroups, indicating a pronounced risk of bias propagation and a\nlack of real-world grounding for medical applications of LLMs. Furthermore, we\nobserve that various alignment methods minimally resolve inconsistencies in the\nmodels' representation of disease prevalence across different languages. For\nfurther exploration and analysis, we make all data and a data visualization\ntool available at: www.crosscare.net.\n","authors":["Shan Chen","Jack Gallifant","Mingye Gao","Pedro Moreira","Nikolaj Munch","Ajay Muthukkumar","Arvind Rajan","Jaya Kolluri","Amelia Fiske","Janna Hastings","Hugo Aerts","Brian Anthony","Leo Anthony Celi","William G. La Cava","Danielle S. Bitterman"],"pdf_url":"https://arxiv.org/pdf/2405.05506v2.pdf","comment":"Submitted for review, data visualization tool available at:\n  www.crosscare.net"},{"id":"http://arxiv.org/abs/2402.19350v4","updated":"2024-06-24T23:03:18Z","published":"2024-02-29T16:56:36Z","title":"Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process","summary":"  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to\nsimulate human reasoning and inference processes, achieving proficient\nperformance in multi-hop QA. However, a gap persists between PLMs' reasoning\nabilities and those of humans when tackling complex problems. Psychological\nstudies suggest a vital connection between explicit information in passages and\nhuman prior knowledge during reading. Nevertheless, current research has given\ninsufficient attention to linking input passages and PLMs' pre-training-based\nknowledge from the perspective of human cognition studies. In this study, we\nintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, which\nuses prompts to connect explicit and implicit knowledge, aligning with human\nreading process for multi-hop QA. We consider the input passages as explicit\nknowledge, employing them to elicit implicit knowledge through unified prompt\nreasoning. Furthermore, our model incorporates type-specific reasoning via\nprompts, a form of implicit knowledge. Experimental results show that PEI\nperforms comparably to the state-of-the-art on HotpotQA. Ablation studies\nconfirm the efficacy of our model in bridging and integrating explicit and\nimplicit knowledge.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Jiaxing Shen","Xia Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19350v4.pdf","comment":"This paper has been accepted at COLING 2024"},{"id":"http://arxiv.org/abs/2406.17169v1","updated":"2024-06-24T23:02:56Z","published":"2024-06-24T23:02:56Z","title":"Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability\n  of Large Language Models","summary":"  As Large Language Models (LLMs) continue to exhibit remarkable performance in\nnatural language understanding tasks, there is a crucial need to measure their\nability for human-like multi-step logical reasoning. Existing logical reasoning\nevaluation benchmarks often focus primarily on simplistic single-step or\nmulti-step reasoning with a limited set of inference rules. Furthermore, the\nlack of datasets for evaluating non-monotonic reasoning represents a crucial\ngap since it aligns more closely with human-like reasoning. To address these\nlimitations, we propose Multi-LogiEval, a comprehensive evaluation dataset\nencompassing multi-step logical reasoning with various inference rules and\ndepths. Multi-LogiEval covers three logic types--propositional, first-order,\nand non-monotonic--consisting of more than 30 inference rules and more than 60\nof their combinations with various depths. Leveraging this dataset, we conduct\nevaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,\nand Mistral, employing a zero-shot chain-of-thought. Experimental results show\nthat there is a significant drop in the performance of LLMs as the reasoning\nsteps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).\nWe further conduct a thorough investigation of reasoning chains generated by\nLLMs which reveals several important findings. We believe that Multi-LogiEval\nfacilitates future research for evaluating and enhancing the logical reasoning\nability of LLMs. Data is available at\nhttps://github.com/Mihir3009/Multi-LogiEval.\n","authors":["Nisarg Patel","Mohith Kulkarni","Mihir Parmar","Aashna Budhiraja","Mutsumi Nakamura","Neeraj Varshney","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2406.17169v1.pdf","comment":"23 Pages"},{"id":"http://arxiv.org/abs/2402.11532v2","updated":"2024-06-24T22:43:57Z","published":"2024-02-18T10:10:40Z","title":"Chain-of-Instructions: Compositional Instruction Tuning on Large\n  Language Models","summary":"  Fine-tuning large language models (LLMs) with a collection of large and\ndiverse instructions has improved the model's generalization to different\ntasks, even for unseen tasks. However, most existing instruction datasets\ninclude only single instructions, and they struggle to follow complex\ninstructions composed of multiple subtasks. In this work, we propose a novel\nconcept of compositional instructions called chain-of-instructions (CoI), where\nthe output of one instruction becomes an input for the next like a chain.\nUnlike the conventional practice of solving single instruction tasks, our\nproposed method encourages a model to solve each subtask step by step until the\nfinal answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions)\nimproves the model's ability to handle instructions composed of multiple\nsubtasks as well as unseen composite tasks such as multilingual summarization.\nOverall, our study find that simple CoI tuning of existing instruction data can\nprovide consistent generalization to solve more complex, unseen, and longer\nchains of instructions.\n","authors":["Shirley Anugrah Hayati","Taehee Jung","Tristan Bodding-Long","Sudipta Kar","Abhinav Sethy","Joo-Kyung Kim","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2402.11532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17163v1","updated":"2024-06-24T22:30:26Z","published":"2024-06-24T22:30:26Z","title":"Paraphrase and Aggregate with Large Language Models for Minimizing\n  Intent Classification Errors","summary":"  Large language models (LLM) have achieved remarkable success in natural\nlanguage generation but lesser focus has been given to their applicability in\ndecision making tasks such as classification. We show that LLMs like LLaMa can\nachieve high performance on large multi-class classification tasks but still\nmake classification errors and worse, generate out-of-vocabulary class labels.\nTo address these critical issues, we introduce Paraphrase and AGgregate\n(PAG)-LLM approach wherein an LLM generates multiple paraphrases of the input\nquery (parallel queries), performs multi-class classification for the original\nquery and each paraphrase, and at the end aggregate all the classification\nlabels based on their confidence scores. We evaluate PAG-LLM on two large\nmulti-class classication datasets: CLINC, and Banking and show 22.7% and 15.1%\nerror reduction. We show that PAG-LLM is especially effective for hard examples\nwhere LLM is uncertain, and reduces the critical misclassification and\nhallucinated label generation errors\n","authors":["Vikas Yadav","Zheng Tang","Vijay Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2406.17163v1.pdf","comment":"Accepted at SIGIR 2024"},{"id":"http://arxiv.org/abs/2406.17158v1","updated":"2024-06-24T22:09:50Z","published":"2024-06-24T22:09:50Z","title":"DEXTER: A Benchmark for open-domain Complex Question Answering using\n  LLMs","summary":"  Open-domain complex Question Answering (QA) is a difficult task with\nchallenges in evidence retrieval and reasoning. The complexity of such\nquestions could stem from questions being compositional, hybrid evidence, or\nambiguity in questions. While retrieval performance for classical QA tasks is\nwell explored, their capabilities for heterogeneous complex retrieval tasks,\nespecially in an open-domain setting, and the impact on downstream QA\nperformance, are relatively unexplored. To address this, in this work, we\npropose a benchmark composing diverse complex QA tasks and provide a toolkit to\nevaluate state-of-the-art pre-trained dense and sparse retrieval models in an\nopen-domain setting. We observe that late interaction models and surprisingly\nlexical models like BM25 perform well compared to other pre-trained dense\nretrieval models. In addition, since context-based reasoning is critical for\nsolving complex QA tasks, we also evaluate the reasoning capabilities of LLMs\nand the impact of retrieval performance on their reasoning capabilities.\nThrough experiments, we observe that much progress is to be made in retrieval\nfor complex QA to improve downstream QA performance. Our software and related\ndata can be accessed at https://github.com/VenkteshV/DEXTER\n","authors":["Venktesh V. Deepali Prabhu","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2406.17158v1.pdf","comment":"under submission, 22 pages"},{"id":"http://arxiv.org/abs/2308.16137v7","updated":"2024-06-24T21:22:00Z","published":"2023-08-30T16:47:51Z","title":"LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language\n  Models","summary":"  Today's large language models (LLMs) typically train on short text segments\n(e.g., <4K tokens) due to the quadratic complexity of their Transformer\narchitectures. As a result, their performance suffers drastically on inputs\nlonger than those encountered during training, substantially limiting their\napplications in real-world tasks involving long contexts such as encoding\nscientific articles, code repositories, or long dialogues. Through theoretical\nanalysis and empirical investigation, this work identifies three major factors\ncontributing to this length generalization failure. Our theoretical analysis\nfurther reveals that commonly used techniques like truncating the attention\nwindow or relative positional encodings are inadequate to address them.\nAnswering these challenges, we propose LM-Infinite, a simple and effective\nmethod for enhancing LLMs' capabilities of handling long contexts. LM-Infinite\nis highly flexible and can be used with most modern LLMs off-the-shelf. Without\nany parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments\nto generalize to up to 200M length inputs while retaining perplexity. It also\nimproves performance on downstream tasks such as Passkey Retrieval and Qasper\nin the zero-shot setting. LM-Infinite brings substantial efficiency\nimprovements: it achieves 2.7x decoding speed up and 7.5x memory saving over\nthe original model. Our codes are released at\n\\url{https://github.com/Glaciohound/LM-Infinite}.\n","authors":["Chi Han","Qifan Wang","Hao Peng","Wenhan Xiong","Yu Chen","Heng Ji","Sinong Wang"],"pdf_url":"https://arxiv.org/pdf/2308.16137v7.pdf","comment":"NAACL 2024 Outstanding paper, 9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2309.00789v2","updated":"2024-06-24T21:01:58Z","published":"2023-09-02T01:45:27Z","title":"LinkTransformer: A Unified Package for Record Linkage with Transformer\n  Language Models","summary":"  Linking information across sources is fundamental to a variety of analyses in\nsocial science, business, and government. While large language models (LLMs)\noffer enormous promise for improving record linkage in noisy datasets, in many\ndomains approximate string matching packages in popular softwares such as R and\nStata remain predominant. These packages have clean, simple interfaces and can\nbe easily extended to a diversity of languages. Our open-source package\nLinkTransformer aims to extend the familiarity and ease-of-use of popular\nstring matching methods to deep learning. It is a general purpose package for\nrecord linkage with transformer LLMs that treats record linkage as a text\nretrieval problem. At its core is an off-the-shelf toolkit for applying\ntransformer models to record linkage with four lines of code. LinkTransformer\ncontains a rich repository of pre-trained transformer semantic similarity\nmodels for multiple languages and supports easy integration of any transformer\nlanguage model from Hugging Face or OpenAI. It supports standard functionality\nsuch as blocking and linking on multiple noisy fields. LinkTransformer APIs\nalso perform other common text data processing tasks, e.g., aggregation, noisy\nde-duplication, and translation-free cross-lingual linkage. Importantly,\nLinkTransformer also contains comprehensive tools for efficient model tuning,\nto facilitate different levels of customization when off-the-shelf models do\nnot provide the required accuracy. Finally, to promote reusability,\nreproducibility, and extensibility, LinkTransformer makes it easy for users to\ncontribute their custom-trained models to its model hub. By combining\ntransformer language models with intuitive APIs that will be familiar to many\nusers of popular string matching packages, LinkTransformer aims to democratize\nthe benefits of LLMs among those who may be less familiar with deep learning\nframeworks.\n","authors":["Abhishek Arora","Melissa Dell"],"pdf_url":"https://arxiv.org/pdf/2309.00789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14452v2","updated":"2024-06-24T20:59:05Z","published":"2023-03-25T12:12:30Z","title":"COFFEE: A Contrastive Oracle-Free Framework for Event Extraction","summary":"  Event extraction is a complex information extraction task that involves\nextracting events from unstructured text. Prior classification-based methods\nrequire comprehensive entity annotations for joint training, while newer\ngeneration-based methods rely on heuristic templates containing oracle\ninformation such as event type, which is often unavailable in real-world\nscenarios. In this study, we consider a more realistic setting of this task,\nnamely the Oracle-Free Event Extraction (OFEE) task, where only the input\ncontext is given without any oracle information, including event type, event\nontology and trigger word. To solve this task, we propose a new framework,\ncalled COFFEE, which extracts the events solely based on the document context\nwithout referring to any oracle information. In particular, a contrastive\nselection model is introduced in COFFEE to rectify the generated triggers and\nhandle multi-event instances. The proposed COFFEE outperforms state-of-the-art\napproaches under the oracle-free setting of the event extraction task, as\nevaluated on a public event extraction benchmark ACE05.\n","authors":["Meiru Zhang","Yixuan Su","Zaiqiao Meng","Zihao Fu","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2303.14452v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17135v1","updated":"2024-06-24T20:54:32Z","published":"2024-06-24T20:54:32Z","title":"Testing network clustering algorithms with Natural Language Processing","summary":"  The advent of online social networks has led to the development of an\nabundant literature on the study of online social groups and their relationship\nto individuals' personalities as revealed by their textual productions. Social\nstructures are inferred from a wide range of social interactions. Those\ninteractions form complex -- sometimes multi-layered -- networks, on which\ncommunity detection algorithms are applied to extract higher order structures.\nThe choice of the community detection algorithm is however hardily questioned\nin relation with the cultural production of the individual they classify. In\nthis work, we assume the entangled nature of social networks and their cultural\nproduction to propose a definition of cultural based online social groups as\nsets of individuals whose online production can be categorized as social\ngroup-related. We take advantage of this apparently self-referential\ndescription of online social groups with a hybrid methodology that combines a\ncommunity detection algorithm and a natural language processing classification\nalgorithm. A key result of this analysis is the possibility to score community\ndetection algorithms using their agreement with the natural language processing\nclassification. A second result is that we can assign the opinion of a random\nuser at >85% accuracy.\n","authors":["Ixandra Achitouv","David Chavalarias","Bruno Gaume"],"pdf_url":"https://arxiv.org/pdf/2406.17135v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.18239v4","updated":"2024-06-24T20:24:53Z","published":"2024-04-28T16:31:32Z","title":"SOUL: Unlocking the Power of Second-Order Optimization for LLM\n  Unlearning","summary":"  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n","authors":["Jinghan Jia","Yihua Zhang","Yimeng Zhang","Jiancheng Liu","Bharat Runwal","James Diffenderfer","Bhavya Kailkhura","Sijia Liu"],"pdf_url":"https://arxiv.org/pdf/2404.18239v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.04359v2","updated":"2024-06-24T20:01:19Z","published":"2022-10-09T22:02:58Z","title":"Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years\n  of German Parliamentary Debates","summary":"  Solidarity is a crucial concept to understand social relations in societies.\nIn this paper, we explore fine-grained solidarity frames to study solidarity\ntowards women and migrants in German parliamentary debates between 1867 and\n2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k\nEuro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and\nGPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation\nquality. Using GPT-4, we automatically annotate more than 18k further instances\n(with a cost of around 500 Euro) across 155 years and find that solidarity with\nmigrants outweighs anti-solidarity but that frequencies and solidarity types\nshift over time. Most importantly, group-based notions of (anti-)solidarity\nfade in favor of compassionate solidarity, focusing on the vulnerability of\nmigrant groups, and exchange-based anti-solidarity, focusing on the lack of\n(economic) contribution. Our study highlights the interplay of historical\nevents, socio-economic needs, and political ideologies in shaping migration\ndiscourse and social cohesion. We also show that powerful LLMs, if carefully\nprompted, can be cost-effective alternatives to human annotation for hard\nsocial scientific tasks.\n","authors":["Aida Kostikova","Benjamin Paassen","Dominik Beese","Ole Pütz","Gregor Wiedemann","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2210.04359v2.pdf","comment":"Note title and author changes"},{"id":"http://arxiv.org/abs/2406.17104v1","updated":"2024-06-24T19:45:12Z","published":"2024-06-24T19:45:12Z","title":"Automated Adversarial Discovery for Safety Classifiers","summary":"  Safety classifiers are critical in mitigating toxicity on online forums such\nas social media and in chatbots. Still, they continue to be vulnerable to\nemergent, and often innumerable, adversarial attacks. Traditional automated\nadversarial data generation methods, however, tend to produce attacks that are\nnot diverse, but variations of previously observed harm types. We formalize the\ntask of automated adversarial discovery for safety classifiers - to find new\nattacks along previously unseen harm dimensions that expose new weaknesses in\nthe classifier. We measure progress on this task along two key axes (1)\nadversarial success: does the attack fool the classifier? and (2) dimensional\ndiversity: does the attack represent a previously unseen harm type? Our\nevaluation of existing attack generation methods on the CivilComments toxicity\ntask reveals their limitations: Word perturbation attacks fail to fool\nclassifiers, while prompt-based LLM attacks have more adversarial success, but\nlack dimensional diversity. Even our best-performing prompt-based method finds\nnew successful attacks on unseen harm dimensions of attacks only 5\\% of the\ntime. Automatically finding new harmful dimensions of attack is crucial and\nthere is substantial headroom for future research on our new task.\n","authors":["Yash Kumar Lal","Preethi Lahoti","Aradhana Sinha","Yao Qin","Ananth Balashankar"],"pdf_url":"https://arxiv.org/pdf/2406.17104v1.pdf","comment":"Published at Fourth Workshop on TrustworthyNLP (TrustNLP) at NAACL\n  2024"},{"id":"http://arxiv.org/abs/2406.17095v1","updated":"2024-06-24T19:35:11Z","published":"2024-06-24T19:35:11Z","title":"Attention Instruction: Amplifying Attention in the Middle via Prompting","summary":"  The context window of large language models has been extended to 128k tokens\nor more. However, language models still suffer from position bias and have\ndifficulty in accessing and using the middle part of the context due to the\nlack of attention. We examine the relative position awareness of LLMs and the\nfeasibility of mitigating disproportional attention through prompting. We\naugment the original task instruction with $\\texttt{attention instructions}$\nthat direct language models to allocate more attention towards a selected\nsegment of the context. We conduct a comprehensive investigation on\nmulti-document question answering task with both position-based and index-based\ninstructions. We find that language models do not have relative position\nawareness of the context. Nevertheless, they demonstrate the capacity to adapt\nattention to a specific segment using matching indexes. Our analysis\ncontributes to a deeper understanding of position bias in LLMs and provides a\npathway to mitigate this bias by instruction, thus benefiting LLMs in locating\nand utilizing relevant information from retrieved documents in RAG\napplications.\n","authors":["Meiru Zhang","Zaiqiao Meng","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2406.17095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02416v4","updated":"2024-06-24T18:55:16Z","published":"2024-02-04T09:24:51Z","title":"Aligner: Efficient Alignment by Learning to Correct","summary":"  With the rapid development of large language models (LLMs) and ever-evolving\npractical requirements, finding an efficient and effective alignment method has\nnever been more critical. However, the tension between the complexity of\ncurrent alignment methods and the need for rapid iteration in deployment\nscenarios necessitates the development of a model-agnostic alignment approach\nthat can operate under these constraints. In this paper, we introduce Aligner,\na novel and simple alignment paradigm that learns the correctional residuals\nbetween preferred and dispreferred answers using a small model. Designed as a\nmodel-agnostic, plug-and-play module, Aligner can be directly applied to\nvarious open-source and API-based models with only one-off training, making it\nsuitable for rapid iteration. Notably, Aligner can be applied to any powerful,\nlarge-scale upstream models. Moreover, it can even iteratively bootstrap the\nupstream models using corrected responses as synthetic human preference data,\nbreaking through the model's performance ceiling. Our experiments demonstrate\nperformance improvements by deploying the same Aligner model across 11\ndifferent LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and\nhonesty). Specifically, Aligner-7B has achieved an average improvement of 68.9%\nin helpfulness and 23.8% in harmlessness across the tested LLMs while also\neffectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking\nAligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%,\nsurpassing GPT-4 Omni's 57.5% Win Rate (community report).\n","authors":["Jiaming Ji","Boyuan Chen","Hantao Lou","Donghai Hong","Borong Zhang","Xuehai Pan","Juntao Dai","Tianyi Qiu","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2402.02416v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17055v1","updated":"2024-06-24T18:15:27Z","published":"2024-06-24T18:15:27Z","title":"Large Language Models Assume People are More Rational than We Really are","summary":"  In order for AI systems to communicate effectively with people, they must\nunderstand how we make decisions. However, people's decisions are not always\nrational, so the implicit internal models of human decision-making in Large\nLanguage Models (LLMs) must account for this. Previous empirical evidence seems\nto suggest that these implicit models are accurate -- LLMs offer believable\nproxies of human behavior, acting how we expect humans would in everyday\ninteractions. However, by comparing LLM behavior and predictions to a large\ndataset of human decisions, we find that this is actually not the case: when\nboth simulating and predicting people's choices, a suite of cutting-edge LLMs\n(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more\nrational than we really are. Specifically, these models deviate from human\nbehavior and align more closely with a classic model of rational choice --\nexpected value theory. Interestingly, people also tend to assume that other\npeople are rational when interpreting their behavior. As a consequence, when we\ncompare the inferences that LLMs and people draw from the decisions of others\nusing another psychological dataset, we find that these inferences are highly\ncorrelated. Thus, the implicit decision-making models of LLMs appear to be\naligned with the human expectation that other people will act rationally,\nrather than with how people actually act.\n","authors":["Ryan Liu","Jiayi Geng","Joshua C. Peterson","Ilia Sucholutsky","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2406.17055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13179v4","updated":"2024-06-24T18:13:16Z","published":"2024-05-21T20:03:40Z","title":"RAG-RLRC-LaySum at BioLaySumm: Integrating Retrieval-Augmented\n  Generation and Readability Control for Layman Summarization of Biomedical\n  Texts","summary":"  This paper introduces the RAG-RLRC-LaySum framework, designed to make complex\nbiomedical research understandable to laymen through advanced Natural Language\nProcessing (NLP) techniques. Our Retrieval Augmented Generation (RAG) solution,\nenhanced by a reranking method, utilizes multiple knowledge sources to ensure\nthe precision and pertinence of lay summaries. Additionally, our Reinforcement\nLearning for Readability Control (RLRC) strategy improves readability, making\nscientific content comprehensible to non-specialists. Evaluations using the\npublicly accessible PLOS and eLife datasets show that our methods surpass Plain\nGemini model, demonstrating a 20% increase in readability scores, a 15%\nimprovement in ROUGE-2 relevance scores, and a 10% enhancement in factual\naccuracy. The RAG-RLRC-LaySum framework effectively democratizes scientific\nknowledge, enhancing public engagement with biomedical discoveries.\n","authors":["Yuelyu Ji","Zhuochun Li","Rui Meng","Sonish Sivarajkumar","Yanshan Wang","Zeshui Yu","Hui Ji","Yushui Han","Hanyu Zeng","Daqing He"],"pdf_url":"https://arxiv.org/pdf/2405.13179v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17038v1","updated":"2024-06-24T18:00:59Z","published":"2024-06-24T18:00:59Z","title":"modeLing: A Novel Dataset for Testing Linguistic Reasoning in Language\n  Models","summary":"  We introduce modeLing, a novel benchmark of Linguistics Olympiad-style\npuzzles which tests few-shot reasoning in AI systems. Solving these puzzles\nnecessitates inferring aspects of a language's grammatical structure from a\nsmall number of examples. Such puzzles provide a natural testbed for language\nmodels, as they require compositional generalization and few-shot inductive\nreasoning. Consisting solely of new puzzles written specifically for this work,\nmodeLing has no risk of appearing in the training data of existing AI systems:\nthis ameliorates the risk of data leakage, a potential confounder for many\nprior evaluations of reasoning. Evaluating several large open source language\nmodels and GPT on our benchmark, we observe non-negligible accuracy,\ndemonstrating few-shot emergent reasoning ability which cannot merely be\nattributed to shallow memorization. However, imperfect model performance\nsuggests that modeLing can be used to measure further progress in linguistic\nreasoning.\n","authors":["Nathan A. Chi","Teodor Malchev","Riley Kong","Ryan A. Chi","Lucas Huang","Ethan A. Chi","R. Thomas McCoy","Dragomir Radev"],"pdf_url":"https://arxiv.org/pdf/2406.17038v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.16828v1","updated":"2024-06-24T17:37:52Z","published":"2024-06-24T17:37:52Z","title":"Ragnarök: A Reusable RAG Framework and Baselines for TREC 2024\n  Retrieval-Augmented Generation Track","summary":"  Did you try out the new Bing Search? Or maybe you fiddled around with Google\nAI~Overviews? These might sound familiar because the modern-day search stack\nhas recently evolved to include retrieval-augmented generation (RAG) systems.\nThey allow searching and incorporating real-time data into large language\nmodels (LLMs) to provide a well-informed, attributed, concise summary in\ncontrast to the traditional search paradigm that relies on displaying a ranked\nlist of documents. Therefore, given these recent advancements, it is crucial to\nhave an arena to build, test, visualize, and systematically evaluate RAG-based\nsearch systems. With this in mind, we propose the TREC 2024 RAG Track to foster\ninnovation in evaluating RAG systems. In our work, we lay out the steps we've\nmade towards making this track a reality -- we describe the details of our\nreusable framework, Ragnar\\\"ok, explain the curation of the new MS MARCO V2.1\ncollection choice, release the development topics for the track, and\nstandardize the I/O definitions which assist the end user. Next, using\nRagnar\\\"ok, we identify and provide key industrial baselines such as OpenAI's\nGPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface\nfor an interactive arena allowing benchmarking pairwise RAG systems by\ncrowdsourcing. We open-source our Ragnar\\\"ok framework and baselines to achieve\na unified standard for future RAG systems.\n","authors":["Ronak Pradeep","Nandan Thakur","Sahel Sharifymoghaddam","Eric Zhang","Ryan Nguyen","Daniel Campos","Nick Craswell","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2406.16828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16629v1","updated":"2024-06-24T13:23:00Z","published":"2024-06-24T13:23:00Z","title":"Meta-experiments: Improving experimentation through experimentation","summary":"  A/B testing is widexly used in the industry to optimize customer facing\nwebsites. Many companies employ experimentation specialists to facilitate and\nimprove the process of A/B testing. Here, we present the application of A/B\ntesting to this improvement effort itself, by running experiments on the\nexperimentation process, which we call 'meta-experiments'. We discuss the\nchallenges of this approach using the example of one of our meta-experiments,\nwhich helped experimenters to run more sufficiently powered A/B tests. We also\npoint out the benefits of 'dog fooding' for the experimentation specialists\nwhen running their own experiments.\n","authors":["Melanie J. I. Müller"],"pdf_url":"https://arxiv.org/pdf/2406.16629v1.pdf","comment":"6 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.19181v2","updated":"2024-06-24T13:22:22Z","published":"2024-03-28T07:22:16Z","title":"Make Large Language Model a Better Ranker","summary":"  Large Language Models (LLMs) demonstrate robust capabilities across various\nfields, leading to a paradigm shift in LLM-enhanced Recommender System (RS).\nResearch to date focuses on point-wise and pair-wise recommendation paradigms,\nwhich are inefficient for LLM-based recommenders due to high computational\ncosts. However, existing list-wise approaches also fall short in ranking tasks\ndue to misalignment between ranking objectives and next-token prediction.\nMoreover, these LLM-based methods struggle to effectively address the order\nrelation among candidates, particularly given the scale of ratings. To address\nthese challenges, this paper introduces the large language model framework with\nAligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap\nbetween the capabilities of LLMs and the nuanced requirements of ranking tasks.\nSpecifically, ALRO employs explicit feedback in a listwise manner by\nintroducing soft lambda loss, a customized adaptation of lambda loss designed\nfor optimizing order relations. This mechanism provides more accurate\noptimization goals, enhancing the ranking process. Additionally, ALRO\nincorporates a permutation-sensitive learning mechanism that addresses position\nbias, a prevalent issue in generative models, without imposing additional\ncomputational burdens during inference. Our evaluative studies reveal that ALRO\noutperforms both existing embedding-based recommendation methods and LLM-based\nrecommendation baselines.\n","authors":["Wenshuo Chao","Zhi Zheng","Hengshu Zhu","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2403.19181v2.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.15351v2","updated":"2024-06-24T12:08:06Z","published":"2024-01-27T08:52:19Z","title":"A Survey on Neural Topic Models: Methods, Applications, and Challenges","summary":"  Topic models have been prevalent for decades to discover latent topics and\ninfer topic proportions of documents in an unsupervised fashion. They have been\nwidely used in various applications like text analysis and context\nrecommendation. Recently, the rise of neural networks has facilitated the\nemergence of a new research field -- Neural Topic Models (NTMs). Different from\nconventional topic models, NTMs directly optimize parameters without requiring\nmodel-specific derivations. This endows NTMs with better scalability and\nflexibility, resulting in significant research attention and plentiful new\nmethods and applications. In this paper, we present a comprehensive survey on\nneural topic models concerning methods, applications, and challenges.\nSpecifically, we systematically organize current NTM methods according to their\nnetwork structures and introduce the NTMs for various scenarios like short\ntexts and bilingual documents. We also discuss a wide range of popular\napplications built on NTMs. Finally, we highlight the challenges confronted by\nNTMs to inspire future research. We accompany this survey with a repository for\neasier access to the mentioned paper resources:\nhttps://github.com/bobxwu/Paper-Neural-Topic-Models.\n","authors":["Xiaobao Wu","Thong Nguyen","Anh Tuan Luu"],"pdf_url":"https://arxiv.org/pdf/2401.15351v2.pdf","comment":"Accepted to Artificial Intelligence Review. See\n  https://doi.org/10.1007/s10462-023-10661-7 and a paper list at\n  https://github.com/BobXWu/Paper-Neural-Topic-Models"},{"id":"http://arxiv.org/abs/2403.17209v4","updated":"2024-06-24T12:04:06Z","published":"2024-03-25T21:37:30Z","title":"Generation of Asset Administration Shell with Large Language Model\n  Agents: Toward Semantic Interoperability in Digital Twins in the Context of\n  Industry 4.0","summary":"  This research introduces a novel approach for achieving semantic\ninteroperability in digital twins and assisting the creation of Asset\nAdministration Shell (AAS) as digital twin model within the context of Industry\n4.0. The foundational idea of our research is that the communication based on\nsemantics and the generation of meaningful textual data are directly linked,\nand we posit that these processes are equivalent if the exchanged information\ncan be serialized in text form. Based on this, we construct a \"semantic node\"\ndata structure in our research to capture the semantic essence of textual data.\nThen, a system powered by large language models is designed and implemented to\nprocess the \"semantic node\" and generate standardized digital twin models from\nraw textual data collected from datasheets describing technical assets. Our\nevaluation demonstrates an effective generation rate of 62-79%, indicating a\nsubstantial proportion of the information from the source text can be\ntranslated error-free to the target digital twin instance model with the\ngenerative capability of large language models. This result has a direct\napplication in the context of Industry 4.0, and the designed system is\nimplemented as a data model generation tool for reducing the manual effort in\ncreating AAS model. In our evaluation, a comparative analysis of different LLMs\nand an in-depth ablation study of Retrieval-Augmented Generation (RAG)\nmechanisms provide insights into the effectiveness of LLM systems for\ninterpreting technical concepts and translating data. Our findings emphasize\nLLMs' capability to automate AAS instance creation and contribute to the\nbroader field of semantic interoperability for digital twins in industrial\napplications. The prototype implementation and evaluation results are presented\non our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.\n","authors":["Yuchen Xia","Zhewen Xiao","Nasser Jazdi","Michael Weyrich"],"pdf_url":"https://arxiv.org/pdf/2403.17209v4.pdf","comment":"Published in IEEE Access"},{"id":"http://arxiv.org/abs/2406.16568v1","updated":"2024-06-24T12:03:35Z","published":"2024-06-24T12:03:35Z","title":"Star+: A New Multi-Domain Model for CTR Prediction","summary":"  In this paper, we introduce Star+, a novel multi-domain model for\nclick-through rate (CTR) prediction inspired by the Star model. Traditional\nsingle-domain approaches and existing multi-task learning techniques face\nchallenges in multi-domain environments due to their inability to capture\ndomain-specific data distributions and complex inter-domain relationships.\nStar+ addresses these limitations by enhancing the interaction between shared\nand domain-specific information through various fusion strategies, such as add,\nadaptive add, concatenation, and gating fusions, to find the optimal balance\nbetween domain-specific and shared information. We also investigate the impact\nof different normalization techniques, including layer normalization, batch\nnormalization, and partition normalization, on the performance of our model.\nOur extensive experiments on both industrial and public datasets demonstrate\nthat Star+ significantly improves prediction accuracy and efficiency. This work\ncontributes to the advancement of recommendation systems by providing a robust,\nscalable, and adaptive solution for multi-domain environments.\n","authors":["Çağrı Yeşil","Kaya Turgut"],"pdf_url":"https://arxiv.org/pdf/2406.16568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16494v1","updated":"2024-06-24T10:02:24Z","published":"2024-06-24T10:02:24Z","title":"Cross-domain Transfer of Valence Preferences via a Meta-optimization\n  Approach","summary":"  Cross-domain recommendation offers a potential avenue for alleviating data\nsparsity and cold-start problems. Embedding and mapping, as a classic\ncross-domain research genre, aims to identify a common mapping function to\nperform representation transformation between two domains. Nevertheless,\nprevious coarse-grained preference representations, non-personalized mapping\nfunctions, and excessive reliance on overlapping users limit their performance,\nespecially in scenarios where overlapping users are sparse. To address\naforementioned challenges, we propose a novel cross-domain approach, namely\nCVPM. CVPM formalizes cross-domain interest transfer as a hybrid architecture\nof parametric meta-learning and self-supervised learning, which not only\ntransfers user preferences at a finer level, but also enables signal\nenhancement with the knowledge of non-overlapping users. Specifically, with\ndeep insights into user preferences and valence preference theory, we believe\nthat there exists significant difference between users' positive preferences\nand negative behaviors, and thus employ differentiated encoders to learn their\ndistributions. In particular, we further utilize the pre-trained model and item\npopularity to sample pseudo-interaction items to ensure the integrity of both\ndistributions. To guarantee the personalization of preference transfer, we\ntreat each user's mapping as two parts, the common transformation and the\npersonalized bias, where the network used to generate the personalized bias is\noutput by a meta-learner. Furthermore, in addition to the supervised loss for\noverlapping users, we design contrastive tasks for non-overlapping users from\nboth group and individual-levels to avoid model skew and enhance the semantics\nof representations. Exhaustive data analysis and extensive experimental results\ndemonstrate the effectiveness and advancement of our proposed framework.\n","authors":["Chuang Zhao","Hongke Zhao","Ming He","Xiaomeng Li","Jianping Fan"],"pdf_url":"https://arxiv.org/pdf/2406.16494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09234v4","updated":"2024-06-24T09:26:42Z","published":"2023-10-13T16:37:53Z","title":"ClickPrompt: CTR Models are Strong Prompt Generators for Adapting\n  Language Models to CTR Prediction","summary":"  Click-through rate (CTR) prediction has become increasingly indispensable for\nvarious Internet applications. Traditional CTR models convert the multi-field\ncategorical data into ID features via one-hot encoding, and extract the\ncollaborative signals among features. Such a paradigm suffers from the problem\nof semantic information loss. Another line of research explores the potential\nof pretrained language models (PLMs) for CTR prediction by converting input\ndata into textual sentences through hard prompt templates. Although semantic\nsignals are preserved, they generally fail to capture the collaborative\ninformation (e.g., feature interactions, pure ID features), not to mention the\nunacceptable inference overhead brought by the huge model size. In this paper,\nwe aim to model both the semantic knowledge and collaborative knowledge for\naccurate CTR estimation, and meanwhile address the inference inefficiency\nissue. To benefit from both worlds and close their gaps, we propose a novel\nmodel-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models\nto generate interaction-aware soft prompts for PLMs. We design a\nprompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM\nhas to recover the masked tokens based on the language context, as well as the\nsoft prompts generated by CTR model. The collaborative and semantic knowledge\nfrom ID and textual features would be explicitly aligned and interacted via the\nprompt interface. Then, we can either tune the CTR model with PLM for superior\nperformance, or solely tune the CTR model without PLM for inference efficiency.\nExperiments on four real-world datasets validate the effectiveness of\nClickPrompt compared with existing baselines.\n","authors":["Jianghao Lin","Bo Chen","Hangyu Wang","Yunjia Xi","Yanru Qu","Xinyi Dai","Kangning Zhang","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.09234v4.pdf","comment":"Accepted by WWW 2024"},{"id":"http://arxiv.org/abs/2403.15279v2","updated":"2024-06-24T09:18:48Z","published":"2024-03-22T15:22:06Z","title":"Fundus: A Simple-to-Use News Scraper Optimized for High Quality\n  Extractions","summary":"  This paper introduces Fundus, a user-friendly news scraper that enables users\nto obtain millions of high-quality news articles with just a few lines of code.\nUnlike existing news scrapers, we use manually crafted, bespoke content\nextractors that are specifically tailored to the formatting guidelines of each\nsupported online newspaper. This allows us to optimize our scraping for quality\nsuch that retrieved news articles are textually complete and without HTML\nartifacts. Further, our framework combines both crawling (retrieving HTML from\nthe web or large web archives) and content extraction into a single pipeline.\nBy providing a unified interface for a predefined collection of newspapers, we\naim to make Fundus broadly usable even for non-technical users. This paper\ngives an overview of the framework, discusses our design choices, and presents\na comparative evaluation against other popular news scrapers. Our evaluation\nshows that Fundus yields significantly higher quality extractions (complete and\nartifact-free news articles) than prior work. The framework is available on\nGitHub under https://github.com/flairNLP/fundus and can be simply installed\nusing pip.\n","authors":["Max Dallabetta","Conrad Dobberstein","Adrian Breiding","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2403.15279v2.pdf","comment":"10 pages, 4 figures, ACL 2024, for a screencast see\n  https://www.youtube.com/watch?v=9GJExMelhdI"},{"id":"http://arxiv.org/abs/2406.16383v1","updated":"2024-06-24T07:52:05Z","published":"2024-06-24T07:52:05Z","title":"Context-augmented Retrieval: A Novel Framework for Fast Information\n  Retrieval based Response Generation using Large Language Model","summary":"  Generating high-quality answers consistently by providing contextual\ninformation embedded in the prompt passed to the Large Language Model (LLM) is\ndependent on the quality of information retrieval. As the corpus of contextual\ninformation grows, the answer/inference quality of Retrieval Augmented\nGeneration (RAG) based Question Answering (QA) systems declines. This work\nsolves this problem by combining classical text classification with the Large\nLanguage Model (LLM) to enable quick information retrieval from the vector\nstore and ensure the relevancy of retrieved information. For the same, this\nwork proposes a new approach Context Augmented retrieval (CAR), where\npartitioning of vector database by real-time classification of information\nflowing into the corpus is done. CAR demonstrates good quality answer\ngeneration along with significant reduction in information retrieval and answer\ngeneration time.\n","authors":["Sai Ganesh","Anupam Purwar","Gautam B"],"pdf_url":"https://arxiv.org/pdf/2406.16383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16367v1","updated":"2024-06-24T07:17:59Z","published":"2024-06-24T07:17:59Z","title":"On the Role of Long-tail Knowledge in Retrieval Augmented Large Language\n  Models","summary":"  Retrieval augmented generation (RAG) exhibits outstanding performance in\npromoting the knowledge capabilities of large language models (LLMs) with\nretrieved documents related to user queries. However, RAG only focuses on\nimproving the response quality of LLMs via enhancing queries indiscriminately\nwith retrieved information, paying little attention to what type of knowledge\nLLMs really need to answer original queries more accurately. In this paper, we\nsuggest that long-tail knowledge is crucial for RAG as LLMs have already\nremembered common world knowledge during large-scale pre-training. Based on our\nobservation, we propose a simple but effective long-tail knowledge detection\nmethod for LLMs. Specifically, the novel Generative Expected Calibration Error\n(GECE) metric is derived to measure the ``long-tailness'' of knowledge based on\nboth statistics and semantics. Hence, we retrieve relevant documents and infuse\nthem into the model for patching knowledge loopholes only when the input query\nrelates to long-tail knowledge. Experiments show that, compared to existing RAG\npipelines, our method achieves over 4x speedup in average inference time and\nconsistent performance improvement in downstream tasks.\n","authors":["Dongyang Li","Junbing Yan","Taolin Zhang","Chengyu Wang","Xiaofeng He","Longtao Huang","Hui Xue","Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16581v3","updated":"2024-06-24T06:53:23Z","published":"2023-12-27T14:13:42Z","title":"Continuous-time Autoencoders for Regular and Irregular Time Series\n  Imputation","summary":"  Time series imputation is one of the most fundamental tasks for time series.\nReal-world time series datasets are frequently incomplete (or irregular with\nmissing observations), in which case imputation is strongly required. Many\ndifferent time series imputation methods have been proposed. Recent\nself-attention-based methods show the state-of-the-art imputation performance.\nHowever, it has been overlooked for a long time to design an imputation method\nbased on continuous-time recurrent neural networks (RNNs), i.e., neural\ncontrolled differential equations (NCDEs). To this end, we redesign time series\n(variational) autoencoders based on NCDEs. Our method, called continuous-time\nautoencoder (CTA), encodes an input time series sample into a continuous hidden\npath (rather than a hidden vector) and decodes it to reconstruct and impute the\ninput. In our experiments with 4 datasets and 19 baselines, our method shows\nthe best imputation performance in almost all cases.\n","authors":["Hyowon Wi","Yehjin Shin","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2312.16581v3.pdf","comment":"Published as a WSDM'24 full paper (oral presentation)"},{"id":"http://arxiv.org/abs/2406.16350v1","updated":"2024-06-24T06:46:32Z","published":"2024-06-24T06:46:32Z","title":"A Survey on Intent-aware Recommender Systems","summary":"  Many modern online services feature personalized recommendations. A central\nchallenge when providing such recommendations is that the reason why an\nindividual user accesses the service may change from visit to visit or even\nduring an ongoing usage session. To be effective, a recommender system should\ntherefore aim to take the users' probable intent of using the service at a\ncertain point in time into account. In recent years, researchers have thus\nstarted to address this challenge by incorporating intent-awareness into\nrecommender systems. Correspondingly, a number of technical approaches were put\nforward, including diversification techniques, intent prediction models or\nlatent intent modeling approaches. In this paper, we survey and categorize\nexisting approaches to building the next generation of Intent-Aware Recommender\nSystems (IARS). Based on an analysis of current evaluation practices, we\noutline open gaps and possible future directions in this area, which in\nparticular include the consideration of additional interaction signals and\ncontextual information to further improve the effectiveness of such systems.\n","authors":["Dietmar Jannach","Markus Zanker"],"pdf_url":"https://arxiv.org/pdf/2406.16350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16332v1","updated":"2024-06-24T06:10:13Z","published":"2024-06-24T06:10:13Z","title":"DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task","summary":"  Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly apply a\ndemonstration retriever to retrieve demonstrations and use top-$k$\ndemonstrations for in-context learning (ICL). Although effective, this approach\noverlooks the dependencies between demonstrations, leading to inferior\nperformance of few-shot ICL in the passage ranking task. In this paper, we\nformulate the demonstration selection as a \\textit{retrieve-then-rerank}\nprocess and introduce the DemoRank framework. In this framework, we first use\nLLM feedback to train a demonstration retriever and construct a novel\ndependency-aware training samples to train a demonstration reranker to improve\nfew-shot ICL. The construction of such training samples not only considers\ndemonstration dependencies but also performs in an efficient way. Extensive\nexperiments demonstrate DemoRank's effectiveness in in-domain scenarios and\nstrong generalization to out-of-domain scenarios. Our codes are available\nat~\\url{https://github.com/8421BCD/DemoRank}.\n","authors":["Wenhan Liu","Yutao Zhu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2406.16332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11131v5","updated":"2024-06-24T02:44:28Z","published":"2023-08-22T02:25:04Z","title":"ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential\n  Behavior Comprehension in Recommendation","summary":"  With large language models (LLMs) achieving remarkable breakthroughs in\nnatural language processing (NLP) domains, LLM-enhanced recommender systems\nhave received much attention and have been actively explored currently. In this\npaper, we focus on adapting and empowering a pure large language model for\nzero-shot and few-shot recommendation tasks. First and foremost, we identify\nand formulate the lifelong sequential behavior incomprehension problem for LLMs\nin recommendation domains, i.e., LLMs fail to extract useful information from a\ntextual context of long user behavior sequence, even if the length of context\nis far from reaching the context limitation of LLMs. To address such an issue\nand improve the recommendation performance of LLMs, we propose a novel\nframework, namely Retrieval-enhanced Large Language models (ReLLa) for\nrecommendation tasks in both zero-shot and few-shot settings. For zero-shot\nrecommendation, we perform semantic user behavior retrieval (SUBR) to improve\nthe data quality of testing samples, which greatly reduces the difficulty for\nLLMs to extract the essential knowledge from user behavior sequences. As for\nfew-shot recommendation, we further design retrieval-enhanced instruction\ntuning (ReiT) by adopting SUBR as a data augmentation technique for training\nsamples. Specifically, we develop a mixed training dataset consisting of both\nthe original data samples and their retrieval-enhanced counterparts. We conduct\nextensive experiments on three real-world public datasets to demonstrate the\nsuperiority of ReLLa compared with existing baseline models, as well as its\ncapability for lifelong sequential behavior comprehension. To be highlighted,\nwith only less than 10% training samples, few-shot ReLLa can outperform\ntraditional CTR models that are trained on the entire training set (e.g.,\nDCNv2, DIN, SIM). The code is available\n\\url{https://github.com/LaVieEnRose365/ReLLa}.\n","authors":["Jianghao Lin","Rong Shan","Chenxu Zhu","Kounianhua Du","Bo Chen","Shigang Quan","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.11131v5.pdf","comment":"Accepted by WWW 2024. Full and More Readable Version"},{"id":"http://arxiv.org/abs/2308.07269v3","updated":"2024-06-24T02:17:57Z","published":"2023-08-14T16:52:42Z","title":"EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language\n  Models","summary":"  Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy\nissues, which means they are unaware of unseen events or generate text with\nincorrect facts owing to outdated/noisy data. To this end, many knowledge\nediting approaches for LLMs have emerged -- aiming to subtly inject/edit\nupdated knowledge or adjust undesired behavior while minimizing the impact on\nunrelated inputs. Nevertheless, due to significant differences among various\nknowledge editing methods and the variations in task setups, there is no\nstandard implementation framework available for the community, which hinders\npractitioners from applying knowledge editing to applications. To address these\nissues, we propose EasyEdit, an easy-to-use knowledge editing framework for\nLLMs. It supports various cutting-edge knowledge editing approaches and can be\nreadily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.\nEmpirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,\ndemonstrating that knowledge editing surpasses traditional fine-tuning in terms\nof reliability and generalization. We have released the source code on GitHub,\nalong with Google Colab tutorials and comprehensive documentation for beginners\nto get started. Besides, we present an online system for real-time knowledge\nediting, and a demo video.\n","authors":["Peng Wang","Ningyu Zhang","Bozhong Tian","Zekun Xi","Yunzhi Yao","Ziwen Xu","Mengru Wang","Shengyu Mao","Xiaohan Wang","Siyuan Cheng","Kangwei Liu","Yuansheng Ni","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2308.07269v3.pdf","comment":"ACL 2024 System Demonstrations; Code:\n  https://github.com/zjunlp/EasyEdit HF Demo:\n  https://huggingface.co/spaces/zjunlp/EasyEdit Video:\n  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit"},{"id":"http://arxiv.org/abs/2402.03049v4","updated":"2024-06-24T02:10:23Z","published":"2024-02-05T14:33:56Z","title":"EasyInstruct: An Easy-to-use Instruction Processing Framework for Large\n  Language Models","summary":"  In recent years, instruction tuning has gained increasing attention and\nemerged as a crucial technique to enhance the capabilities of Large Language\nModels (LLMs). To construct high-quality instruction datasets, many instruction\nprocessing approaches have been proposed, aiming to achieve a delicate balance\nbetween data quantity and data quality. Nevertheless, due to inconsistencies\nthat persist among various instruction processing methods, there is no standard\nopen-source instruction processing implementation framework available for the\ncommunity, which hinders practitioners from further developing and advancing.\nTo facilitate instruction processing research and development, we present\nEasyInstruct, an easy-to-use instruction processing framework for LLMs, which\nmodularizes instruction generation, selection, and prompting, while also\nconsidering their combination and interaction. EasyInstruct is publicly\nreleased and actively maintained at https://github.com/zjunlp/EasyInstruct,\nalong with an online demo app and a demo video for quick-start, calling for\nbroader research centered on instruction data and synthetic data.\n","authors":["Yixin Ou","Ningyu Zhang","Honghao Gui","Ziwen Xu","Shuofei Qiao","Yida Xue","Runnan Fang","Kangwei Liu","Lei Li","Zhen Bi","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2402.03049v4.pdf","comment":"ACL 2024 System Demonstrations; Project website:\n  https://zjunlp.github.io/project/EasyInstruct Code:\n  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo\n  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct"},{"id":"http://arxiv.org/abs/2406.10289v2","updated":"2024-06-24T23:53:05Z","published":"2024-06-12T21:23:48Z","title":"VeraCT Scan: Retrieval-Augmented Fake News Detection with Justifiable\n  Reasoning","summary":"  The proliferation of fake news poses a significant threat not only by\ndisseminating misleading information but also by undermining the very\nfoundations of democracy. The recent advance of generative artificial\nintelligence has further exacerbated the challenge of distinguishing genuine\nnews from fabricated stories. In response to this challenge, we introduce\nVeraCT Scan, a novel retrieval-augmented system for fake news detection. This\nsystem operates by extracting the core facts from a given piece of news and\nsubsequently conducting an internet-wide search to identify corroborating or\nconflicting reports. Then sources' credibility is leveraged for information\nverification. Besides determining the veracity of news, we also provide\ntransparent evidence and reasoning to support its conclusions, resulting in the\ninterpretability and trust in the results. In addition to GPT-4 Turbo, Llama-2\n13B is also fine-tuned for news content understanding, information\nverification, and reasoning. Both implementations have demonstrated\nstate-of-the-art accuracy in the realm of fake news detection.\n","authors":["Cheng Niu","Yang Guan","Yuanhao Wu","Juno Zhu","Juntong Song","Randy Zhong","Kaihua Zhu","Siliang Xu","Shizhe Diao","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.10289v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17182v1","updated":"2024-06-24T23:42:18Z","published":"2024-06-24T23:42:18Z","title":"Debiased Recommendation with Noisy Feedback","summary":"  Ratings of a user to most items in recommender systems are usually missing\nnot at random (MNAR), largely because users are free to choose which items to\nrate. To achieve unbiased learning of the prediction model under MNAR data,\nthree typical solutions have been proposed, including error-imputation-based\n(EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods.\nHowever, these methods ignore an alternative form of bias caused by the\ninconsistency between the observed ratings and the users' true preferences,\nalso known as noisy feedback or outcome measurement errors (OME), e.g., due to\npublic opinion or low-quality data collection process. In this work, we study\nintersectional threats to the unbiased learning of the prediction model from\ndata MNAR and OME in the collected data. First, we design OME-EIB, OME-IPS, and\nOME-DR estimators, which largely extend the existing estimators to combat OME\nin real-world recommendation scenarios. Next, we theoretically prove the\nunbiasedness and generalization bound of the proposed estimators. We further\npropose an alternate denoising training approach to achieve unbiased learning\nof the prediction model under MNAR data with OME. Extensive experiments are\nconducted on three real-world datasets and one semi-synthetic dataset to show\nthe effectiveness of our proposed approaches. The code is available at\nhttps://github.com/haoxuanli-pku/KDD24-OME-DR.\n","authors":["Haoxuan Li","Chunyuan Zheng","Wenjie Wang","Hao Wang","Fuli Feng","Xiao-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.17182v1.pdf","comment":"KDD 24 Research Track Paper"},{"id":"http://arxiv.org/abs/2403.19651v2","updated":"2024-06-24T23:41:29Z","published":"2024-03-28T17:59:20Z","title":"MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions","summary":"  Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent works leverage text\ninstructions to allow users to more freely express their search intents.\nHowever, they primarily focus on image pairs that are visually similar and/or\ncan be characterized by a small set of pre-defined relations. The core thesis\nof this paper is that text instructions can enable retrieving images with\nricher relations beyond visual similarity. To show this, we introduce\nMagicLens, a series of self-supervised image retrieval models that support\nopen-ended instructions. MagicLens is built on a key novel insight: image pairs\nthat naturally occur on the same web pages contain a wide range of implicit\nrelations (e.g., inside view of), and we can bring those implicit relations\nexplicit by synthesizing instructions via foundation models. Trained on 36.7M\n(query image, instruction, target image) triplets with rich semantic relations\nmined from the web, MagicLens achieves results comparable with or better than\nprior best on eight benchmarks of various image retrieval tasks, while\nmaintaining high parameter efficiency with a significantly smaller model size.\nAdditional human analyses on a 1.4M-image unseen corpus further demonstrate the\ndiversity of search intents supported by MagicLens. Code and models are\npublicly available at https://open-vision-language.github.io/MagicLens/.\n","authors":["Kai Zhang","Yi Luan","Hexiang Hu","Kenton Lee","Siyuan Qiao","Wenhu Chen","Yu Su","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2403.19651v2.pdf","comment":"ICML 2024 (Oral); Project Website:\n  https://open-vision-language.github.io/MagicLens/"},{"id":"http://arxiv.org/abs/2406.17158v1","updated":"2024-06-24T22:09:50Z","published":"2024-06-24T22:09:50Z","title":"DEXTER: A Benchmark for open-domain Complex Question Answering using\n  LLMs","summary":"  Open-domain complex Question Answering (QA) is a difficult task with\nchallenges in evidence retrieval and reasoning. The complexity of such\nquestions could stem from questions being compositional, hybrid evidence, or\nambiguity in questions. While retrieval performance for classical QA tasks is\nwell explored, their capabilities for heterogeneous complex retrieval tasks,\nespecially in an open-domain setting, and the impact on downstream QA\nperformance, are relatively unexplored. To address this, in this work, we\npropose a benchmark composing diverse complex QA tasks and provide a toolkit to\nevaluate state-of-the-art pre-trained dense and sparse retrieval models in an\nopen-domain setting. We observe that late interaction models and surprisingly\nlexical models like BM25 perform well compared to other pre-trained dense\nretrieval models. In addition, since context-based reasoning is critical for\nsolving complex QA tasks, we also evaluate the reasoning capabilities of LLMs\nand the impact of retrieval performance on their reasoning capabilities.\nThrough experiments, we observe that much progress is to be made in retrieval\nfor complex QA to improve downstream QA performance. Our software and related\ndata can be accessed at https://github.com/VenkteshV/DEXTER\n","authors":["Venktesh V. Deepali Prabhu","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2406.17158v1.pdf","comment":"under submission, 22 pages"}]},"2024-06-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2310.02905v3","updated":"2024-06-23T23:59:53Z","published":"2023-10-02T02:01:16Z","title":"Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural\n  bandits Coupled with Transformers","summary":"  Large language models (LLMs) have shown remarkable instruction-following\ncapabilities and achieved impressive performances in various applications.\nHowever, the performances of LLMs depend heavily on the instructions given to\nthem, which are typically manually tuned with substantial human efforts. Recent\nwork has used the query-efficient Bayesian optimization (BO) algorithm to\nautomatically optimize the instructions given to black-box LLMs. However, BO\nusually falls short when optimizing highly sophisticated (e.g.,\nhigh-dimensional) objective functions, such as the functions mapping an\ninstruction to the performance of an LLM. This is mainly due to the limited\nexpressive power of the Gaussian process (GP) which is used by BO as a\nsurrogate to model the objective function. Meanwhile, it has been repeatedly\nshown that neural networks (NNs), especially pre-trained transformers, possess\nstrong expressive power and can model highly complex functions. So, we adopt a\nneural bandit algorithm which replaces the GP in BO by an NN surrogate to\noptimize instructions for black-box LLMs. More importantly, the neural bandit\nalgorithm allows us to naturally couple the NN surrogate with the hidden\nrepresentation learned by a pre-trained transformer (i.e., an open-source LLM),\nwhich significantly boosts its performance. These motivate us to propose our\nINSTruction optimization usIng Neural bandits Coupled with Transformers\n(INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use\nextensive experiments to show that INSTINCT consistently outperforms baselines\nin different tasks, e.g., various instruction induction tasks and the task of\nimproving zero-shot chain-of-thought instructions. Our code is available at\nhttps://github.com/xqlin98/INSTINCT.\n","authors":["Xiaoqiang Lin","Zhaoxuan Wu","Zhongxiang Dai","Wenyang Hu","Yao Shu","See-Kiong Ng","Patrick Jaillet","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2310.02905v3.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2406.05080v2","updated":"2024-06-23T22:58:46Z","published":"2024-06-07T16:52:57Z","title":"I2EDL: Interactive Instruction Error Detection and Localization","summary":"  In the Vision-and-Language Navigation in Continuous Environments (VLN-CE)\ntask, the human user guides an autonomous agent to reach a target goal via a\nseries of low-level actions following a textual instruction in natural\nlanguage. However, most existing methods do not address the likely case where\nusers may make mistakes when providing such instruction (e.g. \"turn left\"\ninstead of \"turn right\"). In this work, we address a novel task of Interactive\nVLN in Continuous Environments (IVLN-CE), which allows the agent to interact\nwith the user during the VLN-CE navigation to verify any doubts regarding the\ninstruction errors. We propose an Interactive Instruction Error Detector and\nLocalizer (I2EDL) that triggers the user-agent interaction upon the detection\nof instruction errors during the navigation. We leverage a pre-trained module\nto detect instruction errors and pinpoint them in the instruction by\ncross-referencing the textual input and past observations. In such way, the\nagent is able to query the user for a timely correction, without demanding the\nuser's cognitive load, as we locate the probable errors to a precise part of\nthe instruction. We evaluate the proposed I2EDL on a dataset of instructions\ncontaining errors, and further devise a novel metric, the Success weighted by\nInteraction Number (SIN), to reflect both the navigation performance and the\ninteraction effectiveness. We show how the proposed method can ask focused\nrequests for corrections to the user, which in turn increases the navigation\nsuccess, while minimizing the interactions.\n","authors":["Francesco Taioli","Stefano Rosa","Alberto Castellini","Lorenzo Natale","Alessio Del Bue","Alessandro Farinelli","Marco Cristani","Yiming Wang"],"pdf_url":"https://arxiv.org/pdf/2406.05080v2.pdf","comment":"Accepted at IEEE RO-MAN 2024"},{"id":"http://arxiv.org/abs/2406.16235v1","updated":"2024-06-23T22:53:47Z","published":"2024-06-23T22:53:47Z","title":"Preference Tuning For Toxicity Mitigation Generalizes Across Languages","summary":"  Detoxifying multilingual Large Language Models (LLMs) has become crucial due\nto their increasing global use. In this work, we explore zero-shot\ncross-lingual generalization of preference tuning in detoxifying LLMs. Unlike\nprevious studies that show limited cross-lingual generalization for other\nsafety tasks, we demonstrate that Direct Preference Optimization (DPO) training\nwith only English data can significantly reduce toxicity in multilingual\nopen-ended generations. For example, the probability of mGPT-1.3B generating\ntoxic continuations drops from 46.8% to 3.9% across 17 different languages\nafter training. Our results also extend to other multilingual LLMs, such as\nBLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal\nintervention and activation analysis, we identified the dual multilinguality\nproperty of MLP layers in LLMs, which explains the cross-lingual generalization\nof DPO. Finally, we show that bilingual sentence retrieval can predict the\ncross-lingual transferability of DPO preference tuning.\n","authors":["Xiaochen Li","Zheng-Xin Yong","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2406.16235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16229v1","updated":"2024-06-23T21:56:48Z","published":"2024-06-23T21:56:48Z","title":"Multi-Objective Linguistic Control of Large Language Models","summary":"  Large language models (LLMs), despite their breakthroughs on many challenging\nbenchmark tasks, lean to generate verbose responses and lack the\ncontrollability of output complexity, which is usually preferred by human users\nin practice. In this paper, we study how to precisely control multiple\nlinguistic complexities of LLM output by finetuning using off-the-shelf data.\nTo this end, we propose multi-control tuning (MCTune), which includes multiple\nlinguistic complexity values of ground-truth responses as controls in the input\nfor instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM\ndatasets. Evaluations on widely used benchmarks demonstrate that our method\ndoes not only improve LLMs' multi-complexity controllability substantially but\nalso retains or even enhances the quality of the responses as a side benefit.\n","authors":["Dang Nguyen","Jiuhai Chen","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.16229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13446v2","updated":"2024-06-23T21:51:45Z","published":"2024-02-21T00:44:04Z","title":"Large Language Models for Data Annotation: A Survey","summary":"  Data annotation generally refers to the labeling or generating of raw data\nwith relevant information, which could be used for improving the efficacy of\nmachine learning models. The process, however, is labor-intensive and costly.\nThe emergence of advanced Large Language Models (LLMs), exemplified by GPT-4,\npresents an unprecedented opportunity to automate the complicated process of\ndata annotation. While existing surveys have extensively covered LLM\narchitecture, training, and general applications, we uniquely focus on their\nspecific utility for data annotation. This survey contributes to three core\naspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment,\nand LLM-Generated Annotations Utilization. Furthermore, this survey includes an\nin-depth taxonomy of data types that LLMs can annotate, a comprehensive review\nof learning strategies for models utilizing LLM-generated annotations, and a\ndetailed discussion of the primary challenges and limitations associated with\nusing LLMs for data annotation. Serving as a key guide, this survey aims to\nassist researchers and practitioners in exploring the potential of the latest\nLLMs for data annotation, thereby fostering future advancements in this\ncritical field.\n","authors":["Zhen Tan","Dawei Li","Song Wang","Alimohammad Beigi","Bohan Jiang","Amrita Bhattacharjee","Mansooreh Karami","Jundong Li","Lu Cheng","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2402.13446v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16223v1","updated":"2024-06-23T21:32:15Z","published":"2024-06-23T21:32:15Z","title":"Continuous Output Personality Detection Models via Mixed Strategy\n  Training","summary":"  The traditional personality models only yield binary results. This paper\npresents a novel approach for training personality detection models that\nproduce continuous output values, using mixed strategies. By leveraging the\nPANDORA dataset, which includes extensive personality labeling of Reddit\ncomments, we developed models that predict the Big Five personality traits with\nhigh accuracy. Our approach involves fine-tuning a RoBERTa-base model with\nvarious strategies such as Multi-Layer Perceptron (MLP) integration, and\nhyperparameter tuning. The results demonstrate that our models significantly\noutperform traditional binary classification methods, offering precise\ncontinuous outputs for personality traits, thus enhancing applications in AI,\npsychology, human resources, marketing and health care fields.\n","authors":["Rong Wang","Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2406.16223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07115v2","updated":"2024-06-23T19:53:33Z","published":"2024-01-13T16:41:40Z","title":"Open Models, Closed Minds? On Agents Capabilities in Mimicking Human\n  Personalities through Open Large Language Models","summary":"  The emergence of unveiling human-like behaviors in Large Language Models\n(LLMs) has led to a closer connection between NLP and human psychology.\nScholars have been studying the inherent personalities exhibited by LLMs and\nattempting to incorporate human traits and behaviors into them. However, these\nefforts have primarily focused on commercially-licensed LLMs, neglecting the\nwidespread use and notable advancements seen in Open LLMs. This work aims to\naddress this gap by employing a set of 12 LLM Agents based on the most\nrepresentative Open models and subject them to a series of assessments\nconcerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five\nInventory (BFI) test. Our approach involves evaluating the intrinsic\npersonality traits of Open LLM agents and determining the extent to which these\nagents can mimic human personalities when conditioned by specific personalities\nand roles. Our findings unveil that $(i)$ each Open LLM agent showcases\ndistinct human personalities; $(ii)$ personality-conditioned prompting produces\nvarying effects on the agents, with only few successfully mirroring the imposed\npersonality, while most of them being ``closed-minded'' (i.e., they retain\ntheir intrinsic traits); and $(iii)$ combining role and personality\nconditioning can enhance the agents' ability to mimic human personalities. Our\nwork represents a step up in understanding the dense relationship between NLP\nand human psychology through the lens of Open LLMs.\n","authors":["Lucio La Cava","Andrea Tagarelli"],"pdf_url":"https://arxiv.org/pdf/2401.07115v2.pdf","comment":"Enhanced methodology and evaluation based on BFI in addition to MBTI,\n  with expanded set of LLM agents. Author list changed w.r.t. the previous\n  version (v1), see Acknowledgements"},{"id":"http://arxiv.org/abs/2406.16203v1","updated":"2024-06-23T19:49:10Z","published":"2024-06-23T19:49:10Z","title":"LLMs' Classification Performance is Overclaimed","summary":"  In many classification tasks designed for AI or human to solve, gold labels\nare typically included within the label space by default, often posed as \"which\nof the following is correct?\" This standard setup has traditionally highlighted\nthe strong performance of advanced AI, particularly top-performing Large\nLanguage Models (LLMs), in routine classification tasks. However, when the gold\nlabel is intentionally excluded from the label space, it becomes evident that\nLLMs still attempt to select from the available label candidates, even when\nnone are correct. This raises a pivotal question: Do LLMs truly demonstrate\ntheir intelligence in understanding the essence of classification tasks?\n  In this study, we evaluate both closed-source and open-source LLMs across\nrepresentative classification tasks, arguing that the perceived performance of\nLLMs is overstated due to their inability to exhibit the expected comprehension\nof the task. This paper makes a threefold contribution: i) To our knowledge,\nthis is the first work to identify the limitations of LLMs in classification\ntasks when gold labels are absent. We define this task as Classify-w/o-Gold and\npropose it as a new testbed for LLMs. ii) We introduce a benchmark, Know-No,\ncomprising two existing classification tasks and one new task, to evaluate\nClassify-w/o-Gold. iii) This work defines and advocates for a new evaluation\nmetric, OmniAccuracy, which assesses LLMs' performance in classification tasks\nboth when gold labels are present and absent.\n","authors":["Hanzi Xu","Renze Lou","Jiangshu Du","Vahid Mahzoon","Elmira Talebianaraki","Zhuoan Zhou","Elizabeth Garrison","Slobodan Vucetic","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2406.16203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16201v1","updated":"2024-06-23T19:40:11Z","published":"2024-06-23T19:40:11Z","title":"Blind Baselines Beat Membership Inference Attacks for Foundation Models","summary":"  Membership inference (MI) attacks try to determine if a data sample was used\nto train a machine learning model. For foundation models trained on unknown Web\ndata, MI attacks can be used to detect copyrighted training materials, measure\ntest set contamination, or audit machine unlearning. Unfortunately, we find\nthat evaluations of MI attacks for foundation models are flawed, because they\nsample members and non-members from different distributions. For 8 published MI\nevaluation datasets, we show that blind attacks -- that distinguish the member\nand non-member distributions without looking at any trained model -- outperform\nstate-of-the-art MI attacks. Existing evaluations thus tell us nothing about\nmembership leakage of a foundation model's training data.\n","authors":["Debeshee Das","Jie Zhang","Florian Tramèr"],"pdf_url":"https://arxiv.org/pdf/2406.16201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14092v2","updated":"2024-06-23T18:30:26Z","published":"2024-05-23T01:43:45Z","title":"Large Language Models Can Self-Correct with Minimal Effort","summary":"  Intrinsic self-correct was a method that instructed large language models\n(LLMs) to verify and correct their responses without external feedback.\nUnfortunately, the study concluded that the LLMs could not self-correct\nreasoning yet. We find that a simple yet effective verification method can\nunleash inherent capabilities of the LLMs. That is to mask a key condition in\nthe question, add the current response to construct a verification question,\nand predict the condition to verify the response. The condition can be an\nentity in an open-domain question or a numeric value in a math question, which\nrequires minimal effort (via prompting) to identify. We propose an iterative\nverify-then-correct framework to progressively identify and correct (probably)\nfalse responses, named ProCo. We conduct experiments on three reasoning tasks.\nOn average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields $+6.8$ exact\nmatch on four open-domain question answering datasets, $+14.1$ accuracy on\nthree arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense\nreasoning dataset, compared to Self-Correct.\n","authors":["Zhenyu Wu","Qingkai Zeng","Zhihan Zhang","Zhaoxuan Tan","Chao Shen","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2405.14092v2.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2404.02261v2","updated":"2024-06-23T18:21:01Z","published":"2024-04-02T19:34:22Z","title":"LLMs in the Loop: Leveraging Large Language Model Annotations for Active\n  Learning in Low-Resource Languages","summary":"  Low-resource languages face significant barriers in AI development due to\nlimited linguistic resources and expertise for data labeling, rendering them\nrare and costly. The scarcity of data and the absence of preexisting tools\nexacerbate these challenges, especially since these languages may not be\nadequately represented in various NLP datasets. To address this gap, we propose\nleveraging the potential of LLMs in the active learning loop for data\nannotation. Initially, we conduct evaluations to assess inter-annotator\nagreement and consistency, facilitating the selection of a suitable LLM\nannotator. The chosen annotator is then integrated into a training loop for a\nclassifier using an active learning paradigm, minimizing the amount of queried\ndata required. Empirical evaluations, notably employing GPT-4-Turbo,\ndemonstrate near-state-of-the-art performance with significantly reduced data\nrequirements, as indicated by estimated potential cost savings of at least\n42.45 times compared to human annotation. Our proposed solution shows promising\npotential to substantially reduce both the monetary and computational costs\nassociated with automation in low-resource settings. By bridging the gap\nbetween low-resource languages and AI, this approach fosters broader inclusion\nand shows the potential to enable automation across diverse linguistic\nlandscapes.\n","authors":["Nataliia Kholodna","Sahib Julka","Mohammad Khodadadi","Muhammed Nurullah Gumus","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2404.02261v2.pdf","comment":"20 pages, 6 tables. The source code related to this paper is\n  available at https://github.com/mkandai/llms-in-the-loop. This paper has been\n  accepted for publication at ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2406.16176v1","updated":"2024-06-23T18:01:56Z","published":"2024-06-23T18:01:56Z","title":"GraphEval2000: Benchmarking and Improving Large Language Models on Graph\n  Datasets","summary":"  Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP), demonstrating significant capabilities in processing\nand understanding text data. However, recent studies have identified\nlimitations in LLMs' ability to reason about graph-structured data. To address\nthis gap, we introduce GraphEval2000, the first comprehensive graph dataset,\ncomprising 40 graph data structure problems along with 2000 test cases.\nAdditionally, we introduce an evaluation framework based on GraphEval2000,\ndesigned to assess the graph reasoning abilities of LLMs through coding\nchallenges. Our dataset categorizes test cases into four primary and four\nsub-categories, ensuring a comprehensive evaluation. We evaluate eight popular\nLLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of\ndirected graphs compared to undirected ones. While private LLMs consistently\noutperform open-source models, the performance gap is narrowing. Furthermore,\nto improve the usability of our evaluation framework, we propose Structured\nSymbolic Decomposition (SSD), an instruction-based method designed to enhance\nLLM performance on GraphEval2000. Results show that SSD improves the\nperformance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an\nincrease of 11.11\\%, 33.37\\%, and 33.37\\%, respectively.\n","authors":["Qiming Wu","Zichen Chen","Will Corcoran","Misha Sra","Ambuj K. Singh"],"pdf_url":"https://arxiv.org/pdf/2406.16176v1.pdf","comment":"Submitted to NeurIPs 2024 Dataset and Benchmark track, under review"},{"id":"http://arxiv.org/abs/2406.16167v1","updated":"2024-06-23T17:18:19Z","published":"2024-06-23T17:18:19Z","title":"FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy\n  in Large Language Models","summary":"  We present a novel extension to Retrieval Augmented Generation with the goal\nof mitigating factual inaccuracies in the output of large language models.\nSpecifically, our method draws on the cognitive linguistic theory of frame\nsemantics for the indexing and retrieval of factual information relevant to\nhelping large language models answer queries. We conduct experiments to\ndemonstrate the effectiveness of this method both in terms of retrieval\neffectiveness and in terms of the relevance of the frames and frame relations\nautomatically generated. Our results show that this novel mechanism of Frame\nSemantic-based retrieval, designed to improve Retrieval Augmented Generation\n(FS-RAG), is effective and offers potential for providing data-driven insights\ninto frame semantics theory. We provide open access to our program code and\nprompts.\n","authors":["Harish Tayyar Madabushi"],"pdf_url":"https://arxiv.org/pdf/2406.16167v1.pdf","comment":"program code and prompts available at\n  https://github.com/H-TayyarMadabushi/A-Frame-Semantics-based-approach-for-Improved-Factual-Accuracy-in-Large-Language-Models"},{"id":"http://arxiv.org/abs/2406.13990v2","updated":"2024-06-23T16:46:00Z","published":"2024-06-20T04:35:59Z","title":"Inference-Time Decontamination: Reusing Leaked Benchmarks for Large\n  Language Model Evaluation","summary":"  The training process of large language models (LLMs) often involves varying\ndegrees of test data contamination. Although current LLMs are achieving\nincreasingly better performance on various benchmarks, their performance in\npractical applications does not always match their benchmark results. Leakage\nof benchmarks can prevent the accurate assessment of LLMs' true performance.\nHowever, constructing new benchmarks is costly, labor-intensive and still\ncarries the risk of leakage. Therefore, in this paper, we ask the question, Can\nwe reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time\nDecontamination (ITD) to address this issue by detecting and rewriting leaked\nsamples without altering their difficulties. ITD can mitigate performance\ninflation caused by memorizing leaked benchmarks. Our proof-of-concept\nexperiments demonstrate that ITD reduces inflated accuracy by 22.9% on GSM8K\nand 19.0% on MMLU. On MMLU, using Inference-time Decontamination can lead to a\ndecrease in the results of Phi3 and Mistral by 6.7% and 3.6% respectively. We\nhope that ITD can provide more truthful evaluation results for large language\nmodels.\n","authors":["Qin Zhu","Qingyuan Cheng","Runyu Peng","Xiaonan Li","Tengxiao Liu","Ru Peng","Xipeng Qiu","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.13990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16152v1","updated":"2024-06-23T16:26:27Z","published":"2024-06-23T16:26:27Z","title":"Towards Region-aware Bias Evaluation Metrics","summary":"  When exposed to human-generated data, language models are known to learn and\namplify societal biases. While previous works introduced benchmarks that can be\nused to assess the bias in these models, they rely on assumptions that may not\nbe universally true. For instance, a gender bias dimension commonly used by\nthese metrics is that of family--career, but this may not be the only common\nbias in certain regions of the world. In this paper, we identify topical\ndifferences in gender bias across different regions and propose a region-aware\nbottom-up approach for bias assessment. Our proposed approach uses\ngender-aligned topics for a given region and identifies gender bias dimensions\nin the form of topic pairs that are likely to capture gender societal biases.\nSeveral of our proposed bias topic pairs are on par with human perception of\ngender biases in these regions in comparison to the existing ones, and we also\nidentify new pairs that are more aligned than the existing ones. In addition,\nwe use our region-aware bias topic pairs in a Word Embedding Association Test\n(WEAT)-based evaluation metric to test for gender biases across different\nregions in different data domains. We also find that LLMs have a higher\nalignment to bias pairs for highly-represented regions showing the importance\nof region-aware bias evaluation metric.\n","authors":["Angana Borah","Aparna Garimella","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2406.16152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01574v4","updated":"2024-06-23T15:57:16Z","published":"2024-06-03T17:53:00Z","title":"MMLU-Pro: A More Robust and Challenging Multi-Task Language\n  Understanding Benchmark","summary":"  In the age of large-scale language models, benchmarks like the Massive\nMultitask Language Understanding (MMLU) have been pivotal in pushing the\nboundaries of what AI can achieve in language comprehension and reasoning\nacross diverse domains. However, as models continue to improve, their\nperformance on these benchmarks has begun to plateau, making it increasingly\ndifficult to discern differences in model capabilities. This paper introduces\nMMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven\nMMLU benchmark by integrating more challenging, reasoning-focused questions and\nexpanding the choice set from four to ten options. Additionally, MMLU-Pro\neliminates the trivial and noisy questions in MMLU. Our experimental results\nshow that MMLU-Pro not only raises the challenge, causing a significant drop in\naccuracy by 16% to 33% compared to MMLU but also demonstrates greater stability\nunder varying prompts. With 24 different prompt styles tested, the sensitivity\nof model scores to prompt variations decreased from 4-5% in MMLU to just 2% in\nMMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)\nreasoning achieved better performance on MMLU-Pro compared to direct answering,\nwhich is in stark contrast to the findings on the original MMLU, indicating\nthat MMLU-Pro includes more complex reasoning questions. Our assessments\nconfirm that MMLU-Pro is a more discriminative benchmark to better track\nprogress in the field.\n","authors":["Yubo Wang","Xueguang Ma","Ge Zhang","Yuansheng Ni","Abhranil Chandra","Shiguang Guo","Weiming Ren","Aaran Arulraj","Xuan He","Ziyan Jiang","Tianle Li","Max Ku","Kai Wang","Alex Zhuang","Rongqi Fan","Xiang Yue","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.01574v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14932v3","updated":"2024-06-23T15:50:48Z","published":"2024-03-22T03:23:58Z","title":"Extending Token Computation for LLM Reasoning","summary":"  Large Language Models (LLMs) are pivotal in advancing natural language\nprocessing but often struggle with complex reasoning tasks due to inefficient\nattention distributions. In this paper, we explore the effect of increased\ncomputed tokens on LLM performance and introduce a novel method for extending\ncomputed tokens in the Chain-of-Thought (CoT) process, utilizing attention\nmechanism optimization. By fine-tuning an LLM on a domain-specific, highly\nstructured dataset, we analyze attention patterns across layers, identifying\ninefficiencies caused by non-semantic tokens with outlier high attention\nscores. To address this, we propose an algorithm that emulates early layer\nattention patterns across downstream layers to re-balance skewed attention\ndistributions and enhance knowledge abstraction. Our findings demonstrate that\nour approach not only facilitates a deeper understanding of the internal\ndynamics of LLMs but also significantly improves their reasoning capabilities,\nparticularly in non-STEM domains. Our study lays the groundwork for further\ninnovations in LLM design, aiming to create more powerful, versatile, and\nresponsible models capable of tackling a broad range of real-world\napplications.\n","authors":["Bingli Liao","Danilo Vasconcellos Vargas"],"pdf_url":"https://arxiv.org/pdf/2403.14932v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16144v1","updated":"2024-06-23T15:50:22Z","published":"2024-06-23T15:50:22Z","title":"Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step","summary":"  Current research found the issue of Early Answering in large language models\n(LLMs), where the models already have an answer before generating the\nChain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary\ndependency between the predicted answer and the reasoning process.\nConsequently, two important questions arise: (1) Is CoT still necessary if the\nmodel already has an answer? (2) Can the correctness of the answer serve as\nvalid evidence for the correctness of CoT? To address these questions, we\npropose a method, namely Chain-of-Probe (CoP), to probe changes in the mind\nduring the model's reasoning. The probing results show that in a significant\nnumber of question-answer cases, CoT appears to be unnecessary, and this\nnecessity correlates with the simplicity of the task, defined by reasoning\nsteps required. Furthermore, by analyzing patterns in mind change, we examine\nthe correctness of the model's reasoning. Our validation reveals that many\nresponses, although correct in their final answer, contain errors in their\nreasoning process. To this end, we propose a strategic approach based on CoP to\nprioritize answers with correct reasoning among multiple candidates, thereby\nbolstering the reliability of the model's reasoning.\n","authors":["Zezhong Wang","Xingshan Zeng","Weiwen Liu","Yufei Wang","Liangyou Li","Yasheng Wang","Lifeng Shang","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2406.16144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16135v1","updated":"2024-06-23T15:15:17Z","published":"2024-06-23T15:15:17Z","title":"Crosslingual Capabilities and Knowledge Barriers in Multilingual Large\n  Language Models","summary":"  Large language models (LLMs) are typically multilingual due to pretraining on\ndiverse multilingual corpora. But can these models relate corresponding\nconcepts across languages, effectively being crosslingual? This study evaluates\nsix state-of-the-art LLMs on inherently crosslingual tasks. We observe that\nwhile these models show promising surface-level crosslingual abilities on\nmachine translation and embedding space analyses, they struggle with deeper\ncrosslingual knowledge transfer, revealing a crosslingual knowledge barrier in\nboth general (MMLU benchmark) and domain-specific (Harry Potter quiz) contexts.\nWe observe that simple inference-time mitigation methods offer only limited\nimprovement. On the other hand, we propose fine-tuning of LLMs on\nmixed-language data, which effectively reduces these gaps, even when using\nout-of-domain datasets like WikiText. Our findings suggest the need for\nexplicit optimization to unlock the full crosslingual potential of LLMs. Our\ncode is publicly available at\nhttps://github.com/google-research/crosslingual-knowledge-barriers.\n","authors":["Lynn Chua","Badih Ghazi","Yangsibo Huang","Pritish Kamath","Ravi Kumar","Pasin Manurangsi","Amer Sinha","Chulin Xie","Chiyuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.16135v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2405.04614v2","updated":"2024-06-23T22:05:46Z","published":"2024-05-07T18:58:32Z","title":"Multi-Margin Loss: Proposal and Application in Recommender Systems","summary":"  Recommender systems guide users through vast amounts of information by\nsuggesting items based on their predicted preferences. Collaborative\nfiltering-based deep learning techniques have regained popularity due to their\nsimplicity, using only user-item interactions. Typically, these systems consist\nof three main components: an interaction module, a loss function, and a\nnegative sampling strategy. Initially, researchers focused on enhancing\nperformance by developing complex interaction modules with techniques like\nmulti-layer perceptrons, transformers, or graph neural networks. However, there\nhas been a recent shift toward refining loss functions and negative sampling\nstrategies. This shift has increased interest in contrastive learning, which\npulls similar pairs closer while pushing dissimilar ones apart. Contrastive\nlearning involves key practices such as heavy data augmentation, large batch\nsizes, and hard-negative sampling, but these also bring challenges like high\nmemory demands and under-utilization of some negative samples. The proposed\nMulti-Margin Loss (MML) addresses these challenges by introducing multiple\nmargins and varying weights for negative samples. MML efficiently utilizes not\nonly the hardest negatives but also other non-trivial negatives, offering a\nsimpler yet effective loss function that outperforms more complex methods,\nespecially when resources are limited. Experiments on two well-known datasets\nshowed MML achieved up to a 20\\% performance improvement compared to a baseline\ncontrastive loss function with fewer negative samples.\n","authors":["Makbule Gulcin Ozsoy"],"pdf_url":"https://arxiv.org/pdf/2405.04614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16212v1","updated":"2024-06-23T20:33:49Z","published":"2024-06-23T20:33:49Z","title":"A Mechanism for Optimizing Media Recommender Systems","summary":"  A mechanism is described that addresses the fundamental trade off between\nmedia producers who want to increase reach and consumers who provide attention\nbased on the rate of utility received, and where overreach negatively impacts\nthat rate. An optimal solution can be achieved when the media source considers\nthe impact of overreach in a cost function used in determining the optimal\ndistribution of content to maximize individual consumer utility and\nparticipation. The result is a Nash equilibrium between producer and consumer\nthat is also Pareto efficient. Comparison with the literature on Recommender\nsystems highlights the advantages of the mechanism.The review suggests\nadvancements over that literature including identifying an optimal content\nvolume for the consumer and improvements for handling multiple objectives A\npractical algorithm to generate the optimal distribution for each consumer is\nprovided.\n","authors":["Brian McFadden"],"pdf_url":"https://arxiv.org/pdf/2406.16212v1.pdf","comment":"Main Paper: 20 pages, Appendix with proofs and additional material:\n  26 pages"},{"id":"http://arxiv.org/abs/2404.02261v2","updated":"2024-06-23T18:21:01Z","published":"2024-04-02T19:34:22Z","title":"LLMs in the Loop: Leveraging Large Language Model Annotations for Active\n  Learning in Low-Resource Languages","summary":"  Low-resource languages face significant barriers in AI development due to\nlimited linguistic resources and expertise for data labeling, rendering them\nrare and costly. The scarcity of data and the absence of preexisting tools\nexacerbate these challenges, especially since these languages may not be\nadequately represented in various NLP datasets. To address this gap, we propose\nleveraging the potential of LLMs in the active learning loop for data\nannotation. Initially, we conduct evaluations to assess inter-annotator\nagreement and consistency, facilitating the selection of a suitable LLM\nannotator. The chosen annotator is then integrated into a training loop for a\nclassifier using an active learning paradigm, minimizing the amount of queried\ndata required. Empirical evaluations, notably employing GPT-4-Turbo,\ndemonstrate near-state-of-the-art performance with significantly reduced data\nrequirements, as indicated by estimated potential cost savings of at least\n42.45 times compared to human annotation. Our proposed solution shows promising\npotential to substantially reduce both the monetary and computational costs\nassociated with automation in low-resource settings. By bridging the gap\nbetween low-resource languages and AI, this approach fosters broader inclusion\nand shows the potential to enable automation across diverse linguistic\nlandscapes.\n","authors":["Nataliia Kholodna","Sahib Julka","Mohammad Khodadadi","Muhammed Nurullah Gumus","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2404.02261v2.pdf","comment":"20 pages, 6 tables. The source code related to this paper is\n  available at https://github.com/mkandai/llms-in-the-loop. This paper has been\n  accepted for publication at ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2406.16170v1","updated":"2024-06-23T17:24:07Z","published":"2024-06-23T17:24:07Z","title":"SimCE: Simplifying Cross-Entropy Loss for Collaborative Filtering","summary":"  The learning objective is integral to collaborative filtering systems, where\nthe Bayesian Personalized Ranking (BPR) loss is widely used for learning\ninformative backbones. However, BPR often experiences slow convergence and\nsuboptimal local optima, partially because it only considers one negative item\nfor each positive item, neglecting the potential impacts of other unobserved\nitems. To address this issue, the recently proposed Sampled Softmax\nCross-Entropy (SSM) compares one positive sample with multiple negative\nsamples, leading to better performance. Our comprehensive experiments confirm\nthat recommender systems consistently benefit from multiple negative samples\nduring training. Furthermore, we introduce a \\underline{Sim}plified Sampled\nSoftmax \\underline{C}ross-\\underline{E}ntropy Loss (SimCE), which simplifies\nthe SSM using its upper bound. Our validation on 12 benchmark datasets, using\nboth MF and LightGCN backbones, shows that SimCE significantly outperforms both\nBPR and SSM.\n","authors":["Xiaodong Yang","Huiyuan Chen","Yuchen Yan","Yuxin Tang","Yuying Zhao","Eric Xu","Yiwei Cai","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2406.16170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16106v1","updated":"2024-06-23T13:40:50Z","published":"2024-06-23T13:40:50Z","title":"Evaluating Ensemble Methods for News Recommender Systems","summary":"  News recommendation is crucial for facilitating individuals' access to\narticles, particularly amid the increasingly digital landscape of news\nconsumption. Consequently, extensive research is dedicated to News Recommender\nSystems (NRS) with increasingly sophisticated algorithms. Despite this\nsustained scholarly inquiry, there exists a notable research gap regarding the\npotential synergy achievable by amalgamating these algorithms to yield superior\noutcomes. This paper endeavours to address this gap by demonstrating how\nensemble methods can be used to combine many diverse state-of-the-art\nalgorithms to achieve superior results on the Microsoft News dataset (MIND).\nAdditionally, we identify scenarios where ensemble methods fail to improve\nresults and offer explanations for this occurrence. Our findings demonstrate\nthat a combination of NRS algorithms can outperform individual algorithms,\nprovided that the base learners are sufficiently diverse, with improvements of\nup to 5\\% observed for an ensemble consisting of a content-based BERT approach\nand the collaborative filtering LSTUR algorithm. Additionally, our results\ndemonstrate the absence of any improvement when combining insufficiently\ndistinct methods. These findings provide insight into successful approaches of\nensemble methods in NRS and advocates for the development of better systems\nthrough appropriate ensemble solutions.\n","authors":["Alexander Gray","Noorhan Abbas"],"pdf_url":"https://arxiv.org/pdf/2406.16106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16048v1","updated":"2024-06-23T08:24:08Z","published":"2024-06-23T08:24:08Z","title":"Evaluating D-MERIT of Partial-annotation on Information Retrieval","summary":"  Retrieval models are often evaluated on partially-annotated datasets. Each\nquery is mapped to a few relevant texts and the remaining corpus is assumed to\nbe irrelevant. As a result, models that successfully retrieve false negatives\nare punished in evaluation. Unfortunately, completely annotating all texts for\nevery query is not resource efficient. In this work, we show that using\npartially-annotated datasets in evaluation can paint a distorted picture. We\ncurate D-MERIT, a passage retrieval evaluation set from Wikipedia, aspiring to\ncontain all relevant passages for each query. Queries describe a group (e.g.,\n``journals about linguistics'') and relevant passages are evidence that\nentities belong to the group (e.g., a passage indicating that Language is a\njournal about linguistics). We show that evaluating on a dataset containing\nannotations for only a subset of the relevant passages might result in\nmisleading ranking of the retrieval systems and that as more relevant texts are\nincluded in the evaluation set, the rankings converge. We propose our dataset\nas a resource for evaluation and our study as a recommendation for balance\nbetween resource-efficiency and reliable evaluation when annotating evaluation\nsets for text retrieval.\n","authors":["Royi Rassin","Yaron Fairstein","Oren Kalinsky","Guy Kushilevitz","Nachshon Cohen","Alexander Libov","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2406.16048v1.pdf","comment":"Our dataset can be downloaded from https://D-MERIT.github.io"},{"id":"http://arxiv.org/abs/2404.10496v4","updated":"2024-06-23T06:23:12Z","published":"2024-04-16T12:10:01Z","title":"Spiral of Silence: How is Large Language Model Killing Information\n  Retrieval? -- A Case Study on Open Domain Question Answering","summary":"  The practice of Retrieval-Augmented Generation (RAG), which integrates Large\nLanguage Models (LLMs) with retrieval systems, has become increasingly\nprevalent. However, the repercussions of LLM-derived content infiltrating the\nweb and influencing the retrieval-generation feedback loop are largely\nuncharted territories. In this study, we construct and iteratively run a\nsimulation pipeline to deeply investigate the short-term and long-term effects\nof LLM text on RAG systems. Taking the trending Open Domain Question Answering\n(ODQA) task as a point of entry, our findings reveal a potential digital\n\"Spiral of Silence\" effect, with LLM-generated text consistently outperforming\nhuman-authored content in search rankings, thereby diminishing the presence and\nimpact of human contributions online. This trend risks creating an imbalanced\ninformation ecosystem, where the unchecked proliferation of erroneous\nLLM-generated content may result in the marginalization of accurate\ninformation. We urge the academic community to take heed of this potential\nissue, ensuring a diverse and authentic digital information landscape.\n","authors":["Xiaoyang Chen","Ben He","Hongyu Lin","Xianpei Han","Tianshu Wang","Boxi Cao","Le Sun","Yingfei Sun"],"pdf_url":"https://arxiv.org/pdf/2404.10496v4.pdf","comment":"Accepted to ACL2024"},{"id":"http://arxiv.org/abs/2406.05898v2","updated":"2024-06-23T05:43:41Z","published":"2024-06-09T19:35:20Z","title":"Async Learned User Embeddings for Ads Delivery Optimization","summary":"  In recommendation systems, high-quality user embeddings can capture subtle\npreferences, enable precise similarity calculations, and adapt to changing\npreferences over time to maintain relevance. The effectiveness of\nrecommendation systems depends on the quality of user embedding. We propose to\nasynchronously learn high fidelity user embeddings for billions of users each\nday from sequence based multimodal user activities through a Transformer-like\nlarge scale feature learning module. The async learned user representations\nembeddings (ALURE) are further converted to user similarity graphs through\ngraph learning and then combined with user realtime activities to retrieval\nhighly related ads candidates for the ads delivery system. Our method shows\nsignificant gains in both offline and online experiments.\n","authors":["Mingwei Tang","Meng Liu","Hong Li","Junjie Yang","Chenglin Wei","Boyang Li","Dai Li","Rengan Xu","Yifan Xu","Zehua Zhang","Xiangyu Wang","Linfeng Liu","Yuelei Xie","Chengye Liu","Labib Fawaz","Li Li","Hongnan Wang","Bill Zhu","Sri Reddy"],"pdf_url":"https://arxiv.org/pdf/2406.05898v2.pdf","comment":"Accepted by workshop on Multimodal Representation and Retrieval at\n  SIGIR 2024, Washington DC"},{"id":"http://arxiv.org/abs/2406.16013v1","updated":"2024-06-23T05:02:21Z","published":"2024-06-23T05:02:21Z","title":"Database-Augmented Query Representation for Information Retrieval","summary":"  Information retrieval models that aim to search for the documents relevant to\nthe given query have shown many successes, which have been applied to diverse\ntasks. However, the query provided by the user is oftentimes very short, which\nchallenges the retrievers to correctly fetch relevant documents. To tackle\nthis, existing studies have proposed expanding the query with a couple of\nadditional (user-related) features related to the query. Yet, they may be\nsuboptimal to effectively augment the query, though there is plenty of\ninformation available to augment it in a relational database. Motivated by\nthis, we present a novel retrieval framework called Database-Augmented Query\nrepresentation (DAQu), which augments the original query with various\n(query-related) metadata across multiple tables. In addition, as the number of\nfeatures in the metadata can be very large and there is no order among them, we\nencode them with our graph-based set encoding strategy, which considers\nhierarchies of features in the database without order. We validate DAQu in\ndiverse retrieval scenarios that can incorporate metadata from the relational\ndatabase, demonstrating that ours significantly enhances overall retrieval\nperformance, compared to existing query augmentation methods.\n","authors":["Soyeong Jeong","Jinheon Baek","Sukmin Cho","Sung Ju Hwang","Jong C. Park"],"pdf_url":"https://arxiv.org/pdf/2406.16013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13375v2","updated":"2024-06-23T04:09:03Z","published":"2024-04-20T13:28:34Z","title":"Beyond Collaborative Filtering: A Relook at Task Formulation in\n  Recommender Systems","summary":"  Recommender Systems (RecSys) have become indispensable in numerous\napplications, profoundly influencing our everyday experiences. Despite their\npractical significance, academic research in RecSys often abstracts the\nformulation of research tasks from real-world contexts, aiming for a clean\nproblem formulation and more generalizable findings. However, it is observed\nthat there is a lack of collective understanding in RecSys academic research.\nThe root of this issue may lie in the simplification of research task\ndefinitions, and an overemphasis on modeling the decision outcomes rather than\nthe decision-making process. That is, we often conceptualize RecSys as the task\nof predicting missing values in a static user-item interaction matrix, rather\nthan predicting a user's decision on the next interaction within a dynamic,\nchanging, and application-specific context. There exists a mismatch between the\ninputs accessible to a model and the information available to users during\ntheir decision-making process, yet the model is tasked to predict users'\ndecisions. While collaborative filtering is effective in learning general\npreferences from historical records, it is crucial to also consider the dynamic\ncontextual factors in practical settings. Defining research tasks based on\napplication scenarios using domain-specific datasets may lead to more\ninsightful findings. Accordingly, viable solutions and effective evaluations\ncan emerge for different application scenarios.\n","authors":["Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2404.13375v2.pdf","comment":"Published in ACM SIGWEB Newsletter, Spring 2024:\n  https://dl.acm.org/doi/10.1145/3663752.3663756"},{"id":"http://arxiv.org/abs/2404.17313v2","updated":"2024-06-23T03:13:57Z","published":"2024-04-26T10:45:34Z","title":"Towards Group-aware Search Success","summary":"  Traditional measures of search success often overlook the varying information\nneeds of different demographic groups. To address this gap, we introduce a\nnovel metric, named Group-aware Search Success (GA-SS). GA-SS redefines search\nsuccess to ensure that all demographic groups achieve satisfaction from search\noutcomes. We introduce a comprehensive mathematical framework to calculate\nGA-SS, incorporating both static and stochastic ranking policies and\nintegrating user browsing models for a more accurate assessment. In addition,\nwe have proposed Group-aware Most Popular Completion (gMPC) ranking model to\naccount for demographic variances in user intent, aligning more closely with\nthe diverse needs of all user groups. We empirically validate our metric and\napproach with two real-world datasets: one focusing on query auto-completion\nand the other on movie recommendations, where the results highlight the impact\nof stochasticity and the complex interplay among various search success\nmetrics. Our findings advocate for a more inclusive approach in measuring\nsearch success, as well as inspiring future investigations into the quality of\nservice of search.\n","authors":["Haolun Wu","Bhaskar Mitra","Nick Craswell"],"pdf_url":"https://arxiv.org/pdf/2404.17313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15983v1","updated":"2024-06-23T02:24:50Z","published":"2024-06-23T02:24:50Z","title":"Learning k-Determinantal Point Processes for Personalized Ranking","summary":"  The key to personalized recommendation is to predict a personalized ranking\non a catalog of items by modeling the user's preferences. There are many\npersonalized ranking approaches for item recommendation from implicit feedback\nlike Bayesian Personalized Ranking (BPR) and listwise ranking. Despite these\nmethods have shown performance benefits, there are still limitations affecting\nrecommendation performance. First, none of them directly optimize ranking of\nsets, causing inadequate exploitation of correlations among multiple items.\nSecond, the diversity aspect of recommendations is insufficiently addressed\ncompared to relevance.\n  In this work, we present a new optimization criterion LkP based on set\nprobability comparison for personalized ranking that moves beyond traditional\nranking-based methods. It formalizes set-level relevance and diversity ranking\ncomparisons through a Determinantal Point Process (DPP) kernel decomposition.\nTo confer ranking interpretability to the DPP set probabilities and prioritize\nthe practicality of LkP, we condition the standard DPP on the cardinality k of\nthe DPP-distributed set, known as k-DPP, a less-explored extension of DPP. The\ngeneric stochastic gradient descent based technique can be directly applied to\noptimizing models that employ LkP. We implement LkP in the context of both\nMatrix Factorization (MF) and neural networks approaches, on three real-world\ndatasets, obtaining improved relevance and diversity performances. LkP is\nbroadly applicable, and when applied to existing recommendation models it also\nyields strong performance improvements, suggesting that LkP holds significant\nvalue to the field of recommender systems.\n","authors":["Yuli Liu","Christian Walder","Lexing Xie"],"pdf_url":"https://arxiv.org/pdf/2406.15983v1.pdf","comment":"14 pages, accepted at ICDE 2024 (40th IEEE International Conference\n  on Data Engineering)"}]},"2024-06-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.17764v1","updated":"2024-06-25T17:48:56Z","published":"2024-06-25T17:48:56Z","title":"BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context\n  Learning","summary":"  Large language models (LLMs) possess extensive parametric knowledge, but this\nknowledge is difficult to update with new information because retraining is\nvery expensive and infeasible for closed-source models. Knowledge editing (KE)\nhas emerged as a viable solution for updating the knowledge of LLMs without\ncompromising their overall performance. On-the-fly KE methods, inspired by\nin-context learning (ICL), have shown great promise and allow LLMs to be\ntreated as black boxes. In the past, KE was primarily employed in English\ncontexts, whereas the potential for cross-lingual KE in current English-centric\nLLMs has not been fully explored. To foster more research in this direction, we\nintroduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverse\nlanguages across three KE task types. We also propose a gradient-free KE method\ncalled Multilingual In-context Knowledge Editing (MIKE) and evaluate it on\nBMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in terms\nof reliability, generality, locality, and portability, offering valuable\ninsights and a framework for future research in cross-lingual KE. Our code and\ndata are publicly accessible via the anonymous repository at\nhttps://anonymous.4open.science/r/MIKE.\n","authors":["Ercong Nie","Bo Shao","Zifeng Ding","Mingyang Wang","Helmut Schmid","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2406.17764v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.17761v1","updated":"2024-06-25T17:45:26Z","published":"2024-06-25T17:45:26Z","title":"CaLMQA: Exploring culturally specific long-form question answering\n  across 23 languages","summary":"  Large language models (LLMs) are commonly used for long-form question\nanswering, which requires them to generate paragraph-length answers to complex\nquestions. While long-form QA has been well-studied in English via many\ndifferent datasets and evaluation metrics, this research has not been extended\nto cover most other languages. To bridge this gap, we introduce CaLMQA, a\ncollection of 2.6K complex questions spanning 23 languages, including\nunder-resourced, rarely-studied languages such as Fijian and Kirundi. Our\ndataset includes both naturally-occurring questions collected from community\nweb forums as well as questions written by native speakers, whom we hire for\nthis purpose. Our process yields diverse, complex questions that reflect\ncultural topics (e.g. traditions, laws, news) and the language usage of native\nspeakers. We conduct automatic evaluation across a suite of open- and\nclosed-source models using our novel metric CaLMScore, which detects incorrect\nlanguage and token repetitions in answers, and observe that the quality of\nLLM-generated answers degrades significantly for some low-resource languages.\nWe perform human evaluation on a subset of models and see that model\nperformance is significantly worse for culturally specific questions than for\nculturally agnostic questions. Our findings highlight the need for further\nresearch in LLM multilingual capabilities and non-English LFQA evaluation.\n","authors":["Shane Arora","Marzena Karpinska","Hung-Ting Chen","Ipsita Bhattacharjee","Mohit Iyyer","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2406.17761v1.pdf","comment":"39 pages, 16 figures. Code and data available at\n  https://github.com/2015aroras/CaLMQA"},{"id":"http://arxiv.org/abs/2406.06582v2","updated":"2024-06-25T17:44:00Z","published":"2024-06-04T20:08:25Z","title":"Discrete Multimodal Transformers with a Pretrained Large Language Model\n  for Mixed-Supervision Speech Processing","summary":"  Recent work on discrete speech tokenization has paved the way for models that\ncan seamlessly perform multiple tasks across modalities, e.g., speech\nrecognition, text to speech, speech to speech translation. Moreover, large\nlanguage models (LLMs) pretrained from vast text corpora contain rich\nlinguistic information that can improve accuracy in a variety of tasks. In this\npaper, we present a decoder-only Discrete Multimodal Language Model (DMLM),\nwhich can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and\nmodalities (text, speech, vision). We explore several critical aspects of\ndiscrete multi-modal models, including the loss function, weight\ninitialization, mixed training supervision, and codebook. Our results show that\nDMLM benefits significantly, across multiple tasks and datasets, from a\ncombination of supervised and unsupervised training. Moreover, for ASR, it\nbenefits from initializing DMLM from a pretrained LLM, and from a codebook\nderived from Whisper activations.\n","authors":["Viet Anh Trinh","Rosy Southwell","Yiwen Guan","Xinlu He","Zhiyong Wang","Jacob Whitehill"],"pdf_url":"https://arxiv.org/pdf/2406.06582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17755v1","updated":"2024-06-25T17:41:52Z","published":"2024-06-25T17:41:52Z","title":"Accelerating Clinical Evidence Synthesis with Large Language Models","summary":"  Automatic medical discovery by AI is a dream of many. One step toward that\ngoal is to create an AI model to understand clinical studies and synthesize\nclinical evidence from the literature. Clinical evidence synthesis currently\nrelies on systematic reviews of clinical trials and retrospective analyses from\nmedical literature. However, the rapid expansion of publications presents\nchallenges in efficiently identifying, summarizing, and updating evidence. We\nintroduce TrialMind, a generative AI-based pipeline for conducting medical\nsystematic reviews, encompassing study search, screening, and data extraction\nphases. We utilize large language models (LLMs) to drive each pipeline\ncomponent while incorporating human expert oversight to minimize errors. To\nfacilitate evaluation, we also create a benchmark dataset TrialReviewBench, a\ncustom dataset with 870 annotated clinical studies from 25 meta-analysis papers\nacross various medical treatments. Our results demonstrate that TrialMind\nsignificantly improves the literature review process, achieving high recall\nrates (0.897-1.000) in study searching from over 20 million PubMed studies and\noutperforming traditional language model embeddings-based methods in screening\n(Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpasses\ndirect GPT-4 performance in result extraction, with accuracy ranging from 0.65\nto 0.84. We also support clinical evidence synthesis in forest plots, as\nvalidated by eight human annotators who preferred TrialMind over the GPT-4\nbaseline with a winning rate of 62.5%-100% across the involved reviews. Our\nfindings suggest that an LLM-based clinical evidence synthesis approach, such\nas TrialMind, can enable reliable and high-quality clinical evidence synthesis\nto improve clinical research efficiency.\n","authors":["Zifeng Wang","Lang Cao","Benjamin Danek","Yichi Zhang","Qiao Jin","Zhiyong Lu","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2406.17755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17753v1","updated":"2024-06-25T17:40:47Z","published":"2024-06-25T17:40:47Z","title":"Measuring and Benchmarking Large Language Models' Capabilities to\n  Generate Persuasive Language","summary":"  We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive text - both when explicitly instructed to rewrite text\nto be more or less persuasive and when only instructed to paraphrase. To this\nend, we construct a new dataset, Persuasive-Pairs, of pairs each consisting of\na short text and of a text rewritten by an LLM to amplify or diminish\npersuasive language. We multi-annotate the pairs on a relative scale for\npersuasive language. This data is not only a valuable resource in itself, but\nwe also show that it can be used to train a regression model to predict a score\nof persuasive language between text pairs. This model can score and benchmark\nnew LLMs across domains, thereby facilitating the comparison of different LLMs.\nFinally, we discuss effects observed for different system prompts. Notably, we\nfind that different 'personas' in the system prompt of LLaMA3 change the\npersuasive language in the text substantially, even when only instructed to\nparaphrase. These findings underscore the importance of investigating\npersuasive language in LLM generated text.\n","authors":["Amalie Brogaard Pauli","Isabelle Augenstein","Ira Assent"],"pdf_url":"https://arxiv.org/pdf/2406.17753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17746v1","updated":"2024-06-25T17:32:16Z","published":"2024-06-25T17:32:16Z","title":"Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted\n  Phenomenon","summary":"  Memorization in language models is typically treated as a homogenous\nphenomenon, neglecting the specifics of the memorized data. We instead model\nmemorization as the effect of a set of complex factors that describe each\nsample and relate it to the model and corpus. To build intuition around these\nfactors, we break memorization down into a taxonomy: recitation of highly\nduplicated sequences, reconstruction of inherently predictable sequences, and\nrecollection of sequences that are neither. We demonstrate the usefulness of\nour taxonomy by using it to construct a predictive model for memorization. By\nanalyzing dependencies and inspecting the weights of the predictive model, we\nfind that different factors influence the likelihood of memorization\ndifferently depending on the taxonomic category.\n","authors":["USVSN Sai Prashanth","Alvin Deng","Kyle O'Brien","Jyothir S V","Mohammad Aflah Khan","Jaydeep Borkar","Christopher A. Choquette-Choo","Jacob Ray Fuehne","Stella Biderman","Tracy Ke","Katherine Lee","Naomi Saphra"],"pdf_url":"https://arxiv.org/pdf/2406.17746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17744v1","updated":"2024-06-25T17:29:52Z","published":"2024-06-25T17:29:52Z","title":"Following Length Constraints in Instructions","summary":"  Aligned instruction following models can better fulfill user requests than\ntheir unaligned counterparts. However, it has been shown that there is a length\nbias in evaluation of such models, and that training algorithms tend to exploit\nthis bias by learning longer responses. In this work we show how to train\nmodels that can be controlled at inference time with instructions containing\ndesired length constraints. Such models are superior in length instructed\nevaluations, outperforming standard instruction following models such as GPT4,\nLlama 3 and Mixtral.\n","authors":["Weizhe Yuan","Ilia Kulikov","Ping Yu","Kyunghyun Cho","Sainbayar Sukhbaatar","Jason Weston","Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2406.17744v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2406.17739v1","updated":"2024-06-25T17:25:02Z","published":"2024-06-25T17:25:02Z","title":"Find Parent then Label Children: A Two-stage Taxonomy Completion Method\n  with Pre-trained Language Model","summary":"  Taxonomies, which organize domain concepts into hierarchical structures, are\ncrucial for building knowledge systems and downstream applications. As domain\nknowledge evolves, taxonomies need to be continuously updated to include new\nconcepts. Previous approaches have mainly focused on adding concepts to the\nleaf nodes of the existing hierarchical tree, which does not fully utilize the\ntaxonomy's knowledge and is unable to update the original taxonomy structure\n(usually involving non-leaf nodes). In this paper, we propose a two-stage\nmethod called ATTEMPT for taxonomy completion. Our method inserts new concepts\ninto the correct position by finding a parent node and labeling child nodes.\nSpecifically, by combining local nodes with prompts to generate natural\nsentences, we take advantage of pre-trained language models for\nhypernym/hyponymy recognition. Experimental results on two public datasets\n(including six domains) show that ATTEMPT performs best on both taxonomy\ncompletion and extension tasks, surpassing existing methods.\n","authors":["Fei Xia","Yixuan Weng","Shizhu He","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.17739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17737v1","updated":"2024-06-25T17:24:07Z","published":"2024-06-25T17:24:07Z","title":"LLM Targeted Underperformance Disproportionately Impacts Vulnerable\n  Users","summary":"  While state-of-the-art Large Language Models (LLMs) have shown impressive\nperformance on many tasks, there has been extensive research on undesirable\nmodel behavior such as hallucinations and bias. In this work, we investigate\nhow the quality of LLM responses changes in terms of information accuracy,\ntruthfulness, and refusals depending on three user traits: English proficiency,\neducation level, and country of origin. We present extensive experimentation on\nthree state-of-the-art LLMs and two different datasets targeting truthfulness\nand factuality. Our findings suggest that undesirable behaviors in\nstate-of-the-art LLMs occur disproportionately more for users with lower\nEnglish proficiency, of lower education status, and originating from outside\nthe US, rendering these models unreliable sources of information towards their\nmost vulnerable users.\n","authors":["Elinor Poole-Dayan","Deb Roy","Jad Kabbara"],"pdf_url":"https://arxiv.org/pdf/2406.17737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00716v2","updated":"2024-06-25T17:23:22Z","published":"2024-04-25T15:51:06Z","title":"Large Language Models in Healthcare: A Comprehensive Benchmark","summary":"  The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and complex clinical tasks that are close to\nreal-world practice, i.e., referral QA, treatment recommendation,\nhospitalization (long document) summarization, patient education, pharmacology\nQA and drug interaction for emerging drugs. We conduct an extensive evaluation\nof twenty-two LLMs under both zero-shot and few-shot settings. Finally, we\ninvite medical experts to evaluate the clinical usefulness of LLMs.\n","authors":["Andrew Liu","Hongjian Zhou","Yining Hua","Omid Rohanian","Anshul Thakur","Lei Clifton","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2405.00716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15422v2","updated":"2024-06-25T17:02:10Z","published":"2024-02-23T16:32:28Z","title":"A Data-Centric Approach To Generate Faithful and High Quality Patient\n  Summaries with Large Language Models","summary":"  Patients often face difficulties in understanding their hospitalizations,\nwhile healthcare workers have limited resources to provide explanations. In\nthis work, we investigate the potential of large language models to generate\npatient summaries based on doctors' notes and study the effect of training data\non the faithfulness and quality of the generated summaries. To this end, we\nrelease (i) a rigorous labeling protocol for errors in medical texts and (ii) a\npublicly available dataset of annotated hallucinations in 100 doctor-written\nand 100 generated summaries. We show that fine-tuning on hallucination-free\ndata effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama\n2, while preserving relevant information. We observe a similar effect on GPT-4\n(0.70 to 0.40), when the few-shot examples are hallucination-free. We also\nconduct a qualitative evaluation using hallucination-free and improved training\ndata. We find that common quantitative metrics do not correlate well with\nfaithfulness and quality. Finally, we test GPT-4 for automatic hallucination\ndetection, which clearly outperforms common baselines.\n","authors":["Stefan Hegselmann","Shannon Zejiang Shen","Florian Gierse","Monica Agrawal","David Sontag","Xiaoyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.15422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17716v1","updated":"2024-06-25T16:58:19Z","published":"2024-06-25T16:58:19Z","title":"ViANLI: Adversarial Natural Language Inference for Vietnamese","summary":"  The development of Natural Language Processing (NLI) datasets and models has\nbeen inspired by innovations in annotation design. With the rapid development\nof machine learning models today, the performance of existing machine learning\nmodels has quickly reached state-of-the-art results on a variety of tasks\nrelated to natural language processing, including natural language inference\ntasks. By using a pre-trained model during the annotation process, it is\npossible to challenge current NLI models by having humans produce\npremise-hypothesis combinations that the machine model cannot correctly\npredict. To remain attractive and challenging in the research of natural\nlanguage inference for Vietnamese, in this paper, we introduce the adversarial\nNLI dataset to the NLP research community with the name ViANLI. This data set\ncontains more than 10K premise-hypothesis pairs and is built by a continuously\nadjusting process to obtain the most out of the patterns generated by the\nannotators. ViANLI dataset has brought many difficulties to many current SOTA\nmodels when the accuracy of the most powerful model on the test set only\nreached 48.4%. Additionally, the experimental results show that the models\ntrained on our dataset have significantly improved the results on other\nVietnamese NLI datasets.\n","authors":["Tin Van Huynh","Kiet Van Nguyen","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.17716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15193v3","updated":"2024-06-25T16:55:03Z","published":"2024-06-21T14:35:16Z","title":"Reward Steering with Evolutionary Heuristics for Decoding-time Alignment","summary":"  The widespread applicability and increasing omnipresence of LLMs have\ninstigated a need to align LLM responses to user and stakeholder preferences.\nMany preference optimization approaches have been proposed that fine-tune LLM\nparameters to achieve good alignment. However, such parameter tuning is known\nto interfere with model performance on many tasks. Moreover, keeping up with\nshifting user preferences is tricky in such a situation. Decoding-time\nalignment with reward model guidance solves these issues at the cost of\nincreased inference time. However, most of such methods fail to strike the\nright balance between exploration and exploitation of reward -- often due to\nthe conflated formulation of these two aspects - to give well-aligned\nresponses. To remedy this we decouple these two aspects and implement them in\nan evolutionary fashion: exploration is enforced by decoding from mutated\ninstructions and exploitation is represented as the periodic replacement of\npoorly-rewarded generations with well-rewarded ones. Empirical evidences\nindicate that this strategy outperforms many preference optimization and\ndecode-time alignment approaches on two widely accepted alignment benchmarks\nAlpacaEval 2 and MT-Bench. Our implementation will be available at:\nhttps://darwin-alignment.github.io.\n","authors":["Chia-Yu Hung","Navonil Majumder","Ambuj Mehrish","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2406.15193v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17706v1","updated":"2024-06-25T16:45:47Z","published":"2024-06-25T16:45:47Z","title":"FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model","summary":"  Large language models (LLMs) show amazing performance on many domain-specific\ntasks after fine-tuning with some appropriate data. However, many\ndomain-specific data are privately distributed across multiple owners. Thus,\nthis dilemma raises the interest in how to perform LLM fine-tuning in federated\nlearning (FL). However, confronted with limited computation and communication\ncapacities, FL clients struggle to fine-tune an LLM effectively. To this end,\nwe introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL.\nSpecifically, our method involves the server generating a compressed LLM and\naligning its performance with the full model. Subsequently, the clients\nfine-tune a lightweight yet important part of the compressed model, referred to\nas an adapter. Notice that as the server has no access to the private data\nowned by the clients, the data used for alignment by the server has a different\ndistribution from the one used for fine-tuning by clients. We formulate the\nproblem into a bi-level optimization problem to minimize the negative effect of\ndata discrepancy and derive the updating rules for the server and clients. We\nconduct extensive experiments on LLaMA-2, empirically showing that the adapter\nhas exceptional performance when reintegrated into the global LLM. The results\nalso indicate that the proposed FedBiOT significantly reduces resource\nconsumption compared to existing benchmarks, all while achieving comparable\nperformance levels.\n","authors":["Feijie Wu","Zitao Li","Yaliang Li","Bolin Ding","Jing Gao"],"pdf_url":"https://arxiv.org/pdf/2406.17706v1.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2406.17692v1","updated":"2024-06-25T16:32:33Z","published":"2024-06-25T16:32:33Z","title":"From Distributional to Overton Pluralism: Investigating Large Language\n  Model Alignment","summary":"  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n","authors":["Thom Lake","Eunsol Choi","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2406.17692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17681v1","updated":"2024-06-25T16:13:53Z","published":"2024-06-25T16:13:53Z","title":"VarBench: Robust Language Model Benchmarking Through Dynamic Variable\n  Perturbation","summary":"  As large language models achieve impressive scores on traditional benchmarks,\nan increasing number of researchers are becoming concerned about benchmark data\nleakage during pre-training, commonly known as the data contamination problem.\nTo ensure fair evaluation, recent benchmarks release only the training and\nvalidation sets, keeping the test set labels closed-source. They require anyone\nwishing to evaluate his language model to submit the model's predictions for\ncentralized processing and then publish the model's result on their\nleaderboard. However, this submission process is inefficient and prevents\neffective error analysis. To address this issue, we propose to variabilize\nbenchmarks and evaluate language models dynamically. Specifically, we extract\nvariables from each test case and define a value range for each variable. For\neach evaluation, we sample new values from these value ranges to create unique\ntest cases, thus ensuring a fresh evaluation each time. We applied this\nvariable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and\nTruthfulQA, which cover mathematical generation and multiple-choice tasks. Our\nexperimental results demonstrate that this approach provides a more accurate\nassessment of the true capabilities of language models, effectively mitigating\nthe contamination problem.\n","authors":["Kun Qian","Shunji Wan","Claudia Tang","Youzhi Wang","Xuanming Zhang","Maximillian Chen","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2406.17681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00541v5","updated":"2024-06-25T16:13:12Z","published":"2023-11-01T14:20:18Z","title":"An Embedded Diachronic Sense Change Model with a Case Study from Ancient\n  Greek","summary":"  Word meanings change over time, and word senses evolve, emerge or die out in\nthe process. For ancient languages, where the corpora are often small and\nsparse, modelling such changes accurately proves challenging, and quantifying\nuncertainty in sense-change estimates consequently becomes important. GASC\n(Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing\ngenerative models that have been used to analyse sense change for target words\nfrom an ancient Greek text corpus, using unsupervised learning without the help\nof any pre-training. These models represent the senses of a given target word\nsuch as \"kosmos\" (meaning decoration, order or world) as distributions over\ncontext words, and sense prevalence as a distribution over senses. The models\nare fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal\nchanges in these representations. This paper introduces EDiSC, an Embedded DiSC\nmodel, which combines word embeddings with DiSC to provide superior model\nperformance. It is shown empirically that EDiSC offers improved predictive\naccuracy, ground-truth recovery and uncertainty quantification, as well as\nbetter sampling efficiency and scalability properties with MCMC methods. The\nchallenges of fitting these models are also discussed.\n","authors":["Schyan Zafar","Geoff K. Nicholls"],"pdf_url":"https://arxiv.org/pdf/2311.00541v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17675v1","updated":"2024-06-25T16:09:08Z","published":"2024-06-25T16:09:08Z","title":"Quantifying AI Psychology: A Psychometrics Benchmark for Large Language\n  Models","summary":"  Large Language Models (LLMs) have demonstrated exceptional task-solving\ncapabilities, increasingly adopting roles akin to human-like assistants. The\nbroader integration of LLMs into society has sparked interest in whether they\nmanifest psychological attributes, and whether these attributes are\nstable-inquiries that could deepen the understanding of their behaviors.\nInspired by psychometrics, this paper presents a framework for investigating\npsychology in LLMs, including psychological dimension identification,\nassessment dataset curation, and assessment with results validation. Following\nthis framework, we introduce a comprehensive psychometrics benchmark for LLMs\nthat covers six psychological dimensions: personality, values, emotion, theory\nof mind, motivation, and intelligence. This benchmark includes thirteen\ndatasets featuring diverse scenarios and item types. Our findings indicate that\nLLMs manifest a broad spectrum of psychological attributes. We also uncover\ndiscrepancies between LLMs' self-reported traits and their behaviors in\nreal-world scenarios. This paper demonstrates a thorough psychometric\nassessment of LLMs, providing insights into reliable evaluation and potential\napplications in AI and social sciences.\n","authors":["Yuan Li","Yue Huang","Hongyi Wang","Xiangliang Zhang","James Zou","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.17675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17667v1","updated":"2024-06-25T15:57:02Z","published":"2024-06-25T15:57:02Z","title":"This Paper Had the Smartest Reviewers -- Flattery Detection Utilising an\n  Audio-Textual Transformer-Based Approach","summary":"  Flattery is an important aspect of human communication that facilitates\nsocial bonding, shapes perceptions, and influences behavior through strategic\ncompliments and praise, leveraging the power of speech to build rapport\neffectively. Its automatic detection can thus enhance the naturalness of\nhuman-AI interactions. To meet this need, we present a novel audio textual\ndataset comprising 20 hours of speech and train machine learning models for\nautomatic flattery detection. In particular, we employ pretrained AST,\nWav2Vec2, and Whisper models for the speech modality, and Whisper TTS models\ncombined with a RoBERTa text classifier for the textual modality. Subsequently,\nwe build a multimodal classifier by combining text and audio representations.\nEvaluation on unseen test data demonstrates promising results, with Unweighted\nAverage Recall scores reaching 82.46% in audio-only experiments, 85.97% in\ntext-only experiments, and 87.16% using a multimodal approach.\n","authors":["Lukas Christ","Shahin Amiriparian","Friederike Hawighorst","Ann-Kathrin Schill","Angelo Boutalikakis","Lorenz Graf-Vlachy","Andreas König","Björn W. Schuller"],"pdf_url":"https://arxiv.org/pdf/2406.17667v1.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.17663v1","updated":"2024-06-25T15:52:15Z","published":"2024-06-25T15:52:15Z","title":"LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic","summary":"  We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the\nlogical reasoning capabilities of Large Language Models (LLMs), by combining\nthem with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic\nmethod where the LLM Actor generates declarative logic programs along with\ntests for semantic correctness, while the Automated Reasoning Critic evaluates\nthe code, runs the tests and provides feedback on test failures for iterative\nrefinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a\nnew state-of-the-art accuracy of 88.32% on the FOLIO benchmark which tests\ncomplex logical reasoning capabilities. Our experiments demonstrate significant\nimprovements over LLM-only baselines, highlighting the importance of logic test\ngeneration and iterative self-refinement. We achieve our best result using a\nfully automated self-supervised training loop where the Actor is trained on\nend-to-end dialog traces with Critic feedback. We discuss potential\nenhancements and provide a detailed error analysis, showcasing the robustness\nand efficacy of LLM-ARC for complex natural language reasoning tasks.\n","authors":["Aditya Kalyanpur","Kailash Saravanakumar","Victor Barres","Jennifer Chu-Carroll","David Melville","David Ferrucci"],"pdf_url":"https://arxiv.org/pdf/2406.17663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17650v1","updated":"2024-06-25T15:41:40Z","published":"2024-06-25T15:41:40Z","title":"ELIZA Reinterpreted: The world's first chatbot was not intended as a\n  chatbot at all","summary":"  ELIZA, often considered the world's first chatbot, was written by Joseph\nWeizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot,\nbut rather to build a platform for research into human-machine conversation and\nthe important cognitive processes of interpretation and misinterpretation. His\npurpose was obscured by ELIZA's fame, resulting in large part from the\nfortuitous timing of it's creation, and it's escape into the wild. In this\npaper I provide a rich historical context for ELIZA's creation, demonstrating\nthat ELIZA arose from the intersection of some of the central threads in the\ntechnical history of AI. I also briefly discuss how ELIZA escaped into the\nworld, and how its accidental escape, along with several coincidental turns of\nthe programming language screws, led both to the misapprehension that ELIZA was\nintended as a chatbot, and to the loss of the original ELIZA to history for\nover 50 years.\n","authors":["Jeff Shrager"],"pdf_url":"https://arxiv.org/pdf/2406.17650v1.pdf","comment":"In review in IEEE Annals of the History of Computing (submitted Apr\n  2024)"},{"id":"http://arxiv.org/abs/2406.17647v1","updated":"2024-06-25T15:41:07Z","published":"2024-06-25T15:41:07Z","title":"Variationist: Exploring Multifaceted Variation and Bias in Written\n  Language Data","summary":"  Exploring and understanding language data is a fundamental stage in all areas\ndealing with human language. It allows NLP practitioners to uncover quality\nconcerns and harmful biases in data before training, and helps linguists and\nsocial scientists to gain insight into language use and human behavior. Yet,\nthere is currently a lack of a unified, customizable tool to seamlessly inspect\nand visualize language variation and bias across multiple variables, language\nunits, and diverse metrics that go beyond descriptive statistics. In this\npaper, we introduce Variationist, a highly-modular, extensible, and\ntask-agnostic tool that fills this gap. Variationist handles at once a\npotentially unlimited combination of variable types and semantics across\ndiversity and association metrics with regards to the language unit of choice,\nand orchestrates the creation of up to five-dimensional interactive charts for\nover 30 variable type-semantics combinations. Through our case studies on\ncomputational dialectology, human label variation, and text generation, we show\nhow Variationist enables researchers from different disciplines to effortlessly\nanswer specific research questions or unveil undesired associations in language\ndata. A Python library, code, documentation, and tutorials are made publicly\navailable to the research community.\n","authors":["Alan Ramponi","Camilla Casula","Stefano Menini"],"pdf_url":"https://arxiv.org/pdf/2406.17647v1.pdf","comment":"ACL 2024 (System Demonstrations)"},{"id":"http://arxiv.org/abs/2406.17642v1","updated":"2024-06-25T15:31:01Z","published":"2024-06-25T15:31:01Z","title":"Banishing LLM Hallucinations Requires Rethinking Generalization","summary":"  Despite their powerful chat, coding, and reasoning abilities, Large Language\nModels (LLMs) frequently hallucinate. Conventional wisdom suggests that\nhallucinations are a consequence of a balance between creativity and\nfactuality, which can be mitigated, but not eliminated, by grounding the LLM in\nexternal knowledge sources. Through extensive systematic experiments, we show\nthat these traditional approaches fail to explain why LLMs hallucinate in\npractice. Specifically, we show that LLMs augmented with a massive Mixture of\nMemory Experts (MoME) can easily memorize large datasets of random numbers. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple neural networks trained to predict the next token hallucinate when\nthe training loss is above a threshold as it usually does in practice when\ntraining on internet scale data. We interpret our findings by comparing against\ntraditional retrieval methods for mitigating hallucinations. We use our\nfindings to design a first generation model for removing hallucinations --\nLamini-1 -- that stores facts in a massive mixture of millions of memory\nexperts that are retrieved dynamically.\n","authors":["Johnny Li","Saksham Consul","Eda Zhou","James Wong","Naila Farooqui","Yuxin Ye","Nithyashree Manohar","Zhuxiaona Wei","Tian Wu","Ben Echols","Sharon Zhou","Gregory Diamos"],"pdf_url":"https://arxiv.org/pdf/2406.17642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17639v1","updated":"2024-06-25T15:24:02Z","published":"2024-06-25T15:24:02Z","title":"Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP","summary":"  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering two\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? We design AlignCLIP, in\norder to answer these questions and show that answers to both questions are\npositive. Through extensive experiments, we show that AlignCLIP achieves\nnoticeable enhancements in the cross-modal alignment of the embeddings, and\nthereby, reduces the modality gap, while maintaining the performance across\nseveral downstream evaluations, such as zero-shot image classification,\nzero-shot multi-modal retrieval and zero-shot semantic text similarity.\n","authors":["Sedigheh Eslami","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2406.17639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17633v1","updated":"2024-06-25T15:20:25Z","published":"2024-06-25T15:20:25Z","title":"Knowledge Distillation in Automated Annotation: Supervised Text\n  Classification with LLM-Generated Training Labels","summary":"  Computational social science (CSS) practitioners often rely on human-labeled\ndata to fine-tune supervised text classifiers. We assess the potential for\nresearchers to augment or replace human-generated training data with surrogate\ntraining labels from generative large language models (LLMs). We introduce a\nrecommended workflow and test this LLM application by replicating 14\nclassification tasks and measuring performance. We employ a novel corpus of\nEnglish-language text classification data sets from recent CSS articles in\nhigh-impact journals. Because these data sets are stored in password-protected\narchives, our analyses are less prone to issues of contamination. For each\ntask, we compare supervised classifiers fine-tuned using GPT-4 labels against\nclassifiers fine-tuned with human annotations and against labels from GPT-4 and\nMistral-7B with few-shot in-context learning. Our findings indicate that\nsupervised classification models fine-tuned on LLM-generated labels perform\ncomparably to models fine-tuned with labels from human annotators. Fine-tuning\nmodels using LLM-generated labels can be a fast, efficient and cost-effective\nmethod of building supervised text classifiers.\n","authors":["Nicholas Pangakis","Samuel Wolken"],"pdf_url":"https://arxiv.org/pdf/2406.17633v1.pdf","comment":"In Proceedings of the Sixth Workshop on Natural Language Processing\n  and Computational Social Science"},{"id":"http://arxiv.org/abs/2406.17626v1","updated":"2024-06-25T15:13:02Z","published":"2024-06-25T15:13:02Z","title":"CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue\n  Coreference","summary":"  As large language models (LLMs) constantly evolve, ensuring their safety\nremains a critical research problem. Previous red-teaming approaches for LLM\nsafety have primarily focused on single prompt attacks or goal hijacking. To\nthe best of our knowledge, we are the first to study LLM safety in multi-turn\ndialogue coreference. We created a dataset of 1,400 questions across 14\ncategories, each featuring multi-turn coreference safety attacks. We then\nconducted detailed evaluations on five widely used open-source LLMs. The\nresults indicated that under multi-turn coreference safety attacks, the highest\nattack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was\n13.9% with the Mistral-7B-Instruct model. These findings highlight the safety\nvulnerabilities in LLMs during dialogue coreference interactions.\n","authors":["Erxin Yu","Jing Li","Ming Liao","Siqi Wang","Zuchen Gao","Fei Mi","Lanqing Hong"],"pdf_url":"https://arxiv.org/pdf/2406.17626v1.pdf","comment":"Submitted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2406.17624v1","updated":"2024-06-25T15:08:44Z","published":"2024-06-25T15:08:44Z","title":"Self-assessment, Exhibition, and Recognition: a Review of Personality in\n  Large Language Models","summary":"  As large language models (LLMs) appear to behave increasingly human-like in\ntext-based interactions, more and more researchers become interested in\ninvestigating personality in LLMs. However, the diversity of psychological\npersonality research and the rapid development of LLMs have led to a broad yet\nfragmented landscape of studies in this interdisciplinary field. Extensive\nstudies across different research focuses, different personality psychometrics,\nand different LLMs make it challenging to have a holistic overview and further\npose difficulties in applying findings to real-world applications. In this\npaper, we present a comprehensive review by categorizing current studies into\nthree research problems: self-assessment, exhibition, and recognition, based on\nthe intrinsic characteristics and external manifestations of personality in\nLLMs. For each problem, we provide a thorough analysis and conduct in-depth\ncomparisons of their corresponding solutions. Besides, we summarize research\nfindings and open challenges from current studies and further discuss their\nunderlying causes. We also collect extensive publicly available resources to\nfacilitate interested researchers and developers. Lastly, we discuss the\npotential future research directions and application scenarios. Our paper is\nthe first comprehensive survey of up-to-date literature on personality in LLMs.\nBy presenting a clear taxonomy, in-depth analysis, promising future directions,\nand extensive resource collections, we aim to provide a better understanding\nand facilitate further advancements in this emerging field.\n","authors":["Zhiyuan Wen","Yu Yang","Jiannong Cao","Haoming Sun","Ruosong Yang","Shuaiqi Liu"],"pdf_url":"https://arxiv.org/pdf/2406.17624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13069v2","updated":"2024-06-25T15:02:40Z","published":"2024-06-18T21:31:19Z","title":"Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG","summary":"  How novel are texts generated by language models (LMs) relative to their\ntraining corpora? In this work, we investigate the extent to which modern LMs\ngenerate $n$-grams from their training data, evaluating both (i) the\nprobability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the\nproportion of $n$-grams generated by an LM that did not appear in the training\ndata (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search\nover a corpus in constant time, we develop Rusty-DAWG, a novel search tool\ninspired by indexing of genomic data. We compare the novelty of LM-generated\ntext to human-written text and explore factors that affect generation novelty,\nfocusing on the Pythia models. We find that, for $n > 4$, LM-generated text is\nless novel than human-written text, though it is more novel for smaller $n$.\nLarger LMs and more constrained decoding strategies both decrease novelty.\nFinally, we show that LMs complete $n$-grams with lower loss if they are more\nfrequent in the training data. Overall, our results reveal factors influencing\nthe novelty of LM-generated text, and we release Rusty-DAWG to facilitate\nfurther pretraining data research.\n","authors":["William Merrill","Noah A. Smith","Yanai Elazar"],"pdf_url":"https://arxiv.org/pdf/2406.13069v2.pdf","comment":"8 page preprint + appendix. Minor fixes and appendix changes June 25,\n  2024"},{"id":"http://arxiv.org/abs/2406.17618v1","updated":"2024-06-25T15:02:32Z","published":"2024-06-25T15:02:32Z","title":"Towards Building an End-to-End Multilingual Automatic Lyrics\n  Transcription Model","summary":"  Multilingual automatic lyrics transcription (ALT) is a challenging task due\nto the limited availability of labelled data and the challenges introduced by\nsinging, compared to multilingual automatic speech recognition. Although some\nmultilingual singing datasets have been released recently, English continues to\ndominate these collections. Multilingual ALT remains underexplored due to the\nscale of data and annotation quality. In this paper, we aim to create a\nmultilingual ALT system with available datasets. Inspired by architectures that\nhave been proven effective for English ALT, we adapt these techniques to the\nmultilingual scenario by expanding the target vocabulary set. We then evaluate\nthe performance of the multilingual model in comparison to its monolingual\ncounterparts. Additionally, we explore various conditioning methods to\nincorporate language information into the model. We apply analysis by language\nand combine it with the language classification performance. Our findings\nreveal that the multilingual model performs consistently better than the\nmonolingual models trained on the language subsets. Furthermore, we demonstrate\nthat incorporating language information significantly enhances performance.\n","authors":["Jiawen Huang","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2406.17618v1.pdf","comment":"Accepted at EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2406.17600v1","updated":"2024-06-25T14:42:17Z","published":"2024-06-25T14:42:17Z","title":"\"Seeing the Big through the Small\": Can LLMs Approximate Human Judgment\n  Distributions on NLI from a Few Explanations?","summary":"  Human label variation (HLV) is a valuable source of information that arises\nwhen multiple human annotators provide different labels for valid reasons. In\nNatural Language Inference (NLI) earlier approaches to capturing HLV involve\neither collecting annotations from many crowd workers to represent human\njudgment distribution (HJD) or use expert linguists to provide detailed\nexplanations for their chosen labels. While the former method provides denser\nHJD information, obtaining it is resource-intensive. In contrast, the latter\noffers richer textual information but it is challenging to scale up to many\nhuman judges. Besides, large language models (LLMs) are increasingly used as\nevaluators (``LLM judges'') but with mixed results, and few works aim to study\nHJDs. This study proposes to exploit LLMs to approximate HJDs using a small\nnumber of expert labels and explanations. Our experiments show that a few\nexplanations significantly improve LLMs' ability to approximate HJDs with and\nwithout explicit labels, thereby providing a solution to scale up annotations\nfor HJD. However, fine-tuning smaller soft-label aware models with the\nLLM-generated model judgment distributions (MJDs) presents partially\ninconsistent results: while similar in distance, their resulting fine-tuned\nmodels and visualized distributions differ substantially. We show the\nimportance of complementing instance-level distance measures with a\nglobal-level shape metric and visualization to more effectively evaluate MJDs\nagainst human judgment distributions.\n","authors":["Beiduo Chen","Xinpeng Wang","Siyao Peng","Robert Litschko","Anna Korhonen","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2406.17600v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.17588v1","updated":"2024-06-25T14:31:26Z","published":"2024-06-25T14:31:26Z","title":"LongIns: A Challenging Long-context Instruction-based Exam for LLMs","summary":"  The long-context capabilities of large language models (LLMs) have been a hot\ntopic in recent years. To evaluate the performance of LLMs in different\nscenarios, various assessment benchmarks have emerged. However, as most of\nthese benchmarks focus on identifying key information to answer questions,\nwhich mainly requires the retrieval ability of LLMs, these benchmarks can\npartially represent the reasoning performance of LLMs from large amounts of\ninformation. Meanwhile, although LLMs often claim to have context windows of\n32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual\nsupported length of these LLMs. To address these issues, we propose the LongIns\nbenchmark dataset, a challenging long-context instruction-based exam for LLMs,\nwhich is built based on the existing instruction datasets. Specifically, in our\nLongIns, we introduce three evaluation settings: Global Instruction & Single\nTask (GIST), Local Instruction & Single Task (LIST), and Local Instruction &\nMultiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations\non existing LLMs and have the following important findings: (1). The\ntop-performing GPT-4 with 128k context length performs poorly on the evaluation\ncontext window of 16k in our LongIns. (2). For the multi-hop reasoning ability\nof many existing LLMs, significant efforts are still needed under short context\nwindows (less than 4k).\n","authors":["Shawn Gavin","Tuney Zheng","Jiaheng Liu","Quehry Que","Noah Wang","Jian Yang","Chenchen Zhang","Wenhao Huang","Wenhu Chen","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16751v2","updated":"2024-06-25T14:18:21Z","published":"2024-06-24T15:58:15Z","title":"Towards Zero-Shot Text-To-Speech for Arabic Dialects","summary":"  Zero-shot multi-speaker text-to-speech (ZS-TTS) systems have advanced for\nEnglish, however, it still lags behind due to insufficient resources. We\naddress this gap for Arabic, a language of more than 450 million native\nspeakers, by first adapting a sizeable existing dataset to suit the needs of\nspeech synthesis. Additionally, we employ a set of Arabic dialect\nidentification models to explore the impact of pre-defined dialect labels on\nimproving the ZS-TTS model in a multi-dialect setting. Subsequently, we\nfine-tune the\nXTTS\\footnote{https://docs.coqui.ai/en/latest/models/xtts.html}\\footnote{https://medium.com/machine-learns/xtts-v2-new-version-of-the-open-source-text-to-speech-model-af73914db81f}\\footnote{https://medium.com/@erogol/xtts-v1-techincal-notes-eb83ff05bdc}\nmodel, an open-source architecture. We then evaluate our models on a dataset\ncomprising 31 unseen speakers and an in-house dialectal dataset. Our automated\nand human evaluation results show convincing performance while capable of\ngenerating dialectal speech. Our study highlights significant potential for\nimprovements in this emerging area of research in Arabic.\n","authors":["Khai Duy Doan","Abdul Waheed","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2406.16751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17574v1","updated":"2024-06-25T14:14:35Z","published":"2024-06-25T14:14:35Z","title":"Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for\n  Querying and Classifying IoT Threats","summary":"  Recognizing the promise of natural language interfaces to databases, prior\nstudies have emphasized the development of text-to-SQL systems. While\nsubstantial progress has been made in this field, existing research has\nconcentrated on generating SQL statements from text queries. The broader\nchallenge, however, lies in inferring new information about the returned data.\nOur research makes two major contributions to address this gap. First, we\nintroduce a novel Internet-of-Things (IoT) text-to-SQL dataset comprising\n10,985 text-SQL pairs and 239,398 rows of network traffic activity. The dataset\ncontains additional query types limited in prior text-to-SQL datasets, notably\ntemporal-related queries. Our dataset is sourced from a smart building's IoT\necosystem exploring sensor read and network traffic data. Second, our dataset\nallows two-stage processing, where the returned data (network traffic) from a\ngenerated SQL can be categorized as malicious or not. Our results show that\njoint training to query and infer information about the data can improve\noverall text-to-SQL performance, nearly matching substantially larger models.\nWe also show that current large language models (e.g., GPT3.5) struggle to\ninfer new information about returned data, thus our dataset provides a novel\ntest bed for integrating complex domain-specific reasoning into LLMs.\n","authors":["Ryan Pavlich","Nima Ebadi","Richard Tarbell","Billy Linares","Adrian Tan","Rachael Humphreys","Jayanta Kumar Das","Rambod Ghandiparsi","Hannah Haley","Jerris George","Rocky Slavin","Kim-Kwang Raymond Choo","Glenn Dietrich","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2406.17574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17566v1","updated":"2024-06-25T14:02:11Z","published":"2024-06-25T14:02:11Z","title":"FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating\n  Toxicity in French Texts","summary":"  Large language models (LLMs) are increasingly popular but are also prone to\ngenerating bias, toxic or harmful language, which can have detrimental effects\non individuals and communities. Although most efforts is put to assess and\nmitigate toxicity in generated content, it is primarily concentrated on\nEnglish, while it's essential to consider other languages as well. For\naddressing this issue, we create and release FrenchToxicityPrompts, a dataset\nof 50K naturally occurring French prompts and their continuations, annotated\nwith toxicity scores from a widely used toxicity classifier. We evaluate 14\ndifferent models from four prevalent open-sourced families of LLMs against our\ndataset to assess their potential toxicity across various dimensions. We hope\nthat our contribution will foster future research on toxicity detection and\nmitigation beyond Englis\n","authors":["Caroline Brun","Vassilina Nikoulina"],"pdf_url":"https://arxiv.org/pdf/2406.17566v1.pdf","comment":"TRAC-2024, Fourth Workshop on Threat, Aggression and Cyberbullying.\n  20 May 2024"},{"id":"http://arxiv.org/abs/2406.17563v1","updated":"2024-06-25T14:00:42Z","published":"2024-06-25T14:00:42Z","title":"Multi-property Steering of Large Language Models with Dynamic Activation\n  Composition","summary":"  Activation steering methods were shown to be effective in conditioning\nlanguage model generation by additively intervening over models' intermediate\nrepresentations. However, the evaluation of these techniques has so far been\nlimited to single conditioning properties and synthetic settings. In this work,\nwe conduct a comprehensive evaluation of various activation steering\nstrategies, highlighting the property-dependent nature of optimal parameters to\nensure a robust effect throughout generation. To address this issue, we propose\nDynamic Activation Composition, an information-theoretic approach to modulate\nthe steering intensity of one or more properties throughout generation. Our\nexperiments on multi-property steering show that our method successfully\nmaintains high conditioning while minimizing the impact of conditioning on\ngeneration fluency.\n","authors":["Daniel Scalena","Gabriele Sarti","Malvina Nissim"],"pdf_url":"https://arxiv.org/pdf/2406.17563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17557v1","updated":"2024-06-25T13:50:56Z","published":"2024-06-25T13:50:56Z","title":"The FineWeb Datasets: Decanting the Web for the Finest Text Data at\n  Scale","summary":"  The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments.\n","authors":["Guilherme Penedo","Hynek Kydlíček","Loubna Ben allal","Anton Lozhkov","Margaret Mitchell","Colin Raffel","Leandro Von Werra","Thomas Wolf"],"pdf_url":"https://arxiv.org/pdf/2406.17557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14425v2","updated":"2024-06-25T13:48:41Z","published":"2024-06-20T15:49:28Z","title":"SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource\n  Languages","summary":"  Question Answering (QA) datasets have been instrumental in developing and\nevaluating Large Language Model (LLM) capabilities. However, such datasets are\nscarce for languages other than English due to the cost and difficulties of\ncollection and manual annotation. This means that producing novel models and\nmeasuring the performance of multilingual LLMs in low-resource languages is\nchallenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a\nmethod for generating and validating QA datasets for low-resource languages. We\nutilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs\nbetween English and the target language. We use the English data as context to\n$\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which\nare automatically translated and further validated for quality. Combining these\nwith their designated non-English $\\textit{human-curated}$ paragraphs form the\nfinal QA dataset. The method allows to maintain the content quality, reduces\nthe likelihood of factual errors, and circumvents the need for costly\nannotation. To test the method, we created a QA dataset with $1.2$K samples for\nthe Armenian language. The human evaluation shows that $98\\%$ of the generated\nEnglish data maintains quality and diversity in the question types and topics,\nwhile the translation validation pipeline can filter out $\\sim70\\%$ of data\nwith poor quality. We use the dataset to benchmark state-of-the-art LLMs,\nshowing their inability to achieve human accuracy with some model performances\ncloser to random chance. This shows that the generated dataset is non-trivial\nand can be used to evaluate reasoning capabilities in low-resource language.\n","authors":["Gayane Ghazaryan","Erik Arakelyan","Pasquale Minervini","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2406.14425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16797v2","updated":"2024-06-25T13:46:41Z","published":"2024-06-24T16:58:23Z","title":"Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs","summary":"  Existing methods for adapting large language models (LLMs) to new tasks are\nnot suited to multi-task adaptation because they modify all the model weights\n-- causing destructive interference between tasks. The resulting effects, such\nas catastrophic forgetting of earlier tasks, make it challenging to obtain good\nperformance on multiple tasks at the same time. To mitigate this, we propose\nLottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies\nand optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide\nrange of challenging tasks such as instruction following, reasoning, math, and\nsummarization. LoTA obtains better performance than full fine-tuning and\nlow-rank adaptation (LoRA), and maintains good performance even after training\non other tasks -- thus, avoiding catastrophic forgetting. By extracting and\nfine-tuning over lottery tickets (or sparse task vectors), LoTA also enables\nmodel merging over highly dissimilar tasks. Our code is made publicly available\nat https://github.com/kiddyboots216/lottery-ticket-adaptation.\n","authors":["Ashwinee Panda","Berivan Isik","Xiangyu Qi","Sanmi Koyejo","Tsachy Weissman","Prateek Mittal"],"pdf_url":"https://arxiv.org/pdf/2406.16797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12036v2","updated":"2024-06-25T13:45:49Z","published":"2024-06-17T19:07:21Z","title":"MedCalc-Bench: Evaluating Large Language Models for Medical Calculations","summary":"  As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.\n","authors":["Nikhil Khandekar","Qiao Jin","Guangzhi Xiong","Soren Dunn","Serina S Applebaum","Zain Anwar","Maame Sarfo-Gyamfi","Conrad W Safranek","Abid A Anwar","Andrew Zhang","Aidan Gilson","Maxwell B Singer","Amisha Dave","Andrew Taylor","Aidong Zhang","Qingyu Chen","Zhiyong Lu"],"pdf_url":"https://arxiv.org/pdf/2406.12036v2.pdf","comment":"Github link: https://github.com/ncbi-nlp/MedCalc-Bench HuggingFace\n  link: https://huggingface.co/datasets/nsk7153/MedCalc-Bench"},{"id":"http://arxiv.org/abs/2406.17553v1","updated":"2024-06-25T13:43:24Z","published":"2024-06-25T13:43:24Z","title":"Retrieval-Augmented Code Generation for Situated Action Generation: A\n  Case Study on Minecraft","summary":"  In the Minecraft Collaborative Building Task, two players collaborate: an\nArchitect (A) provides instructions to a Builder (B) to assemble a specified\nstructure using 3D blocks. In this work, we investigate the use of large\nlanguage models (LLMs) to predict the sequence of actions taken by the Builder.\nLeveraging LLMs' in-context learning abilities, we use few-shot prompting\ntechniques, that significantly improve performance over baseline methods.\nAdditionally, we present a detailed analysis of the gaps in performance for\nfuture work\n","authors":["Chalamalasetti Kranti","Sherzod Hakimov","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2406.17553v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2402.11253v3","updated":"2024-06-25T13:39:52Z","published":"2024-02-17T11:25:26Z","title":"Aligning Large Language Models by On-Policy Self-Judgment","summary":"  Existing approaches for aligning large language models with human preferences\nface a trade-off that requires a separate reward model (RM) for on-policy\nlearning. In this paper, we present a novel alignment framework, SELF-JUDGE\nthat (1) does on-policy learning and 2) is parameter efficient, as it does not\nrequire an additional RM for evaluating the samples for on-policy learning. To\nthis end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a\nsingle model to act as both a policy and a judge. Specifically, we view the\npairwise judgment task, choosing the better response from a response pair, as a\nspecial case of the instruction-following task. The resulting model can judge\npreferences of on-the-fly responses from current policy initialized from\nitself. Experimental results show the efficacy of SELF-JUDGE, outperforming\nbaselines in preference benchmarks. We also show that the rejecting sampling by\nitself can improve performance further without an additional evaluator.\n","authors":["Sangkyu Lee","Sungdong Kim","Ashkan Yousefpour","Minjoon Seo","Kang Min Yoo","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2402.11253v3.pdf","comment":"Published as a main conference paper at ACL 2024"},{"id":"http://arxiv.org/abs/2402.14762v2","updated":"2024-06-25T13:38:41Z","published":"2024-02-22T18:21:59Z","title":"MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues","summary":"  The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}.\n","authors":["Ge Bai","Jie Liu","Xingyuan Bu","Yancheng He","Jiaheng Liu","Zhanhui Zhou","Zhuoran Lin","Wenbo Su","Tiezheng Ge","Bo Zheng","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2402.14762v2.pdf","comment":"[ACL 2024] The first three authors contribute equally, 34 pages, repo\n  at https://github.com/mtbench101/mt-bench-101"},{"id":"http://arxiv.org/abs/2406.17542v1","updated":"2024-06-25T13:29:14Z","published":"2024-06-25T13:29:14Z","title":"CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained\n  Models using Greedy Coordinate Descent","summary":"  Large language models (LLMs) have recently demonstrated remarkable\nperformance across diverse language tasks. But their deployment is often\nconstrained by their substantial computational and storage requirements.\nQuantization has emerged as a key technique for addressing this challenge,\nenabling the compression of large models with minimal impact on performance.\nThe recent GPTQ algorithm, a post-training quantization (PTQ) method, has\nproven highly effective for compressing LLMs, sparking a wave of research that\nleverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the\nPTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ\nwith improved performance. CDQuant uses coordinate descent to minimize the\nlayer-wise reconstruction loss to achieve high-quality quantized weights. Our\nalgorithm is easy to implement and scales efficiently to models with hundreds\nof billions of parameters. Through extensive evaluation on the PaLM2 model\nfamily, we demonstrate that CDQuant consistently outperforms GPTQ across\ndiverse model sizes and quantization levels. In particular, for INT2\nquantization of PaLM2-Otter, CDQuant achieves a 10% reduction in perplexity\ncompared to GPTQ.\n","authors":["Pranav Ajit Nair","Arun Sai Suggala"],"pdf_url":"https://arxiv.org/pdf/2406.17542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17535v1","updated":"2024-06-25T13:20:08Z","published":"2024-06-25T13:20:08Z","title":"Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian\n  Benchmark","summary":"  Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to generate and manipulate human language, highlighting\ntheir potential across various applications. Evaluating LLMs in languages other\nthan English is crucial for ensuring their linguistic versatility, cultural\nrelevance, and applicability in diverse global contexts, thus broadening their\nusability and effectiveness. We tackle this challenge by introducing a\nstructured benchmark using the INVALSI tests, a set of well-established\nassessments designed to measure educational competencies across Italy. Our\nstudy makes three primary contributions: Firstly, we adapt the INVALSI\nbenchmark for automated LLM evaluation, which involves rigorous adaptation of\nthe test format to suit automated processing while retaining the essence of the\noriginal tests. Secondly, we provide a detailed assessment of current LLMs,\noffering a crucial reference point for the academic community. Finally, we\nvisually compare the performance of these models against human results.\nAdditionally, researchers are invited to submit their models for ongoing\nevaluation, ensuring the benchmark remains a current and valuable resource.\n","authors":["Fabio Mercorio","Mario Mezzanzanica","Daniele Potertì","Antonio Serino","Andrea Seveso"],"pdf_url":"https://arxiv.org/pdf/2406.17535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17534v1","updated":"2024-06-25T13:19:41Z","published":"2024-06-25T13:19:41Z","title":"Retrieval-style In-Context Learning for Few-shot Hierarchical Text\n  Classification","summary":"  Hierarchical text classification (HTC) is an important task with broad\napplications, while few-shot HTC has gained increasing interest recently. While\nin-context learning (ICL) with large language models (LLMs) has achieved\nsignificant success in few-shot learning, it is not as effective for HTC\nbecause of the expansive hierarchical label sets and extremely-ambiguous\nlabels. In this work, we introduce the first ICL-based framework with LLM for\nfew-shot HTC. We exploit a retrieval database to identify relevant\ndemonstrations, and an iterative policy to manage multi-layer hierarchical\nlabels. Particularly, we equip the retrieval database with HTC label-aware\nrepresentations for the input texts, which is achieved by continual training on\na pretrained language model with masked language modeling (MLM), layer-wise\nclassification (CLS, specifically for HTC), and a novel divergent contrastive\nlearning (DCL, mainly for adjacent semantically-similar labels) objective.\nExperimental results on three benchmark datasets demonstrate superior\nperformance of our method, and we can achieve state-of-the-art results in\nfew-shot HTC.\n","authors":["Huiyao Chen","Yu Zhao","Zulong Chen","Mengjia Wang","Liangyue Li","Meishan Zhang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17534v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2406.17532v1","updated":"2024-06-25T13:16:34Z","published":"2024-06-25T13:16:34Z","title":"Can Large Language Models Understand DL-Lite Ontologies? An Empirical\n  Study","summary":"  Large language models (LLMs) have shown significant achievements in solving a\nwide range of tasks. Recently, LLMs' capability to store, retrieve and infer\nwith symbolic knowledge has drawn a great deal of attention, showing their\npotential to understand structured information. However, it is not yet known\nwhether LLMs can understand Description Logic (DL) ontologies. In this work, we\nempirically analyze the LLMs' capability of understanding DL-Lite ontologies\ncovering 6 representative tasks from syntactic and semantic aspects. With\nextensive experiments, we demonstrate both the effectiveness and limitations of\nLLMs in understanding DL-Lite ontologies. We find that LLMs can understand\nformal syntax and model-theoretic semantics of concepts and roles. However,\nLLMs struggle with understanding TBox NI transitivity and handling ontologies\nwith large ABoxes. We hope that our experiments and analyses provide more\ninsights into LLMs and inspire to build more faithful knowledge engineering\nsolutions.\n","authors":["Keyu Wang","Guilin Qi","Jiaqi Li","Songlin Zhai"],"pdf_url":"https://arxiv.org/pdf/2406.17532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17526v1","updated":"2024-06-25T13:08:35Z","published":"2024-06-25T13:08:35Z","title":"LumberChunker: Long-Form Narrative Document Segmentation","summary":"  Modern NLP tasks increasingly rely on dense retrieval methods to access\nup-to-date and relevant contextual information. We are motivated by the premise\nthat retrieval benefits from segments that can vary in size such that a\ncontent's semantic independence is better captured. We propose LumberChunker, a\nmethod leveraging an LLM to dynamically segment documents, which iteratively\nprompts the LLM to identify the point within a group of sequential passages\nwhere the content begins to shift. To evaluate our method, we introduce\nGutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer\npairs derived from 100 public domain narrative books available on Project\nGutenberg. Our experiments show that LumberChunker not only outperforms the\nmost competitive baseline by 7.37% in retrieval performance (DCG@20) but also\nthat, when integrated into a RAG pipeline, LumberChunker proves to be more\neffective than other chunking methods and competitive baselines, such as the\nGemini 1.5M Pro. Our Code and Data are available at\nhttps://github.com/joaodsmarques/LumberChunker\n","authors":["André V. Duarte","João Marques","Miguel Graça","Miguel Freire","Lei Li","Arlindo L. Oliveira"],"pdf_url":"https://arxiv.org/pdf/2406.17526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09631v5","updated":"2024-06-25T13:00:08Z","published":"2024-02-15T00:20:30Z","title":"Representation Surgery: Theory and Practice of Affine Steering","summary":"  Language models often exhibit undesirable behavior, e.g., generating toxic or\ngender-biased text. In the case of neural language models, an encoding of the\nundesirable behavior is often present in the model's representations. Thus, one\nnatural (and common) approach to prevent the model from exhibiting undesirable\nbehavior is to steer the model's representations in a manner that reduces the\nprobability of it generating undesirable text. This paper investigates the\nformal and empirical properties of steering functions, i.e., transformation of\nthe neural language model's representations that alter its behavior. First, we\nderive two optimal, in the least-squares sense, affine steering functions under\ndifferent constraints. Our theory provides justification for existing\napproaches and offers a novel, improved steering approach. Second, we offer a\nseries of experiments that demonstrate the empirical effectiveness of the\nmethods in mitigating bias and reducing toxic generation.\n","authors":["Shashwat Singh","Shauli Ravfogel","Jonathan Herzig","Roee Aharoni","Ryan Cotterell","Ponnurangam Kumaraguru"],"pdf_url":"https://arxiv.org/pdf/2402.09631v5.pdf","comment":"Accepted in ICML 2024"},{"id":"http://arxiv.org/abs/2406.17519v1","updated":"2024-06-25T12:59:38Z","published":"2024-06-25T12:59:38Z","title":"Entropy-Based Decoding for Retrieval-Augmented Large Language Models","summary":"  Augmenting Large Language Models (LLMs) with retrieved external knowledge has\nproven effective for improving the factual accuracy of generated responses.\nDespite their success, retrieval-augmented LLMs still face the distractibility\nissue, where the generated responses are negatively influenced by noise from\nboth external and internal knowledge sources. In this paper, we introduce a\nnovel, training-free decoding method guided by entropy considerations to\nmitigate this issue. Our approach utilizes entropy-based document-parallel\nensemble decoding to prioritize low-entropy distributions from retrieved\ndocuments, thereby enhancing the extraction of relevant information of context.\nAdditionally, it incorporates a contrastive decoding mechanism that contrasts\nthe obtained low-entropy ensemble distribution with the high-entropy\ndistribution derived from the model's internal knowledge across layers, which\nensures a greater emphasis on reliable external information. Extensive\nexperiments on open-domain question answering datasets demonstrate the\nsuperiority of our method.\n","authors":["Zexuan Qiu","Zijing Ou","Bin Wu","Jingjing Li","Aiwei Liu","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2406.17519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17513v1","updated":"2024-06-25T12:51:06Z","published":"2024-06-25T12:51:06Z","title":"Benchmarking Mental State Representations in Language Models","summary":"  While numerous works have assessed the generative performance of language\nmodels (LMs) on tasks requiring Theory of Mind reasoning, research into the\nmodels' internal representation of mental states remains limited. Recent work\nhas used probing to demonstrate that LMs can represent beliefs of themselves\nand others. However, these claims are accompanied by limited evaluation, making\nit difficult to assess how mental state representations are affected by model\ndesign and training choices. We report an extensive benchmark with various LM\ntypes with different model sizes, fine-tuning approaches, and prompt designs to\nstudy the robustness of mental state representations and memorisation issues\nwithin the probes. Our results show that the quality of models' internal\nrepresentations of the beliefs of others increases with model size and, more\ncrucially, with fine-tuning. We are the first to study how prompt variations\nimpact probing performance on theory of mind tasks. We demonstrate that models'\nrepresentations are sensitive to prompt variations, even when such variations\nshould be beneficial. Finally, we complement previous activation editing\nexperiments on Theory of Mind tasks and show that it is possible to improve\nmodels' reasoning performance by steering their activations without the need to\ntrain any probe.\n","authors":["Matteo Bortoletto","Constantin Ruhdorfer","Lei Shi","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2406.17513v1.pdf","comment":"ICML 2024 Workshop on Mechanistic Interpretability"},{"id":"http://arxiv.org/abs/2311.06062v3","updated":"2024-06-25T12:36:02Z","published":"2023-11-10T13:55:05Z","title":"Practical Membership Inference Attacks against Fine-tuned Large Language\n  Models via Self-prompt Calibration","summary":"  Membership Inference Attacks (MIA) aim to infer whether a target data record\nhas been utilized for model training or not. Prior attempts have quantified the\nprivacy risks of language models (LMs) via MIAs, but there is still no\nconsensus on whether existing MIA algorithms can cause remarkable privacy\nleakage on practical Large Language Models (LLMs). Existing MIAs designed for\nLMs can be classified into two categories: reference-free and reference-based\nattacks. They are both based on the hypothesis that training records\nconsistently strike a higher probability of being sampled. Nevertheless, this\nhypothesis heavily relies on the overfitting of target models, which will be\nmitigated by multiple regularization methods and the generalization of LLMs.\nThe reference-based attack seems to achieve promising effectiveness in LLMs,\nwhich measures a more reliable membership signal by comparing the probability\ndiscrepancy between the target model and the reference model. However, the\nperformance of reference-based attack is highly dependent on a reference\ndataset that closely resembles the training dataset, which is usually\ninaccessible in the practical scenario. Overall, existing MIAs are unable to\neffectively unveil privacy leakage over practical fine-tuned LLMs that are\noverfitting-free and private. We propose a Membership Inference Attack based on\nSelf-calibrated Probabilistic Variation (SPV-MIA). Specifically, since\nmemorization in LLMs is inevitable during the training process and occurs\nbefore overfitting, we introduce a more reliable membership signal,\nprobabilistic variation, which is based on memorization rather than\noverfitting. Furthermore, we introduce a self-prompt approach, which constructs\nthe dataset to fine-tune the reference model by prompting the target LLM\nitself. In this manner, the adversary can collect a dataset with a similar\ndistribution from public APIs.\n","authors":["Wenjie Fu","Huandong Wang","Chen Gao","Guanghua Liu","Yong Li","Tao Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.06062v3.pdf","comment":"Repo: https://github.com/wjfu99/MIA-LLMs"},{"id":"http://arxiv.org/abs/2406.16020v2","updated":"2024-06-25T12:27:06Z","published":"2024-06-23T05:40:26Z","title":"AudioBench: A Universal Benchmark for Audio Large Language Models","summary":"  We introduce AudioBench, a new benchmark designed to evaluate audio large\nlanguage models (AudioLLMs). AudioBench encompasses 8 distinct tasks and 26\ncarefully selected or newly curated datasets, focusing on speech understanding,\nvoice interpretation, and audio scene understanding. Despite the rapid\nadvancement of large language models, including multimodal versions, a\nsignificant gap exists in comprehensive benchmarks for thoroughly evaluating\ntheir capabilities. AudioBench addresses this gap by providing relevant\ndatasets and evaluation metrics. In our study, we evaluated the capabilities of\nfour models across various aspects and found that no single model excels\nconsistently across all tasks. We outline the research outlook for AudioLLMs\nand anticipate that our open-source code, data, and leaderboard will offer a\nrobust testbed for future model developments.\n","authors":["Bin Wang","Xunlong Zou","Geyu Lin","Shuo Sun","Zhuohan Liu","Wenyu Zhang","Zhengyuan Liu","AiTi Aw","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16020v2.pdf","comment":"20 pages; v2 - typo update; Code:\n  https://github.com/AudioLLMs/AudioBench"},{"id":"http://arxiv.org/abs/2404.07900v2","updated":"2024-06-25T12:23:00Z","published":"2024-04-11T16:39:00Z","title":"High-Dimension Human Value Representation in Large Language Models","summary":"  The widespread application of Large Language Models (LLMs) across various\ntasks and fields has necessitated the alignment of these models with human\nvalues and preferences. Given various approaches of human value alignment,\nranging from Reinforcement Learning with Human Feedback (RLHF), to\nconstitutional learning, etc. there is an urgent need to understand the scope\nand nature of human values injected into these models before their release.\nThere is also a need for model alignment without a costly large scale human\nannotation effort. We propose UniVaR, a high-dimensional representation of\nhuman value distributions in LLMs, orthogonal to model architecture and\ntraining data. Trained from the value-relevant output of eight multilingual\nLLMs and tested on the output from four multilingual LLMs, namely LlaMA2,\nChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the\ndistribution of human values embedded in different LLMs with different langauge\nsources. Through UniVaR, we explore how different LLMs prioritize various\nvalues in different languages and cultures, shedding light on the complex\ninterplay between human values and language modeling.\n","authors":["Samuel Cahyawijaya","Delong Chen","Yejin Bang","Leila Khalatbari","Bryan Wilie","Ziwei Ji","Etsuko Ishii","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2404.07900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01677v3","updated":"2024-06-25T12:08:41Z","published":"2024-01-20T08:44:34Z","title":"Embedding Ontologies via Incorporating Extensional and Intensional\n  Knowledge","summary":"  Ontologies contain rich knowledge within domain, which can be divided into\ntwo categories, namely extensional knowledge and intensional knowledge.\nExtensional knowledge provides information about the concrete instances that\nbelong to specific concepts in the ontology, while intensional knowledge\ndetails inherent properties, characteristics, and semantic associations among\nconcepts. However, existing ontology embedding approaches fail to take both\nextensional knowledge and intensional knowledge into fine consideration\nsimultaneously. In this paper, we propose a novel ontology embedding approach\nnamed EIKE (Extensional and Intensional Knowledge Embedding) by representing\nontologies in two spaces, called extensional space and intensional space. EIKE\npresents a unified framework for embedding instances, concepts and their\nrelations in an ontology, applying a geometry-based method to model extensional\nknowledge and a pretrained language model to model intensional knowledge, which\ncan capture both structure information and textual information. Experimental\nresults show that EIKE significantly outperforms state-of-the-art methods in\nthree datasets for both triple classification and link prediction, indicating\nthat EIKE provides a more comprehensive and representative perspective of the\ndomain.\n","authors":["Keyu Wang","Guilin Qi","Jiaoyan Chen","Yi Huang","Tianxing Wu"],"pdf_url":"https://arxiv.org/pdf/2402.01677v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17484v1","updated":"2024-06-25T12:05:56Z","published":"2024-06-25T12:05:56Z","title":"MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment\n  and Knowledge Aggregation","summary":"  Large language models (LLMs) have shown substantial progress in natural\nlanguage understanding and generation, proving valuable especially in the\nmedical field. Despite advancements, challenges persist due to the complexity\nand diversity inherent in medical tasks, which can be categorized as\nknowledge-intensive tasks and alignment-required tasks. Previous approaches\neither ignore the latter task or focus on a minority of tasks and hence lose\ngeneralization. To address these drawbacks, we propose a progressive\nfine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise\naggregator to encode diverse knowledge in the first stage and filter out\ndetrimental information. In the second stage, we drop the Noise Aggregator to\navoid the interference of suboptimal representation and leverage an additional\nalignment module optimized towards an orthogonal direction to the knowledge\nspace to mitigate knowledge forgetting. Based on this two-stage paradigm, we\nproposed a Medical LLM through decoupling Clinical Alignment and Knowledge\nAggregation (MedCare), which is designed to achieve state-of-the-art (SOTA)\nperformance on over 20 medical tasks, as well as SOTA results on specific\nmedical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all\ndemonstrate significant improvements over existing models with similar model\nsizes.\n","authors":["Yusheng Liao","Shuyang Jiang","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17484v1.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.16512v5","updated":"2024-06-25T11:54:23Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages. Our code is publicly\nreleased at https://github.com/SamuelCahyawijaya/in-context-alignment\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17474v1","updated":"2024-06-25T11:41:16Z","published":"2024-06-25T11:41:16Z","title":"Transformer-based Named Entity Recognition with Combined Data\n  Representation","summary":"  This study examines transformer-based models and their effectiveness in named\nentity recognition tasks. The study investigates data representation\nstrategies, including single, merged, and context, which respectively use one\nsentence, multiple sentences, and sentences joined with attention to context\nper vector. Analysis shows that training models with a single strategy may lead\nto poor performance on different data representations. To address this\nlimitation, the study proposes a combined training procedure that utilizes all\nthree strategies to improve model stability and adaptability. The results of\nthis approach are presented and discussed for four languages (English, Polish,\nCzech, and German) across various datasets, demonstrating the effectiveness of\nthe combined strategy.\n","authors":["Michał Marcińczuk"],"pdf_url":"https://arxiv.org/pdf/2406.17474v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.17465v1","updated":"2024-06-25T11:12:01Z","published":"2024-06-25T11:12:01Z","title":"Enhancing Tool Retrieval with Iterative Feedback from Large Language\n  Models","summary":"  Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.\n","authors":["Qiancheng Xu","Yongqi Li","Heming Xia","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2406.17465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10918v3","updated":"2024-06-25T10:50:09Z","published":"2024-06-16T12:46:40Z","title":"Embodied Question Answering via Multi-LLM Systems","summary":"  Embodied Question Answering (EQA) is an important problem, which involves an\nagent exploring the environment to answer user queries. In the existing\nliterature, EQA has exclusively been studied in single-agent scenarios, where\nexploration can be time-consuming and costly. In this work, we consider EQA in\na multi-agent framework involving multiple large language models (LLM) based\nagents independently answering queries about a household environment. To\ngenerate one answer for each query, we use the individual responses to train a\nCentral Answer Model (CAM) that aggregates responses for a robust answer. Using\nCAM, we observe a $50\\%$ higher EQA accuracy when compared against aggregation\nmethods for ensemble LLM, such as voting schemes and debates. CAM does not\nrequire any form of agent communication, alleviating it from the associated\ncosts. We ablate CAM with various nonlinear (neural network, random forest,\ndecision tree, XGBoost) and linear (logistic regression classifier, SVM)\nalgorithms. Finally, we present a feature importance analysis for CAM via\npermutation feature importance (PFI), quantifying CAMs reliance on each\nindependent agent and query context.\n","authors":["Bhrij Patel","Vishnu Sashank Dorbala","Dinesh Manocha","Amrit Singh Bedi"],"pdf_url":"https://arxiv.org/pdf/2406.10918v3.pdf","comment":"17 pages, 13 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2406.17456v1","updated":"2024-06-25T10:49:56Z","published":"2024-06-25T10:49:56Z","title":"Improving Grammatical Error Correction via Contextual Data Augmentation","summary":"  Nowadays, data augmentation through synthetic data has been widely used in\nthe field of Grammatical Error Correction (GEC) to alleviate the problem of\ndata scarcity. However, these synthetic data are mainly used in the\npre-training phase rather than the data-limited fine-tuning phase due to\ninconsistent error distribution and noisy labels. In this paper, we propose a\nsynthetic data construction method based on contextual augmentation, which can\nensure an efficient augmentation of the original data with a more consistent\nerror distribution. Specifically, we combine rule-based substitution with\nmodel-based generation, using the generative model to generate a richer context\nfor the extracted error patterns. Besides, we also propose a relabeling-based\ndata cleaning method to mitigate the effects of noisy labels in synthetic data.\nExperiments on CoNLL14 and BEA19-Test show that our proposed augmentation\nmethod consistently and substantially outperforms strong baselines and achieves\nthe state-of-the-art level with only a few synthetic data.\n","authors":["Yixuan Wang","Baoxin Wang","Yijun Liu","Qingfu Zhu","Dayong Wu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2406.17456v1.pdf","comment":"Accepted as Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2406.17453v1","updated":"2024-06-25T10:44:01Z","published":"2024-06-25T10:44:01Z","title":"Learning to Ask Informative Questions: Enhancing LLMs with Preference\n  Optimization and Expected Information Gain","summary":"  Questions are essential tools for acquiring the necessary information to\ncomplete information-seeking tasks. However, large language models (LLMs),\nespecially open-source models, often perform poorly in generating informative\nquestions, as measured by expected information gain (EIG). In this paper, we\npropose a method to enhance the informativeness of LLM-generated questions in\n20-question game dialogues. We sample multiple questions from the same model\n(LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG\nquestions to apply a Direct Preference Optimization (DPO) algorithm. Our\nresults show that this method produces more effective questions (in terms of\nEIG), even in domains different from those used to train the DPO model.\n","authors":["Davide Mazzaccara","Alberto Testoni","Raffaella Bernardi"],"pdf_url":"https://arxiv.org/pdf/2406.17453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19220v3","updated":"2024-06-25T10:41:43Z","published":"2024-05-29T16:00:46Z","title":"WRDScore: New Metric for Evaluation of Natural Language Generation\n  Models","summary":"  The problem of natural language generation, and, more specifically, method\nname prediction, faces significant difficulties when proposed models need to be\nevaluated on test data. Such a metric would need to consider the versatility\nwith which a single method can be named, with respect to both semantics and\nsyntax. Measuring the direct overlap between the predicted and reference (true)\nsequences will not be able to capture these subtleties. Other existing\nembedding based metrics either do not measure precision and recall or impose\nstrict unrealistic assumptions on both sequences. To address these issues, we\npropose a new metric that, on the one hand, is very simple and lightweight,\nand, on the other hand, is able to calculate precision and recall without\nresorting to any assumptions while obtaining good performance with respect to\nthe human judgement.\n","authors":["Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2405.19220v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09910v2","updated":"2024-06-25T10:33:41Z","published":"2024-02-15T12:17:15Z","title":"DE-COP: Detecting Copyrighted Content in Language Models Training Data","summary":"  How can we detect if copyrighted content was used in the training process of\na language model, considering that the training data is typically undisclosed?\nWe are motivated by the premise that a language model is likely to identify\nverbatim excerpts from its training text. We propose DE-COP, a method to\ndetermine whether a piece of copyrighted content was included in training.\nDE-COP's core approach is to probe an LLM with multiple-choice questions, whose\noptions include both verbatim text and their paraphrases. We construct\nBookTection, a benchmark with excerpts from 165 books published prior and\nsubsequent to a model's training cutoff, along with their paraphrases. Our\nexperiments show that DE-COP surpasses the prior best method by 9.6% in\ndetection performance (AUC) on models with logits available. Moreover, DE-COP\nalso achieves an average accuracy of 72% for detecting suspect books on fully\nblack-box models where prior methods give approximately 4% accuracy. The code\nand datasets are available at https://github.com/LeiLiLab/DE-COP.\n","authors":["André V. Duarte","Xuandong Zhao","Arlindo L. Oliveira","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2402.09910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17430v1","updated":"2024-06-25T10:08:45Z","published":"2024-06-25T10:08:45Z","title":"Towards Probing Speech-Specific Risks in Large Multimodal Models: A\n  Taxonomy, Benchmark, and Insights","summary":"  Large Multimodal Models (LMMs) have achieved great success recently,\ndemonstrating a strong capability to understand multimodal information and to\ninteract with human users. Despite the progress made, the challenge of\ndetecting high-risk interactions in multimodal settings, and in particular in\nspeech modality, remains largely unexplored. Conventional research on risk for\nspeech modality primarily emphasises the content (e.g., what is captured as\ntranscription). However, in speech-based interactions, paralinguistic cues in\naudio can significantly alter the intended meaning behind utterances. In this\nwork, we propose a speech-specific risk taxonomy, covering 8 risk categories\nunder hostility (malicious sarcasm and threats), malicious imitation (age,\ngender, ethnicity), and stereotypical biases (age, gender, ethnicity). Based on\nthe taxonomy, we create a small-scale dataset for evaluating current LMMs\ncapability in detecting these categories of risk. We observe even the latest\nmodels remain ineffective to detect various paralinguistic-specific risks in\nspeech (e.g., Gemini 1.5 Pro is performing only slightly above random\nbaseline). Warning: this paper contains biased and offensive examples.\n","authors":["Hao Yang","Lizhen Qu","Ehsan Shareghi","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.17430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11725v2","updated":"2024-06-25T09:53:44Z","published":"2023-05-19T15:01:48Z","title":"S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid\n  Question Answering","summary":"  Answering multi-hop questions over hybrid factual knowledge from the given\ntext and table (TextTableQA) is a challenging task. Existing models mainly\nadopt a retriever-reader framework, which have several deficiencies, such as\nnoisy labeling in training retriever, insufficient utilization of heterogeneous\ninformation over text and table, and deficient ability for different reasoning\noperations. In this paper, we propose a three-stage TextTableQA framework\nS3HQA, which comprises of retriever, selector, and reasoner. We use a retriever\nwith refinement training to solve the noisy labeling problem. Then, a hybrid\nselector considers the linked relationships between heterogeneous data to\nselect the most relevant factual knowledge. For the final stage, instead of\nadapting a reading comprehension module like in previous methods, we employ a\ngeneration-based reasoner to obtain answers. This includes two approaches: a\nrow-wise generator and an LLM prompting generator~(first time used in this\ntask). The experimental results demonstrate that our method achieves\ncompetitive results in the few-shot setting. When trained on the full dataset,\nour approach outperforms all baseline methods, ranking first on the HybridQA\nleaderboard.\n","authors":["Fangyu Lei","Xiang Li","Yifan Wei","Shizhu He","Yiming Huang","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2305.11725v2.pdf","comment":"ACL 2023"},{"id":"http://arxiv.org/abs/2406.17419v1","updated":"2024-06-25T09:42:56Z","published":"2024-06-25T09:42:56Z","title":"Leave No Document Behind: Benchmarking Long-Context LLMs with Extended\n  Multi-Doc QA","summary":"  Long-context modeling capabilities have garnered widespread attention,\nleading to the emergence of Large Language Models (LLMs) with ultra-context\nwindows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually\ncatching up. However, existing benchmarks employ irrelevant noise texts to\nartificially extend the length of test cases, diverging from the real-world\nscenarios of long-context applications. To bridge this gap, we propose a novel\nlong-context benchmark, Loong, aligning with realistic scenarios through\nextended multi-document question answering (QA). Unlike typical document QA, in\nLoong's test cases, each document is relevant to the final answer, ignoring any\ndocument will lead to the failure of the answer. Furthermore, Loong introduces\nfour types of tasks with a range of context lengths: Spotlight Locating,\nComparison, Clustering, and Chain of Reasoning, to facilitate a more realistic\nand comprehensive evaluation of long-context understanding. Extensive\nexperiments indicate that existing long-context language models still exhibit\nconsiderable potential for enhancement. Retrieval augmented generation (RAG)\nachieves poor performance, demonstrating that Loong can reliably assess the\nmodel's long-context modeling capabilities.\n","authors":["Minzheng Wang","Longze Chen","Cheng Fu","Shengyi Liao","Xinghua Zhang","Bingli Wu","Haiyang Yu","Nan Xu","Lei Zhang","Run Luo","Yunshui Li","Min Yang","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2406.17419v1.pdf","comment":"We release our code and data publicly at\n  https://github.com/MozerWang/Loong"},{"id":"http://arxiv.org/abs/2406.17415v1","updated":"2024-06-25T09:37:15Z","published":"2024-06-25T09:37:15Z","title":"Variable Layer-Wise Quantization: A Simple and Effective Approach to\n  Quantize LLMs","summary":"  We present a simple variable quantization approach that quantizes different\nlayers of a large language model (LLM) at different bit levels. Specifically,\nwe quantize the most important layers to higher bit precision and less\nimportant layers to lower bits to achieve floating point quantization levels.\nWe propose two effective strategies to measure the importance of layers within\nLLMs: the first measures the importance of a layer based on how different its\noutput embeddings are from the input embeddings (the higher the better); the\nsecond estimates the importance of a layer using the number of layer weights\nthat are much larger than average (the smaller the better). We show that\nquantizing different layers at varying bits according to our importance scores\nresults in minimal performance drop with a far more compressed model size.\nFinally, we present several practical key takeaways from our variable\nlayer-wise quantization experiments: (a) LLM performance under variable\nquantization remains close to the original model until 25-50% of layers are\nmoved in lower quantization using our proposed ordering but only until 5-10% if\nmoved using no specific ordering; (b) Quantizing LLMs to lower bits performs\nsubstantially better than pruning unless extreme quantization (2-bit) is used;\nand (c) Layer-wise quantization to lower bits works better in the case of\nlarger LLMs with more layers compared to smaller LLMs with fewer layers. The\ncode used to run the experiments is available at:\nhttps://github.com/RazvanDu/LayerwiseQuant.\n","authors":["Razvan-Gabriel Dumitru","Vikas Yadav","Rishabh Maheshwary","Paul-Ioan Clotan","Sathwik Tejaswi Madhusudhan","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2406.17415v1.pdf","comment":"submitted to EMNLP, 15 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.04666v2","updated":"2024-06-25T09:28:43Z","published":"2024-03-07T17:13:12Z","title":"Telecom Language Models: Must They Be Large?","summary":"  The increasing interest in Large Language Models (LLMs) within the\ntelecommunications sector underscores their potential to revolutionize\noperational efficiency. However, the deployment of these sophisticated models\nis often hampered by their substantial size and computational demands, raising\nconcerns about their viability in resource-constrained environments. Addressing\nthis challenge, recent advancements have seen the emergence of small language\nmodels that surprisingly exhibit performance comparable to their larger\ncounterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a\ncompact yet powerful model, exemplifies this new wave of efficient small\nlanguage models. This paper conducts a comprehensive evaluation of Phi-2's\nintrinsic understanding of the telecommunications domain. Recognizing the\nscale-related limitations, we enhance Phi-2's capabilities through a\nRetrieval-Augmented Generation approach, meticulously integrating an extensive\nknowledge base specifically curated with telecom standard specifications. The\nenhanced Phi-2 model demonstrates a profound improvement in accuracy, answering\nquestions about telecom standards with a precision that closely rivals the more\nresource-intensive GPT-3.5. The paper further explores the refined capabilities\nof Phi-2 in addressing problem-solving scenarios within the telecom sector,\nhighlighting its potential and limitations.\n","authors":["Nicola Piovesan","Antonio De Domenico","Fadhel Ayed"],"pdf_url":"https://arxiv.org/pdf/2403.04666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17404v1","updated":"2024-06-25T09:25:39Z","published":"2024-06-25T09:25:39Z","title":"Make Some Noise: Unlocking Language Model Parallel Inference Capability\n  through Noisy Training","summary":"  Existing speculative decoding methods typically require additional model\nstructure and training processes to assist the model for draft token\ngeneration. This makes the migration of acceleration methods to the new model\nmore costly and more demanding on device memory. To address this problem, we\npropose the Make Some Noise (MSN) training framework as a replacement for the\nsupervised fine-tuning stage of the large language model. The training method\nsimply introduces some noise at the input for the model to learn the denoising\ntask. It significantly enhances the parallel decoding capability of the model\nwithout affecting the original task capability. In addition, we propose a\ntree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further\nimprove the inference speed of MSN models. Experiments in both the general and\ncode domains have shown that MSN can improve inference speed by 2.3-2.7x times\nwithout compromising model performance. The MSN model also achieves comparable\nacceleration ratios to the SOTA model with additional model structure on\nSpec-Bench.\n","authors":["Yixuan Wang","Xianzhen Luo","Fuxuan Wei","Yijun Liu","Qingfu Zhu","Xuanyu Zhang","Qing Yang","Dongliang Xu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2406.17404v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.02659v2","updated":"2024-06-25T09:24:01Z","published":"2024-05-04T12:59:10Z","title":"R4: Reinforced Retriever-Reorder-Responder for Retrieval-Augmented Large\n  Language Models","summary":"  Retrieval-augmented large language models (LLMs) leverage relevant content\nretrieved by information retrieval systems to generate correct responses,\naiming to alleviate the hallucination problem. However, existing\nretriever-responder methods typically append relevant documents to the prompt\nof LLMs to perform text generation tasks without considering the interaction of\nfine-grained structural semantics between the retrieved documents and the LLMs.\nThis issue is particularly important for accurate response generation as LLMs\ntend to \"lose in the middle\" when dealing with input prompts augmented with\nlengthy documents. In this work, we propose a new pipeline named \"Reinforced\nRetriever-Reorder-Responder\" (R$^4$) to learn document orderings for\nretrieval-augmented LLMs, thereby further enhancing their generation abilities\nwhile the large numbers of parameters of LLMs remain frozen. The reordering\nlearning process is divided into two steps according to the quality of the\ngenerated responses: document order adjustment and document representation\nenhancement. Specifically, document order adjustment aims to organize retrieved\ndocument orderings into beginning, middle, and end positions based on graph\nattention learning, which maximizes the reinforced reward of response quality.\nDocument representation enhancement further refines the representations of\nretrieved documents for responses of poor quality via document-level gradient\nadversarial learning. Extensive experiments demonstrate that our proposed\npipeline achieves better factual question-answering performance on\nknowledge-intensive tasks compared to strong baselines across various public\ndatasets. The source codes and trained models will be released upon paper\nacceptance.\n","authors":["Taolin Zhang","Dongyang Li","Qizhou Chen","Chengyu Wang","Longtao Huang","Hui Xue","Xiaofeng He","Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2405.02659v2.pdf","comment":"need to further experiment"},{"id":"http://arxiv.org/abs/2406.17385v1","updated":"2024-06-25T09:04:21Z","published":"2024-06-25T09:04:21Z","title":"Native Design Bias: Studying the Impact of English Nativeness on\n  Language Model Performance","summary":"  Large Language Models (LLMs) excel at providing information acquired during\npretraining on large-scale corpora and following instructions through user\nprompts. This study investigates whether the quality of LLM responses varies\ndepending on the demographic profile of users. Considering English as the\nglobal lingua franca, along with the diversity of its dialects among speakers\nof different native languages, we explore whether non-native English speakers\nreceive lower-quality or even factually incorrect responses from LLMs more\nfrequently. Our results show that performance discrepancies occur when LLMs are\nprompted by native versus non-native English speakers and persist when\ncomparing native speakers from Western countries with others. Additionally, we\nfind a strong anchoring effect when the model recognizes or is made aware of\nthe user's nativeness, which further degrades the response quality when\ninteracting with non-native speakers. Our analysis is based on a newly\ncollected dataset with over 12,000 unique annotations from 124 annotators,\nincluding information on their native language and English proficiency.\n","authors":["Manon Reusens","Philipp Borchert","Jochen De Weerdt","Bart Baesens"],"pdf_url":"https://arxiv.org/pdf/2406.17385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17378v1","updated":"2024-06-25T08:55:12Z","published":"2024-06-25T08:55:12Z","title":"A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens","summary":"  Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nembedding LLMs, the obtained text embedding will be able to be aligned with the\nkey tokens in the input text. We first fully analyze this phenomenon on eight\nembedding LLMs and show that this phenomenon is universal and is not affected\nby model architecture, training strategy, and embedding method. With a deeper\nanalysis, we then find that the main change in embedding space between the\nembedding LLMs and their original generative LLMs is in the first principal\ncomponent. By adjusting the first principal component, we can align text\nembedding with the key tokens. Finally, we give several examples to demonstrate\nthe vast application potential of this finding: (1) we propose a simple and\npractical sparse retrieval method based on the aligned tokens, which can\nachieve 80\\% of the dense retrieval effect of the same model while reducing the\ncomputation significantly; (2) we show that our findings provide a fresh\nperspective to help understand fuzzy concepts (e.g., semantic relatedness vs.\nsemantic similarity) and emerging technologies (e.g., instruction-following\nembedding) in this field.\n","authors":["Zhijie Nie","Richong Zhang","Zhanyu Wu"],"pdf_url":"https://arxiv.org/pdf/2406.17378v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2406.17377v1","updated":"2024-06-25T08:53:46Z","published":"2024-06-25T08:53:46Z","title":"A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual\n  LLMs","summary":"  Low-resource languages, by its very definition, tend to be under represented\nin the pre-training corpora of Large Language Models. In this work, we\ninvestigate three low-resource cross-lingual approaches that enable an LLM\nadapt to tasks in previously unseen languages. Llama-2 is an LLM where Indic\nlanguages, among many other language families, contribute to less than\n$0.005\\%$ of the total $2$ trillion token pre-training corpora. In this work,\nwe experiment with the English-dominated Llama-2 for cross-lingual transfer to\nthree Indic languages, Bengali, Hindi, and Tamil as target languages. We study\nthree approaches for cross-lingual transfer, under ICL and fine-tuning. One, we\nfind that adding additional supervisory signals via a dominant language in the\nLLM, leads to improvements, both under in-context learning and fine-tuning.\nTwo, adapting the target languages to word reordering may be beneficial under\nICL, but its impact diminishes with fine tuning. Finally, continued\npre-training in one low-resource language can improve model performance for\nother related low-resource languages.\n","authors":["Vaibhav Singh","Amrith Krishna","Karthika NJ","Ganesh Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2406.17377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17375v1","updated":"2024-06-25T08:49:11Z","published":"2024-06-25T08:49:11Z","title":"An Empirical Study on the Characteristics of Bias upon Context Length\n  Variation for Bangla","summary":"  Pretrained language models inherently exhibit various social biases,\nprompting a crucial examination of their social impact across various\nlinguistic contexts due to their widespread usage. Previous studies have\nprovided numerous methods for intrinsic bias measurements, predominantly\nfocused on high-resource languages. In this work, we aim to extend these\ninvestigations to Bangla, a low-resource language. Specifically, in this study,\nwe (1) create a dataset for intrinsic gender bias measurement in Bangla, (2)\ndiscuss necessary adaptations to apply existing bias measurement methods for\nBangla, and (3) examine the impact of context length variation on bias\nmeasurement, a factor that has been overlooked in previous studies. Through our\nexperiments, we demonstrate a clear dependency of bias metrics on context\nlength, highlighting the need for nuanced considerations in Bangla bias\nanalysis. We consider our work as a stepping stone for bias measurement in the\nBangla Language and make all of our resources publicly available to support\nfuture research.\n","authors":["Jayanta Sadhu","Ayan Antik Khan","Abhik Bhattacharjee","Rifat Shahriyar"],"pdf_url":"https://arxiv.org/pdf/2406.17375v1.pdf","comment":"Accepted in Findings of ACL, 2024"},{"id":"http://arxiv.org/abs/2406.14868v2","updated":"2024-06-25T08:44:24Z","published":"2024-06-21T05:13:20Z","title":"Direct Multi-Turn Preference Optimization for Language Agents","summary":"  Adapting Large Language Models (LLMs) for agent tasks is critical in\ndeveloping language agents. Direct Preference Optimization (DPO) is a promising\ntechnique for this adaptation with the alleviation of compounding errors,\noffering a means to directly optimize Reinforcement Learning (RL) objectives.\nHowever, applying DPO to multi-turn tasks presents challenges due to the\ninability to cancel the partition function. Overcoming this obstacle involves\nmaking the partition function independent of the current state and addressing\nlength disparities between preferred and dis-preferred trajectories. In this\nlight, we replace the policy constraint with the state-action occupancy measure\nconstraint in the RL objective and add length normalization to the\nBradley-Terry model, yielding a novel loss function named DMPO for multi-turn\nagent tasks with theoretical explanations. Extensive experiments on three\nmulti-turn agent task datasets confirm the effectiveness and superiority of the\nDMPO loss.\n","authors":["Wentao Shi","Mengqi Yuan","Junkang Wu","Qifan Wang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2406.14868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00697v2","updated":"2024-06-25T08:42:53Z","published":"2024-06-02T10:25:02Z","title":"Comprehensive Evaluation of Large Language Models for Topic Modeling","summary":"  Recent work utilizes Large Language Models (LLMs) for topic modeling,\ngenerating comprehensible topic labels for given documents. However, their\nperformance has mainly been evaluated qualitatively, and there remains room for\nquantitative investigation of their capabilities. In this paper, we\nquantitatively evaluate LLMs from multiple perspectives: the quality of topics,\nthe impact of LLM-specific concerns, such as hallucination and shortcuts for\nlimited documents, and LLMs' controllability of topic categories via prompts.\nOur findings show that LLMs can identify coherent and diverse topics with few\nhallucinations but may take shortcuts by focusing only on parts of documents.\nWe also found that their controllability is limited.\n","authors":["Tomoki Doi","Masaru Isonuma","Hitomi Yanaka"],"pdf_url":"https://arxiv.org/pdf/2406.00697v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17363v1","updated":"2024-06-25T08:26:41Z","published":"2024-06-25T08:26:41Z","title":"Leveraging Synthetic Audio Data for End-to-End Low-Resource Speech\n  Translation","summary":"  This paper describes our system submission to the International Conference on\nSpoken Language Translation (IWSLT 2024) for Irish-to-English speech\ntranslation. We built end-to-end systems based on Whisper, and employed a\nnumber of data augmentation techniques, such as speech back-translation and\nnoise augmentation. We investigate the effect of using synthetic audio data and\ndiscuss several methods for enriching signal diversity.\n","authors":["Yasmin Moslem"],"pdf_url":"https://arxiv.org/pdf/2406.17363v1.pdf","comment":"IWSLT 2024"},{"id":"http://arxiv.org/abs/2402.13414v2","updated":"2024-06-25T08:26:19Z","published":"2024-02-20T22:50:41Z","title":"Harnessing Large Language Models as Post-hoc Correctors","summary":"  As Machine Learning (ML) models grow in size and demand higher-quality\ntraining data, the expenses associated with re-training and fine-tuning these\nmodels are escalating rapidly. Inspired by recent impressive achievements of\nLarge Language Models (LLMs) in different fields, this paper delves into the\nquestion: can LLMs efficiently improve an ML's performance at a minimal cost?\nWe show that, through our proposed training-free framework LlmCorr, an LLM can\nwork as a post-hoc corrector to propose corrections for the predictions of an\narbitrary ML model. In particular, we form a contextual knowledge database by\nincorporating the dataset's label information and the ML model's predictions on\nthe validation dataset. Leveraging the in-context learning capability of LLMs,\nwe ask the LLM to summarise the instances in which the ML model makes mistakes\nand the correlation between primary predictions and true labels. Following\nthis, the LLM can transfer its acquired knowledge to suggest corrections for\nthe ML model's predictions. Our experimental results on text analysis and the\nchallenging molecular predictions show that \\model improves the performance of\na number of models by up to 39%.\n","authors":["Zhiqiang Zhong","Kuangyu Zhou","Davide Mottin"],"pdf_url":"https://arxiv.org/pdf/2402.13414v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06589v2","updated":"2024-06-25T08:23:03Z","published":"2024-06-05T13:55:27Z","title":"PatentEval: Understanding Errors in Patent Generation","summary":"  In this work, we introduce a comprehensive error typology specifically\ndesigned for evaluating two distinct tasks in machine-generated patent texts:\nclaims-to-abstract generation, and the generation of the next claim given\nprevious ones. We have also developed a benchmark, PatentEval, for\nsystematically assessing language models in this context. Our study includes a\ncomparative analysis, annotated by humans, of various models. These range from\nthose specifically adapted during training for tasks within the patent domain\nto the latest general-purpose large language models (LLMs). Furthermore, we\nexplored and evaluated some metrics to approximate human judgments in patent\ntext evaluation, analyzing the extent to which these metrics align with expert\nassessments. These approaches provide valuable insights into the capabilities\nand limitations of current language models in the specialized field of patent\ntext generation.\n","authors":["You Zuo","Kim Gerdes","Eric Villemonte de La Clergerie","Benoît Sagot"],"pdf_url":"https://arxiv.org/pdf/2406.06589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13275v2","updated":"2024-06-25T08:07:36Z","published":"2024-06-19T07:09:46Z","title":"Enhancing Automated Audio Captioning via Large Language Models with\n  Optimized Audio Encoding","summary":"  Automated audio captioning (AAC) is an audio-to-text task to describe audio\ncontents in natural language. Recently, the advancements in large language\nmodels (LLMs), with improvements in training approaches for audio encoders,\nhave opened up possibilities for improving AAC. Thus, we explore enhancing AAC\nfrom three aspects: 1) a pre-trained audio encoder via consistent ensemble\ndistillation (CED) is used to improve the effectivity of acoustic tokens, with\na querying transformer (Q-Former) bridging the modality gap to LLM and compress\nacoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B\nparameters as the decoder; 3) another pre-trained LLM corrects text errors\ncaused by insufficient training data and annotation ambiguities. Both the audio\nencoder and text decoder are optimized by low-rank adaptation (LoRA).\nExperiments show that each of these enhancements is effective. Our method\nobtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.\n","authors":["Jizhong Liu","Gang Li","Junbo Zhang","Heinrich Dinkel","Yongqing Wang","Zhiyong Yan","Yujun Wang","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.13275v2.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.15485v2","updated":"2024-06-25T07:37:22Z","published":"2024-06-17T11:00:04Z","title":"SegHist: A General Segmentation-based Framework for Chinese Historical\n  Document Text Line Detection","summary":"  Text line detection is a key task in historical document analysis facing many\nchallenges of arbitrary-shaped text lines, dense texts, and text lines with\nhigh aspect ratios, etc. In this paper, we propose a general framework for\nhistorical document text detection (SegHist), enabling existing\nsegmentation-based text detection methods to effectively address the\nchallenges, especially text lines with high aspect ratios. Integrating the\nSegHist framework with the commonly used method DB++, we develop DB-SegHist.\nThis approach achieves SOTA on the CHDAC, MTHv2, and competitive results on\nHDRC datasets, with a significant improvement of 1.19% on the most challenging\nCHDAC dataset which features more text lines with high aspect ratios. Moreover,\nour method attains SOTA on rotated MTHv2 and rotated HDRC, demonstrating its\nrotational robustness. The code is available at\nhttps://github.com/LumionHXJ/SegHist.\n","authors":["Xingjian Hu","Baole Wei","Liangcai Gao"],"pdf_url":"https://arxiv.org/pdf/2406.15485v2.pdf","comment":"Accepted by ICDAR2024"},{"id":"http://arxiv.org/abs/2406.17328v1","updated":"2024-06-25T07:25:15Z","published":"2024-06-25T07:25:15Z","title":"Dual-Space Knowledge Distillation for Large Language Models","summary":"  Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.\n","authors":["Songming Zhang","Xue Zhang","Zengkui Sun","Yufeng Chen","Jinan Xu"],"pdf_url":"https://arxiv.org/pdf/2406.17328v1.pdf","comment":"17 pages, 11 figures, code available at:\n  https://github.com/songmzhang/DSKD"},{"id":"http://arxiv.org/abs/2406.17324v1","updated":"2024-06-25T07:15:10Z","published":"2024-06-25T07:15:10Z","title":"Delving into the Utilisation of ChatGPT in Scientific Publications in\n  Astronomy","summary":"  Rapid progress in the capabilities of machine learning approaches in natural\nlanguage processing has culminated in the rise of large language models over\nthe last two years. Recent works have shown unprecedented adoption of these for\nacademic writing, especially in some fields, but their pervasiveness in\nastronomy has not been studied sufficiently. To remedy this, we extract words\nthat ChatGPT uses more often than humans when generating academic text and\nsearch a total of 1 million articles for them. This way, we assess the\nfrequency of word occurrence in published works in astronomy tracked by the\nNASA Astrophysics Data System since 2000. We then perform a statistical\nanalysis of the occurrences. We identify a list of words favoured by ChatGPT\nand find a statistically significant increase for these words against a control\ngroup in 2024, which matches the trend in other disciplines. These results\nsuggest a widespread adoption of these models in the writing of astronomy\npapers. We encourage organisations, publishers, and researchers to work\ntogether to identify ethical and pragmatic guidelines to maximise the benefits\nof these systems while maintaining scientific rigour.\n","authors":["Simone Astarita","Sandor Kruk","Jan Reerink","Pablo Gómez"],"pdf_url":"https://arxiv.org/pdf/2406.17324v1.pdf","comment":"Submitted to SPAICE"},{"id":"http://arxiv.org/abs/2406.14825v2","updated":"2024-06-25T07:13:15Z","published":"2024-06-21T01:52:37Z","title":"TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction\n  in RAG-based Crowdsourcing Systems","summary":"  Temporal relation extraction (TRE) aims to grasp the evolution of events or\nactions, and thus shape the workflow of associated tasks, so it holds promise\nin helping understand task requests initiated by requesters in crowdsourcing\nsystems. However, existing methods still struggle with limited and unevenly\ndistributed annotated data. Therefore, inspired by the abundant global\nknowledge stored within pre-trained language models (PLMs), we propose a\nmulti-task prompt learning framework for TRE (TemPrompt), incorporating prompt\ntuning and contrastive learning to tackle these issues. To elicit more\neffective prompts for PLMs, we introduce a task-oriented prompt construction\napproach that thoroughly takes the myriad factors of TRE into consideration for\nautomatic prompt generation. In addition, we present temporal event reasoning\nas a supplement to bolster the model's focus on events and temporal cues. The\nexperimental results demonstrate that TemPrompt outperforms all compared\nbaselines across the majority of metrics under both standard and few-shot\nsettings. A case study is provided to validate its effectiveness in\ncrowdsourcing scenarios.\n","authors":["Jing Yang","Yu Zhao","Yang Linyao","Xiao Wang","Long Chen","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14825v2.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.07710v2","updated":"2024-06-25T07:08:17Z","published":"2023-10-11T17:57:35Z","title":"A Resilient and Accessible Distribution-Preserving Watermark for Large\n  Language Models","summary":"  Watermarking techniques offer a promising way to identify machine-generated\ncontent via embedding covert information into the contents generated from\nlanguage models. A challenge in the domain lies in preserving the distribution\nof original generated content after watermarking. Our research extends and\nimproves upon existing watermarking framework, placing emphasis on the\nimportance of a \\textbf{Di}stribution-\\textbf{P}reserving (DiP) watermark.\nContrary to the current strategies, our proposed DiPmark simultaneously\npreserves the original token distribution during watermarking\n(distribution-preserving), is detectable without access to the language model\nAPI and prompts (accessible), and is provably robust to moderate changes of\ntokens (resilient). DiPmark operates by selecting a random set of tokens prior\nto the generation of a word, then modifying the token distribution through a\ndistribution-preserving reweight function to enhance the probability of these\nselected tokens during the sampling process. Extensive empirical evaluation on\nvarious language models and tasks demonstrates our approach's\ndistribution-preserving property, accessibility, and resilience, making it a\neffective solution for watermarking tasks that demand impeccable quality\npreservation.\n","authors":["Yihan Wu","Zhengmian Hu","Junfeng Guo","Hongyang Zhang","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2310.07710v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.17312v1","updated":"2024-06-25T06:49:16Z","published":"2024-06-25T06:49:16Z","title":"Not All Preference Pairs Are Created Equal: A Recipe for\n  Annotation-Efficient Iterative Preference Learning","summary":"  Iterative preference learning, though yielding superior performances,\nrequires online annotated preference labels. In this work, we study strategies\nto select worth-annotating response pairs for cost-efficient annotation while\nachieving competitive or even better performances compared with the random\nselection baseline for iterative preference learning. Built on assumptions\nregarding uncertainty and distribution shifts, we propose a comparative view to\nrank the implicit reward margins as predicted by DPO to select the response\npairs that yield more benefits. Through extensive experiments, we show that\nannotating those response pairs with small margins is generally better than\nlarge or random, under both single- and multi-iteration scenarios. Besides, our\nempirical results suggest allocating more annotation budgets in the earlier\niterations rather than later across multiple iterations.\n","authors":["Sen Yang","Leyang Cui","Deng Cai","Xinting Huang","Shuming Shi","Wai Lam"],"pdf_url":"https://arxiv.org/pdf/2406.17312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01290v2","updated":"2024-06-25T06:25:41Z","published":"2023-10-02T15:43:53Z","title":"Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language\n  Models","summary":"  We propose Knowledge Crosswords, a geometric knowledge reasoning benchmark\nconsisting of incomplete knowledge networks bounded by structured factual\nconstraints, where LLMs are tasked with inferring the missing facts to meet all\nconstraints. The novel setting of geometric knowledge reasoning necessitates\nnew LM abilities beyond existing atomic/linear multi-hop QA, such as\nbacktracking, verifying facts and constraints, reasoning with uncertainty, and\nmore. Knowledge Crosswords contains 2,101 individual problems, covering diverse\nknowledge domains, and is further divided into three difficulty levels. We\nconduct extensive experiments to evaluate existing LLMs and approaches on\nKnowledge Crosswords. Results demonstrate that baseline approaches struggle\nwith larger knowledge networks and semantically-equivalent entity distractors.\nIn light of their limitations, we propose two new approaches, Staged Prompting\nand Verify-All, to augment LLMs' abilities for error-aware backtracking and\nconstraint verification. Our Verify-All significantly outperforms prior methods\nand is more robust towards problems in the hard subset. Further analysis shows\nthat geometric knowledge reasoning poses new challenges to LLMs' knowledge\nabilities, particularly in robustness towards varying option orders, complex\nstructural constraints in knowledge networks, \"none of the above\" scenarios,\nand more.\n","authors":["Wenxuan Ding","Shangbin Feng","Yuhan Liu","Zhaoxuan Tan","Vidhisha Balachandran","Tianxing He","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2310.01290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17305v1","updated":"2024-06-25T06:24:50Z","published":"2024-06-25T06:24:50Z","title":"Retrieval Augmented Instruction Tuning for Open NER with Large Language\n  Models","summary":"  The strong capability of large language models (LLMs) has been applied to\ninformation extraction (IE) through either retrieval augmented prompting or\ninstruction tuning (IT). However, the best way to incorporate information with\nLLMs for IE remains an open question. In this paper, we explore Retrieval\nAugmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named\nentity recognition (NER). Specifically, for each training sample, we retrieve\nsemantically similar examples from the training dataset as the context and\nprepend them to the input of the original instruction. To evaluate our RA-IT\napproach more thoroughly, we construct a Chinese IT dataset for open NER and\nevaluate RA-IT in both English and Chinese scenarios. Experimental results\nverify the effectiveness of RA-IT across various data sizes and in both English\nand Chinese scenarios. We also conduct thorough studies to explore the impacts\nof various retrieval strategies in the proposed RA-IT framework. Code and data\nare available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER\n","authors":["Tingyu Xie","Jian Zhang","Yan Zhang","Yuanyuan Liang","Qi Li","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17304v1","updated":"2024-06-25T06:19:47Z","published":"2024-06-25T06:19:47Z","title":"Leveraging LLMs for Dialogue Quality Measurement","summary":"  In task-oriented conversational AI evaluation, unsupervised methods poorly\ncorrelate with human judgments, and supervised approaches lack generalization.\nRecent advances in large language models (LLMs) show robust zeroshot and\nfew-shot capabilities across NLP tasks. This paper explores using LLMs for\nautomated dialogue quality evaluation, experimenting with various\nconfigurations on public and proprietary datasets. Manipulating factors such as\nmodel size, in-context examples, and selection techniques, we examine\n\"chain-of-thought\" (CoT) reasoning and label extraction procedures. Our results\nshow that (1) larger models yield more accurate dialogue labels; (2)\nalgorithmic selection of in-context examples outperforms random selection; (3)\nCoT reasoning where an LLM is asked to provide justifications before outputting\nfinal labels improves performance; and (4) fine-tuned LLMs outperform\nout-of-the-box ones. Our results indicate that LLMs that are suitably\nfine-tuned and have sufficient reasoning capabilities can be leveraged for\nautomated dialogue evaluation.\n","authors":["Jinghan Jia","Abi Komma","Timothy Leffel","Xujun Peng","Ajay Nagesh","Tamer Soliman","Aram Galstyan","Anoop Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.17304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17300v1","updated":"2024-06-25T06:08:16Z","published":"2024-06-25T06:08:16Z","title":"CausalScore: An Automatic Reference-Free Metric for Assessing Response\n  Relevance in Open-Domain Dialogue Systems","summary":"  Automatically evaluating the quality of responses in open-domain dialogue\nsystems is a challenging but crucial task. Current evaluation metrics often\nfail to align with human judgments, especially when assessing responses that\nare grammatically correct. To address this issue, we propose a novel metric,\ncalled CausalScore, which assesses the relevance of responses by measuring the\ncausal strength between dialogue histories and responses. The causal strength\nis estimated by utilizing both unconditional dependence and conditional\ndependencies from the dialogue history to responses. We compare our metric with\nthe existing competitive metrics in terms of their alignment with human\njudgements. Our experimental results demonstrate that CausalScore significantly\nsurpasses existing state-of-the-art metrics by aligning better with human\njudgements. Additionally, we collect a new dialogue dataset CGDIALOG+ with\nhuman-annotated causal relations and a set of pairwise human judgements to\nfacilitate the development of future automatic metrics.\n","authors":["Tao Feng","Lizhen Qu","Xiaoxi Kang","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2406.17300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10882v3","updated":"2024-06-25T05:56:40Z","published":"2024-02-16T18:36:36Z","title":"Universal Prompt Optimizer for Safe Text-to-Image Generation","summary":"  Text-to-Image (T2I) models have shown great performance in generating images\nbased on textual prompts. However, these models are vulnerable to unsafe input\nto generate unsafe content like sexual, harassment and illegal-activity images.\nExisting studies based on image checker, model fine-tuning and embedding\nblocking are impractical in real-world applications. Hence, we propose the\nfirst universal prompt optimizer for safe T2I (POSI) generation in black-box\nscenario. We first construct a dataset consisting of toxic-clean prompt pairs\nby GPT-3.5 Turbo. To guide the optimizer to have the ability of converting\ntoxic prompt to clean prompt while preserving semantic information, we design a\nnovel reward function measuring toxicity and text alignment of generated images\nand train the optimizer through Proximal Policy Optimization. Experiments show\nthat our approach can effectively reduce the likelihood of various T2I models\nin generating inappropriate images, with no significant impact on text\nalignment. It is also flexible to be combined with methods to achieve better\nperformance. Our code is available at https://github.com/wzongyu/POSI.\n","authors":["Zongyu Wu","Hongcheng Gao","Yueze Wang","Xiang Zhang","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.10882v3.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2406.17294v1","updated":"2024-06-25T05:43:21Z","published":"2024-06-25T05:43:21Z","title":"Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large\n  Language Models","summary":"  Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, particularly in textual mathematical problem-solving. However,\nexisting open-source image instruction fine-tuning datasets, containing limited\nquestion-answer pairs per image, do not fully exploit visual information to\nenhance the multimodal mathematical reasoning capabilities of Multimodal LLMs\n(MLLMs). To bridge this gap, we address the lack of high-quality, diverse\nmultimodal mathematical datasets by collecting 40K high-quality images with\nquestion-answer pairs from 24 existing datasets and synthesizing 320K new\npairs, creating the MathV360K dataset, which enhances both the breadth and\ndepth of multimodal mathematical questions. We introduce Math-LLaVA, a\nLLaVA-1.5-based model fine-tuned with MathV360K. This novel approach\nsignificantly improves the multimodal mathematical reasoning capabilities of\nLLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V\non MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced\ngeneralizability, showing substantial improvements on the MMMU benchmark. Our\nresearch highlights the importance of dataset diversity and synthesis in\nadvancing MLLMs' mathematical reasoning abilities. The code and data are\navailable at: \\url{https://github.com/HZQ950419/Math-LLaVA}.\n","authors":["Wenhao Shi","Zhiqiang Hu","Yi Bin","Junhua Liu","Yang Yang","See-Kiong Ng","Lidong Bing","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2406.17294v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2406.17287v1","updated":"2024-06-25T05:30:55Z","published":"2024-06-25T05:30:55Z","title":"Predicting the Big Five Personality Traits in Chinese Counselling\n  Dialogues Using Large Language Models","summary":"  Accurate assessment of personality traits is crucial for effective\npsycho-counseling, yet traditional methods like self-report questionnaires are\ntime-consuming and biased. This study exams whether Large Language Models\n(LLMs) can predict the Big Five personality traits directly from counseling\ndialogues and introduces an innovative framework to perform the task. Our\nframework applies role-play and questionnaire-based prompting to condition LLMs\non counseling sessions, simulating client responses to the Big Five Inventory.\nWe evaluated our framework on 853 real-world counseling sessions, finding a\nsignificant correlation between LLM-predicted and actual Big Five traits,\nproving the validity of framework. Moreover, ablation studies highlight the\nimportance of role-play simulations and task simplification via questionnaires\nin enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model,\nutilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves\na 130.95\\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\\%\nin personality prediction validity. In conclusion, LLMs can predict personality\nbased on counseling dialogues. Our code and model are publicly available at\n\\url{https://github.com/kuri-leo/BigFive-LLM-Predictor}, providing a valuable\ntool for future research in computational psychometrics.\n","authors":["Yang Yan","Lizhi Ma","Anqi Li","Jingsong Ma","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2406.17287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17283v1","updated":"2024-06-25T05:18:12Z","published":"2024-06-25T05:18:12Z","title":"A Recursive Encoding for Cuneiform Signs","summary":"  One of the most significant problems in cuneiform pedagogy is the process of\nlooking up unknown signs, which often involves a tedious page-by-page search\nthrough a sign list. This paper proposes a new \"recursive encoding\" for signs,\nwhich represents the arrangement of strokes in a way a computer can process. A\nseries of new algorithms then offers students a new way to look up signs by any\ndistinctive component, as well as providing new ways to render signs and\ntablets electronically.\n","authors":["Daniel M. Stelzer"],"pdf_url":"https://arxiv.org/pdf/2406.17283v1.pdf","comment":"27 pages, 29 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.17282v1","updated":"2024-06-25T05:14:54Z","published":"2024-06-25T05:14:54Z","title":"BERT, Neural Information Retrieval, Boolean Retrieval, Negation\n  Retrieval","summary":"  We introduce SetBERT, a fine-tuned BERT-based model designed to enhance query\nembeddings for set operations and Boolean logic queries, such as Intersection\n(AND), Difference (NOT), and Union (OR). SetBERT significantly improves\nretrieval performance for logic-structured queries, an area where both\ntraditional and neural retrieval methods typically underperform. We propose an\ninnovative use of inversed-contrastive loss, focusing on identifying the\nnegative sentence, and fine-tuning BERT with a dataset generated via prompt\nGPT. Furthermore, we demonstrate that, unlike other BERT-based models,\nfine-tuning with triplet loss actually degrades performance for this specific\ntask. Our experiments reveal that SetBERT-base not only significantly\noutperforms BERT-base (up to a 63% improvement in Recall) but also achieves\nperformance comparable to the much larger BERT-large model, despite being only\none-third the size.\n","authors":["Quan Mai","Susan Gauch","Douglas Adams"],"pdf_url":"https://arxiv.org/pdf/2406.17282v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2309.05519v3","updated":"2024-06-25T05:01:09Z","published":"2023-09-11T15:02:25Z","title":"NExT-GPT: Any-to-Any Multimodal LLM","summary":"  While recently Multimodal Large Language Models (MM-LLMs) have made exciting\nstrides, they mostly fall prey to the limitation of only input-side multimodal\nunderstanding, without the ability to produce content in multiple modalities.\nAs we humans always perceive the world and communicate with people through\nvarious modalities, developing any-to-any MM-LLMs capable of accepting and\ndelivering content in any modality becomes essential to human-level AI. To fill\nthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,\nNExT-GPT. We connect an LLM with multimodal adaptors and different diffusion\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in\narbitrary combinations of text, images, videos, and audio. By leveraging the\nexisting well-trained highly-performing encoders and decoders, NExT-GPT is\ntuned with only a small amount of parameter (1%) of certain projection layers,\nwhich not only benefits low-cost training and also facilitates convenient\nexpansion to more potential modalities. Moreover, we introduce a\nmodality-switching instruction tuning (MosIT) and manually curate a\nhigh-quality dataset for MosIT, based on which NExT-GPT is empowered with\ncomplex cross-modal semantic understanding and content generation. Overall, our\nresearch showcases the promising possibility of building an AI agent capable of\nmodeling universal modalities, paving the way for more human-like AI research\nin the community. Project page: https://next-gpt.github.io/\n","authors":["Shengqiong Wu","Hao Fei","Leigang Qu","Wei Ji","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2309.05519v3.pdf","comment":"ICML 2024 (Oral)"},{"id":"http://arxiv.org/abs/2406.17276v1","updated":"2024-06-25T04:45:53Z","published":"2024-06-25T04:45:53Z","title":"OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure","summary":"  Autoregressive language models demonstrate excellent performance in various\nscenarios. However, the inference efficiency is limited by its\none-step-one-word generation mode, which has become a pressing problem recently\nas the models become increasingly larger. Speculative decoding employs a \"draft\nand then verify\" mechanism to allow multiple tokens to be generated in one\nstep, realizing lossless acceleration. Existing methods mainly adopt fixed\nheuristic draft structures, which fail to adapt to different situations to\nmaximize the acceptance length during verification. To alleviate this dilemma,\nwe proposed OPT-Tree, an algorithm to construct adaptive and scalable draft\ntrees. It searches the optimal tree structure that maximizes the mathematical\nexpectation of the acceptance length in each decoding step. Experimental\nresults reveal that OPT-Tree outperforms the existing draft structures and\nachieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.\nIf the draft model is powerful enough and the node budget is sufficient, it can\ngenerate more than ten tokens in a single step. Our code is available at\nhttps://github.com/Jikai0Wang/OPT-Tree.\n","authors":["Jikai Wang","Yi Su","Juntao Li","Qinrong Xia","Zi Ye","Xinyu Duan","Zhefeng Wang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13476v3","updated":"2024-06-25T04:45:19Z","published":"2024-06-19T11:57:42Z","title":"LLMs Are Zero-Shot Context-Aware Simultaneous Translators","summary":"  The advent of transformers has fueled progress in machine translation. More\nrecently large language models (LLMs) have come to the spotlight thanks to\ntheir generality and strong performance in a wide range of language tasks,\nincluding translation. Here we show that open-source LLMs perform on par with\nor better than some state-of-the-art baselines in simultaneous machine\ntranslation (SiMT) tasks, zero-shot. We also demonstrate that injection of\nminimal background information, which is easy with an LLM, brings further\nperformance gains, especially on challenging technical subject-matter. This\nhighlights LLMs' potential for building next generation of massively\nmultilingual, context-aware and terminologically accurate SiMT systems that\nrequire no resource-intensive training or fine-tuning.\n","authors":["Roman Koshkin","Katsuhito Sudoh","Satoshi Nakamura"],"pdf_url":"https://arxiv.org/pdf/2406.13476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17274v1","updated":"2024-06-25T04:41:17Z","published":"2024-06-25T04:41:17Z","title":"Can We Trust the Performance Evaluation of Uncertainty Estimation\n  Methods in Text Summarization?","summary":"  Text summarization, a key natural language generation (NLG) task, is vital in\nvarious domains. However, the high cost of inaccurate summaries in\nrisk-critical applications, particularly those involving human-in-the-loop\ndecision-making, raises concerns about the reliability of uncertainty\nestimation on text summarization (UE-TS) evaluation methods. This concern stems\nfrom the dependency of uncertainty model metrics on diverse and potentially\nconflicting NLG metrics. To address this issue, we introduce a comprehensive\nUE-TS benchmark incorporating 31 NLG metrics across four dimensions. The\nbenchmark evaluates the uncertainty estimation capabilities of two large\nlanguage models and one pre-trained language model on three datasets, with\nhuman-annotation analysis incorporated where applicable. We also assess the\nperformance of 14 common uncertainty estimation methods within this benchmark.\nOur findings emphasize the importance of considering multiple uncorrelated NLG\nmetrics and diverse uncertainty estimation methods to ensure reliable and\nefficient evaluation of UE-TS techniques.\n","authors":["Jianfeng He","Runing Yang","Linlin Yu","Changbin Li","Ruoxi Jia","Feng Chen","Ming Jin","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2406.17274v1.pdf","comment":"63 pages, 41 figures, 11 tables"},{"id":"http://arxiv.org/abs/2404.13071v2","updated":"2024-06-25T04:36:08Z","published":"2024-04-15T05:30:26Z","title":"Modeling Emotions and Ethics with Large Language Models","summary":"  This paper explores the integration of human-like emotions and ethical\nconsiderations into Large Language Models (LLMs). We first model eight\nfundamental human emotions, presented as opposing pairs, and employ\ncollaborative LLMs to reinterpret and express these emotions across a spectrum\nof intensity. Our focus extends to embedding a latent ethical dimension within\nLLMs, guided by a novel self-supervised learning algorithm with human feedback\n(SSHF). This approach enables LLMs to perform self-evaluations and adjustments\nconcerning ethical guidelines, enhancing their capability to generate content\nthat is not only emotionally resonant but also ethically aligned. The\nmethodologies and case studies presented herein illustrate the potential of\nLLMs to transcend mere text and image generation, venturing into the realms of\nempathetic interaction and principled decision-making, thereby setting a new\nprecedent in the development of emotionally aware and ethically conscious AI\nsystems.\n","authors":["Edward Y. Chang"],"pdf_url":"https://arxiv.org/pdf/2404.13071v2.pdf","comment":"8 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2406.17271v1","updated":"2024-06-25T04:27:53Z","published":"2024-06-25T04:27:53Z","title":"DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning\n  Graph","summary":"  The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce\nDynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing\ndata. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four\ndomains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to\ndynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.\n","authors":["Zhehao Zhang","Jiaao Chen","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2406.17271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17266v1","updated":"2024-06-25T04:20:49Z","published":"2024-06-25T04:20:49Z","title":"AG-LSEC: Audio Grounded Lexical Speaker Error Correction","summary":"  Speaker Diarization (SD) systems are typically audio-based and operate\nindependently of the ASR system in traditional speech transcription pipelines\nand can have speaker errors due to SD and/or ASR reconciliation, especially\naround speaker turns and regions of speech overlap. To reduce these errors, a\nLexical Speaker Error Correction (LSEC), in which an external language model\nprovides lexical information to correct the speaker errors, was recently\nproposed. Though the approach achieves good Word Diarization error rate (WDER)\nimprovements, it does not use any additional acoustic information and is prone\nto miscorrections. In this paper, we propose to enhance and acoustically ground\nthe LSEC system with speaker scores directly derived from the existing SD\npipeline. This approach achieves significant relative WDER reductions in the\nrange of 25-40% over the audio-based SD, ASR system and beats the LSEC system\nby 15-25% relative on RT03-CTS, Callhome American English and Fisher datasets.\n","authors":["Rohit Paturi","Xiang Li","Sundararajan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2406.17266v1.pdf","comment":"Accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.17262v1","updated":"2024-06-25T04:03:04Z","published":"2024-06-25T04:03:04Z","title":"D2LLM: Decomposed and Distilled Large Language Models for Semantic\n  Search","summary":"  The key challenge in semantic search is to create models that are both\naccurate and efficient in pinpointing relevant sentences for queries. While\nBERT-style bi-encoders excel in efficiency with pre-computed embeddings, they\noften miss subtle nuances in search tasks. Conversely, GPT-style LLMs with\ncross-encoder designs capture these nuances but are computationally intensive,\nhindering real-time applications. In this paper, we present D2LLMs-Decomposed\nand Distilled LLMs for semantic search-that combines the best of both worlds.\nWe decompose a cross-encoder into an efficient bi-encoder integrated with\nPooling by Multihead Attention and an Interaction Emulation Module, achieving\nnuanced understanding and pre-computability. Knowledge from the LLM is\ndistilled into this model using contrastive, rank, and feature imitation\ntechniques. Our experiments show that D2LLM surpasses five leading baselines in\nterms of all metrics across three tasks, particularly improving NLI task\nperformance by at least 6.45%. The source code is available at\nhttps://github.com/codefuse-ai/D2LLM.\n","authors":["Zihan Liao","Hang Yu","Jianguo Li","Jun Wang","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17261v1","updated":"2024-06-25T04:01:32Z","published":"2024-06-25T04:01:32Z","title":"TRAWL: Tensor Reduced and Approximated Weights for Large Language Models","summary":"  Large language models (LLMs) have fundamentally transformed artificial\nintelligence, catalyzing recent advancements while imposing substantial\nenvironmental and computational burdens. We introduce TRAWL (Tensor Reduced and\nApproximated Weights for Large Language Models), a novel methodology for\noptimizing LLMs through tensor decomposition. TRAWL leverages diverse\nstrategies to exploit matrices within transformer-based architectures,\nrealizing notable performance enhancements without necessitating retraining.\nThe most significant improvements were observed through a layer-by-layer\nintervention strategy, particularly when applied to fully connected weights of\nthe final layers, yielding up to 16% enhancement in accuracy without the need\nfor additional data or fine-tuning. These results underscore the importance of\ntargeted and adaptive techniques in increasing the efficiency and effectiveness\nof large language model optimization, thereby promoting the development of more\nsustainable and accessible AI systems.\n","authors":["Yiran Luo","Het Patel","Yu Fu","Dawon Ahn","Jia Chen","Yue Dong","Evangelos E. Papalexakis"],"pdf_url":"https://arxiv.org/pdf/2406.17261v1.pdf","comment":"8 pages, 5 figures. Submitted to EMNLP 2024 and under review"},{"id":"http://arxiv.org/abs/2406.17260v1","updated":"2024-06-25T03:56:33Z","published":"2024-06-25T03:56:33Z","title":"Mitigating Hallucination in Fictional Character Role-Play","summary":"  Role-playing has wide-ranging applications in customer support, embodied\nagents, computational social science, etc. The influence of parametric world\nknowledge of large language models (LLMs) often causes role-playing characters\nto act out of character and hallucinate about things outside the scope of their\nknowledge. In this work, we focus on the evaluation and mitigation of\nhallucination in fictional character role-play. We introduce a dataset with\nmore than 2,000 characters and 72,000 interviews, including 18,000 adversarial\nquestions. We propose RoleFact, a role-playing method that mitigates\nhallucination by modulating the influence of parametric knowledge using a\npre-calibrated confidence threshold. Experiments show that the proposed method\nimproves the factual precision of generated responses by 18% for adversarial\nquestions with a 44% reduction in temporal hallucination for time-sensitive\ninterviews. The code and the dataset will be available at\nhttps://github.com/NafisSadeq/rolefact.git.\n","authors":["Nafis Sadeq","Zhouhang Xie","Byungkyu Kang","Prarit Lamba","Xiang Gao","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2406.17260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17257v1","updated":"2024-06-25T03:50:54Z","published":"2024-06-25T03:50:54Z","title":"Leveraging Parameter-Efficient Transfer Learning for Multi-Lingual\n  Text-to-Speech Adaptation","summary":"  Different languages have distinct phonetic systems and vary in their prosodic\nfeatures making it challenging to develop a Text-to-Speech (TTS) model that can\neffectively synthesise speech in multilingual settings. Furthermore, TTS\narchitecture needs to be both efficient enough to capture nuances in multiple\nlanguages and efficient enough to be practical for deployment. The standard\napproach is to build transformer based model such as SpeechT5 and train it on\nlarge multilingual dataset. As the size of these models grow the conventional\nfine-tuning for adapting these model becomes impractical due to heavy\ncomputational cost. In this paper, we proposes to integrate parameter-efficient\ntransfer learning (PETL) methods such as adapters and hypernetwork with TTS\narchitecture for multilingual speech synthesis. Notably, in our experiments\nPETL methods able to achieve comparable or even better performance compared to\nfull fine-tuning with only $\\sim$2.5\\% tunable parameters.The code and samples\nare available at: https://anonymous.4open.science/r/multilingualTTS-BA4C.\n","authors":["Yingting Li","Ambuj Mehrish","Bryan Chew","Bo Cheng","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2406.17257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17255v1","updated":"2024-06-25T03:45:28Z","published":"2024-06-25T03:45:28Z","title":"MPCODER: Multi-user Personalized Code Generator with Explicit and\n  Implicit Style Representation Learning","summary":"  Large Language Models (LLMs) have demonstrated great potential for assisting\ndevelopers in their daily development. However, most research focuses on\ngenerating correct code, how to use LLMs to generate personalized code has\nseldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user\nPersonalized Code Generator) to generate personalized code for multiple users.\nTo better learn coding style features, we utilize explicit coding style\nresidual learning to capture the syntax code style standards and implicit style\nlearning to capture the semantic code style conventions. We train a multi-user\nstyle adapter to better differentiate the implicit feature representations of\ndifferent users through contrastive learning, ultimately enabling personalized\ncode generation for multiple users. We further propose a novel evaluation\nmetric for estimating similarities between codes of different coding styles.\nThe experimental results show the effectiveness of our approach for this novel\ntask.\n","authors":["Zhenlong Dai","Chang Yao","WenKang Han","Ying Yuan","Zhipeng Gao","Jingyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2406.17255v1.pdf","comment":"Accepted by ACL 2024, Main Conference"},{"id":"http://arxiv.org/abs/2406.17253v1","updated":"2024-06-25T03:41:02Z","published":"2024-06-25T03:41:02Z","title":"How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?","summary":"  As large language models (LLMs) are widely deployed, targeted editing of\ntheir knowledge has become a critical challenge. Recently, advancements in\nmodel editing techniques, such as Rank-One Model Editing (ROME), have paved the\nway for updating LLMs with new knowledge. However, the efficacy of these\nmethods varies across different types of knowledge. This study investigates the\ncapability of knowledge editing methods to incorporate new knowledge with\nvarying degrees of \"perplexingness\", a term we use to describe the initial\ndifficulty LLMs have in understanding new concepts. We begin by quantifying the\n\"perplexingness\" of target knowledge using pre-edit conditional probabilities,\nand assess the efficacy of edits through post-edit conditional probabilities.\nUtilizing the widely-used CounterFact dataset, we find significant negative\ncorrelations between the \"perplexingness\" of the new knowledge and the edit\nefficacy across all 12 scenarios. To dive deeper into this phenomenon, we\nintroduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym\npairs across diverse categories. Our analysis reveal that more abstract\nconcepts (hypernyms) tend to be more perplexing than their specific\ncounterparts (hyponyms). Further exploration into the influence of knowledge\nhierarchy on editing outcomes indicates that knowledge positioned at higher\nhierarchical levels is more challenging to modify in some scenarios. Our\nresearch highlights a previously overlooked aspect of LLM editing: the variable\nefficacy of editing methods in handling perplexing knowledge. By revealing how\nhierarchical relationships can influence editing outcomes, our findings offer\nnew insights into the challenges of updating LLMs and pave the way for more\nnuanced approaches to model editing in the future.\n","authors":["Huaizhi Ge","Frank Rudzicz","Zining Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.17253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17245v1","updated":"2024-06-25T03:24:06Z","published":"2024-06-25T03:24:06Z","title":"Unlocking Continual Learning Abilities in Language Models","summary":"  Language models (LMs) exhibit impressive performance and generalization\ncapabilities. However, LMs struggle with the persistent challenge of\ncatastrophic forgetting, which undermines their long-term sustainability in\ncontinual learning (CL). Existing approaches usually address the issue by\nincorporating old task data or task-wise inductive bias into LMs. However, old\ndata and accurate task information are often unavailable or costly to collect,\nhindering the availability of current CL approaches for LMs. To address this\nlimitation, we introduce $\\textbf{MIGU}$ ($\\textbf{M}$agn$\\textbf{I}$tude-based\n$\\textbf{G}$radient $\\textbf{U}$pdating for continual learning), a\nrehearsal-free and task-label-free method that only updates the model\nparameters with large magnitudes of output in LMs' linear layers. MIGU is based\non our observation that the L1-normalized magnitude distribution of the output\nin LMs' linear layers is different when the LM models deal with different task\ndata. By imposing this simple constraint on the gradient update process, we can\nleverage the inherent behaviors of LMs, thereby unlocking their innate CL\nabilities. Our experiments demonstrate that MIGU is universally applicable to\nall three LM architectures (T5, RoBERTa, and Llama2), delivering\nstate-of-the-art or on-par performance across continual finetuning and\ncontinual pre-training settings on four CL benchmarks. For example, MIGU brings\na 15.2% average accuracy improvement over conventional parameter-efficient\nfinetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly\nintegrate with all three existing CL types to further enhance performance. Code\nis available at \\href{https://github.com/wenyudu/MIGU}{this https URL}.\n","authors":["Wenyu Du","Shuang Cheng","Tongxu Luo","Zihan Qiu","Zeyu Huang","Ka Chun Cheung","Reynold Cheng","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2406.17245v1.pdf","comment":"preprint, 19 pages"},{"id":"http://arxiv.org/abs/2405.17441v2","updated":"2024-06-25T03:23:00Z","published":"2024-05-14T10:46:33Z","title":"When Large Language Models Meet Optical Networks: Paving the Way for\n  Automation","summary":"  Since the advent of GPT, large language models (LLMs) have brought about\nrevolutionary advancements in all walks of life. As a superior natural language\nprocessing (NLP) technology, LLMs have consistently achieved state-of-the-art\nperformance on numerous areas. However, LLMs are considered to be\ngeneral-purpose models for NLP tasks, which may encounter challenges when\napplied to complex tasks in specialized fields such as optical networks. In\nthis study, we propose a framework of LLM-empowered optical networks,\nfacilitating intelligent control of the physical layer and efficient\ninteraction with the application layer through an LLM-driven agent (AI-Agent)\ndeployed in the control layer. The AI-Agent can leverage external tools and\nextract domain knowledge from a comprehensive resource library specifically\nestablished for optical networks. This is achieved through user input and\nwell-crafted prompts, enabling the generation of control instructions and\nresult representations for autonomous operation and maintenance in optical\nnetworks. To improve LLM's capability in professional fields and stimulate its\npotential on complex tasks, the details of performing prompt engineering,\nestablishing domain knowledge library, and implementing complex tasks are\nillustrated in this study. Moreover, the proposed framework is verified on two\ntypical tasks: network alarm analysis and network performance optimization. The\ngood response accuracies and sematic similarities of 2,400 test situations\nexhibit the great potential of LLM in optical networks.\n","authors":["Danshi Wang","Yidi Wang","Xiaotian Jiang","Yao Zhang","Yue Pang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.17441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10963v2","updated":"2024-06-25T03:14:10Z","published":"2024-02-13T20:16:29Z","title":"GLoRe: When, Where, and How to Improve LLM Reasoning via Global and\n  Local Refinements","summary":"  State-of-the-art language models can exhibit impressive reasoning refinement\ncapabilities on math, science or coding tasks. However, recent work\ndemonstrates that even the best models struggle to identify \\textit{when and\nwhere to refine} without access to external feedback. Outcome-based Reward\nModels (\\textbf{ORMs}), trained to predict correctness of the final answer\nindicating when to refine, offer one convenient solution for deciding when to\nrefine. Process Based Reward Models (\\textbf{PRMs}), trained to predict\ncorrectness of intermediate steps, can then be used to indicate where to\nrefine. But they are expensive to train, requiring extensive human annotations.\nIn this paper, we propose Stepwise ORMs (\\textbf{SORMs}) which are trained,\nonly on synthetic data, to approximate the expected future reward of the\noptimal policy or $V^{\\star}$. More specifically, SORMs are trained to predict\nthe correctness of the final answer when sampling the current policy many times\n(rather than only once as in the case of ORMs). Our experiments show that SORMs\ncan more accurately detect incorrect reasoning steps compared to ORMs, thus\nimproving downstream accuracy when doing refinements. We then train\n\\textit{global} refinement models, which take only the question and a draft\nsolution as input and predict a corrected solution, and \\textit{local}\nrefinement models which also take as input a critique indicating the location\nof the first reasoning error. We generate training data for both models\nsynthetically by reusing data used to train the SORM. We find combining global\nand local refinements, using the ORM as a reranker, significantly outperforms\neither one individually, as well as a best of three sample baseline. With this\nstrategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned\nwith RL) on GSM8K from 53\\% to 65\\% when greedily sampled.\n","authors":["Alex Havrilla","Sharath Raparthy","Christoforus Nalmpantis","Jane Dwivedi-Yu","Maksym Zhuravinskyi","Eric Hambro","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2402.10963v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17241v1","updated":"2024-06-25T03:09:53Z","published":"2024-06-25T03:09:53Z","title":"What Do the Circuits Mean? A Knowledge Edit View","summary":"  In the field of language model interpretability, circuit discovery is gaining\npopularity. Despite this, the true meaning of these circuits remain largely\nunanswered. We introduce a novel method to learn their meanings as a holistic\nobject through the lens of knowledge editing. We extract circuits in the\nGPT2-XL model using diverse text classification datasets, and use hierarchical\nrelations datasets to explore knowledge editing in the circuits. Our findings\nindicate that these circuits contain entity knowledge but resist new knowledge\nmore than complementary circuits during knowledge editing. Additionally, we\nexamine the impact of circuit size, discovering that an ideal \"theoretical\ncircuit\" where essential knowledge is concentrated likely incorporates more\nthan 5% but less than 50% of the model's parameters. We also assess the overlap\nbetween circuits from different datasets, finding moderate similarities. What\nconstitutes these circuits, then? We find that up to 60% of the circuits\nconsist of layer normalization modules rather than attention or MLP modules,\nadding evidence to the ongoing debates regarding knowledge localization. In\nsummary, our findings offer new insights into the functions of the circuits,\nand introduce research directions for further interpretability and safety\nresearch of language models.\n","authors":["Huaizhi Ge","Frank Rudzicz","Zining Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.17241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04834v2","updated":"2024-06-25T02:54:19Z","published":"2024-06-07T11:01:15Z","title":"Annotating FrameNet via Structure-Conditioned Language Generation","summary":"  Despite the remarkable generative capabilities of language models in\nproducing naturalistic language, their effectiveness on explicit manipulation\nand generation of linguistic structures remain understudied. In this paper, we\ninvestigate the task of generating new sentences preserving a given semantic\nstructure, following the FrameNet formalism. We propose a framework to produce\nnovel frame-semantically annotated sentences following an\novergenerate-and-filter approach. Our results show that conditioning on rich,\nexplicit semantic information tends to produce generations with high human\nacceptance, under both prompting and finetuning. Our generated frame-semantic\nstructured annotations are effective at training data augmentation for\nframe-semantic role labeling in low-resource settings; however, we do not see\nbenefits under higher resource settings. Our study concludes that while\ngenerating high-quality, semantically rich data might be within reach, the\ndownstream utility of such generations remains to be seen, highlighting the\noutstanding challenges with automating linguistic annotation tasks.\n","authors":["Xinyue Cui","Swabha Swayamdipta"],"pdf_url":"https://arxiv.org/pdf/2406.04834v2.pdf","comment":"This paper has been accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.17233v1","updated":"2024-06-25T02:37:53Z","published":"2024-06-25T02:37:53Z","title":"Self-Constructed Context Decompilation with Fined-grained Alignment\n  Enhancement","summary":"  Decompilation transforms compiled code back into a high-level programming\nlanguage for analysis when source code is unavailable. Previous work has\nprimarily focused on enhancing decompilation performance by increasing the\nscale of model parameters or training data for pre-training. Based on the\ncharacteristics of the decompilation task, we propose two methods: (1) Without\nfine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method\nrecompiles the LLM's decompilation results to construct pairs for in-context\nlearning, helping the model improve decompilation performance. (2) Fine-grained\nAlignment Enhancement (FAE), which meticulously aligns assembly code with\nsource code at the statement level by leveraging debugging information, is\nemployed during the fine-tuning phase to achieve further improvements in\ndecompilation. By integrating these two methods, we achieved a Re-Executability\nperformance improvement of approximately 7.35\\% on the Decompile-Eval\nbenchmark, establishing a new state-of-the-art performance of 55.03\\%.\n","authors":["Yunlong Feng","Yang Xu","Dechuan Teng","Honglin Mu","Xiao Xu","Libo Qin","Wanxiang Che","Qingfu Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.17233v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.17232v1","updated":"2024-06-25T02:37:29Z","published":"2024-06-25T02:37:29Z","title":"Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human\n  Belief Networks","summary":"  Creating human-like large language model (LLM) agents is crucial for faithful\nsocial simulation. Having LLMs role-play based on demographic information\nsometimes improves human likeness but often does not. This study assessed\nwhether LLM alignment with human behavior can be improved by integrating\ninformation from empirically-derived human belief networks. Using data from a\nhuman survey, we estimated a belief network encompassing 18 topics loading on\ntwo non-overlapping latent factors. We then seeded LLM-based agents with an\nopinion on one topic, and assessed the alignment of its expressed opinions on\nremaining test topics with corresponding human data. Role-playing based on\ndemographic information alone did not align LLM and human opinions, but seeding\nthe agent with a single belief greatly improved alignment for topics related in\nthe belief network, and not for topics outside the network. These results\nsuggest a novel path for human-LLM belief alignment in work seeking to simulate\nand understand patterns of belief distributions in society.\n","authors":["Yun-Shiuan Chuang","Zach Studdiford","Krirk Nirunwiroj","Agam Goyal","Vincent V. Frigo","Sijia Yang","Dhavan Shah","Junjie Hu","Timothy T. Rogers"],"pdf_url":"https://arxiv.org/pdf/2406.17232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17231v1","updated":"2024-06-25T02:37:12Z","published":"2024-06-25T02:37:12Z","title":"CogMG: Collaborative Augmentation Between Large Language Model and\n  Knowledge Graph","summary":"  Large language models have become integral to question-answering applications\ndespite their propensity for generating hallucinations and factually inaccurate\ncontent. Querying knowledge graphs to reduce hallucinations in LLM meets the\nchallenge of incomplete knowledge coverage in knowledge graphs. On the other\nhand, updating knowledge graphs by information extraction and knowledge graph\ncompletion faces the knowledge update misalignment issue. In this work, we\nintroduce a collaborative augmentation framework, CogMG, leveraging knowledge\ngraphs to address the limitations of LLMs in QA scenarios, explicitly targeting\nthe problems of incomplete knowledge coverage and knowledge update\nmisalignment. The LLMs identify and decompose required knowledge triples that\nare not present in the KG, enriching them and aligning updates with real-world\ndemands. We demonstrate the efficacy of this approach through a supervised\nfine-tuned LLM within an agent framework, showing significant improvements in\nreducing hallucinations and enhancing factual accuracy in QA responses. Our\ncode and video are publicly available.\n","authors":["Tong Zhou","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.17231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16495v2","updated":"2024-06-25T02:20:14Z","published":"2024-06-24T10:05:23Z","title":"OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to\n  construct Observer-Thinker-Conceiver-Expresser","summary":"  Recent research has shown that combining Mamba with Transformer architecture,\nwhich has selective state space and quadratic self-attention mechanism,\noutperforms using Mamba or Transformer architecture alone in language modeling\ntasks. The quadratic self-attention mechanism effectively alleviates the\nshortcomings of selective state space in handling long-term dependencies of any\nelement in the sequence. We propose a position information injection method\nthat connects the selective state space model with the quadratic attention, and\nintegrates these two architectures with hybrid experts with cross-sharing\ndomains, so that we can enjoy the advantages of both. We design a new\narchitecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser\n(OTCE), which can compete with well-known medium-scale open-source language\nmodels on a small scale in language modeling tasks.\n","authors":["Jingze Shi","Ting Xie","Bingheng Wu","Chunjun Zheng","Kai Wang"],"pdf_url":"https://arxiv.org/pdf/2406.16495v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17224v1","updated":"2024-06-25T02:18:15Z","published":"2024-06-25T02:18:15Z","title":"Large Language Models are Interpretable Learners","summary":"  The trade-off between expressiveness and interpretability remains a core\nchallenge when building human-centric predictive models for classification and\ndecision-making. While symbolic rules offer interpretability, they often lack\nexpressiveness, whereas neural networks excel in performance but are known for\nbeing black boxes. In this paper, we show a combination of Large Language\nModels (LLMs) and symbolic programs can bridge this gap. In the proposed\nLLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language\nprompts provides a massive set of interpretable modules that can transform raw\ninput into natural language concepts. Symbolic programs then integrate these\nmodules into an interpretable decision rule. To train LSPs, we develop a\ndivide-and-conquer approach to incrementally build the program from scratch,\nwhere the learning process of each step is guided by LLMs. To evaluate the\neffectiveness of LSPs in extracting interpretable and accurate knowledge from\ndata, we introduce IL-Bench, a collection of diverse tasks, including both\nsynthetic and real-world scenarios across different modalities. Empirical\nresults demonstrate LSP's superior performance compared to traditional\nneurosymbolic programs and vanilla automatic prompt tuning methods. Moreover,\nas the knowledge learned by LSP is a combination of natural language\ndescriptions and symbolic rules, it is easily transferable to humans\n(interpretable), and other LLMs, and generalizes well to out-of-distribution\nsamples.\n","authors":["Ruochen Wang","Si Si","Felix Yu","Dorothea Wiesmann","Cho-Jui Hsieh","Inderjit Dhillon"],"pdf_url":"https://arxiv.org/pdf/2406.17224v1.pdf","comment":"Preliminary Version, Code at [this\n  url](https://github.com/ruocwang/llm-symbolic-program)"},{"id":"http://arxiv.org/abs/2406.17213v1","updated":"2024-06-25T01:56:47Z","published":"2024-06-25T01:56:47Z","title":"Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence\n  Coverage","summary":"  News media structure their reporting of events or issues using certain\nperspectives.\n  When describing an incident involving gun violence, for example, some\njournalists may focus on mental health or gun regulation, while others may\nemphasize the discussion of gun rights. Such perspectives are called\n\\say{frames} in communication research. We study, for the first time, the value\nof combining lead images and their contextual information with text to identify\nthe frame of a given news article. We observe that using multiple modes of\ninformation(article- and image-derived features) improves prediction of news\nframes over any single mode of information when the images are relevant to the\nframes of the headlines. We also observe that frame image relevance is related\nto the ease of conveying frames via images, which we call frame concreteness.\nAdditionally, we release the first multimodal news framing dataset related to\ngun violence in the U.S., curated and annotated by communication researchers.\nThe dataset will allow researchers to further examine the use of multiple\ninformation modalities for studying media framing.\n","authors":["Isidora Chara Tourni","Lei Guo","Hengchang Hu","Edward Halim","Prakash Ishwar","Taufiq Daryanto","Mona Jalal","Boqi Chen","Margrit Betke","Fabian Zhafransyah","Sha Lai","Derry Tanti Wijaya"],"pdf_url":"https://arxiv.org/pdf/2406.17213v1.pdf","comment":"published at Findings of the Association for Computational\n  Linguistics: EMNLP 2021"},{"id":"http://arxiv.org/abs/2405.18400v3","updated":"2024-06-25T01:49:45Z","published":"2024-05-28T17:40:48Z","title":"Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass","summary":"  Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.\n","authors":["Ethan Shen","Alan Fan","Sarah M. Pratt","Jae Sung Park","Matthew Wallingford","Sham M. Kakade","Ari Holtzman","Ranjay Krishna","Ali Farhadi","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2405.18400v3.pdf","comment":"22 pages, 15 figures"},{"id":"http://arxiv.org/abs/2404.14740v2","updated":"2024-06-25T01:02:16Z","published":"2024-04-23T04:47:22Z","title":"Modeling the Sacred: Considerations when Using Religious Texts in\n  Natural Language Processing","summary":"  This position paper concerns the use of religious texts in Natural Language\nProcessing (NLP), which is of special interest to the Ethics of NLP. Religious\ntexts are expressions of culturally important values, and machine learned\nmodels have a propensity to reproduce cultural values encoded in their training\ndata. Furthermore, translations of religious texts are frequently used by NLP\nresearchers when language data is scarce. This repurposes the translations from\ntheir original uses and motivations, which often involve attracting new\nfollowers. This paper argues that NLP's use of such texts raises considerations\nthat go beyond model biases, including data provenance, cultural contexts, and\ntheir use in proselytism. We argue for more consideration of researcher\npositionality, and of the perspectives of marginalized linguistic and religious\ncommunities.\n","authors":["Ben Hutchinson"],"pdf_url":"https://arxiv.org/pdf/2404.14740v2.pdf","comment":"Findings of NAACL2024"},{"id":"http://arxiv.org/abs/2406.17982v1","updated":"2024-06-25T23:36:16Z","published":"2024-06-25T23:36:16Z","title":"EDEN: Empathetic Dialogues for English learning","summary":"  Dialogue systems have been used as conversation partners in English learning,\nbut few have studied whether these systems improve learning outcomes. Student\npassion and perseverance, or grit, has been associated with language learning\nsuccess. Recent work establishes that as students perceive their English\nteachers to be more supportive, their grit improves. Hypothesizing that the\nsame pattern applies to English-teaching chatbots, we create EDEN, a robust\nopen-domain chatbot for spoken conversation practice that provides empathetic\nfeedback. To construct EDEN, we first train a specialized spoken utterance\ngrammar correction model and a high-quality social chit-chat conversation\nmodel. We then conduct a preliminary user study with a variety of strategies\nfor empathetic feedback. Our experiment suggests that using adaptive empathetic\nfeedback leads to higher perceived affective support, which, in turn, predicts\nincreased student grit.\n","authors":["Li Siyan","Teresa Shao","Zhou Yu","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2406.17982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17975v1","updated":"2024-06-25T23:12:07Z","published":"2024-06-25T23:12:07Z","title":"Inherent Challenges of Post-Hoc Membership Inference for Large Language\n  Models","summary":"  Large Language Models (LLMs) are often trained on vast amounts of undisclosed\ndata, motivating the development of post-hoc Membership Inference Attacks\n(MIAs) to gain insight into their training data composition. However, in this\npaper, we identify inherent challenges in post-hoc MIA evaluation due to\npotential distribution shifts between collected member and non-member datasets.\nUsing a simple bag-of-words classifier, we demonstrate that datasets used in\nrecent post-hoc MIAs suffer from significant distribution shifts, in some cases\nachieving near-perfect distinction between members and non-members. This\nimplies that previously reported high MIA performance may be largely\nattributable to these shifts rather than model memorization. We confirm that\nrandomized, controlled setups eliminate such shifts and thus enable the\ndevelopment and fair evaluation of new MIAs. However, we note that such\nrandomized setups are rarely available for the latest LLMs, making post-hoc\ndata collection still required to infer membership for real-world LLMs. As a\npotential solution, we propose a Regression Discontinuity Design (RDD) approach\nfor post-hoc data collection, which substantially mitigates distribution\nshifts. Evaluating various MIA methods on this RDD setup yields performance\nbarely above random guessing, in stark contrast to previously reported results.\nOverall, our findings highlight the challenges in accurately measuring LLM\nmemorization and the need for careful experimental design in (post-hoc)\nmembership inference tasks.\n","authors":["Matthieu Meeus","Shubham Jain","Marek Rei","Yves-Alexandre de Montjoye"],"pdf_url":"https://arxiv.org/pdf/2406.17975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17974v1","updated":"2024-06-25T23:11:39Z","published":"2024-06-25T23:11:39Z","title":"Evaluating Fairness in Large Vision-Language Models Across Diverse\n  Demographic Attributes and Prompts","summary":"  Large vision-language models (LVLMs) have recently achieved significant\nprogress, demonstrating strong capabilities in open-world visual understanding.\nHowever, it is not yet clear how LVLMs address demographic biases in real life,\nespecially the disparities across attributes such as gender, skin tone, and\nage. In this paper, we empirically investigate \\emph{visual fairness} in\nseveral mainstream LVLMs and audit their performance disparities across\nsensitive demographic attributes, based on public fairness benchmark datasets\n(e.g., FACET). To disclose the visual bias in LVLMs, we design a fairness\nevaluation framework with direct questions and single-choice\nquestion-instructed prompts on visual question-answering/classification tasks.\nThe zero-shot prompting results indicate that, despite enhancements in visual\nunderstanding, both open-source and closed-source LVLMs exhibit prevalent\nfairness issues across different instruct prompts and demographic attributes.\n","authors":["Xuyang Wu","Yuan Wang","Hsin-Tai Wu","Zhiqiang Tao","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2406.17974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17972v1","updated":"2024-06-25T23:07:18Z","published":"2024-06-25T23:07:18Z","title":"LABOR-LLM: Language-Based Occupational Representations with Large\n  Language Models","summary":"  Many empirical studies of labor market questions rely on estimating\nrelatively simple predictive models using small, carefully constructed\nlongitudinal survey datasets based on hand-engineered features. Large Language\nModels (LLMs), trained on massive datasets, encode vast quantities of world\nknowledge and can be used for the next job prediction problem. However, while\nan off-the-shelf LLM produces plausible career trajectories when prompted, the\nprobability with which an LLM predicts a particular job transition conditional\non career history will not, in general, align with the true conditional\nprobability in a given population. Recently, Vafa et al. (2024) introduced a\ntransformer-based \"foundation model\", CAREER, trained using a large,\nunrepresentative resume dataset, that predicts transitions between jobs; it\nfurther demonstrated how transfer learning techniques can be used to leverage\nthe foundation model to build better predictive models of both transitions and\nwages that reflect conditional transition probabilities found in nationally\nrepresentative survey datasets. This paper considers an alternative where the\nfine-tuning of the CAREER foundation model is replaced by fine-tuning LLMs. For\nthe task of next job prediction, we demonstrate that models trained with our\napproach outperform several alternatives in terms of predictive performance on\nthe survey data, including traditional econometric models, CAREER, and LLMs\nwith in-context learning, even though the LLM can in principle predict job\ntitles that are not allowed in the survey data. Further, we show that our\nfine-tuned LLM-based models' predictions are more representative of the career\ntrajectories of various workforce subpopulations than off-the-shelf LLM models\nand CAREER. We conduct experiments and analyses that highlight the sources of\nthe gains in the performance of our models for representative predictions.\n","authors":["Tianyu Du","Ayush Kanodia","Herman Brunborg","Keyon Vafa","Susan Athey"],"pdf_url":"https://arxiv.org/pdf/2406.17972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17969v1","updated":"2024-06-25T22:51:08Z","published":"2024-06-25T22:51:08Z","title":"Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a\n  Feature Decorrelation Perspective","summary":"  To better interpret the intrinsic mechanism of large language models (LLMs),\nrecent studies focus on monosemanticity on its basic units. A monosemantic\nneuron is dedicated to a single and specific concept, which forms a one-to-one\ncorrelation between neurons and concepts. Despite extensive research in\nmonosemanticity probing, it remains unclear whether monosemanticity is\nbeneficial or harmful to model capacity. To explore this question, we revisit\nmonosemanticity from the feature decorrelation perspective and advocate for its\nencouragement. We experimentally observe that the current conclusion by\nwang2024learning, which suggests that decreasing monosemanticity enhances model\nperformance, does not hold when the model changes. Instead, we demonstrate that\nmonosemanticity consistently exhibits a positive correlation with model\ncapacity, in the preference alignment process. Consequently, we apply feature\ncorrelation as a proxy for monosemanticity and incorporate a feature\ndecorrelation regularizer into the dynamic preference optimization process. The\nexperiments show that our method not only enhances representation diversity and\nactivation sparsity but also improves preference alignment performance.\n","authors":["Hanqi Yan","Yanzheng Xiang","Guangyi Chen","Yifei Wang","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2406.17969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17967v1","updated":"2024-06-25T22:49:17Z","published":"2024-06-25T22:49:17Z","title":"Unmasking the Imposters: In-Domain Detection of Human vs.\n  Machine-Generated Tweets","summary":"  The rapid development of large language models (LLMs) has significantly\nimproved the generation of fluent and convincing text, raising concerns about\ntheir misuse on social media platforms. We present a methodology using Twitter\ndatasets to examine the generative capabilities of four LLMs: Llama 3, Mistral,\nQwen2, and GPT4o. We evaluate 7B and 8B parameter base-instruction models of\nthe three open-source LLMs and validate the impact of further fine-tuning and\n\"uncensored\" versions. Our findings show that \"uncensored\" models with\nadditional in-domain fine-tuning dramatically reduce the effectiveness of\nautomated detection methods. This study addresses a gap by exploring smaller\nopen-source models and the effects of \"uncensoring,\" providing insights into\nhow fine-tuning and content moderation influence machine-generated text\ndetection.\n","authors":["Bryan E. Tuck","Rakesh M. Verma"],"pdf_url":"https://arxiv.org/pdf/2406.17967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17962v1","updated":"2024-06-25T22:44:17Z","published":"2024-06-25T22:44:17Z","title":"SimsChat: A Customisable Persona-Driven Role-Playing Agent","summary":"  Large Language Models (LLMs) possess the remarkable capability to understand\nhuman instructions and generate high-quality text, enabling them to act as\nagents that simulate human behaviours. This capability allows LLMs to emulate\nhuman beings in a more advanced manner, beyond merely replicating simple human\nbehaviours. However, there is a lack of exploring into leveraging LLMs to craft\ncharacters from several aspects. In this work, we introduce the Customisable\nConversation Agent Framework, which employs LLMs to simulate real-world\ncharacters that can be freely customised according to different user\npreferences. The customisable framework is helpful for designing customisable\ncharacters and role-playing agents according to human's preferences. We first\npropose the SimsConv dataset, which comprises 68 different customised\ncharacters, 1,360 multi-turn role-playing dialogues, and encompasses 13,971\ninteraction dialogues in total. The characters are created from several\nreal-world elements, such as career, aspiration, trait, and skill. Building on\nthese foundations, we present SimsChat, a freely customisable role-playing\nagent. It incorporates different real-world scenes and topic-specific character\ninteraction dialogues, simulating characters' life experiences in various\nscenarios and topic-specific interactions with specific emotions. Experimental\nresults show that our proposed framework achieves desirable performance and\nprovides helpful guideline for building better simulacra of human beings in the\nfuture. Our data and code are available at\nhttps://github.com/Bernard-Yang/SimsChat.\n","authors":["Bohao Yang","Dong Liu","Chen Tang","Chenghao Xiao","Kun Zhao","Chao Li","Lin Yuan","Guang Yang","Lanxiao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2406.17962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17961v1","updated":"2024-06-25T22:40:03Z","published":"2024-06-25T22:40:03Z","title":"NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data\n  Normalization","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in parsing textual data and generating code. However, their\nperformance in tasks involving tabular data, especially those requiring\nsymbolic reasoning, faces challenges due to the structural variance and\ninconsistency in table cell values often found in web tables. In this paper, we\nintroduce NormTab, a novel framework aimed at enhancing the symbolic reasoning\nperformance of LLMs by normalizing web tables. We study table normalization as\na stand-alone, one-time preprocessing step using LLMs to support symbolic\nreasoning on tabular data. Our experimental evaluation, conducted on\nchallenging web table datasets such as WikiTableQuestion and TabFact,\ndemonstrates that leveraging NormTab significantly improves symbolic reasoning\nperformance, showcasing the importance and effectiveness of web table\nnormalization for enhancing LLM-based symbolic reasoning tasks.\n","authors":["Md Mahadi Hasan Nahid","Davood Rafiei"],"pdf_url":"https://arxiv.org/pdf/2406.17961v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2406.17947v1","updated":"2024-06-25T21:47:53Z","published":"2024-06-25T21:47:53Z","title":"Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias","summary":"  The variations between in-group and out-group speech (intergroup bias) are\nsubtle and could underlie many social phenomena like stereotype perpetuation\nand implicit bias. In this paper, we model the intergroup bias as a tagging\ntask on English sports comments from forums dedicated to fandom for NFL teams.\nWe curate a unique dataset of over 6 million game-time comments from opposing\nperspectives (the teams in the game), each comment grounded in a non-linguistic\ndescription of the events that precipitated these comments (live win\nprobabilities for each team). Expert and crowd annotations justify modeling the\nbias through tagging of implicit and explicit referring expressions and reveal\nthe rich, contextual understanding of language and the world required for this\ntask. For large-scale analysis of intergroup variation, we use LLMs for\nautomated tagging, and discover that some LLMs perform best when prompted with\nlinguistic descriptions of the win probability at the time of the comment,\nrather than numerical probability. Further, large-scale tagging of comments\nusing LLMs uncovers linear variations in the form of referent across win\nprobabilities that distinguish in-group and out-group utterances. Code and data\nare available at https://github.com/venkatasg/intergroup-nfl .\n","authors":["Venkata S Govindarajan","Matianyu Zang","Kyle Mahowald","David Beaver","Junyi Jessy Li"],"pdf_url":"https://arxiv.org/pdf/2406.17947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13805v3","updated":"2024-06-25T21:25:07Z","published":"2022-04-28T22:33:36Z","title":"Investigating writing style as a contributor to gender gaps in science\n  and technology","summary":"  A growing stream of research finds that scientific contributions are\nevaluated differently depending on the gender of the author. In this article,\nwe consider whether gender differences in writing styles - how men and women\ncommunicate their work - may contribute to these observed gender gaps. We\nground our investigation in a framework for characterizing the linguistic style\nof written text, with two sets of features - informational (i.e., features that\nemphasize facts) and involved (i.e., features that emphasize relationships).\nUsing a large sample of academic papers and patents, we find significant\ndifferences in writing style by gender, with women using more involved features\nin their writing. Papers and patents with more involved features also tend to\nbe cited more by women. Our findings suggest that scientific text is not devoid\nof personal character, which could contribute to bias in evaluation, thereby\ncompromising the norm of universalism as a foundational principle of science.\n","authors":["Kara Kedrick","Ekaterina Levitskaya","Russell J. Funk"],"pdf_url":"https://arxiv.org/pdf/2204.13805v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11709v2","updated":"2024-06-25T21:17:41Z","published":"2024-06-17T16:28:21Z","title":"Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\n  Questioning for Socratic Code Debugging","summary":"  Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning.\n","authors":["Priyanka Kargupta","Ishika Agarwal","Dilek Hakkani-Tur","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2406.11709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18060v4","updated":"2024-06-25T21:17:09Z","published":"2024-02-28T05:44:41Z","title":"Benchmarking Large Language Models on Answering and Explaining\n  Challenging Medical Questions","summary":"  LLMs have demonstrated impressive performance in answering medical questions,\nsuch as achieving passing scores on medical licensing examinations. However,\nmedical board exam or general clinical questions do not capture the complexity\nof realistic clinical cases. Moreover, the lack of reference explanations means\nwe cannot easily evaluate the reasoning of model decisions, a crucial component\nof supporting doctors in making complex medical decisions. To address these\nchallenges, we construct two new datasets: JAMA Clinical Challenge and\nMedbullets. JAMA Clinical Challenge consists of questions based on challenging\nclinical cases, while Medbullets comprises simulated clinical questions. Both\ndatasets are structured as multiple-choice question-answering tasks,\naccompanied by expert-written explanations. We evaluate seven LLMs on the two\ndatasets using various prompts. Experiments demonstrate that our datasets are\nharder than previous benchmarks. Human and automatic evaluations of\nmodel-generated explanations provide insights into the promise and deficiency\nof LLMs for explainable medical QA.\n","authors":["Hanjie Chen","Zhouxiang Fang","Yash Singla","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2402.18060v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03192v2","updated":"2024-06-25T20:54:16Z","published":"2024-04-04T04:23:19Z","title":"Do Large Language Models Rank Fairly? An Empirical Study on the Fairness\n  of LLMs as Rankers","summary":"  The integration of Large Language Models (LLMs) in information retrieval has\nraised a critical reevaluation of fairness in the text-ranking models. LLMs,\nsuch as GPT models and Llama2, have shown effectiveness in natural language\nunderstanding tasks, and prior works (e.g., RankGPT) have also demonstrated\nthat the LLMs exhibit better performance than the traditional ranking models in\nthe ranking task. However, their fairness remains largely unexplored. This\npaper presents an empirical study evaluating these LLMs using the TREC Fair\nRanking dataset, focusing on the representation of binary protected attributes\nsuch as gender and geographic location, which are historically underrepresented\nin search outcomes. Our analysis delves into how these LLMs handle queries and\ndocuments related to these attributes, aiming to uncover biases in their\nranking algorithms. We assess fairness from both user and content perspectives,\ncontributing an empirical benchmark for evaluating LLMs as the fair ranker.\n","authors":["Yuan Wang","Xuyang Wu","Hsin-Tai Wu","Zhiqiang Tao","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2404.03192v2.pdf","comment":"Accepted at NAACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2406.17935v1","updated":"2024-06-25T20:52:09Z","published":"2024-06-25T20:52:09Z","title":"Sequential Editing for Lifelong Training of Speech Recognition Models","summary":"  Automatic Speech Recognition (ASR) traditionally assumes known domains, but\nadding data from a new domain raises concerns about computational\ninefficiencies linked to retraining models on both existing and new domains.\nFine-tuning solely on new domain risks Catastrophic Forgetting (CF). To address\nthis, Lifelong Learning (LLL) algorithms have been proposed for ASR. Prior\nresearch has explored techniques such as Elastic Weight Consolidation,\nKnowledge Distillation, and Replay, all of which necessitate either additional\nparameters or access to prior domain data. We propose Sequential Model Editing\nas a novel method to continually learn new domains in ASR systems. Different\nthan previous methods, our approach does not necessitate access to prior\ndatasets or the introduction of extra parameters. Our study demonstrates up to\n15% Word Error Rate Reduction (WERR) over fine-tuning baseline, and superior\nefficiency over other LLL techniques on CommonVoice English multi-accent\ndataset.\n","authors":["Devang Kulshreshtha","Saket Dingliwal","Brady Houston","Nikolaos Pappas","Srikanth Ronanki"],"pdf_url":"https://arxiv.org/pdf/2406.17935v1.pdf","comment":"INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.17926v1","updated":"2024-06-25T20:37:16Z","published":"2024-06-25T20:37:16Z","title":"FASA: a Flexible and Automatic Speech Aligner for Extracting\n  High-quality Aligned Children Speech Data","summary":"  Automatic Speech Recognition (ASR) for adults' speeches has made significant\nprogress by employing deep neural network (DNN) models recently, but\nimprovement in children's speech is still unsatisfactory due to children's\nspeech's distinct characteristics. DNN models pre-trained on adult data often\nstruggle in generalizing children's speeches with fine tuning because of the\nlack of high-quality aligned children's speeches. When generating datasets,\nhuman annotations are not scalable, and existing forced-alignment tools are not\nusable as they make impractical assumptions about the quality of the input\ntranscriptions. To address these challenges, we propose a new forced-alignment\ntool, FASA, as a flexible and automatic speech aligner to extract high-quality\naligned children's speech data from many of the existing noisy children's\nspeech data. We demonstrate its usage on the CHILDES dataset and show that FASA\ncan improve data quality by 13.6$\\times$ over human annotations.\n","authors":["Dancheng Liu","Jinjun Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.17926v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.17923v1","updated":"2024-06-25T20:11:37Z","published":"2024-06-25T20:11:37Z","title":"PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning","summary":"  Large language models (LLMs) have shown remarkable abilities in diverse\nnatural language processing (NLP) tasks. The LLMs generally undergo supervised\nfine-tuning (SFT) followed by preference alignment to be usable in downstream\napplications. However, this sequential training pipeline leads to alignment tax\nthat degrades the LLM performance.\n  This paper introduces PAFT, a new PArallel training paradigm for effective\nLLM Fine-Tuning, which independently performs SFT and preference alignment\n(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective\ndatasets. The model produced by SFT and the model from preference alignment are\nthen merged into a final model by parameter fusing for use in downstream\napplications. This work reveals important findings that preference alignment\nlike DPO naturally results in a sparse model while SFT leads to a natural dense\nmodel which needs to be sparsified for effective model merging. This paper\nintroduces an effective interference resolution which reduces the redundancy by\nsparsifying the delta parameters. The LLM resulted from the new training\nparadigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.\nComprehensive evaluation shows the effectiveness of the parallel training\nparadigm.\n","authors":["Shiva Kumar Pentyala","Zhichao Wang","Bin Bi","Kiran Ramnath","Xiang-Bo Mao","Regunathan Radhakrishnan","Sitaram Asur"," Na"," Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.17923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17911v1","updated":"2024-06-25T19:52:01Z","published":"2024-06-25T19:52:01Z","title":"X-ray Made Simple: Radiology Report Generation and Evaluation with\n  Layman's Terms","summary":"  Radiology Report Generation (RRG) has achieved significant progress with the\nadvancements of multimodal generative models. However, the evaluation in the\ndomain suffers from a lack of fair and robust metrics. We reveal that, high\nperformance on RRG with existing lexical-based metrics (e.g. BLEU) might be\nmore of a mirage - a model can get a high BLEU only by learning the template of\nreports. This has become an urgent problem for RRG due to the highly\npatternized nature of these reports. In this work, we un-intuitively approach\nthis problem by proposing the Layman's RRG framework, a layman's terms-based\ndataset, evaluation and training framework that systematically improves RRG\nwith day-to-day language. We first contribute the translated Layman's terms\ndataset. Building upon the dataset, we then propose a semantics-based\nevaluation method, which is proved to mitigate the inflated numbers of BLEU and\nprovides fairer evaluation. Last, we show that training on the layman's terms\ndataset encourages models to focus on the semantics of the reports, as opposed\nto overfitting to learning the report templates. We reveal a promising scaling\nlaw between the number of training examples and semantics gain provided by our\ndataset, compared to the inverse pattern brought by the original formats. Our\ncode is available at \\url{https://github.com/hegehongcha/LaymanRRG}.\n","authors":["Kun Zhao","Chenghao Xiao","Chen Tang","Bohao Yang","Kai Ye","Noura Al Moubayed","Liang Zhan","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2406.17911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02532v2","updated":"2024-06-25T19:35:06Z","published":"2024-06-04T17:53:36Z","title":"SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices","summary":"  As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.\n","authors":["Ruslan Svirschevski","Avner May","Zhuoming Chen","Beidi Chen","Zhihao Jia","Max Ryabinin"],"pdf_url":"https://arxiv.org/pdf/2406.02532v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2406.17903v1","updated":"2024-06-25T19:34:00Z","published":"2024-06-25T19:34:00Z","title":"Mapping the Past: Geographically Linking an Early 20th Century Swedish\n  Encyclopedia with Wikidata","summary":"  In this paper, we describe the extraction of all the location entries from a\nprominent Swedish encyclopedia from the early 20th century, the \\textit{Nordisk\nFamiljebok} `Nordic Family Book.' We focused on the second edition called\n\\textit{Uggleupplagan}, which comprises 38 volumes and over 182,000 articles.\nThis makes it one of the most extensive Swedish encyclopedias. Using a\nclassifier, we first determined the category of the entries. We found that\napproximately 22 percent of them were locations. We applied a named entity\nrecognition to these entries and we linked them to Wikidata. Wikidata enabled\nus to extract their precise geographic locations resulting in almost 18,000\nvalid coordinates. We then analyzed the distribution of these locations and the\nentry selection process. It showed a higher density within Sweden, Germany, and\nthe United Kingdom. The paper sheds light on the selection and representation\nof geographic information in the \\textit{Nordisk Familjebok}, providing\ninsights into historical and societal perspectives. It also paves the way for\nfuture investigations into entry selection in different time periods and\ncomparative analyses among various encyclopedias.\n","authors":["Axel Ahlin","Alfred Myrne","Pierre Nugues"],"pdf_url":"https://arxiv.org/pdf/2406.17903v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.17901v1","updated":"2024-06-25T19:23:42Z","published":"2024-06-25T19:23:42Z","title":"Script-Agnostic Language Identification","summary":"  Language identification is used as the first step in many data collection and\ncrawling efforts because it allows us to sort online text into\nlanguage-specific buckets. However, many modern languages, such as Konkani,\nKashmiri, Punjabi etc., are synchronically written in several scripts.\nMoreover, languages with different writing systems do not share significant\nlexical, semantic, and syntactic properties in neural representation spaces,\nwhich is a disadvantage for closely related languages and low-resource\nlanguages, especially those from the Indian Subcontinent. To counter this, we\npropose learning script-agnostic representations using several different\nexperimental strategies (upscaling, flattening, and script mixing) focusing on\nfour major Dravidian languages (Tamil, Telugu, Kannada, and Malayalam). We find\nthat word-level script randomization and exposure to a language written in\nmultiple scripts is extremely valuable for downstream script-agnostic language\nidentification, while also maintaining competitive performance on naturally\noccurring text.\n","authors":["Milind Agarwal","Joshua Otten","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2406.17901v1.pdf","comment":"Under Review in ACL Rolling Review"},{"id":"http://arxiv.org/abs/2406.17888v1","updated":"2024-06-25T18:52:48Z","published":"2024-06-25T18:52:48Z","title":"CTBench: A Comprehensive Benchmark for Evaluating Language Model\n  Capabilities in Clinical Trial Design","summary":"  CTBench is introduced as a benchmark to assess language models (LMs) in\naiding clinical study design. Given study-specific metadata, CTBench evaluates\nAI models' ability to determine the baseline features of a clinical trial (CT),\nwhich include demographic and relevant features collected at the trial's start\nfrom all participants. These baseline features, typically presented in CT\npublications (often as Table 1), are crucial for characterizing study cohorts\nand validating results. Baseline features, including confounders and\ncovariates, are also necessary for accurate treatment effect estimation in\nstudies involving observational data. CTBench consists of two datasets:\n\"CT-Repo,\" containing baseline features from 1,690 clinical trials sourced from\nclinicaltrials.gov, and \"CT-Pub,\" a subset of 100 trials with more\ncomprehensive baseline features gathered from relevant publications. Two\nLM-based evaluation methods are developed to compare the actual baseline\nfeature lists against LM-generated responses. \"ListMatch-LM\" and\n\"ListMatch-BERT\" use GPT-4o and BERT scores (at various thresholds),\nrespectively, for evaluation. To establish baseline results, advanced prompt\nengineering techniques using LLaMa3-70B-Instruct and GPT-4o in zero-shot and\nthree-shot learning settings are applied to generate potential baseline\nfeatures. The performance of GPT-4o as an evaluator is validated through\nhuman-in-the-loop evaluations on the CT-Pub dataset, where clinical experts\nconfirm matches between actual and LM-generated features. The results highlight\na promising direction with significant potential for improvement, positioning\nCTBench as a useful tool for advancing research on AI in CT design and\npotentially enhancing the efficacy and robustness of CTs.\n","authors":["Nafis Neehal","Bowen Wang","Shayom Debopadhaya","Soham Dan","Keerthiram Murugesan","Vibha Anand","Kristin P. Bennett"],"pdf_url":"https://arxiv.org/pdf/2406.17888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06873v2","updated":"2024-06-25T18:38:28Z","published":"2023-08-14T01:01:19Z","title":"SpeechX: Neural Codec Language Model as a Versatile Speech Transformer","summary":"  Recent advancements in generative speech models based on audio-text prompts\nhave enabled remarkable innovations like high-quality zero-shot text-to-speech.\nHowever, existing models still face limitations in handling diverse audio-text\nspeech generation tasks involving transforming input speech and processing\naudio captured in adverse acoustic conditions. This paper introduces SpeechX, a\nversatile speech generation model capable of zero-shot TTS and various speech\ntransformation tasks, dealing with both clean and noisy signals. SpeechX\ncombines neural codec language modeling with multi-task learning using\ntask-dependent prompting, enabling unified and extensible modeling and\nproviding a consistent way for leveraging textual input in speech enhancement\nand transformation tasks. Experimental results show SpeechX's efficacy in\nvarious tasks, including zero-shot TTS, noise suppression, target speaker\nextraction, speech removal, and speech editing with or without background\nnoise, achieving comparable or superior performance to specialized models\nacross tasks. See https://aka.ms/speechx for demo samples.\n","authors":["Xiaofei Wang","Manthan Thakker","Zhuo Chen","Naoyuki Kanda","Sefik Emre Eskimez","Sanyuan Chen","Min Tang","Shujie Liu","Jinyu Li","Takuya Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2308.06873v2.pdf","comment":"To appear in TASLP. See https://aka.ms/speechx for demo samples"},{"id":"http://arxiv.org/abs/2406.04175v2","updated":"2024-06-25T18:37:19Z","published":"2024-06-06T15:32:29Z","title":"Confabulation: The Surprising Value of Large Language Model\n  Hallucinations","summary":"  This paper presents a systematic defense of large language model (LLM)\nhallucinations or 'confabulations' as a potential resource instead of a\ncategorically negative pitfall. The standard view is that confabulations are\ninherently problematic and AI research should eliminate this flaw. In this\npaper, we argue and empirically demonstrate that measurable semantic\ncharacteristics of LLM confabulations mirror a human propensity to utilize\nincreased narrativity as a cognitive resource for sense-making and\ncommunication. In other words, it has potential value. Specifically, we analyze\npopular hallucination benchmarks and reveal that hallucinated outputs display\nincreased levels of narrativity and semantic coherence relative to veridical\noutputs. This finding reveals a tension in our usually dismissive\nunderstandings of confabulation. It suggests, counter-intuitively, that the\ntendency for LLMs to confabulate may be intimately associated with a positive\ncapacity for coherent narrative-text generation.\n","authors":["Peiqi Sui","Eamon Duede","Sophie Wu","Richard Jean So"],"pdf_url":"https://arxiv.org/pdf/2406.04175v2.pdf","comment":"Forthcoming at ACL2024 main conference. 1 figure"},{"id":"http://arxiv.org/abs/2406.17876v1","updated":"2024-06-25T18:35:13Z","published":"2024-06-25T18:35:13Z","title":"ET tu, CLIP? Addressing Common Object Errors for Unseen Environments","summary":"  We introduce a simple method that employs pre-trained CLIP encoders to\nenhance model generalization in the ALFRED task. In contrast to previous\nliterature where CLIP replaces the visual encoder, we suggest using CLIP as an\nadditional module through an auxiliary object detection objective. We validate\nour method on the recently proposed Episodic Transformer architecture and\ndemonstrate that incorporating CLIP improves task performance on the unseen\nvalidation set. Additionally, our analysis results support that CLIP especially\nhelps with leveraging object descriptions, detecting small objects, and\ninterpreting rare words.\n","authors":["Ye Won Byun","Cathy Jiao","Shahriar Noroozizadeh","Jimin Sun","Rosa Vitiello"],"pdf_url":"https://arxiv.org/pdf/2406.17876v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17875v1","updated":"2024-06-25T18:30:25Z","published":"2024-06-25T18:30:25Z","title":"Cloaked Classifiers: Pseudonymization Strategies on Sensitive\n  Classification Tasks","summary":"  Protecting privacy is essential when sharing data, particularly in the case\nof an online radicalization dataset that may contain personal information. In\nthis paper, we explore the balance between preserving data usefulness and\nensuring robust privacy safeguards, since regulations like the European GDPR\nshape how personal information must be handled. We share our method for\nmanually pseudonymizing a multilingual radicalization dataset, ensuring\nperformance comparable to the original data. Furthermore, we highlight the\nimportance of establishing comprehensive guidelines for processing sensitive\nNLP data by sharing our complete pseudonymization process, our guidelines, the\nchallenges we encountered as well as the resulting dataset.\n","authors":["Arij Riabi","Menel Mahamdi","Virginie Mouilleron","Djamé Seddah"],"pdf_url":"https://arxiv.org/pdf/2406.17875v1.pdf","comment":"Proceedings of the fifth Workshop on Privacy in Natural Language\n  Processing"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.17745v1","updated":"2024-06-25T17:31:04Z","published":"2024-06-25T17:31:04Z","title":"Light-weight End-to-End Graph Interest Network for CTR Prediction in\n  E-commerce Search","summary":"  Click-through-rate (CTR) prediction has an essential impact on improving user\nexperience and revenue in e-commerce search. With the development of deep\nlearning, graph-based methods are well exploited to utilize graph structure\nextracted from user behaviors and other information to help embedding learning.\nHowever, most of the previous graph-based methods mainly focus on\nrecommendation scenarios, and therefore their graph structures highly depend on\nitem's sequential information from user behaviors, ignoring query's sequential\nsignal and query-item correlation. In this paper, we propose a new approach\nnamed Light-weight End-to-End Graph Interest Network (EGIN) to effectively mine\nusers' search interests and tackle previous challenges. (i) EGIN utilizes query\nand item's correlation and sequential information from the search system to\nbuild a heterogeneous graph for better CTR prediction in e-commerce search.\n(ii) EGIN's graph embedding learning shares the same training input and is\njointly trained with CTR prediction, making the end-to-end framework effortless\nto deploy in large-scale search systems. The proposed EGIN is composed of three\nparts: query-item heterogeneous graph, light-weight graph sampling, and\nmulti-interest network. The query-item heterogeneous graph captures correlation\nand sequential information of query and item efficiently by the proposed\nlight-weight graph sampling. The multi-interest network is well designed to\nutilize graph embedding to capture various similarity relationships between\nquery and item to enhance the final CTR prediction. We conduct extensive\nexperiments on both public and industrial datasets to demonstrate the\neffectiveness of the proposed EGIN. At the same time, the training cost of\ngraph learning is relatively low compared with the main CTR prediction task,\nensuring efficiency in practical applications.\n","authors":["Pai Peng","Quanxiang Jia","Ziqiang Zhou","Shuang Hong","Zichong Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.17745v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.17526v1","updated":"2024-06-25T13:08:35Z","published":"2024-06-25T13:08:35Z","title":"LumberChunker: Long-Form Narrative Document Segmentation","summary":"  Modern NLP tasks increasingly rely on dense retrieval methods to access\nup-to-date and relevant contextual information. We are motivated by the premise\nthat retrieval benefits from segments that can vary in size such that a\ncontent's semantic independence is better captured. We propose LumberChunker, a\nmethod leveraging an LLM to dynamically segment documents, which iteratively\nprompts the LLM to identify the point within a group of sequential passages\nwhere the content begins to shift. To evaluate our method, we introduce\nGutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer\npairs derived from 100 public domain narrative books available on Project\nGutenberg. Our experiments show that LumberChunker not only outperforms the\nmost competitive baseline by 7.37% in retrieval performance (DCG@20) but also\nthat, when integrated into a RAG pipeline, LumberChunker proves to be more\neffective than other chunking methods and competitive baselines, such as the\nGemini 1.5M Pro. Our Code and Data are available at\nhttps://github.com/joaodsmarques/LumberChunker\n","authors":["André V. Duarte","João Marques","Miguel Graça","Miguel Freire","Lei Li","Arlindo L. Oliveira"],"pdf_url":"https://arxiv.org/pdf/2406.17526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17507v1","updated":"2024-06-25T12:47:04Z","published":"2024-06-25T12:47:04Z","title":"ACE: A Generative Cross-Modal Retrieval Framework with Coarse-To-Fine\n  Semantic Modeling","summary":"  Generative retrieval, which has demonstrated effectiveness in text-to-text\nretrieval, utilizes a sequence-to-sequence model to directly generate candidate\nidentifiers based on natural language queries. Without explicitly computing the\nsimilarity between queries and candidates, generative retrieval surpasses\ndual-tower models in both speed and accuracy on large-scale corpora, providing\nnew insights for cross-modal retrieval. However, constructing identifiers for\nmultimodal data remains an untapped problem, and the modality gap between\nnatural language queries and multimodal candidates hinders retrieval\nperformance due to the absence of additional encoders. To this end, we propose\na pioneering generAtive Cross-modal rEtrieval framework (ACE), which is a\ncomprehensive framework for end-to-end cross-modal retrieval based on\ncoarse-to-fine semantic modeling. We propose combining K-Means and RQ-VAE to\nconstruct coarse and fine tokens, serving as identifiers for multimodal data.\nCorrespondingly, we design the coarse-to-fine feature fusion strategy to\nefficiently align natural language queries and candidate identifiers. ACE is\nthe first work to comprehensively demonstrate the feasibility of generative\napproach on text-to-image/audio/video retrieval, challenging the dominance of\nthe embedding-based dual-tower architecture. Extensive experiments show that\nACE achieves state-of-the-art performance in cross-modal retrieval and\noutperforms the strong baselines on Recall@1 by 15.27% on average.\n","authors":["Minghui Fang","Shengpeng Ji","Jialong Zuo","Hai Huang","Yan Xia","Jieming Zhu","Xize Cheng","Xiaoda Yang","Wenrui Liu","Gang Wang","Zhenhua Dong","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.17507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17475v1","updated":"2024-06-25T11:41:50Z","published":"2024-06-25T11:41:50Z","title":"Performative Debias with Fair-exposure Optimization Driven by Strategic\n  Agents in Recommender Systems","summary":"  Data bias, e.g., popularity impairs the dynamics of two-sided markets within\nrecommender systems. This overshadows the less visible but potentially\nintriguing long-tail items that could capture user interest. Despite the\nabundance of research surrounding this issue, it still poses challenges and\nremains a hot topic in academic circles. Along this line, in this paper, we\ndeveloped a re-ranking approach in dynamic settings with fair-exposure\noptimization driven by strategic agents. Designed for the producer side, the\nexecution of agents assumes content creators can modify item features based on\nstrategic incentives to maximize their exposure. This iterative process entails\nan end-to-end optimization, employing differentiable ranking operators that\nsimultaneously target accuracy and fairness. Joint objectives ensure the\nperformance of recommendations while enhancing the visibility of tail items. We\nalso leveraged the performativity nature of predictions to illustrate how\nstrategic learning influences content creators to shift towards fairness\nefficiently, thereby incentivizing features of tail items. Through\ncomprehensive experiments on both public and industrial datasets, we have\nsubstantiated the effectiveness and dominance of the proposed method especially\non unveiling the potential of tail items.\n","authors":["Zhichen Xiang","Hongke Zhao","Chuang Zhao","Ming He","Jianping Fan"],"pdf_url":"https://arxiv.org/pdf/2406.17475v1.pdf","comment":"SIGKDD 2024 accepted paper"},{"id":"http://arxiv.org/abs/2406.17378v1","updated":"2024-06-25T08:55:12Z","published":"2024-06-25T08:55:12Z","title":"A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens","summary":"  Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nembedding LLMs, the obtained text embedding will be able to be aligned with the\nkey tokens in the input text. We first fully analyze this phenomenon on eight\nembedding LLMs and show that this phenomenon is universal and is not affected\nby model architecture, training strategy, and embedding method. With a deeper\nanalysis, we then find that the main change in embedding space between the\nembedding LLMs and their original generative LLMs is in the first principal\ncomponent. By adjusting the first principal component, we can align text\nembedding with the key tokens. Finally, we give several examples to demonstrate\nthe vast application potential of this finding: (1) we propose a simple and\npractical sparse retrieval method based on the aligned tokens, which can\nachieve 80\\% of the dense retrieval effect of the same model while reducing the\ncomputation significantly; (2) we show that our findings provide a fresh\nperspective to help understand fuzzy concepts (e.g., semantic relatedness vs.\nsemantic similarity) and emerging technologies (e.g., instruction-following\nembedding) in this field.\n","authors":["Zhijie Nie","Richong Zhang","Zhanyu Wu"],"pdf_url":"https://arxiv.org/pdf/2406.17378v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2406.17335v1","updated":"2024-06-25T07:45:00Z","published":"2024-06-25T07:45:00Z","title":"A Thorough Performance Benchmarking on Lightweight Embedding-based\n  Recommender Systems","summary":"  Since the creation of the Web, recommender systems (RSs) have been an\nindispensable mechanism in information filtering. State-of-the-art RSs\nprimarily depend on categorical features, which ecoded by embedding vectors,\nresulting in excessively large embedding tables. To prevent over-parameterized\nembedding tables from harming scalability, both academia and industry have seen\nincreasing efforts in compressing RS embeddings. However, despite the\nprosperity of lightweight embedding-based RSs (LERSs), a wide diversity is seen\nin evaluation protocols, resulting in obstacles when relating LERS performance\nto real-world usability. Moreover, despite the common goal of lightweight\nembeddings, LERSs are evaluated with a single choice between the two main\nrecommendation tasks -- collaborative filtering and content-based\nrecommendation. This lack of discussions on cross-task transferability hinders\nthe development of unified, more scalable solutions. Motivated by these issues,\nthis study investigates various LERSs' performance, efficiency, and cross-task\ntransferability via a thorough benchmarking process. Additionally, we propose\nan efficient embedding compression method using magnitude pruning, which is an\neasy-to-deploy yet highly competitive baseline that outperforms various complex\nLERSs. Our study reveals the distinct performance of LERSs across the two\ntasks, shedding light on their effectiveness and generalizability. To support\nedge-based recommendations, we tested all LERSs on a Raspberry Pi 4, where the\nefficiency bottleneck is exposed. Finally, we conclude this paper with critical\nsummaries of LERS performance, model selection suggestions, and underexplored\nchallenges around LERSs for future research. To encourage future research, we\npublish source codes and artifacts at \\href{this\nlink}{https://github.com/chenxing1999/recsys-benchmark}.\n","authors":["Hung Vinh Tran","Tong Chen","Quoc Viet Hung Nguyen","Zi Huang","Lizhen Cui","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2406.17335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17289v1","updated":"2024-06-25T05:35:02Z","published":"2024-06-25T05:35:02Z","title":"Hyperbolic Knowledge Transfer in Cross-Domain Recommendation System","summary":"  Cross-Domain Recommendation (CDR) seeks to utilize knowledge from different\ndomains to alleviate the problem of data sparsity in the target recommendation\ndomain, and it has been gaining more attention in recent years. Although there\nhave been notable advancements in this area, most current methods represent\nusers and items in Euclidean space, which is not ideal for handling long-tail\ndistributed data in recommendation systems. Additionally, adding data from\nother domains can worsen the long-tail characteristics of the entire dataset,\nmaking it harder to train CDR models effectively. Recent studies have shown\nthat hyperbolic methods are particularly suitable for modeling long-tail\ndistributions, which has led us to explore hyperbolic representations for users\nand items in CDR scenarios. However, due to the distinct characteristics of the\ndifferent domains, applying hyperbolic representation learning to CDR tasks is\nquite challenging. In this paper, we introduce a new framework called\nHyperbolic Contrastive Learning (HCTS), designed to capture the unique features\nof each domain while enabling efficient knowledge transfer between domains. We\nachieve this by embedding users and items from each domain separately and\nmapping them onto distinct hyperbolic manifolds with adjustable curvatures for\nprediction. To improve the representations of users and items in the target\ndomain, we develop a hyperbolic contrastive learning module for knowledge\ntransfer. Extensive experiments on real-world datasets demonstrate that\nhyperbolic manifolds are a promising alternative to Euclidean space for CDR\ntasks.\n","authors":["Xin Yang","Heng Chang","Zhijian La","Jinze Yang","Xingrun Li","Yu Lu","Shuaiqiang Wang","Dawei Yin","Erxue Min"],"pdf_url":"https://arxiv.org/pdf/2406.17289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03096v2","updated":"2024-06-25T23:15:13Z","published":"2023-08-06T12:22:12Z","title":"Gradient Coding with Iterative Block Leverage Score Sampling","summary":"  We generalize the leverage score sampling sketch for $\\ell_2$-subspace\nembeddings, to accommodate sampling subsets of the transformed data, so that\nthe sketching approach is appropriate for distributed settings. This is then\nused to derive an approximate coded computing approach for first-order methods;\nknown as gradient coding, to accelerate linear regression in the presence of\nfailures in distributed computational networks, \\textit{i.e.} stragglers. We\nreplicate the data across the distributed network, to attain the approximation\nguarantees through the induced sampling distribution. The significance and main\ncontribution of this work, is that it unifies randomized numerical linear\nalgebra with approximate coded computing, while attaining an induced\n$\\ell_2$-subspace embedding through uniform sampling. The transition to uniform\nsampling is done without applying a random projection, as in the case of the\nsubsampled randomized Hadamard transform. Furthermore, by incorporating this\ntechnique to coded computing, our scheme is an iterative sketching approach to\napproximately solving linear regression. We also propose weighting when\nsketching takes place through sampling with replacement, for further\ncompression.\n","authors":["Neophytos Charalambides","Mert Pilanci","Alfred Hero"],"pdf_url":"https://arxiv.org/pdf/2308.03096v2.pdf","comment":"26 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2405.10490v3","updated":"2024-06-25T22:52:43Z","published":"2024-05-17T01:44:30Z","title":"Neural Optimization with Adaptive Heuristics for Intelligent Marketing\n  System","summary":"  Computational marketing has become increasingly important in today's digital\nworld, facing challenges such as massive heterogeneous data, multi-channel\ncustomer journeys, and limited marketing budgets. In this paper, we propose a\ngeneral framework for marketing AI systems, the Neural Optimization with\nAdaptive Heuristics (NOAH) framework. NOAH is the first general framework for\nmarketing optimization that considers both to-business (2B) and to-consumer\n(2C) products, as well as both owned and paid channels. We describe key modules\nof the NOAH framework, including prediction, optimization, and adaptive\nheuristics, providing examples for bidding and content optimization. We then\ndetail the successful application of NOAH to LinkedIn's email marketing system,\nshowcasing significant wins over the legacy ranking system. Additionally, we\nshare details and insights that are broadly useful, particularly on: (i)\naddressing delayed feedback with lifetime value, (ii) performing large-scale\nlinear programming with randomization, (iii) improving retrieval with audience\nexpansion, (iv) reducing signal dilution in targeting tests, and (v) handling\nzero-inflated heavy-tail metrics in statistical testing.\n","authors":["Changshuai Wei","Benjamin Zelditch","Joyce Chen","Andre Assuncao Silva T Ribeiro","Jingyi Kenneth Tay","Borja Ocejo Elizondo","Keerthi Selvaraj","Aman Gupta","Licurgo Benemann De Almeida"],"pdf_url":"https://arxiv.org/pdf/2405.10490v3.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2406.17968v1","updated":"2024-06-25T22:50:48Z","published":"2024-06-25T22:50:48Z","title":"Efficient Document Ranking with Learnable Late Interactions","summary":"  Cross-Encoder (CE) and Dual-Encoder (DE) models are two fundamental\napproaches for query-document relevance in information retrieval. To predict\nrelevance, CE models use joint query-document embeddings, while DE models\nmaintain factorized query and document embeddings; usually, the former has\nhigher quality while the latter benefits from lower latency. Recently,\nlate-interaction models have been proposed to realize more favorable\nlatency-quality tradeoffs, by using a DE structure followed by a lightweight\nscorer based on query and document token embeddings. However, these lightweight\nscorers are often hand-crafted, and there is no understanding of their\napproximation power; further, such scorers require access to individual\ndocument token embeddings, which imposes an increased latency and storage\nburden. In this paper, we propose novel learnable late-interaction models\n(LITE) that resolve these issues. Theoretically, we prove that LITE is a\nuniversal approximator of continuous scoring functions, even for relatively\nsmall embedding dimension. Empirically, LITE outperforms previous\nlate-interaction models such as ColBERT on both in-domain and zero-shot\nre-ranking tasks. For instance, experiments on MS MARCO passage re-ranking show\nthat LITE not only yields a model with better generalization, but also lowers\nlatency and requires 0.25x storage compared to ColBERT.\n","authors":["Ziwei Ji","Himanshu Jain","Andreas Veit","Sashank J. Reddi","Sadeep Jayasumana","Ankit Singh Rawat","Aditya Krishna Menon","Felix Yu","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.17968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17961v1","updated":"2024-06-25T22:40:03Z","published":"2024-06-25T22:40:03Z","title":"NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data\n  Normalization","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in parsing textual data and generating code. However, their\nperformance in tasks involving tabular data, especially those requiring\nsymbolic reasoning, faces challenges due to the structural variance and\ninconsistency in table cell values often found in web tables. In this paper, we\nintroduce NormTab, a novel framework aimed at enhancing the symbolic reasoning\nperformance of LLMs by normalizing web tables. We study table normalization as\na stand-alone, one-time preprocessing step using LLMs to support symbolic\nreasoning on tabular data. Our experimental evaluation, conducted on\nchallenging web table datasets such as WikiTableQuestion and TabFact,\ndemonstrates that leveraging NormTab significantly improves symbolic reasoning\nperformance, showcasing the importance and effectiveness of web table\nnormalization for enhancing LLM-based symbolic reasoning tasks.\n","authors":["Md Mahadi Hasan Nahid","Davood Rafiei"],"pdf_url":"https://arxiv.org/pdf/2406.17961v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2404.03192v2","updated":"2024-06-25T20:54:16Z","published":"2024-04-04T04:23:19Z","title":"Do Large Language Models Rank Fairly? An Empirical Study on the Fairness\n  of LLMs as Rankers","summary":"  The integration of Large Language Models (LLMs) in information retrieval has\nraised a critical reevaluation of fairness in the text-ranking models. LLMs,\nsuch as GPT models and Llama2, have shown effectiveness in natural language\nunderstanding tasks, and prior works (e.g., RankGPT) have also demonstrated\nthat the LLMs exhibit better performance than the traditional ranking models in\nthe ranking task. However, their fairness remains largely unexplored. This\npaper presents an empirical study evaluating these LLMs using the TREC Fair\nRanking dataset, focusing on the representation of binary protected attributes\nsuch as gender and geographic location, which are historically underrepresented\nin search outcomes. Our analysis delves into how these LLMs handle queries and\ndocuments related to these attributes, aiming to uncover biases in their\nranking algorithms. We assess fairness from both user and content perspectives,\ncontributing an empirical benchmark for evaluating LLMs as the fair ranker.\n","authors":["Yuan Wang","Xuyang Wu","Hsin-Tai Wu","Zhiqiang Tao","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2404.03192v2.pdf","comment":"Accepted at NAACL 2024 Main Conference"}]},"2024-06-26T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.07544v2","updated":"2024-06-26T17:59:50Z","published":"2024-06-11T17:59:45Z","title":"Situational Awareness Matters in 3D Vision Language Reasoning","summary":"  Being able to carry out complicated vision language reasoning tasks in 3D\nspace represents a significant milestone in developing household robots and\nhuman-centered embodied AI. In this work, we demonstrate that a critical and\ndistinct challenge in 3D vision language reasoning is situational awareness,\nwhich incorporates two key components: (1) The autonomous agent grounds its\nself-location based on a language prompt. (2) The agent answers open-ended\nquestions from the perspective of its calculated position. To address this\nchallenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D\nvision language reasoning. We tokenize the 3D scene into sparse voxel\nrepresentation and propose a language-grounded situation estimator, followed by\na situated question answering module. Experiments on the SQA3D and ScanQA\ndatasets show that SIG3D outperforms state-of-the-art models in situation\nestimation and question answering by a large margin (e.g., an enhancement of\nover 30% on situation estimation accuracy). Subsequent analysis corroborates\nour architectural design choices, explores the distinct functions of visual and\ntextual tokens, and highlights the importance of situational awareness in the\ndomain of 3D question answering.\n","authors":["Yunze Man","Liang-Yan Gui","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2406.07544v2.pdf","comment":"CVPR 2024. Project Page: https://yunzeman.github.io/situation3d"},{"id":"http://arxiv.org/abs/2406.18534v1","updated":"2024-06-26T17:59:30Z","published":"2024-06-26T17:59:30Z","title":"Towards Compositionality in Concept Learning","summary":"  Concept-based interpretability methods offer a lens into the internals of\nfoundation models by decomposing their embeddings into high-level concepts.\nThese concept representations are most useful when they are compositional,\nmeaning that the individual concepts compose to explain the full sample. We\nshow that existing unsupervised concept extraction methods find concepts which\nare not compositional. To automatically discover compositional concept\nrepresentations, we identify two salient properties of such representations,\nand propose Compositional Concept Extraction (CCE) for finding concepts which\nobey these properties. We evaluate CCE on five different datasets over image\nand text data. Our evaluation shows that CCE finds more compositional concept\nrepresentations than baselines and yields better accuracy on four downstream\nclassification tasks. Code and data are available at\nhttps://github.com/adaminsky/compositional_concepts .\n","authors":["Adam Stein","Aaditya Naik","Yinjun Wu","Mayur Naik","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2406.18534v1.pdf","comment":"Accepted at ICML 2024. 26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.18532v1","updated":"2024-06-26T17:59:18Z","published":"2024-06-26T17:59:18Z","title":"Symbolic Learning Enables Self-Evolving Agents","summary":"  The AI community has been exploring a pathway to artificial general\nintelligence (AGI) by developing \"language agents\", which are complex large\nlanguage models (LLMs) pipelines involving both prompting techniques and tool\nusage methods. While language agents have demonstrated impressive capabilities\nfor many real-world tasks, a fundamental limitation of current language agents\nresearch is that they are model-centric, or engineering-centric. That's to say,\nthe progress on prompts, tools, and pipelines of language agents requires\nsubstantial manual engineering efforts from human experts rather than\nautomatically learning from data. We believe the transition from model-centric,\nor engineering-centric, to data-centric, i.e., the ability of language agents\nto autonomously learn and evolve in environments, is the key for them to\npossibly achieve AGI.\n  In this work, we introduce agent symbolic learning, a systematic framework\nthat enables language agents to optimize themselves on their own in a\ndata-centric way using symbolic optimizers. Specifically, we consider agents as\nsymbolic networks where learnable weights are defined by prompts, tools, and\nthe way they are stacked together. Agent symbolic learning is designed to\noptimize the symbolic network within language agents by mimicking two\nfundamental algorithms in connectionist learning: back-propagation and gradient\ndescent. Instead of dealing with numeric weights, agent symbolic learning works\nwith natural language simulacrums of weights, loss, and gradients. We conduct\nproof-of-concept experiments on both standard benchmarks and complex real-world\ntasks and show that agent symbolic learning enables language agents to update\nthemselves after being created and deployed in the wild, resulting in\n\"self-evolving agents\".\n","authors":["Wangchunshu Zhou","Yixin Ou","Shengwei Ding","Long Li","Jialong Wu","Tiannan Wang","Jiamin Chen","Shuai Wang","Xiaohua Xu","Ningyu Zhang","Huajun Chen","Yuchen Eleanor Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.18532v1.pdf","comment":"Code available at https://github.com/aiwaves-cn/agents"},{"id":"http://arxiv.org/abs/2304.02181v2","updated":"2024-06-26T17:58:42Z","published":"2023-04-05T01:09:58Z","title":"On the Impact of Voice Anonymization on Speech Diagnostic Applications:\n  a Case Study on COVID-19 Detection","summary":"  With advances seen in deep learning, voice-based applications are burgeoning,\nranging from personal assistants, affective computing, to remote disease\ndiagnostics. As the voice contains both linguistic and para-linguistic\ninformation (e.g., vocal pitch, intonation, speech rate, loudness), there is\ngrowing interest in voice anonymization to preserve speaker privacy and\nidentity. Voice privacy challenges have emerged over the last few years and\nfocus has been placed on removing speaker identity while keeping linguistic\ncontent intact. For affective computing and disease monitoring applications,\nhowever, the para-linguistic content may be more critical. Unfortunately, the\neffects that anonymization may have on these systems are still largely unknown.\nIn this paper, we fill this gap and focus on one particular health monitoring\napplication: speech-based COVID-19 diagnosis. We test three anonymization\nmethods and their impact on five different state-of-the-art COVID-19 diagnostic\nsystems using three public datasets. We validate the effectiveness of the\nanonymization methods, compare their computational complexity, and quantify the\nimpact across different testing scenarios for both within- and across-dataset\nconditions. Additionally, we provided a comprehensive evaluation of the\nimportance of different speech aspects for diagnostics and showed how they are\naffected by different types of anonymizers. Lastly, we show the benefits of\nusing anonymized external data as a data augmentation tool to help recover some\nof the COVID-19 diagnostic accuracy loss seen with anonymization.\n","authors":["Yi Zhu","Mohamed Imoussaïne-Aïkous","Carolyn Côté-Lussier","Tiago H. Falk"],"pdf_url":"https://arxiv.org/pdf/2304.02181v2.pdf","comment":"Updated version; Published at IEEE Transactions on Information\n  Forensics and Security"},{"id":"http://arxiv.org/abs/2406.18528v1","updated":"2024-06-26T17:56:29Z","published":"2024-06-26T17:56:29Z","title":"PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine\n  Translation and Summarization Evaluation","summary":"  Large language models (LLMs) have revolutionized the field of NLP. Notably,\ntheir in-context learning capabilities also enable their use as evaluation\nmetrics for natural language generation, making them particularly advantageous\nin low-resource scenarios and time-restricted applications. In this work, we\nintroduce PrExMe, a large-scale prompt exploration for metrics, where we\nevaluate more than 720 prompt templates for open-source LLM-based metrics on\nmachine translation (MT) and summarization datasets, totalling over 6.6M\nevaluations. This extensive comparison (1) serves as a benchmark of the\nperformance of recent open-source LLMs as metrics and (2) explores the\nstability and variability of different prompting strategies. We discover that,\non the one hand, there are scenarios for which prompts are stable. For\ninstance, some LLMs show idiosyncratic preferences and favor to grade generated\ntexts with textual labels while others prefer to return numeric scores. On the\nother hand, the stability of prompts and model rankings can be susceptible to\nseemingly innocuous changes. For example, changing the requested output format\nfrom \"0 to 100\" to \"-1 to +1\" can strongly affect the rankings in our\nevaluation. Our study contributes to understanding the impact of different\nprompting approaches on LLM-based metrics for MT and summarization evaluation,\nhighlighting the most stable prompting patterns and potential limitations.\n","authors":["Christoph Leiter","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2406.18528v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.18522v1","updated":"2024-06-26T17:50:47Z","published":"2024-06-26T17:50:47Z","title":"ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of\n  Text-to-Time-lapse Video Generation","summary":"  We propose a novel text-to-video (T2V) generation benchmark,\nChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the\nT2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast\nto existing benchmarks that focus on the visual quality and textual relevance\nof generated videos, ChronoMagic-Bench focuses on the model's ability to\ngenerate time-lapse videos with significant metamorphic amplitude and temporal\ncoherence. The benchmark probes T2V models for their physics, biology, and\nchemistry capabilities, in a free-form text query. For these purposes,\nChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,\ncategorized into four major types of time-lapse videos: biological,\nhuman-created, meteorological, and physical phenomena, which are further\ndivided into 75 subcategories. This categorization comprehensively evaluates\nthe model's capacity to handle diverse and complex transformations. To\naccurately align human preference with the benchmark, we introduce two new\nautomatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic\nattributes and temporal coherence. MTScore measures the metamorphic amplitude,\nreflecting the degree of change over time, while CHScore assesses the temporal\ncoherence, ensuring the generated videos maintain logical progression and\ncontinuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual\nevaluations of ten representative T2V models, revealing their strengths and\nweaknesses across different categories of prompts, and providing a thorough\nevaluation framework that addresses current gaps in video generation research.\nMoreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k\nhigh-quality pairs of 720p time-lapse videos and detailed captions ensuring\nhigh physical pertinence and large metamorphic amplitude.\n","authors":["Shenghai Yuan","Jinfa Huang","Yongqi Xu","Yaoyang Liu","Shaofeng Zhang","Yujun Shi","Ruijie Zhu","Xinhua Cheng","Jiebo Luo","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.18522v1.pdf","comment":"31 pages, 15 figures"},{"id":"http://arxiv.org/abs/2406.18521v1","updated":"2024-06-26T17:50:11Z","published":"2024-06-26T17:50:11Z","title":"CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal\n  LLMs","summary":"  Chart understanding plays a pivotal role when applying Multimodal Large\nLanguage Models (MLLMs) to real-world tasks such as analyzing scientific papers\nor financial reports. However, existing datasets often focus on oversimplified\nand homogeneous charts with template-based questions, leading to an\nover-optimistic measure of progress. We demonstrate that although open-source\nmodels can appear to outperform strong proprietary models on these benchmarks,\na simple stress test with slightly different charts or questions can\ndeteriorate performance by up to 34.5%. In this work, we propose CharXiv, a\ncomprehensive evaluation suite involving 2,323 natural, challenging, and\ndiverse charts from arXiv papers. CharXiv includes two types of questions: 1)\ndescriptive questions about examining basic chart elements and 2) reasoning\nquestions that require synthesizing information across complex visual elements\nin the chart. To ensure quality, all charts and questions are handpicked,\ncurated, and verified by human experts. Our results reveal a substantial,\npreviously underestimated gap between the reasoning skills of the strongest\nproprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the\nstrongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%.\nAll models lag far behind human performance of 80.5%, underscoring weaknesses\nin the chart understanding capabilities of existing MLLMs. We hope CharXiv\nfacilitates future research on MLLM chart understanding by providing a more\nrealistic and faithful measure of progress. Project page and leaderboard:\nhttps://charxiv.github.io/\n","authors":["Zirui Wang","Mengzhou Xia","Luxi He","Howard Chen","Yitao Liu","Richard Zhu","Kaiqu Liang","Xindi Wu","Haotian Liu","Sadhika Malladi","Alexis Chevalier","Sanjeev Arora","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18521v1.pdf","comment":"121 pages, 90 figures"},{"id":"http://arxiv.org/abs/2406.18518v1","updated":"2024-06-26T17:49:11Z","published":"2024-06-26T17:49:11Z","title":"APIGen: Automated Pipeline for Generating Verifiable and Diverse\n  Function-Calling Datasets","summary":"  The advancement of function-calling agent models requires diverse, reliable,\nand high-quality datasets. This paper presents APIGen, an automated data\ngeneration pipeline designed to synthesize verifiable high-quality datasets for\nfunction-calling applications. We leverage APIGen and collect 3,673 executable\nAPIs across 21 different categories to generate diverse function-calling\ndatasets in a scalable and structured manner. Each data in our dataset is\nverified through three hierarchical stages: format checking, actual function\nexecutions, and semantic verification, ensuring its reliability and\ncorrectness. We demonstrate that models trained with our curated datasets, even\nwith only 7B parameters, can achieve state-of-the-art performance on the\nBerkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.\nMoreover, our 1B model achieves exceptional performance, surpassing\nGPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000\nhigh-quality entries, aiming to advance the field of function-calling agent\ndomains. The dataset is available on Huggingface:\nhttps://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the\nproject homepage: https://apigen-pipeline.github.io/\n","authors":["Zuxin Liu","Thai Hoang","Jianguo Zhang","Ming Zhu","Tian Lan","Shirley Kokane","Juntao Tan","Weiran Yao","Zhiwei Liu","Yihao Feng","Rithesh Murthy","Liangwei Yang","Silvio Savarese","Juan Carlos Niebles","Huan Wang","Shelby Heinecke","Caiming Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.18518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00716v3","updated":"2024-06-26T17:48:18Z","published":"2024-04-25T15:51:06Z","title":"Large Language Models in the Clinic: A Comprehensive Benchmark","summary":"  The adoption of large language models (LLMs) to assist clinicians has\nattracted remarkable attention. Existing works mainly adopt the close-ended\nquestion-answering (QA) task with answer options for evaluation. However, many\nclinical decisions involve answering open-ended questions without pre-set\noptions. To better understand LLMs in the clinic, we construct a benchmark\nClinicBench. We first collect eleven existing datasets covering diverse\nclinical language generation, understanding, and reasoning tasks. Furthermore,\nwe construct six novel datasets and complex clinical tasks that are close to\nreal-world practice, i.e., referral QA, treatment recommendation,\nhospitalization (long document) summarization, patient education, pharmacology\nQA and drug interaction for emerging drugs. We conduct an extensive evaluation\nof twenty-two LLMs under both zero-shot and few-shot settings. Finally, we\ninvite medical experts to evaluate the clinical usefulness of LLMs.\n","authors":["Andrew Liu","Hongjian Zhou","Yining Hua","Omid Rohanian","Anshul Thakur","Lei Clifton","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2405.00716v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10552v2","updated":"2024-06-26T17:42:59Z","published":"2024-06-15T08:13:47Z","title":"Large Language Model Enhanced Clustering for News Event Detection","summary":"  The news landscape is continuously evolving, with an ever-increasing volume\nof information from around the world. Automated event detection within this\nvast data repository is essential for monitoring, identifying, and categorizing\nsignificant news occurrences across diverse platforms. This paper presents an\nevent detection framework that leverages Large Language Models (LLMs) combined\nwith clustering analysis to detect news events from the Global Database of\nEvents, Language, and Tone (GDELT). The framework enhances event clustering\nthrough both pre-event detection tasks (keyword extraction and text embedding)\nand post-event detection tasks (event summarization and topic labeling). We\nalso evaluate the impact of various textual embeddings on the quality of\nclustering outcomes, ensuring robust news categorization. Additionally, we\nintroduce a novel Cluster Stability Assessment Index (CSAI) to assess the\nvalidity and robustness of clustering results. CSAI utilizes latent feature\nvectors to provide a new way of measuring clustering quality. Our experiments\nindicate that combining LLM embeddings with clustering algorithms yields the\nbest results, demonstrating greater robustness in terms of CSAI scores.\nMoreover, post-event detection tasks generate meaningful insights, facilitating\neffective interpretation of event clustering results. Overall, our experimental\nresults indicate that the proposed framework offers valuable insights and could\nenhance the accuracy and depth of news reporting.\n","authors":["Adane Nega Tarekegn"],"pdf_url":"https://arxiv.org/pdf/2406.10552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18512v1","updated":"2024-06-26T17:33:51Z","published":"2024-06-26T17:33:51Z","title":"\"Is ChatGPT a Better Explainer than My Professor?\": Evaluating the\n  Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline","summary":"  Explanations form the foundation of knowledge sharing and build upon\ncommunication principles, social dynamics, and learning theories. We focus\nspecifically on conversational approaches for explanations because the context\nis highly adaptive and interactive. Our research leverages previous work on\nexplanatory acts, a framework for understanding the different strategies that\nexplainers and explainees employ in a conversation to both explain, understand,\nand engage with the other party. We use the 5-Levels dataset was constructed\nfrom the WIRED YouTube series by Wachsmuth et al., and later annotated by\nBooshehri et al. with explanatory acts. These annotations provide a framework\nfor understanding how explainers and explainees structure their response when\ncrafting a response.\n  With the rise of generative AI in the past year, we hope to better understand\nthe capabilities of Large Language Models (LLMs) and how they can augment\nexpert explainer's capabilities in conversational settings. To achieve this\ngoal, the 5-Levels dataset (We use Booshehri et al.'s 2023 annotated dataset\nwith explanatory acts.) allows us to audit the ability of LLMs in engaging in\nexplanation dialogues. To evaluate the effectiveness of LLMs in generating\nexplainer responses, we compared 3 different strategies, we asked human\nannotators to evaluate 3 different strategies: human explainer response, GPT4\nstandard response, GPT4 response with Explanation Moves.\n","authors":["Grace Li","Milad Alshomary","Smaranda Muresan"],"pdf_url":"https://arxiv.org/pdf/2406.18512v1.pdf","comment":"6 figures, 5 pages"},{"id":"http://arxiv.org/abs/2406.18510v1","updated":"2024-06-26T17:31:22Z","published":"2024-06-26T17:31:22Z","title":"WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially)\n  Safer Language Models","summary":"  We introduce WildTeaming, an automatic LLM safety red-teaming framework that\nmines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of\nnovel jailbreak tactics, and then composes multiple tactics for systematic\nexploration of novel jailbreaks. Compared to prior work that performed\nred-teaming via recruited human workers, gradient-based optimization, or\niterative revision with LLMs, our work investigates jailbreaks from chatbot\nusers who were not specifically instructed to break the system. WildTeaming\nreveals previously unidentified vulnerabilities of frontier LLMs, resulting in\nup to 4.6x more diverse and successful adversarial attacks compared to\nstate-of-the-art jailbreak methods.\n  While many datasets exist for jailbreak evaluation, very few open-source\ndatasets exist for jailbreak training, as safety training data has been closed\neven when model weights are open. With WildTeaming we create WildJailbreak, a\nlarge-scale open-source synthetic safety dataset with 262K vanilla (direct\nrequest) and adversarial (complex jailbreak) prompt-response pairs. To mitigate\nexaggerated safety behaviors, WildJailbreak provides two contrastive types of\nqueries: 1) harmful queries (vanilla & adversarial) and 2) benign queries that\nresemble harmful queries in form but contain no harm. As WildJailbreak\nconsiderably upgrades the quality and scale of existing safety resources, it\nuniquely enables us to examine the scaling effects of data and the interplay of\ndata properties and model capabilities during safety training. Through\nextensive experiments, we identify the training properties that enable an ideal\nbalance of safety behaviors: appropriate safeguarding without over-refusal,\neffective handling of vanilla and adversarial queries, and minimal, if any,\ndecrease in general capabilities. All components of WildJailbeak contribute to\nachieving balanced safety behaviors of models.\n","authors":["Liwei Jiang","Kavel Rao","Seungju Han","Allyson Ettinger","Faeze Brahman","Sachin Kumar","Niloofar Mireshghallah","Ximing Lu","Maarten Sap","Yejin Choi","Nouha Dziri"],"pdf_url":"https://arxiv.org/pdf/2406.18510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15778v2","updated":"2024-06-26T17:29:46Z","published":"2024-04-24T09:57:11Z","title":"BASS: Batched Attention-optimized Speculative Sampling","summary":"  Speculative decoding has emerged as a powerful method to improve latency and\nthroughput in hosting large language models. However, most existing\nimplementations focus on generating a single sequence. Real-world generative AI\napplications often require multiple responses and how to perform speculative\ndecoding in a batched setting while preserving its latency benefits poses\nnon-trivial challenges. This paper describes a system of batched speculative\ndecoding that sets a new state of the art in multi-sequence generation latency\nand that demonstrates superior GPU utilization as well as quality of\ngenerations within a time budget. For example, for a 7.8B-size model on a\nsingle A100 GPU and with a batch size of 8, each sequence is generated at an\naverage speed of 5.8ms per token, the overall throughput being 1.1K tokens per\nsecond. These results represent state-of-the-art latency and a 2.15X speed-up\nover optimized regular decoding. Within a time budget that regular decoding\ndoes not finish, our system is able to generate sequences with HumanEval\nPass@First of 43% and Pass@All of 61%, far exceeding what's feasible with\nsingle-sequence speculative decoding. Our peak GPU utilization during decoding\nreaches as high as 15.8%, more than 3X the highest of that of regular decoding\nand around 10X of single-sequence speculative decoding.\n","authors":["Haifeng Qian","Sujan Kumar Gonugondla","Sungsoo Ha","Mingyue Shang","Sanjay Krishna Gouda","Ramesh Nallapati","Sudipta Sengupta","Xiaofei Ma","Anoop Deoras"],"pdf_url":"https://arxiv.org/pdf/2404.15778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18505v1","updated":"2024-06-26T17:14:45Z","published":"2024-06-26T17:14:45Z","title":"Mental Modeling of Reinforcement Learning Agents by Language Models","summary":"  Can emergent language models faithfully model the intelligence of\ndecision-making agents? Though modern language models exhibit already some\nreasoning ability, and theoretically can potentially express any probable\ndistribution over tokens, it remains underexplored how the world knowledge\nthese pretrained models have memorized can be utilized to comprehend an agent's\nbehaviour in the physical world. This study empirically examines, for the first\ntime, how well large language models (LLMs) can build a mental model of agents,\ntermed agent mental modelling, by reasoning about an agent's behaviour and its\neffect on states from agent interaction history. This research may unveil the\npotential of leveraging LLMs for elucidating RL agent behaviour, addressing a\nkey challenge in eXplainable reinforcement learning (XRL). To this end, we\npropose specific evaluation metrics and test them on selected RL task datasets\nof varying complexity, reporting findings on agent mental model establishment.\nOur results disclose that LLMs are not yet capable of fully mental modelling\nagents through inference alone without further innovations. This work thus\nprovides new insights into the capabilities and limitations of modern LLMs.\n","authors":["Wenhao Lu","Xufeng Zhao","Josua Spisak","Jae Hee Lee","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2406.18505v1.pdf","comment":"https://lukaswill.github.io/"},{"id":"http://arxiv.org/abs/2406.18501v1","updated":"2024-06-26T17:06:41Z","published":"2024-06-26T17:06:41Z","title":"Is In-Context Learning a Type of Gradient-Based Learning? Evidence from\n  the Inverse Frequency Effect in Structural Priming","summary":"  Large language models (LLMs) have shown the emergent capability of in-context\nlearning (ICL). One line of research has explained ICL as functionally\nperforming gradient descent. In this paper, we introduce a new way of\ndiagnosing whether ICL is functionally equivalent to gradient-based learning.\nOur approach is based on the inverse frequency effect (IFE) -- a phenomenon in\nwhich an error-driven learner is expected to show larger updates when trained\non infrequent examples than frequent ones. The IFE has previously been studied\nin psycholinguistics because humans show this effect in the context of\nstructural priming (the tendency for people to produce sentence structures they\nhave encountered recently); the IFE has been used as evidence that human\nstructural priming must involve error-driven learning mechanisms. In our\nexperiments, we simulated structural priming within ICL and found that LLMs\ndisplay the IFE, with the effect being stronger in larger models. We conclude\nthat ICL is indeed a type of gradient-based learning, supporting the hypothesis\nthat a gradient component is implicitly computed in the forward pass during\nICL. Our results suggest that both humans and LLMs make use of gradient-based,\nerror-driven processing mechanisms.\n","authors":["Zhenghao Zhou","Robert Frank","R. Thomas McCoy"],"pdf_url":"https://arxiv.org/pdf/2406.18501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15877v2","updated":"2024-06-26T17:05:14Z","published":"2024-06-22T15:52:04Z","title":"BigCodeBench: Benchmarking Code Generation with Diverse Function Calls\n  and Complex Instructions","summary":"  Automated software engineering has been greatly empowered by the recent\nadvances in Large Language Models (LLMs) for programming. While current\nbenchmarks have shown that LLMs can perform various software engineering tasks\nlike human developers, the majority of their evaluations are limited to short\nand self-contained algorithmic tasks. Solving challenging and practical\nprogramming tasks requires the capability of utilizing diverse function calls\nas tools to efficiently implement functionalities like data analysis and web\ndevelopment. In addition, using multiple tools to solve a task needs\ncompositional reasoning by accurately understanding complex instructions.\nFulfilling both of these characteristics can pose a great challenge for LLMs.\nTo assess how well LLMs can solve challenging and practical programming tasks,\nwe introduce Bench, a benchmark that challenges LLMs to invoke multiple\nfunction calls as tools from 139 libraries and 7 domains for 1,140 fine-grained\nprogramming tasks. To evaluate LLMs rigorously, each programming task\nencompasses 5.6 test cases with an average branch coverage of 99%. In addition,\nwe propose a natural-language-oriented variant of Bench, Benchi, that\nautomatically transforms the original docstrings into short instructions only\nwith essential information. Our extensive evaluation of 60 LLMs shows that LLMs\nare not yet capable of following complex instructions to use function calls\nprecisely, with scores up to 60%, significantly lower than the human\nperformance of 97%. The results underscore the need for further advancements in\nthis area.\n","authors":["Terry Yue Zhuo","Minh Chien Vu","Jenny Chim","Han Hu","Wenhao Yu","Ratnadira Widyasari","Imam Nur Bani Yusuf","Haolan Zhan","Junda He","Indraneil Paul","Simon Brunner","Chen Gong","Thong Hoang","Armel Randy Zebaze","Xiaoheng Hong","Wen-Ding Li","Jean Kaddour","Ming Xu","Zhihan Zhang","Prateek Yadav","Naman Jain","Alex Gu","Zhoujun Cheng","Jiawei Liu","Qian Liu","Zijian Wang","David Lo","Binyuan Hui","Niklas Muennighoff","Daniel Fried","Xiaoning Du","Harm de Vries","Leandro Von Werra"],"pdf_url":"https://arxiv.org/pdf/2406.15877v2.pdf","comment":"44 pages, 14 figures, 7 tables, built with love by the BigCode\n  community :)"},{"id":"http://arxiv.org/abs/2406.18495v1","updated":"2024-06-26T16:58:20Z","published":"2024-06-26T16:58:20Z","title":"WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks,\n  and Refusals of LLMs","summary":"  We introduce WildGuard -- an open, light-weight moderation tool for LLM\nsafety that achieves three goals: (1) identifying malicious intent in user\nprompts, (2) detecting safety risks of model responses, and (3) determining\nmodel refusal rate. Together, WildGuard serves the increasing needs for\nautomatic safety moderation and evaluation of LLM interactions, providing a\none-stop tool with enhanced accuracy and broad coverage across 13 risk\ncategories. While existing open moderation tools such as Llama-Guard2 score\nreasonably well in classifying straightforward model interactions, they lag far\nbehind a prompted GPT-4, especially in identifying adversarial jailbreaks and\nin evaluating models' refusals, a key measure for evaluating safety behaviors\nin model responses.\n  To address these challenges, we construct WildGuardMix, a large-scale and\ncarefully balanced multi-task safety moderation dataset with 92K labeled\nexamples that cover vanilla (direct) prompts and adversarial jailbreaks, paired\nwith various refusal and compliance responses. WildGuardMix is a combination of\nWildGuardTrain, the training data of WildGuard, and WildGuardTest, a\nhigh-quality human-annotated moderation test set with 5K labeled items covering\nbroad risk scenarios. Through extensive evaluations on WildGuardTest and ten\nexisting public benchmarks, we show that WildGuard establishes state-of-the-art\nperformance in open-source safety moderation across all the three tasks\ncompared to ten strong existing open-source moderation models (e.g., up to\n26.4% improvement on refusal detection). Importantly, WildGuard matches and\nsometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt\nharmfulness identification). WildGuard serves as a highly effective safety\nmoderator in an LLM interface, reducing the success rate of jailbreak attacks\nfrom 79.8% to 2.4%.\n","authors":["Seungju Han","Kavel Rao","Allyson Ettinger","Liwei Jiang","Bill Yuchen Lin","Nathan Lambert","Yejin Choi","Nouha Dziri"],"pdf_url":"https://arxiv.org/pdf/2406.18495v1.pdf","comment":"First two authors contributed equally. Third and fourth authors\n  contributed equally"},{"id":"http://arxiv.org/abs/2406.17294v2","updated":"2024-06-26T16:43:27Z","published":"2024-06-25T05:43:21Z","title":"Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large\n  Language Models","summary":"  Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, particularly in textual mathematical problem-solving. However,\nexisting open-source image instruction fine-tuning datasets, containing limited\nquestion-answer pairs per image, do not fully exploit visual information to\nenhance the multimodal mathematical reasoning capabilities of Multimodal LLMs\n(MLLMs). To bridge this gap, we address the lack of high-quality, diverse\nmultimodal mathematical datasets by collecting 40K high-quality images with\nquestion-answer pairs from 24 existing datasets and synthesizing 320K new\npairs, creating the MathV360K dataset, which enhances both the breadth and\ndepth of multimodal mathematical questions. We introduce Math-LLaVA, a\nLLaVA-1.5-based model fine-tuned with MathV360K. This novel approach\nsignificantly improves the multimodal mathematical reasoning capabilities of\nLLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V\non MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced\ngeneralizability, showing substantial improvements on the MMMU benchmark. Our\nresearch highlights the importance of dataset diversity and synthesis in\nadvancing MLLMs' mathematical reasoning abilities. The code and data are\navailable at: \\url{https://github.com/HZQ950419/Math-LLaVA}.\n","authors":["Wenhao Shi","Zhiqiang Hu","Yi Bin","Junhua Liu","Yang Yang","See-Kiong Ng","Lidong Bing","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2406.17294v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2312.02706v2","updated":"2024-06-26T16:11:55Z","published":"2023-12-05T12:07:30Z","title":"Large Knowledge Model: Perspectives and Challenges","summary":"  Humankind's understanding of the world is fundamentally linked to our\nperception and cognition, with \\emph{human languages} serving as one of the\nmajor carriers of \\emph{world knowledge}. In this vein, \\emph{Large Language\nModels} (LLMs) like ChatGPT epitomize the pre-training of extensive,\nsequence-based world knowledge into neural networks, facilitating the\nprocessing and manipulation of this knowledge in a parametric space. This\narticle explores large models through the lens of \"knowledge\". We initially\ninvestigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in\nenhancing LLMs, covering aspects like knowledge-augmented language model,\nstructure-inducing pre-training, knowledgeable prompts, structured CoT,\nknowledge editing, semantic tools for LLM and knowledgeable AI agents.\nSubsequently, we examine how LLMs can boost traditional symbolic knowledge\nbases, encompassing aspects like using LLM as KG builder and controller,\nstructured knowledge pretraining, and LLM-enhanced symbolic reasoning.\nConsidering the intricate nature of human knowledge, we advocate for the\ncreation of \\emph{Large Knowledge Models} (LKM), specifically engineered to\nmanage diversified spectrum of knowledge structures. This promising undertaking\nwould entail several key challenges, such as disentangling knowledge base from\nlanguage models, cognitive alignment with human knowledge, integration of\nperception and cognition, and building large commonsense models for interacting\nwith physical world, among others. We finally propose a five-\"A\" principle to\ndistinguish the concept of LKM.\n","authors":["Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2312.02706v2.pdf","comment":"Data Intelligence, Published: Jun 18, 2024"},{"id":"http://arxiv.org/abs/2406.18460v1","updated":"2024-06-26T16:10:53Z","published":"2024-06-26T16:10:53Z","title":"Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain\n  Human-Machine Conversation","summary":"  Recently, various methods have been proposed to create open-domain\nconversational agents with Large Language Models (LLMs). These models are able\nto answer user queries, but in a one-way Q&A format rather than a true\nconversation. Fine-tuning on particular datasets is the usual way to modify\ntheir style to increase conversational ability, but this is expensive and\nusually only available in a few languages. In this study, we explore role-play\nzero-shot prompting as an efficient and cost-effective solution for open-domain\nconversation, using capable multilingual LLMs (Beeching et al., 2023) trained\nto obey instructions. We design a prompting system that, when combined with an\ninstruction-following model - here Vicuna (Chiang et al., 2023) - produces\nconversational agents that match and even surpass fine-tuned models in human\nevaluation in French in two different tasks.\n","authors":["Ahmed Njifenjou","Virgile Sucal","Bassam Jabaian","Fabrice Lefèvre"],"pdf_url":"https://arxiv.org/pdf/2406.18460v1.pdf","comment":"Updated version of a paper originally submitted at SIGDIAL 2023"},{"id":"http://arxiv.org/abs/2406.14711v2","updated":"2024-06-26T16:05:20Z","published":"2024-06-20T20:09:37Z","title":"MultiAgent Collaboration Attack: Investigating Adversarial Attacks in\n  Large Language Model Collaborations via Debate","summary":"  Large Language Models (LLMs) have shown exceptional results on current\nbenchmarks when working individually. The advancement in their capabilities,\nalong with a reduction in parameter size and inference times, has facilitated\nthe use of these models as agents, enabling interactions among multiple models\nto execute complex tasks. Such collaborations offer several advantages,\nincluding the use of specialized models (e.g. coding), improved confidence\nthrough multiple computations, and enhanced divergent thinking, leading to more\ndiverse outputs. Thus, the collaborative use of language models is expected to\ngrow significantly in the coming years. In this work, we evaluate the behavior\nof a network of models collaborating through debate under the influence of an\nadversary. We introduce pertinent metrics to assess the adversary's\neffectiveness, focusing on system accuracy and model agreement. Our findings\nhighlight the importance of a model's persuasive ability in influencing others.\nAdditionally, we explore inference-time methods to generate more compelling\narguments and evaluate the potential of prompt-based mitigation as a defensive\nstrategy.\n","authors":["Alfonso Amayuelas","Xianjun Yang","Antonis Antoniades","Wenyue Hua","Liangming Pan","William Wang"],"pdf_url":"https://arxiv.org/pdf/2406.14711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15515v3","updated":"2024-06-26T15:57:22Z","published":"2024-04-23T20:59:03Z","title":"ToM-LM: Delegating Theory of Mind Reasoning to External Symbolic\n  Executors in Large Language Models","summary":"  Theory of Mind (ToM) refers to the ability of individuals to attribute mental\nstates to others. While Large Language Models (LLMs) have shown some promise\nwith ToM ability, they still struggle with complex ToM reasoning. Our approach\nleverages an external symbolic executor, specifically the SMCDEL model checker,\nand fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,\nan LLM is first fine-tuned through pairs of natural language and symbolic\nformulation representation of ToM problems and is then instructed to generate\nthe symbolic formulation with a one-shot in-context example. The generated\nsymbolic formulation is then executed by the SMCDEL model checker to perform\ntransparent and verifiable ToM reasoning and give the final result. We\ndemonstrate that our approach, ToM-LM, shows a significant improvement over all\nthe constructed baselines. Our study proposes a novel view about externalizing\na particular component of ToM reasoning, mainly reasoning about beliefs, and\nsuggests generalizing it to other aspects of ToM reasoning.\n","authors":["Weizhi Tang","Vaishak Belle"],"pdf_url":"https://arxiv.org/pdf/2404.15515v3.pdf","comment":"Accepted at NeSy 2024"},{"id":"http://arxiv.org/abs/2406.18449v1","updated":"2024-06-26T15:53:54Z","published":"2024-06-26T15:53:54Z","title":"Cascading Large Language Models for Salient Event Graph Generation","summary":"  Generating event graphs from long documents is challenging due to the\ninherent complexity of multiple tasks involved such as detecting events,\nidentifying their relationships, and reconciling unstructured input with\nstructured graphs. Recent studies typically consider all events with equal\nimportance, failing to distinguish salient events crucial for understanding\nnarratives. This paper presents CALLMSAE, a CAscading Large Language Model\nframework for SAlient Event graph generation, which leverages the capabilities\nof LLMs and eliminates the need for costly human annotations. We first identify\nsalient events by prompting LLMs to generate summaries, from which salient\nevents are identified. Next, we develop an iterative code refinement prompting\nstrategy to generate event relation graphs, removing hallucinated relations and\nrecovering missing edges. Fine-tuning contextualised graph generation models on\nthe LLM-generated graphs outperforms the models trained on CAEVO-generated\ndata. Experimental results on a human-annotated test set show that the proposed\nmethod generates salient and more accurate graphs, outperforming competitive\nbaselines.\n","authors":["Xingwei Tan","Yuxiang Zhou","Gabriele Pergola","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2406.18449v1.pdf","comment":"9 + 12 pages"},{"id":"http://arxiv.org/abs/2311.14096v2","updated":"2024-06-26T15:26:44Z","published":"2023-11-23T16:45:56Z","title":"Cultural Bias and Cultural Alignment of Large Language Models","summary":"  Culture fundamentally shapes people's reasoning, behavior, and communication.\nAs people increasingly use generative artificial intelligence (AI) to expedite\nand automate personal and professional tasks, cultural values embedded in AI\nmodels may bias people's authentic expression and contribute to the dominance\nof certain cultures. We conduct a disaggregated evaluation of cultural bias for\nfive widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3)\nby comparing the models' responses to nationally representative survey data.\nAll models exhibit cultural values resembling English-speaking and Protestant\nEuropean countries. We test cultural prompting as a control strategy to\nincrease cultural alignment for each country/territory. For recent models\n(GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models'\noutput for 71-81% of countries and territories. We suggest using cultural\nprompting and ongoing evaluation to reduce cultural bias in the output of\ngenerative AI.\n","authors":["Yan Tao","Olga Viberg","Ryan S. Baker","Rene F. Kizilcec"],"pdf_url":"https://arxiv.org/pdf/2311.14096v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17681v2","updated":"2024-06-26T15:21:49Z","published":"2024-06-25T16:13:53Z","title":"VarBench: Robust Language Model Benchmarking Through Dynamic Variable\n  Perturbation","summary":"  As large language models achieve impressive scores on traditional benchmarks,\nan increasing number of researchers are becoming concerned about benchmark data\nleakage during pre-training, commonly known as the data contamination problem.\nTo ensure fair evaluation, recent benchmarks release only the training and\nvalidation sets, keeping the test set labels closed-source. They require anyone\nwishing to evaluate his language model to submit the model's predictions for\ncentralized processing and then publish the model's result on their\nleaderboard. However, this submission process is inefficient and prevents\neffective error analysis. To address this issue, we propose to variabilize\nbenchmarks and evaluate language models dynamically. Specifically, we extract\nvariables from each test case and define a value range for each variable. For\neach evaluation, we sample new values from these value ranges to create unique\ntest cases, thus ensuring a fresh evaluation each time. We applied this\nvariable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and\nTruthfulQA, which cover mathematical generation and multiple-choice tasks. Our\nexperimental results demonstrate that this approach provides a more accurate\nassessment of the true capabilities of language models, effectively mitigating\nthe contamination problem.\n","authors":["Kun Qian","Shunji Wan","Claudia Tang","Youzhi Wang","Xuanming Zhang","Maximillian Chen","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2406.17681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12009v2","updated":"2024-06-26T15:00:52Z","published":"2023-12-19T09:58:54Z","title":"Active Preference Inference using Language Models and Probabilistic\n  Reasoning","summary":"  Actively inferring user preferences, for example by asking good questions, is\nimportant for any human-facing decision-making system. Active inference allows\nsuch systems to adapt and personalize themselves to nuanced individual\npreferences. To enable this ability for instruction-tuned large language models\n(LLMs), one may prompt them to ask users questions to infer their preferences,\ntransforming the language models into more robust, interactive systems.\nHowever, out of the box, these models are not efficient at extracting\npreferences: the questions they generate are not informative, requiring a high\nnumber of user interactions and impeding the usability of the downstream\nsystem. In this work, we introduce an inference-time algorithm that helps LLMs\nquickly infer preferences by using more informative questions. Our algorithm\nuses a probabilistic model whose conditional distributions are defined by\nprompting an LLM, and returns questions that optimize expected entropy and\nexpected model change. Results in a simplified interactive web shopping setting\nwith real product items show that an LLM equipped with our entropy reduction\nalgorithm outperforms baselines with the same underlying LLM on task\nperformance while using fewer user interactions.\n","authors":["Wasu Top Piriyakulkij","Volodymyr Kuleshov","Kevin Ellis"],"pdf_url":"https://arxiv.org/pdf/2312.12009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16772v2","updated":"2024-06-26T15:00:04Z","published":"2024-06-24T16:31:12Z","title":"OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?","summary":"  In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic\nmedal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and\nClaude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to\ncontinuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).\n","authors":["Zhen Huang","Zengzhi Wang","Shijie Xia","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2406.16772v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2406.18406v1","updated":"2024-06-26T14:57:38Z","published":"2024-06-26T14:57:38Z","title":"IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying\n  and Reweighting Context-Aware Neurons","summary":"  It is widely acknowledged that large language models (LLMs) encode a vast\nreservoir of knowledge after being trained on mass data. Recent studies\ndisclose knowledge conflicts in LLM generation, wherein outdated or incorrect\nparametric knowledge (i.e., encoded knowledge) contradicts new knowledge\nprovided in the context. To mitigate such knowledge conflicts, we propose a\nnovel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to\ncapitalize on neurons that are crucial in processing contextual cues.\nSpecifically, IRCAN first identifies neurons that significantly contribute to\ncontext processing, utilizing a context-aware attribution score derived from\nintegrated gradients. Subsequently, the identified context-aware neurons are\nstrengthened via reweighting. In doing so, we steer LLMs to generate\ncontext-sensitive outputs with respect to the new knowledge provided in the\ncontext. Extensive experiments conducted across a variety of models and tasks\ndemonstrate that IRCAN not only achieves remarkable improvements in handling\nknowledge conflicts but also offers a scalable, plug-andplay solution that can\nbe integrated seamlessly with existing models.\n","authors":["Dan Shi","Renren Jin","Tianhao Shen","Weilong Dong","Xinwei Wu","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.18406v1.pdf","comment":"19 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.18403v1","updated":"2024-06-26T14:56:13Z","published":"2024-06-26T14:56:13Z","title":"LLMs instead of Human Judges? A Large Scale Empirical Study across 20\n  NLP Evaluation Tasks","summary":"  There is an increasing trend towards evaluating NLP models with LLM-generated\njudgments instead of human judgments. In the absence of a comparison against\nhuman data, this raises concerns about the validity of these evaluations; in\ncase they are conducted with proprietary models, this also raises concerns over\nreproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with\nhuman annotations, and comprehensively evaluate 11 current LLMs, covering both\nopen-weight and proprietary models, for their ability to replicate the\nannotations. Our evaluations show that each LLM exhibits a large variance\nacross datasets in its correlation to human judgments. We conclude that LLMs\nare not yet ready to systematically replace human judges in NLP.\n","authors":["Anna Bavaresco","Raffaella Bernardi","Leonardo Bertolazzi","Desmond Elliott","Raquel Fernández","Albert Gatt","Esam Ghaleb","Mario Giulianelli","Michael Hanna","Alexander Koller","André F. T. Martins","Philipp Mondorf","Vera Neplenbroek","Sandro Pezzelle","Barbara Plank","David Schlangen","Alessandro Suglia","Aditya K Surikuchi","Ece Takmaz","Alberto Testoni"],"pdf_url":"https://arxiv.org/pdf/2406.18403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05091v2","updated":"2024-06-26T14:50:58Z","published":"2024-04-07T22:16:50Z","title":"MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation\n  and Fine-grained Classification","summary":"  To advance the evaluation of multimodal math reasoning in large multimodal\nmodels (LMMs), this paper introduces a novel benchmark, MM-MATH. MM-MATH\nconsists of 5,929 open-ended middle school math problems with visual contexts,\nwith fine-grained classification across difficulty, grade level, and knowledge\npoints. Unlike existing benchmarks relying on binary answer comparison, MM-MATH\nincorporates both outcome and process evaluations. Process evaluation employs\nLMM-as-a-judge to automatically analyze solution steps, identifying and\ncategorizing errors into specific error types. Extensive evaluation of ten\nmodels on MM-MATH reveals significant challenges for existing LMMs,\nhighlighting their limited utilization of visual information and struggles with\nhigher-difficulty problems. The best-performing model achieves only 31%\naccuracy on MM-MATH, compared to 82% for humans. This highlights the\nchallenging nature of our benchmark for existing models and the significant gap\nbetween the multimodal reasoning capabilities of current models and humans. Our\nprocess evaluation reveals that diagram misinterpretation is the most common\nerror, accounting for more than half of the total error cases, underscoring the\nneed for improved image comprehension in multimodal reasoning.\n","authors":["Kai Sun","Yushi Bai","Ji Qi","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2404.05091v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18400v1","updated":"2024-06-26T14:49:54Z","published":"2024-06-26T14:49:54Z","title":"Do LLMs dream of elephants (when told not to)? Latent concept\n  association and associative memory in transformers","summary":"  Large Language Models (LLMs) have the capacity to store and recall facts.\nThrough experimentation with open-source models, we observe that this ability\nto retrieve facts can be easily manipulated by changing contexts, even without\naltering their factual meanings. These findings highlight that LLMs might\nbehave like an associative memory model where certain tokens in the contexts\nserve as clues to retrieving facts. We mathematically explore this property by\nstudying how transformers, the building blocks of LLMs, can complete such\nmemory tasks. We study a simple latent concept association problem with a\none-layer transformer and we show theoretically and empirically that the\ntransformer gathers information using self-attention and uses the value matrix\nfor associative memory.\n","authors":["Yibo Jiang","Goutham Rajendran","Pradeep Ravikumar","Bryon Aragam"],"pdf_url":"https://arxiv.org/pdf/2406.18400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17357v3","updated":"2024-06-26T14:41:33Z","published":"2024-05-27T17:02:27Z","title":"DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank\n  Distribution","summary":"  Fine-tuning large-scale pre-trained models is inherently a resource-intensive\ntask. While it can enhance the capabilities of the model, it also incurs\nsubstantial computational costs, posing challenges to the practical application\nof downstream tasks. Existing parameter-efficient fine-tuning (PEFT) methods\nsuch as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the\ndifferential parameter budget requirements across weight matrices, which may\nlead to suboptimal fine-tuning outcomes. To address this issue, we introduce\nthe Dynamic Low-Rank Adaptation (DoRA) method. DoRA decomposes high-rank LoRA\nlayers into structured single-rank components, allowing for dynamic pruning of\nparameter budget based on their importance to specific tasks during training,\nwhich makes the most of the limited parameter budget. Experimental results\ndemonstrate that DoRA can achieve competitive performance compared with LoRA\nand full model fine-tuning, and outperform various strong baselines with the\nsame storage parameter budget. Our code is available at\nhttps://github.com/MIkumikumi0116/DoRA\n","authors":["Yulong Mao","Kaiyu Huang","Changhao Guan","Ganglin Bao","Fengran Mo","Jinan Xu"],"pdf_url":"https://arxiv.org/pdf/2405.17357v3.pdf","comment":"Accepted by the main conference of ACL 2024"},{"id":"http://arxiv.org/abs/2406.17282v2","updated":"2024-06-26T14:38:31Z","published":"2024-06-25T05:14:54Z","title":"SetBERT: Enhancing Retrieval Performance for Boolean Logic and Set\n  Operation Queries","summary":"  We introduce SetBERT, a fine-tuned BERT-based model designed to enhance query\nembeddings for set operations and Boolean logic queries, such as Intersection\n(AND), Difference (NOT), and Union (OR). SetBERT significantly improves\nretrieval performance for logic-structured queries, an area where both\ntraditional and neural retrieval methods typically underperform. We propose an\ninnovative use of inversed-contrastive loss, focusing on identifying the\nnegative sentence, and fine-tuning BERT with a dataset generated via prompt\nGPT. Furthermore, we demonstrate that, unlike other BERT-based models,\nfine-tuning with triplet loss actually degrades performance for this specific\ntask. Our experiments reveal that SetBERT-base not only significantly\noutperforms BERT-base (up to a 63% improvement in Recall) but also achieves\nperformance comparable to the much larger BERT-large model, despite being only\none-third the size.\n","authors":["Quan Mai","Susan Gauch","Douglas Adams"],"pdf_url":"https://arxiv.org/pdf/2406.17282v2.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2210.13382v5","updated":"2024-06-26T14:27:49Z","published":"2022-10-24T16:29:55Z","title":"Emergent World Representations: Exploring a Sequence Model Trained on a\n  Synthetic Task","summary":"  Language models show a surprising range of capabilities, but the source of\ntheir apparent competence is unclear. Do these networks just memorize a\ncollection of surface statistics, or do they rely on internal representations\nof the process that generates the sequences they see? We investigate this\nquestion by applying a variant of the GPT model to the task of predicting legal\nmoves in a simple board game, Othello. Although the network has no a priori\nknowledge of the game or its rules, we uncover evidence of an emergent\nnonlinear internal representation of the board state. Interventional\nexperiments indicate this representation can be used to control the output of\nthe network and create \"latent saliency maps\" that can help explain predictions\nin human terms.\n","authors":["Kenneth Li","Aspen K. Hopkins","David Bau","Fernanda Viégas","Hanspeter Pfister","Martin Wattenberg"],"pdf_url":"https://arxiv.org/pdf/2210.13382v5.pdf","comment":"ICLR 2023 oral (notable-top-5%):\n  https://openreview.net/forum?id=DeG07_TcZvT ; code:\n  https://github.com/likenneth/othello_world"},{"id":"http://arxiv.org/abs/2406.10190v2","updated":"2024-06-26T14:22:18Z","published":"2024-06-14T17:23:57Z","title":"CHIRON: Rich Character Representations in Long-Form Narratives","summary":"  Characters are integral to long-form narratives, but are poorly understood by\nexisting story analysis and generation systems. While prior work has simplified\ncharacters via graph-based methods and brief character descriptions, we aim to\nbetter tackle the problem of representing complex characters by taking\ninspiration from advice given to professional writers. We propose CHIRON, a new\n`character sheet' based representation that organizes and filters textual\ninformation about characters. We construct CHIRON sheets in two steps: a\nGeneration Module that prompts an LLM for character information via\nquestion-answering and a Validation Module that uses automated reasoning and a\ndomain-specific entailment model to eliminate false facts about a character. We\nvalidate CHIRON via the downstream task of masked-character prediction, where\nour experiments show CHIRON is better and more flexible than comparable\nsummary-based baselines. We also show that metrics derived from CHIRON can be\nused to automatically infer character-centricity in stories, and that these\nmetrics align with human judgments.\n","authors":["Alexander Gurung","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2406.10190v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18373v1","updated":"2024-06-26T14:17:36Z","published":"2024-06-26T14:17:36Z","title":"Dynamic Data Pruning for Automatic Speech Recognition","summary":"  The recent success of Automatic Speech Recognition (ASR) is largely\nattributed to the ever-growing amount of training data. However, this trend has\nmade model training prohibitively costly and imposed computational demands.\nWhile data pruning has been proposed to mitigate this issue by identifying a\nsmall subset of relevant data, its application in ASR has been barely explored,\nand existing works often entail significant overhead to achieve meaningful\nresults. To fill this gap, this paper presents the first investigation of\ndynamic data pruning for ASR, finding that we can reach the full-data\nperformance by dynamically selecting 70% of data. Furthermore, we introduce\nDynamic Data Pruning for ASR (DDP-ASR), which offers several fine-grained\npruning granularities specifically tailored for speech-related datasets, going\nbeyond the conventional pruning of entire time sequences. Our intensive\nexperiments show that DDP-ASR can save up to 1.6x training time with negligible\nperformance loss.\n","authors":["Qiao Xiao","Pingchuan Ma","Adriana Fernandez-Lopez","Boqian Wu","Lu Yin","Stavros Petridis","Mykola Pechenizkiy","Maja Pantic","Decebal Constantin Mocanu","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2406.18373v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2306.03341v6","updated":"2024-06-26T14:11:53Z","published":"2023-06-06T01:26:53Z","title":"Inference-Time Intervention: Eliciting Truthful Answers from a Language\n  Model","summary":"  We introduce Inference-Time Intervention (ITI), a technique designed to\nenhance the \"truthfulness\" of large language models (LLMs). ITI operates by\nshifting model activations during inference, following a set of directions\nacross a limited number of attention heads. This intervention significantly\nimproves the performance of LLaMA models on the TruthfulQA benchmark. On an\ninstruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from\n32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and\ndemonstrate how to balance it by tuning the intervention strength. ITI is\nminimally invasive and computationally inexpensive. Moreover, the technique is\ndata efficient: while approaches like RLHF require extensive annotations, ITI\nlocates truthful directions using only few hundred examples. Our findings\nsuggest that LLMs may have an internal representation of the likelihood of\nsomething being true, even as they produce falsehoods on the surface.\n","authors":["Kenneth Li","Oam Patel","Fernanda Viégas","Hanspeter Pfister","Martin Wattenberg"],"pdf_url":"https://arxiv.org/pdf/2306.03341v6.pdf","comment":"NeurIPS 2023 spotlight; code:\n  https://github.com/likenneth/honest_llama"},{"id":"http://arxiv.org/abs/2406.18365v1","updated":"2024-06-26T14:04:29Z","published":"2024-06-26T14:04:29Z","title":"Themis: Towards Flexible and Interpretable NLG Evaluation","summary":"  The evaluation of natural language generation (NLG) tasks is a significant\nand longstanding research issue. With the recent emergence of powerful large\nlanguage models (LLMs), some studies have turned to LLM-based automatic\nevaluation methods, which demonstrate great potential to become a new\nevaluation paradigm following traditional string-based and model-based metrics.\nHowever, despite the improved performance of existing methods, they still\npossess some deficiencies, such as dependency on references and limited\nevaluation flexibility. Therefore, in this paper, we meticulously construct a\nlarge-scale NLG evaluation corpus NLG-Eval with human and GPT-4 annotations to\nalleviate the lack of relevant data in this field. Furthermore, we propose\nThemis, an LLM dedicated to NLG evaluation, which has been trained with our\ndesigned multi-perspective consistency and rating-oriented preference alignment\nmethods. Themis can conduct flexible and interpretable evaluations without\nreferences, and it exhibits superior evaluation performance on various NLG\ntasks, simultaneously generalizing well to unseen tasks and surpassing other\nevaluation models, including GPT-4.\n","authors":["Xinyu Hu","Li Lin","Mingqi Gao","Xunjian Yin","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2406.18365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18364v1","updated":"2024-06-26T14:04:15Z","published":"2024-06-26T14:04:15Z","title":"Research on Information Extraction of LCSTS Dataset Based on an Improved\n  BERTSum-LSTM Model","summary":"  With the continuous advancement of artificial intelligence, natural language\nprocessing technology has become widely utilized in various fields. At the same\ntime, there are many challenges in creating Chinese news summaries. First of\nall, the semantics of Chinese news is complex, and the amount of information is\nenormous. Extracting critical information from Chinese news presents a\nsignificant challenge. Second, the news summary should be concise and clear,\nfocusing on the main content and avoiding redundancy. In addition, the\nparticularity of the Chinese language, such as polysemy, word segmentation,\netc., makes it challenging to generate Chinese news summaries. Based on the\nabove, this paper studies the information extraction method of the LCSTS\ndataset based on an improved BERTSum-LSTM model. We improve the BERTSum-LSTM\nmodel to make it perform better in generating Chinese news summaries. The\nexperimental results show that the proposed method has a good effect on\ncreating news summaries, which is of great importance to the construction of\nnews summaries.\n","authors":["Yiming Chen","Haobin Chen","Simin Liu","Yunyun Liu","Fanhao Zhou","Bing Wei"],"pdf_url":"https://arxiv.org/pdf/2406.18364v1.pdf","comment":"submitted to ICMIII 2024"},{"id":"http://arxiv.org/abs/2406.10794v2","updated":"2024-06-26T13:50:32Z","published":"2024-06-16T03:38:48Z","title":"Towards Understanding Jailbreak Attacks in LLMs: A Representation Space\n  Analysis","summary":"  Large language models (LLMs) are susceptible to a type of attack known as\njailbreaking, which misleads LLMs to output harmful contents. Although there\nare diverse jailbreak attack strategies, there is no unified understanding on\nwhy some methods succeed and others fail. This paper explores the behavior of\nharmful and harmless prompts in the LLM's representation space to investigate\nthe intrinsic properties of successful jailbreak attacks. We hypothesize that\nsuccessful attacks share some similar properties: They are effective in moving\nthe representation of the harmful prompt towards the direction to the harmless\nprompts. We leverage hidden representations into the objective of existing\njailbreak attacks to move the attacks along the acceptance direction, and\nconduct experiments to validate the above hypothesis using the proposed\nobjective. We hope this study provides new insights into understanding how LLMs\nunderstand harmfulness information.\n","authors":["Yuping Lin","Pengfei He","Han Xu","Yue Xing","Makoto Yamada","Hui Liu","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2406.10794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18340v1","updated":"2024-06-26T13:35:10Z","published":"2024-06-26T13:35:10Z","title":"Grammar Assistance Using Syntactic Structures (GAUSS)","summary":"  Automatic grammar coaching serves an important purpose of advising on\nstandard grammar varieties while not imposing social pressures or reinforcing\nestablished social roles. Such systems already exist but most of them are for\nEnglish and few of them offer meaningful feedback. Furthermore, they typically\nrely completely on neural methods and require huge computational resources\nwhich most of the world cannot afford. We propose a grammar coaching system for\nSpanish that relies on (i) a rich linguistic formalism capable of giving\ninformative feedback; and (ii) a faster parsing algorithm which makes using\nthis formalism practical in a real-world application. The approach is feasible\nfor any language for which there is a computerized grammar and is less reliant\non expensive and environmentally costly neural methods. We seek to contribute\nto Greener AI and to address global education challenges by raising the\nstandards of inclusivity and engagement in grammar coaching.\n","authors":["Olga Zamaraeva","Lorena S. Allegue","Carlos Gómez-Rodríguez","Anastasiia Ogneva","Margarita Alonso-Ramos"],"pdf_url":"https://arxiv.org/pdf/2406.18340v1.pdf","comment":"5 pages, 4 figures, project summary for CEDI-SEPLN Seminar of the\n  Spanish Society for Natural Language Processing at the 7th Spanish Conference\n  on Informatics, June 19-20, 2024, A Coru\\~na, Spain"},{"id":"http://arxiv.org/abs/2406.17588v2","updated":"2024-06-26T13:28:04Z","published":"2024-06-25T14:31:26Z","title":"LongIns: A Challenging Long-context Instruction-based Exam for LLMs","summary":"  The long-context capabilities of large language models (LLMs) have been a hot\ntopic in recent years. To evaluate the performance of LLMs in different\nscenarios, various assessment benchmarks have emerged. However, as most of\nthese benchmarks focus on identifying key information to answer questions,\nwhich mainly requires the retrieval ability of LLMs, these benchmarks can\npartially represent the reasoning performance of LLMs from large amounts of\ninformation. Meanwhile, although LLMs often claim to have context windows of\n32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual\nsupported length of these LLMs. To address these issues, we propose the LongIns\nbenchmark dataset, a challenging long-context instruction-based exam for LLMs,\nwhich is built based on the existing instruction datasets. Specifically, in our\nLongIns, we introduce three evaluation settings: Global Instruction & Single\nTask (GIST), Local Instruction & Single Task (LIST), and Local Instruction &\nMultiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations\non existing LLMs and have the following important findings: (1). The\ntop-performing GPT-4 with 128k context length performs poorly on the evaluation\ncontext window of 16k in our LongIns. (2). For the multi-hop reasoning ability\nof many existing LLMs, significant efforts are still needed under short context\nwindows (less than 4k).\n","authors":["Shawn Gavin","Tuney Zheng","Jiaheng Liu","Quehry Que","Noah Wang","Jian Yang","Chenchen Zhang","Wenhao Huang","Wenhu Chen","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17588v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18326v1","updated":"2024-06-26T13:12:40Z","published":"2024-06-26T13:12:40Z","title":"PaCoST: Paired Confidence Significance Testing for Benchmark\n  Contamination Detection in Large Language Models","summary":"  Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods.\n","authors":["Huixuan Zhang","Yun Lin","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2406.18326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18321v1","updated":"2024-06-26T13:02:35Z","published":"2024-06-26T13:02:35Z","title":"MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large\n  Language Models Using Odyssey Math Data","summary":"  Large language models (LLMs) have significantly advanced natural language\nunderstanding and demonstrated strong problem-solving abilities. Despite these\nsuccesses, most LLMs still struggle with solving mathematical problems due to\nthe intricate reasoning required. This paper investigates the mathematical\nproblem-solving capabilities of LLMs using the newly developed \"MathOdyssey\"\ndataset. The dataset includes diverse mathematical problems at high school and\nuniversity levels, created by experts from notable institutions to rigorously\ntest LLMs in advanced problem-solving scenarios and cover a wider range of\nsubject areas. By providing the MathOdyssey dataset as a resource to the AI\ncommunity, we aim to contribute to the understanding and improvement of AI\ncapabilities in complex mathematical problem-solving. We conduct benchmarking\non open-source models, such as Llama-3 and DBRX-Instruct, and closed-source\nmodels from the GPT series and Gemini models. Our results indicate that while\nLLMs perform well on routine and moderately difficult tasks, they face\nsignificant challenges with Olympiad-level problems and complex\nuniversity-level questions. Our analysis shows a narrowing performance gap\nbetween open-source and closed-source models, yet substantial challenges\nremain, particularly with the most demanding problems. This study highlights\nthe ongoing need for research to enhance the mathematical reasoning of LLMs.\nThe dataset, results, and code are publicly available.\n","authors":["Meng Fang","Xiangpeng Wan","Fei Lu","Fei Xing","Kai Zou"],"pdf_url":"https://arxiv.org/pdf/2406.18321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18313v1","updated":"2024-06-26T12:54:19Z","published":"2024-06-26T12:54:19Z","title":"Advancing Airport Tower Command Recognition: Integrating\n  Squeeze-and-Excitation and Broadcasted Residual Learning","summary":"  Accurate recognition of aviation commands is vital for flight safety and\nefficiency, as pilots must follow air traffic control instructions precisely.\nThis paper addresses challenges in speech command recognition, such as noisy\nenvironments and limited computational resources, by advancing keyword spotting\ntechnology. We create a dataset of standardized airport tower commands,\nincluding routine and emergency instructions. We enhance broadcasted residual\nlearning with squeeze-and-excitation and time-frame frequency-wise\nsqueeze-and-excitation techniques, resulting in our BC-SENet model. This model\nfocuses on crucial information with fewer parameters. Our tests on five keyword\nspotting models, including BC-SENet, demonstrate superior accuracy and\nefficiency. These findings highlight the effectiveness of our model\nadvancements in improving speech command recognition for aviation safety and\nefficiency in noisy, high-stakes environments. Additionally, BC-SENet shows\ncomparable performance on the common Google Speech Command dataset.\n","authors":["Yuanxi Lin","Tonglin Zhou","Yang Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.18313v1.pdf","comment":"Accepted by IALP 2024"},{"id":"http://arxiv.org/abs/2406.18312v1","updated":"2024-06-26T12:51:37Z","published":"2024-06-26T12:51:37Z","title":"AI-native Memory: A Pathway from LLMs Towards AGI","summary":"  Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.\n","authors":["Jingbo Shang","Zai Zheng","Xiang Ying","Felix Tao","Mindverse Team"],"pdf_url":"https://arxiv.org/pdf/2406.18312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18305v1","updated":"2024-06-26T12:45:43Z","published":"2024-06-26T12:45:43Z","title":"S3: A Simple Strong Sample-effective Multimodal Dialog System","summary":"  In this work, we present a conceptually simple yet powerful baseline for the\nmultimodal dialog task, an S3 model, that achieves near state-of-the-art\nresults on two compelling leaderboards: MMMU and AI Journey Contest 2023. The\nsystem is based on a pre-trained large language model, pre-trained modality\nencoders for image and audio, and a trainable modality projector. The proposed\neffective data mixture for training such an architecture demonstrates that a\nmultimodal model based on a strong language model and trained on a small amount\nof multimodal data can perform efficiently in the task of multimodal dialog.\n","authors":["Elisei Rykov","Egor Malkershin","Alexander Panchenko"],"pdf_url":"https://arxiv.org/pdf/2406.18305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01179v2","updated":"2024-06-26T12:43:56Z","published":"2024-06-03T10:21:48Z","title":"Are AI-Generated Text Detectors Robust to Adversarial Perturbations?","summary":"  The widespread use of large language models (LLMs) has sparked concerns about\nthe potential misuse of AI-generated text, as these models can produce content\nthat closely resembles human-generated text. Current detectors for AI-generated\ntext (AIGT) lack robustness against adversarial perturbations, with even minor\nchanges in characters or words causing a reversal in distinguishing between\nhuman-created and AI-generated text. This paper investigates the robustness of\nexisting AIGT detection methods and introduces a novel detector, the Siamese\nCalibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction\nnetwork to add and remove noise from text, extracting a semantic representation\nthat is robust to local perturbations. We also propose a siamese calibration\ntechnique to train the model to make equally confidence predictions under\ndifferent noise, which improves the model's robustness against adversarial\nperturbations. Experiments on four publicly available datasets show that the\nSCRN outperforms all baseline methods, achieving 6.5\\%-18.25\\% absolute\naccuracy improvement over the best baseline method under adversarial attacks.\nMoreover, it exhibits superior generalizability in cross-domain, cross-genre,\nand mixed-source scenarios. The code is available at\n\\url{https://github.com/CarlanLark/Robust-AIGC-Detector}.\n","authors":["Guanhua Huang","Yuchen Zhang","Zhe Li","Yongjian You","Mingze Wang","Zhouwang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.01179v2.pdf","comment":"Accepted to ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2406.18301v1","updated":"2024-06-26T12:35:12Z","published":"2024-06-26T12:35:12Z","title":"MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of\n  Transcribed Audio for Speech Recognition Research","summary":"  Recently, multilingual artificial intelligence assistants, exemplified by\nChatGPT, have gained immense popularity. As a crucial gateway to human-computer\ninteraction, multilingual automatic speech recognition (ASR) has also garnered\nsignificant attention, as evidenced by systems like Whisper. However, the\nproprietary nature of the training data has impeded researchers' efforts to\nstudy multilingual ASR. This paper introduces MSR-86K, an evolving, large-scale\nmultilingual corpus for speech recognition research. The corpus is derived from\npublicly accessible videos on YouTube, comprising 15 languages and a total of\n86,300 hours of transcribed ASR data. We also introduce how to use the MSR-86K\ncorpus and other open-source corpora to train a robust multilingual ASR model\nthat is competitive with Whisper. MSR-86K will be publicly released on\nHuggingFace, and we believe that such a large corpus will pave new avenues for\nresearch in multilingual ASR.\n","authors":["Song Li","Yongbin You","Xuezhi Wang","Zhengkun Tian","Ke Ding","Guanglu Wan"],"pdf_url":"https://arxiv.org/pdf/2406.18301v1.pdf","comment":"Accepted by InterSpeech 2024"},{"id":"http://arxiv.org/abs/2405.20204v2","updated":"2024-06-26T12:31:48Z","published":"2024-05-30T16:07:54Z","title":"Jina CLIP: Your CLIP Model Is Also Your Text Retriever","summary":"  Contrastive Language-Image Pretraining (CLIP) is widely used to train models\nto align images and texts in a common embedding space by mapping them to\nfixed-sized vectors. These models are key to multimodal information retrieval\nand related tasks. However, CLIP models generally underperform in text-only\ntasks compared to specialized text models. This creates inefficiencies for\ninformation retrieval systems that keep separate embeddings and models for\ntext-only and multimodal tasks. We propose a novel, multi-task contrastive\ntraining method to address this issue, which we use to train the jina-clip-v1\nmodel to achieve the state-of-the-art performance on both text-image and\ntext-text retrieval tasks.\n","authors":["Andreas Koukounas","Georgios Mastrapas","Michael Günther","Bo Wang","Scott Martens","Isabelle Mohr","Saba Sturua","Mohammad Kalim Akram","Joan Fontanals Martínez","Saahil Ognawala","Susana Guzman","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2405.20204v2.pdf","comment":"4 pages, MFM-EAI@ICML2024"},{"id":"http://arxiv.org/abs/2406.18297v1","updated":"2024-06-26T12:31:31Z","published":"2024-06-26T12:31:31Z","title":"FactFinders at CheckThat! 2024: Refining Check-worthy Statement\n  Detection with LLMs through Data Pruning","summary":"  The rapid dissemination of information through social media and the Internet\nhas posed a significant challenge for fact-checking, among others in\nidentifying check-worthy claims that fact-checkers should pay attention to,\ni.e. filtering claims needing fact-checking from a large pool of sentences.\nThis challenge has stressed the need to focus on determining the priority of\nclaims, specifically which claims are worth to be fact-checked. Despite\nadvancements in this area in recent years, the application of large language\nmodels (LLMs), such as GPT, has only recently drawn attention in studies.\nHowever, many open-source LLMs remain underexplored. Therefore, this study\ninvestigates the application of eight prominent open-source LLMs with\nfine-tuning and prompt engineering to identify check-worthy statements from\npolitical transcriptions. Further, we propose a two-step data pruning approach\nto automatically identify high-quality training data instances for effective\nlearning. The efficiency of our approach is demonstrated through evaluations on\nthe English language dataset as part of the check-worthiness estimation task of\nCheckThat! 2024. Further, the experiments conducted with data pruning\ndemonstrate that competitive performance can be achieved with only about 44\\%\nof the training data. Our team ranked first in the check-worthiness estimation\ntask in the English language.\n","authors":["Yufeng Li","Rrubaa Panchendrarajan","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2406.18297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18294v1","updated":"2024-06-26T12:26:16Z","published":"2024-06-26T12:26:16Z","title":"Hierarchical Context Pruning: Optimizing Real-World Code Completion with\n  Repository-Level Pretrained Code LLMs","summary":"  Some recently developed code large language models (Code LLMs) have been\npre-trained on repository-level code data (Repo-Code LLMs), enabling these\nmodels to recognize repository structures and utilize cross-file information\nfor code completion. However, in real-world development scenarios, simply\nconcatenating the entire code repository often exceeds the context window\nlimits of these Repo-Code LLMs, leading to significant performance degradation.\nIn this study, we conducted extensive preliminary experiments and analyses on\nsix Repo-Code LLMs. The results indicate that maintaining the topological\ndependencies of files and increasing the code file content in the completion\nprompts can improve completion accuracy; pruning the specific implementations\nof functions in all dependent files does not significantly reduce the accuracy\nof completions. Based on these findings, we proposed a strategy named\nHierarchical Context Pruning (HCP) to construct completion prompts with high\ninformational code content. The HCP models the code repository at the function\nlevel, maintaining the topological dependencies between code files while\nremoving a large amount of irrelevant code content, significantly reduces the\ninput length for repository-level code completion. We applied the HCP strategy\nin experiments with six Repo-Code LLMs, and the results demonstrate that our\nproposed method can significantly enhance completion accuracy while\nsubstantially reducing the length of input. Our code and data are available at\nhttps://github.com/Hambaobao/HCP-Coder.\n","authors":["Lei Zhang","Yunshui Li","Jiaming Li","Xiaobo Xia","Jiaxi Yang","Run Luo","Minzheng Wang","Longze Chen","Junhao Liu","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2406.18294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18276v1","updated":"2024-06-26T12:00:10Z","published":"2024-06-26T12:00:10Z","title":"Sanskrit Knowledge-based Systems: Annotation and Computational Tools","summary":"  We address the challenges and opportunities in the development of knowledge\nsystems for Sanskrit, with a focus on question answering. By proposing a\nframework for the automated construction of knowledge graphs, introducing\nannotation tools for ontology-driven and general-purpose tasks, and offering a\ndiverse collection of web-interfaces, tools, and software libraries, we have\nmade significant contributions to the field of computational Sanskrit. These\ncontributions not only enhance the accessibility and accuracy of Sanskrit text\nanalysis but also pave the way for further advancements in knowledge\nrepresentation and language processing. Ultimately, this research contributes\nto the preservation, understanding, and utilization of the rich linguistic\ninformation embodied in Sanskrit texts.\n","authors":["Hrishikesh Terdalkar"],"pdf_url":"https://arxiv.org/pdf/2406.18276v1.pdf","comment":"PhD Thesis. 204 pages, 6 publications"},{"id":"http://arxiv.org/abs/2401.09395v3","updated":"2024-06-26T11:43:39Z","published":"2024-01-17T18:13:07Z","title":"Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating\n  LLMs' Mathematical and Coding Competency through Ontology-guided\n  Interventions","summary":"  Recent advancements in Large Language Models (LLMs) have showcased striking\nresults on existing logical reasoning benchmarks, with some models even\nsurpassing human performance. However, the true depth of their competencies and\nrobustness in reasoning tasks remains an open question. To this end, in this\npaper, we focus on two popular reasoning tasks: arithmetic reasoning and code\ngeneration. Particularly, we introduce: (i) a general ontology of perturbations\nfor maths and coding questions, (ii) a semi-automatic method to apply these\nperturbations, and (iii) two datasets, MORE and CORE, respectively, of\nperturbed maths and coding problems to probe the limits of LLM capabilities in\nnumeric reasoning and coding tasks. Through comprehensive evaluations of both\nclosed-source and open-source LLMs, we show a significant performance drop\nacross all the models against the perturbed questions, suggesting that the\ncurrent LLMs lack robust problem solving skills and structured reasoning\nabilities in many areas, as defined by our ontology. We open source the\ndatasets and source codes at: https://github.com/declare-lab/llm_robustness.\n","authors":["Pengfei Hong","Deepanway Ghosal","Navonil Majumder","Somak Aditya","Rada Mihalcea","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2401.09395v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05569v2","updated":"2024-06-26T11:42:10Z","published":"2024-04-08T14:43:13Z","title":"360$^\\circ$REA: Towards A Reusable Experience Accumulation with\n  360° Assessment for Multi-Agent System","summary":"  Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent\nframework inspired by corporate organizational practices. The framework employs\na novel 360$^\\circ$ performance assessment method for multi-perspective\nperformance evaluation with fine-grained assessment. To enhance the capability\nof agents in addressing complex tasks, we introduce dual-level experience pool\nfor agents to accumulate experience through fine-grained assessment. Extensive\nexperiments on complex task datasets demonstrate the effectiveness of\n360$^\\circ$REA.\n","authors":["Shen Gao","Hao Li","Chengrui Huang","Quan Tu","Zhiliang Tian","Minlie Huang","Shuo Shang"],"pdf_url":"https://arxiv.org/pdf/2404.05569v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18266v1","updated":"2024-06-26T11:39:51Z","published":"2024-06-26T11:39:51Z","title":"\"Vorbeşti Româneşte?\" A Recipe to Train Powerful Romanian LLMs\n  with English Instructions","summary":"  In recent years, Large Language Models (LLMs) have achieved almost human-like\nperformance on various tasks. While some LLMs have been trained on multilingual\ndata, most of the training data is in English; hence, their performance in\nEnglish greatly exceeds other languages. To our knowledge, we are the first to\ncollect and translate a large collection of texts, instructions, and benchmarks\nand train, evaluate, and release open-source LLMs tailored for Romanian. We\nevaluate our methods on four different categories, including academic\nbenchmarks, MT-Bench (manually translated), and a professionally built\nhistorical, cultural, and social benchmark adapted to Romanian. We argue for\nthe usefulness and high performance of RoLLMs by obtaining state-of-the-art\nresults across the board. We publicly release all resources (i.e., data,\ntraining and evaluation code, models) to support and encourage research on\nRomanian LLMs while concurrently creating a generalizable recipe, adequate for\nother low or less-resourced languages.\n","authors":["Mihai Masala","Denis C. Ilie-Ablachim","Alexandru Dima","Dragos Corlatescu","Miruna Zavelca","Ovio Olaru","Simina Terian-Dan","Andrei Terian-Dan","Marius Leordeanu","Horia Velicu","Marius Popescu","Mihai Dascalu","Traian Rebedea"],"pdf_url":"https://arxiv.org/pdf/2406.18266v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.07703"},{"id":"http://arxiv.org/abs/2406.18259v1","updated":"2024-06-26T11:11:47Z","published":"2024-06-26T11:11:47Z","title":"Detecting Machine-Generated Texts: Not Just \"AI vs Humans\" and\n  Explainability is Complicated","summary":"  As LLMs rapidly advance, increasing concerns arise regarding risks about\nactual authorship of texts we see online and in real world. The task of\ndistinguishing LLM-authored texts is complicated by the nuanced and overlapping\nbehaviors of both machines and humans. In this paper, we challenge the current\npractice of considering LLM-generated text detection a binary classification\ntask of differentiating human from AI. Instead, we introduce a novel ternary\ntext classification scheme, adding an \"undecided\" category for texts that could\nbe attributed to either source, and we show that this new category is crucial\nto understand how to make the detection result more explainable to lay users.\nThis research shifts the paradigm from merely classifying to explaining\nmachine-generated texts, emphasizing need for detectors to provide clear and\nunderstandable explanations to users. Our study involves creating four new\ndatasets comprised of texts from various LLMs and human authors. Based on new\ndatasets, we performed binary classification tests to ascertain the most\neffective SOTA detection methods and identified SOTA LLMs capable of producing\nharder-to-detect texts. We constructed a new dataset of texts generated by two\ntop-performing LLMs and human authors, and asked three human annotators to\nproduce ternary labels with explanation notes. This dataset was used to\ninvestigate how three top-performing SOTA detectors behave in new ternary\nclassification context. Our results highlight why \"undecided\" category is much\nneeded from the viewpoint of explainability. Additionally, we conducted an\nanalysis of explainability of the three best-performing detectors and the\nexplanation notes of the human annotators, revealing insights about the\ncomplexity of explainable detection of machine-generated texts. Finally, we\npropose guidelines for developing future detection systems with improved\nexplanatory power.\n","authors":["Jiazhou Ji","Ruizhe Li","Shujun Li","Jie Guo","Weidong Qiu","Zheng Huang","Chiyu Chen","Xiaoyu Jiang","Xinru Lu"],"pdf_url":"https://arxiv.org/pdf/2406.18259v1.pdf","comment":"19 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.18256v1","updated":"2024-06-26T11:08:17Z","published":"2024-06-26T11:08:17Z","title":"LLaMIPa: An Incremental Discourse Parser","summary":"  This paper provides the first discourse parsing experiments with a large\nlanguage model (LLM) finetuned on corpora annotated in the style of SDRT\n(Asher, 1993; Asher and Lascarides, 2003). The result is a discourse parser,\nLLaMIPa (LLaMA Incremental Parser), which is able to more fully exploit\ndiscourse context, leading to substantial performance gains over approaches\nthat use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it is able to process\ndiscourse data incrementally, which is essential for the eventual use of\ndiscourse information in downstream tasks.\n","authors":["Kate Thompson","Akshay Chaturvedi","Julie Hunter","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18256v1.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.17639v2","updated":"2024-06-26T10:58:48Z","published":"2024-06-25T15:24:02Z","title":"Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP","summary":"  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering two\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? We design AlignCLIP, in\norder to answer these questions and show that answers to both questions are\npositive. Through extensive experiments, we show that AlignCLIP achieves\nnoticeable enhancements in the cross-modal alignment of the embeddings, and\nthereby, reduces the modality gap, while maintaining the performance across\nseveral downstream evaluations, such as zero-shot image classification,\nzero-shot multi-modal retrieval and zero-shot semantic text similarity.\n","authors":["Sedigheh Eslami","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2406.17639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18245v1","updated":"2024-06-26T10:48:14Z","published":"2024-06-26T10:48:14Z","title":"Weak Reward Model Transforms Generative Models into Robust Causal Event\n  Extraction Systems","summary":"  The inherent ambiguity of cause and effect boundaries poses a challenge in\nevaluating causal event extraction tasks. Traditional metrics like Exact Match\nand BertScore poorly reflect model performance, so we trained evaluation models\nto approximate human evaluation, achieving high agreement. We used them to\nperform Reinforcement Learning with extraction models to align them with human\npreference, prioritising semantic understanding. We successfully explored our\napproach through multiple datasets, including transferring an evaluator trained\non one dataset to another as a way to decrease the reliance on human-annotated\ndata. In that vein, we also propose a weak-to-strong supervision method that\nuses a fraction of the annotated data to train an evaluation model while still\nachieving high performance in training an RL model. Our code is available at\n\\url{https://github.com/oyarsa/event_extraction/tree/causal-event-extraction}.\n","authors":["Italo Luis da Silva","Hanqi Yan","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2406.18245v1.pdf","comment":"13 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.18239v1","updated":"2024-06-26T10:44:02Z","published":"2024-06-26T10:44:02Z","title":"Zero-shot prompt-based classification: topic labeling in times of\n  foundation models in German Tweets","summary":"  Filtering and annotating textual data are routine tasks in many areas, like\nsocial media or news analytics. Automating these tasks allows to scale the\nanalyses wrt. speed and breadth of content covered and decreases the manual\neffort required. Due to technical advancements in Natural Language Processing,\nspecifically the success of large foundation models, a new tool for automating\nsuch annotation processes by using a text-to-text interface given written\nguidelines without providing training samples has become available.\n  In this work, we assess these advancements in-the-wild by empirically testing\nthem in an annotation task on German Twitter data about social and political\nEuropean crises. We compare the prompt-based results with our human annotation\nand preceding classification approaches, including Naive Bayes and a BERT-based\nfine-tuning/domain adaptation pipeline. Our results show that the prompt-based\napproach - despite being limited by local computation resources during the\nmodel selection - is comparable with the fine-tuned BERT but without any\nannotated training data. Our findings emphasize the ongoing paradigm shift in\nthe NLP landscape, i.e., the unification of downstream tasks and elimination of\nthe need for pre-labeled training data.\n","authors":["Simon Münker","Kai Kugler","Achim Rettinger"],"pdf_url":"https://arxiv.org/pdf/2406.18239v1.pdf","comment":"10 pages, 2 tables, 1 figure"},{"id":"http://arxiv.org/abs/2406.18227v1","updated":"2024-06-26T10:24:00Z","published":"2024-06-26T10:24:00Z","title":"GUIDE: A Guideline-Guided Dataset for Instructional Video Comprehension","summary":"  There are substantial instructional videos on the Internet, which provide us\ntutorials for completing various tasks. Existing instructional video datasets\nonly focus on specific steps at the video level, lacking experiential\nguidelines at the task level, which can lead to beginners struggling to learn\nnew tasks due to the lack of relevant experience. Moreover, the specific steps\nwithout guidelines are trivial and unsystematic, making it difficult to provide\na clear tutorial. To address these problems, we present the GUIDE\n(Guideline-Guided) dataset, which contains 3.5K videos of 560 instructional\ntasks in 8 domains related to our daily life. Specifically, we annotate each\ninstructional task with a guideline, representing a common pattern shared by\nall task-related videos. On this basis, we annotate systematic specific steps,\nincluding their associated guideline steps, specific step descriptions and\ntimestamps. Our proposed benchmark consists of three sub-tasks to evaluate\ncomprehension ability of models: (1) Step Captioning: models have to generate\ncaptions for specific steps from videos. (2) Guideline Summarization: models\nhave to mine the common pattern in task-related videos and summarize a\nguideline from them. (3) Guideline-Guided Captioning: models have to generate\ncaptions for specific steps under the guide of guideline. We evaluate plenty of\nfoundation models with GUIDE and perform in-depth analysis. Given the diversity\nand practicality of GUIDE, we believe that it can be used as a better benchmark\nfor instructional video comprehension.\n","authors":["Jiafeng Liang","Shixin Jiang","Zekun Wang","Haojie Pan","Zerui Chen","Zheng Chu","Ming Liu","Ruiji Fu","Zhongyuan Wang","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.18227v1.pdf","comment":"IJCAI 2024"},{"id":"http://arxiv.org/abs/2406.18221v1","updated":"2024-06-26T10:08:47Z","published":"2024-06-26T10:08:47Z","title":"Enhancing Data Privacy in Large Language Models through Private\n  Association Editing","summary":"  Large Language Models (LLMs) are powerful tools with extensive applications,\nbut their tendency to memorize private information raises significant concerns\nas private data leakage can easily happen. In this paper, we introduce Private\nAssociation Editing (PAE), a novel defense approach for private data leakage.\nPAE is designed to effectively remove Personally Identifiable Information (PII)\nwithout retraining the model. Our approach consists of a four-step procedure:\ndetecting memorized PII, applying PAE cards to mitigate memorization of private\ndata, verifying resilience to targeted data extraction (TDE) attacks, and\nensuring consistency in the post-edit LLMs. The versatility and efficiency of\nPAE, which allows for batch modifications, significantly enhance data privacy\nin LLMs. Experimental results demonstrate the effectiveness of PAE in\nmitigating private data leakage. We believe PAE will serve as a critical tool\nin the ongoing effort to protect data privacy in LLMs, encouraging the\ndevelopment of safer models for real-world applications.\n","authors":["Davide Venditti","Elena Sofia Ruzzetti","Giancarlo A. Xompero","Cristina Giannone","Andrea Favalli","Raniero Romagnoli","Fabio Massimo Zanzotto"],"pdf_url":"https://arxiv.org/pdf/2406.18221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18219v1","updated":"2024-06-26T10:07:57Z","published":"2024-06-26T10:07:57Z","title":"A Closer Look into Mixture-of-Experts in Large Language Models","summary":"  Mixture-of-experts (MoE) is gaining increasing attention due to its unique\nproperties and remarkable performance, especially for language tasks. By\nsparsely activating a subset of parameters for each token, MoE architecture\ncould increase the model size without sacrificing computational efficiency,\nachieving a better trade-off between performance and training costs. However,\nthe underlying mechanism of MoE still lacks further exploration, and its\nmodularization degree remains questionable. In this paper, we make an initial\nattempt to understand the inner workings of MoE-based large language models.\nConcretely, we comprehensively study the parametric and behavioral features of\nthree recent MoE-based models and reveal some intriguing observations,\nincluding (1) Neurons act like fine-grained experts. (2) The router of MoE\nusually selects experts with larger output norms. (3) The expert diversity\nincreases as the layer increases, while the last layer is an outlier. Based on\nthe observations, we also provide suggestions for a broad spectrum of MoE\npractitioners, such as router design and expert allocation. We hope this work\ncould shed light on future research on the MoE framework and other modular\narchitectures. Code is available at\nhttps://github.com/kamanphoebe/Look-into-MoEs.\n","authors":["Ka Man Lo","Zeyu Huang","Zihan Qiu","Zili Wang","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2406.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19333v2","updated":"2024-06-26T09:50:28Z","published":"2024-02-29T16:36:51Z","title":"Compact Speech Translation Models via Discrete Speech Units Pretraining","summary":"  We propose a pretraining method to use Self-Supervised Speech (SSS) model to\ncreating more compact Speech-to-text Translation. In contrast to using the SSS\nmodel for initialization, our method is more suitable to memory constrained\nscenario such as on-device deployment. Our method is based on Discrete Speech\nUnits (DSU) extracted from the SSS model. In the first step, our method\npretrains two smaller encoder-decoder models on 1) Filterbank-to-DSU\n(Fbk-to-DSU) and 2) DSU-to-Translation (DSU-to-Trl) data respectively. The DSU\nthus become the distillation inputs of the smaller models. Subsequently, the\nencoder from the Fbk-to-DSU model and the decoder from the DSU-to-Trl model are\ntaken to initialise the compact model. Finally, the compact model is finetuned\non the paired Fbk-Trl data. In addition to being compact, our method requires\nno transcripts, making it applicable to low-resource settings. It also avoids\nspeech discretization in inference and is more robust to the DSU tokenization.\nEvaluation on CoVoST-2 (X-En) shows that our method has consistent improvement\nover the baseline in three metrics while being compact i.e., only half the SSS\nmodel size.\n","authors":["Tsz Kin Lam","Alexandra Birch","Barry Haddow"],"pdf_url":"https://arxiv.org/pdf/2402.19333v2.pdf","comment":"11 pages, accepted at IWSLT 2024"},{"id":"http://arxiv.org/abs/2406.01171v2","updated":"2024-06-26T09:37:48Z","published":"2024-06-03T10:08:23Z","title":"Two Tales of Persona in LLMs: A Survey of Role-Playing and\n  Personalization","summary":"  The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey\n","authors":["Yu-Min Tseng","Yu-Chao Huang","Teng-Yun Hsiao","Wei-Lin Chen","Chao-Wei Huang","Yu Meng","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2406.01171v2.pdf","comment":"8-page version"},{"id":"http://arxiv.org/abs/2406.18200v1","updated":"2024-06-26T09:33:41Z","published":"2024-06-26T09:33:41Z","title":"SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative\n  Decoding","summary":"  Large Language Models (LLMs) demonstrate remarkable emergent abilities across\nvarious tasks, yet fall short of complex reasoning and planning tasks. The\ntree-search-based reasoning methods address this by surpassing the capabilities\nof chain-of-thought prompting, encouraging exploration of intermediate steps.\nHowever, such methods introduce significant inference latency due to the\nsystematic exploration and evaluation of multiple thought paths. This paper\nintroduces SeeD, a novel and efficient inference framework to optimize runtime\nspeed and GPU memory management concurrently. By employing a scheduled\nspeculative execution, SeeD efficiently handles multiple iterations for the\nthought generation and the state evaluation, leveraging a rounds-scheduled\nstrategy to manage draft model dispatching. Extensive experimental evaluations\non three reasoning datasets demonstrate superior speedup performance of SeeD,\nproviding a viable path for batched inference in training-free speculative\ndecoding.\n","authors":["Zhenglin Wang","Jialong Wu","Yilong Lai","Congzhi Zhang","Deyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.18200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17709v2","updated":"2024-06-26T09:25:07Z","published":"2024-02-27T17:41:58Z","title":"Case-Based or Rule-Based: How Do Transformers Do the Math?","summary":"  Despite the impressive performance in a variety of complex tasks, modern\nlarge language models (LLMs) still have trouble dealing with some math problems\nthat are simple and intuitive for humans, such as addition. While we can easily\nlearn basic rules of addition and apply them to new problems of any length,\nLLMs struggle to do the same. Instead, they may rely on similar cases seen in\nthe training corpus for help. We define these two different reasoning\nmechanisms as \"rule-based reasoning\" and \"case-based reasoning\". Since\nrule-based reasoning is essential for acquiring systematic generalization\nability, we aim to explore exactly whether transformers use rule-based or\ncase-based reasoning for math problems. Through carefully designed intervention\nexperiments on five math tasks, we confirm that transformers are performing\ncase-based reasoning, no matter whether scratchpad is used, which aligns with\nthe previous observations that transformers use subgraph matching/shortcut\nlearning to reason. To mitigate such problems, we propose a Rule-Following\nFine-Tuning (RFFT) technique to teach transformers to perform rule-based\nreasoning. Specifically, we provide explicit rules in the input and then\ninstruct transformers to recite and follow the rules step by step. Through\nRFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to\ngeneralize to up to 12-digit addition with over 95% accuracy, which is over 40%\nhigher than scratchpad. The significant improvement demonstrates that teaching\nLLMs to use rules explicitly helps them learn rule-based reasoning and\ngeneralize better in length.\n","authors":["Yi Hu","Xiaojuan Tang","Haotong Yang","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.17709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18192v1","updated":"2024-06-26T09:16:08Z","published":"2024-06-26T09:16:08Z","title":"Methodology of Adapting Large English Language Models for Specific\n  Cultural Contexts","summary":"  The rapid growth of large language models(LLMs) has emerged as a prominent\ntrend in the field of artificial intelligence. However, current\nstate-of-the-art LLMs are predominantly based on English. They encounter\nlimitations when directly applied to tasks in specific cultural domains, due to\ndeficiencies in domain-specific knowledge and misunderstandings caused by\ndifferences in cultural values. To address this challenge, our paper proposes a\nrapid adaptation method for large models in specific cultural contexts, which\nleverages instruction-tuning based on specific cultural knowledge and safety\nvalues data. Taking Chinese as the specific cultural context and utilizing the\nLLaMA3-8B as the experimental English LLM, the evaluation results demonstrate\nthat the adapted LLM significantly enhances its capabilities in domain-specific\nknowledge and adaptability to safety values, while maintaining its original\nexpertise advantages.\n","authors":["Wenjing Zhang","Siqi Xiao","Xuejiao Lei","Ning Wang","Huazheng Zhang","Meijuan An","Bikun Yang","Zhaoxiang Liu","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2406.18192v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.18187v1","updated":"2024-06-26T09:03:52Z","published":"2024-06-26T09:03:52Z","title":"Selective Prompting Tuning for Personalized Conversations with LLMs","summary":"  In conversational AI, personalizing dialogues with persona profiles and\ncontextual understanding is essential. Despite large language models' (LLMs)\nimproved response coherence, effective persona integration remains a challenge.\nIn this work, we first study two common approaches for personalizing LLMs:\ntextual prompting and direct fine-tuning. We observed that textual prompting\noften struggles to yield responses that are similar to the ground truths in\ndatasets, while direct fine-tuning tends to produce repetitive or overly\ngeneric replies. To alleviate those issues, we propose \\textbf{S}elective\n\\textbf{P}rompt \\textbf{T}uning (SPT), which softly prompts LLMs for\npersonalized conversations in a selective way. Concretely, SPT initializes a\nset of soft prompts and uses a trainable dense retriever to adaptively select\nsuitable soft prompts for LLMs according to different input contexts, where the\nprompt retriever is dynamically updated through feedback from the LLMs.\nAdditionally, we propose context-prompt contrastive learning and prompt fusion\nlearning to encourage the SPT to enhance the diversity of personalized\nconversations. Experiments on the CONVAI2 dataset demonstrate that SPT\nsignificantly enhances response diversity by up to 90\\%, along with\nimprovements in other critical performance indicators. Those results highlight\nthe efficacy of SPT in fostering engaging and personalized dialogue generation.\nThe SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available\nfor further exploration.\n","authors":["Qiushi Huang","Xubo Liu","Tom Ko","Bo Wu","Wenwu Wang","Yu Zhang","Lilian Tang"],"pdf_url":"https://arxiv.org/pdf/2406.18187v1.pdf","comment":"Accepted to ACL 2024 findings"},{"id":"http://arxiv.org/abs/2406.18173v1","updated":"2024-06-26T08:44:36Z","published":"2024-06-26T08:44:36Z","title":"UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs","summary":"  Managing long texts is challenging for large language models (LLMs) due to\nlimited context window sizes. This study introduces UIO-LLMs, an unbiased\nincremental optimization approach for memory-enhanced transformers under\nlong-context settings. We initially conceptualize the process as a streamlined\nencoder-decoder framework where the weights-shared encoder and decoder\nrespectively encapsulate a context segment into memories and leverage these\nmemories to predict outputs of the subsequent segment. Subsequently, by\ntreating our memory-enhanced transformers as fully-connected recurrent neural\nnetworks (RNNs), we refine the training process using the Truncated\nBackpropagation Through Time (TBPTT) algorithm, which incorporates innovative\nincremental optimization techniques. These techniques not only diminish time\ncomplexity but also address the bias in gradient computation through an\nunbiased optimization process. UIO-LLMs successfully handle long context, such\nas extending the context window of Llama2-7b-chat from 4K to 100K tokens with\nminimal 2% additional parameters, while keeping the inference cost nearly\nlinear as context length increases.\n","authors":["Wenhao Li","Mingbao Lin","Yunshan Zhong","Shuicheng Yan","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.18173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14159v2","updated":"2024-06-26T08:41:06Z","published":"2024-05-23T04:12:49Z","title":"Super Tiny Language Models","summary":"  The rapid advancement of large language models (LLMs) has led to significant\nimprovements in natural language processing but also poses challenges due to\ntheir high computational and energy demands. This paper introduces a series of\nresearch efforts focused on Super Tiny Language Models (STLMs), which aim to\ndeliver high performance with significantly reduced parameter counts. We\nexplore innovative techniques such as byte-level tokenization with a pooling\nmechanism, weight tying, and efficient training strategies. These methods aim\nto significantly reduce reduce the parameter count compared to traditional\nmodels -- in future works, we aim to build on these in a way that maintains and\nimproves upon the performance of base transformer models. This series of papers\nwill explore into various subproblems, including tokenizer-free models,\nself-play based training, and alternative training objectives. We will target\nmodels with 10M, 50M, and 100M parameters. Our ultimate goal is to make\nhigh-performance language models more accessible and practical for a wide range\nof applications.\n","authors":["Dylan Hillier","Leon Guertler","Cheston Tan","Palaash Agrawal","Chen Ruirui","Bobby Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.14159v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.18164v1","updated":"2024-06-26T08:24:44Z","published":"2024-06-26T08:24:44Z","title":"NeBuLa: A discourse aware Minecraft Builder","summary":"  When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We fine tune an LLM to\npredict actions based on prior context; our model, NeBuLa, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset.\n","authors":["Akshay Chaturvedi","Kate Thompson","Nicholas Asher"],"pdf_url":"https://arxiv.org/pdf/2406.18164v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.15021v2","updated":"2024-06-26T08:23:10Z","published":"2023-10-23T15:19:24Z","title":"Efficient Data Learning for Open Information Extraction with Pre-trained\n  Language Models","summary":"  Open Information Extraction (OpenIE) is a fundamental yet challenging task in\nNatural Language Processing, which involves extracting all triples (subject,\npredicate, object) from a given sentence. While labeling-based methods have\ntheir merits, generation-based techniques offer unique advantages, such as the\nability to generate tokens not present in the original sentence. However, these\ngeneration-based methods often require a significant amount of training data to\nlearn the task form of OpenIE and substantial training time to overcome slow\nmodel convergence due to the order penalty. In this paper, we introduce a novel\nframework, OK-IE, that ingeniously transforms the task form of OpenIE into the\npre-training task form of the T5 model, thereby reducing the need for extensive\ntraining data. Furthermore, we introduce an innovative concept of Anchor to\ncontrol the sequence of model outputs, effectively eliminating the impact of\norder penalty on model convergence and significantly reducing training time.\nExperimental results indicate that, compared to previous SOTA methods, OK-IE\nrequires only 1/100 of the training data (900 instances) and 1/120 of the\ntraining time (3 minutes) to achieve comparable results.\n","authors":["Zhiyuan Fan","Shizhu He"],"pdf_url":"https://arxiv.org/pdf/2310.15021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17415v2","updated":"2024-06-26T08:00:18Z","published":"2024-06-25T09:37:15Z","title":"Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing\n  LLMs Beyond Integer Bit-Levels","summary":"  We present a simple variable quantization approach that quantizes different\nlayers of a large language model (LLM) at different bit levels. Specifically,\nwe quantize the most important layers to higher bit precision and less\nimportant layers to lower bits to achieve floating point quantization levels.\nWe propose two effective strategies to measure the importance of layers within\nLLMs: the first measures the importance of a layer based on how different its\noutput embeddings are from the input embeddings (the higher the better); the\nsecond estimates the importance of a layer using the number of layer weights\nthat are much larger than average (the smaller the better). We show that\nquantizing different layers at varying bits according to our importance scores\nresults in minimal performance drop with a far more compressed model size.\nFinally, we present several practical key takeaways from our variable\nlayer-wise quantization experiments: (a) LLM performance under variable\nquantization remains close to the original model until 25-50% of layers are\nmoved in lower quantization using our proposed ordering but only until 5-10% if\nmoved using no specific ordering; (b) Quantizing LLMs to lower bits performs\nsubstantially better than pruning unless extreme quantization (2-bit) is used;\nand (c) Layer-wise quantization to lower bits works better in the case of\nlarger LLMs with more layers compared to smaller LLMs with fewer layers. The\ncode used to run the experiments is available at:\nhttps://github.com/RazvanDu/LayerwiseQuant.\n","authors":["Razvan-Gabriel Dumitru","Vikas Yadav","Rishabh Maheshwary","Paul-Ioan Clotan","Sathwik Tejaswi Madhusudhan","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2406.17415v2.pdf","comment":"submitted to EMNLP, 15 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.05935v2","updated":"2024-06-26T07:59:03Z","published":"2024-02-08T18:59:48Z","title":"SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models","summary":"  We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory\n","authors":["Dongyang Liu","Renrui Zhang","Longtian Qiu","Siyuan Huang","Weifeng Lin","Shitian Zhao","Shijie Geng","Ziyi Lin","Peng Jin","Kaipeng Zhang","Wenqi Shao","Chao Xu","Conghui He","Junjun He","Hao Shao","Pan Lu","Hongsheng Li","Yu Qiao","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2402.05935v2.pdf","comment":"Accepted by ICML 2024. Code and models are released at\n  https://github.com/Alpha-VLLM/LLaMA2-Accessory"},{"id":"http://arxiv.org/abs/2406.17542v2","updated":"2024-06-26T07:44:42Z","published":"2024-06-25T13:29:14Z","title":"CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained\n  Models using Greedy Coordinate Descent","summary":"  Large language models (LLMs) have recently demonstrated remarkable\nperformance across diverse language tasks. But their deployment is often\nconstrained by their substantial computational and storage requirements.\nQuantization has emerged as a key technique for addressing this challenge,\nenabling the compression of large models with minimal impact on performance.\nThe recent GPTQ algorithm, a post-training quantization (PTQ) method, has\nproven highly effective for compressing LLMs, sparking a wave of research that\nleverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the\nPTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ\nwith improved performance. CDQuant uses coordinate descent to minimize the\nlayer-wise reconstruction loss to achieve high-quality quantized weights. Our\nalgorithm is easy to implement and scales efficiently to models with hundreds\nof billions of parameters. Through extensive evaluation on the PaLM2 model\nfamily, we demonstrate that CDQuant consistently outperforms GPTQ across\ndiverse model sizes and quantization levels. In particular, for INT2\nquantization of PaLM2-Otter, CDQuant achieves a 10% reduction in perplexity\ncompared to GPTQ.\n","authors":["Pranav Ajit Nair","Arun Sai Suggala"],"pdf_url":"https://arxiv.org/pdf/2406.17542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18139v1","updated":"2024-06-26T07:44:24Z","published":"2024-06-26T07:44:24Z","title":"LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal\n  Long-Context Inference","summary":"  Long-context Multimodal Large Language Models (MLLMs) demand substantial\ncomputational resources for inference as the growth of their multimodal\nKey-Value (KV) cache, in response to increasing input lengths, challenges\nmemory and time efficiency. Unlike single-modality LLMs that manage only\ntextual contexts, the KV cache of long-context MLLMs includes representations\nfrom multiple images with temporal and spatial relationships and related\ntextual contexts. The predominance of image tokens means traditional\noptimizations for LLMs' KV caches are unsuitable for multimodal long-context\nsettings, and no prior works have addressed this challenge. In this work, we\nintroduce LOOK-M, a pioneering, fine-tuning-free approach that efficiently\nreduces the multimodal KV cache size while maintaining performance comparable\nto a full cache. We observe that during prompt prefill, the model prioritizes\nmore textual attention over image features, and based on the multimodal\ninteraction observation, a new proposed text-prior method is explored to\ncompress the KV cache. Furthermore, to mitigate the degradation of image\ncontextual information, we propose several compensatory strategies using KV\npairs merging. LOOK-M demonstrates that with a significant reduction in KV\nCache memory usage, such as reducing it by 80% in some cases, it not only\nachieves up to 1.5x faster decoding but also maintains or even enhances\nperformance across a variety of long context multimodal tasks.\n","authors":["Zhongwei Wan","Ziang Wu","Che Liu","Jinfa Huang","Zhihong Zhu","Peng Jin","Longyue Wang","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.18139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18702v2","updated":"2024-06-26T07:44:11Z","published":"2023-11-30T16:52:42Z","title":"CritiqueLLM: Towards an Informative Critique Generation Model for\n  Evaluation of Large Language Model Generation","summary":"  Since the natural language processing (NLP) community started to make large\nlanguage models (LLMs) act as a critic to evaluate the quality of generated\ntexts, most of the existing works train a critique generation model on the\nevaluation data labeled by GPT-4's direct prompting. We observe that these\nmodels lack the ability to generate informative critiques in both pointwise\ngrading and pairwise comparison especially without references. As a result,\ntheir generated critiques cannot provide fine-grained distinguishability on\ngenerated texts, causing unsatisfactory evaluation performance. In this paper,\nwe propose a simple yet effective method called Eval-Instruct, which can first\nacquire pointwise grading critiques with pseudo references and then revise\nthese critiques via multi-path prompting to obtain informative evaluation data\nin different tasks and settings, including pointwise grading and pairwise\ncomparison with / without references. After fine-tuning on these data, the\nresulting model CritiqueLLM is empirically shown to outperform ChatGPT and all\nthe open-source baselines and even achieve comparable evaluation performance to\nGPT-4 in system-level correlations of pointwise grading. We also demonstrate\nthat our generated critiques can act as scalable feedback to further improve\nthe generation quality of strong LLMs like ChatGPT.\n","authors":["Pei Ke","Bosi Wen","Zhuoer Feng","Xiao Liu","Xuanyu Lei","Jiale Cheng","Shengyuan Wang","Aohan Zeng","Yuxiao Dong","Hongning Wang","Jie Tang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2311.18702v2.pdf","comment":"Accepted by ACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2402.04678v3","updated":"2024-06-26T07:43:11Z","published":"2024-02-07T09:09:14Z","title":"FaithLM: Towards Faithful Explanations for Large Language Models","summary":"  Large Language Models (LLMs) have become proficient in addressing complex\ntasks by leveraging their extensive internal knowledge and reasoning\ncapabilities. However, the black-box nature of these models complicates the\ntask of explaining their decision-making processes. While recent advancements\ndemonstrate the potential of leveraging LLMs to self-explain their predictions\nthrough natural language (NL) explanations, their explanations may not\naccurately reflect the LLMs' decision-making process due to a lack of fidelity\noptimization on the derived explanations. Measuring the fidelity of NL\nexplanations is a challenging issue, as it is difficult to manipulate the input\ncontext to mask the semantics of these explanations. To this end, we introduce\nFaithLM to explain the decision of LLMs with NL explanations. Specifically,\nFaithLM designs a method for evaluating the fidelity of NL explanations by\nincorporating the contrary explanations to the query process. Moreover, FaithLM\nconducts an iterative process to improve the fidelity of derived explanations.\nExperiment results on three datasets from multiple domains demonstrate that\nFaithLM can significantly improve the fidelity of derived explanations, which\nalso provides a better alignment with the ground-truth explanations.\n","authors":["Yu-Neng Chuang","Guanchu Wang","Chia-Yuan Chang","Ruixiang Tang","Shaochen Zhong","Fan Yang","Mengnan Du","Xuanting Cai","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2402.04678v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18135v1","updated":"2024-06-26T07:39:20Z","published":"2024-06-26T07:39:20Z","title":"Automatic Speech Recognition for Hindi","summary":"  Automatic speech recognition (ASR) is a key area in computational\nlinguistics, focusing on developing technologies that enable computers to\nconvert spoken language into text. This field combines linguistics and machine\nlearning. ASR models, which map speech audio to transcripts through supervised\nlearning, require handling real and unrestricted text. Text-to-speech systems\ndirectly work with real text, while ASR systems rely on language models trained\non large text corpora. High-quality transcribed data is essential for training\npredictive models. The research involved two main components: developing a web\napplication and designing a web interface for speech recognition. The web\napplication, created with JavaScript and Node.js, manages large volumes of\naudio files and their transcriptions, facilitating collaborative human\ncorrection of ASR transcripts. It operates in real-time using a client-server\narchitecture. The web interface for speech recognition records 16 kHz mono\naudio from any device running the web app, performs voice activity detection\n(VAD), and sends the audio to the recognition engine. VAD detects human speech\npresence, aiding efficient speech processing and reducing unnecessary\nprocessing during non-speech intervals, thus saving computation and network\nbandwidth in VoIP applications. The final phase of the research tested a neural\nnetwork for accurately aligning the speech signal to hidden Markov model (HMM)\nstates. This included implementing a novel backpropagation method that utilizes\nprior statistics of node co-activations.\n","authors":["Anish Saha","A. G. Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2406.18135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18134v1","updated":"2024-06-26T07:38:24Z","published":"2024-06-26T07:38:24Z","title":"Assessing \"Implicit\" Retrieval Robustness of Large Language Models","summary":"  Retrieval-augmented generation has gained popularity as a framework to\nenhance large language models with external knowledge. However, its\neffectiveness hinges on the retrieval robustness of the model. If the model\nlacks retrieval robustness, its performance is constrained by the accuracy of\nthe retriever, resulting in significant compromises when the retrieved context\nis irrelevant. In this paper, we evaluate the \"implicit\" retrieval robustness\nof various large language models, instructing them to directly output the final\nanswer without explicitly judging the relevance of the retrieved context. Our\nfindings reveal that fine-tuning on a mix of gold and distracting context\nsignificantly enhances the model's robustness to retrieval inaccuracies, while\nstill maintaining its ability to extract correct answers when retrieval is\naccurate. This suggests that large language models can implicitly handle\nrelevant or irrelevant retrieved context by learning solely from the\nsupervision of the final answer in an end-to-end manner. Introducing an\nadditional process for explicit relevance judgment can be unnecessary and\ndisrupts the end-to-end approach.\n","authors":["Xiaoyu Shen","Rexhina Blloshmi","Dawei Zhu","Jiahuan Pei","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18133v1","updated":"2024-06-26T07:35:10Z","published":"2024-06-26T07:35:10Z","title":"ConvoCache: Smart Re-Use of Chatbot Responses","summary":"  We present ConvoCache, a conversational caching system that solves the\nproblem of slow and expensive generative AI models in spoken chatbots.\nConvoCache finds a semantically similar prompt in the past and reuses the\nresponse. In this paper we evaluate ConvoCache on the DailyDialog dataset. We\nfind that ConvoCache can apply a UniEval coherence threshold of 90% and respond\nto 89% of prompts using the cache with an average latency of 214ms, replacing\nLLM and voice synthesis that can take over 1s. To further reduce latency we\ntest prefetching and find limited usefulness. Prefetching with 80% of a request\nleads to a 63% hit rate, and a drop in overall coherence. ConvoCache can be\nused with any chatbot to reduce costs by reducing usage of generative AI by up\nto 89%.\n","authors":["Conor Atkins","Ian Wood","Mohamed Ali Kaafar","Hassan Asghar","Nardine Basta","Michal Kepkowski"],"pdf_url":"https://arxiv.org/pdf/2406.18133v1.pdf","comment":"Accepted to appear at Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.18125v1","updated":"2024-06-26T07:25:18Z","published":"2024-06-26T07:25:18Z","title":"ResumeAtlas: Revisiting Resume Classification with Large-Scale Datasets\n  and Large Language Models","summary":"  The increasing reliance on online recruitment platforms coupled with the\nadoption of AI technologies has highlighted the critical need for efficient\nresume classification methods. However, challenges such as small datasets, lack\nof standardized resume templates, and privacy concerns hinder the accuracy and\neffectiveness of existing classification models. In this work, we address these\nchallenges by presenting a comprehensive approach to resume classification. We\ncurated a large-scale dataset of 13,389 resumes from diverse sources and\nemployed Large Language Models (LLMs) such as BERT and Gemma1.1 2B for\nclassification. Our results demonstrate significant improvements over\ntraditional machine learning approaches, with our best model achieving a top-1\naccuracy of 92\\% and a top-5 accuracy of 97.5\\%. These findings underscore the\nimportance of dataset quality and advanced model architectures in enhancing the\naccuracy and robustness of resume classification systems, thus advancing the\nfield of online recruitment practices.\n","authors":["Ahmed Heakl","Youssef Mohamed","Noran Mohamed","Ali Sharkaway","Ahmed Zaky"],"pdf_url":"https://arxiv.org/pdf/2406.18125v1.pdf","comment":"8 pages, 6 figures, 1 table, 6th International Conference on AI in\n  Computational Linguistics"},{"id":"http://arxiv.org/abs/2406.18122v1","updated":"2024-06-26T07:21:02Z","published":"2024-06-26T07:21:02Z","title":"Poisoned LangChain: Jailbreak LLMs by LangChain","summary":"  With the development of natural language processing (NLP), large language\nmodels (LLMs) are becoming increasingly popular. LLMs are integrating more into\neveryday life, raising public concerns about their security vulnerabilities.\nConsequently, the security of large language models is becoming critically\nimportant. Currently, the techniques for attacking and defending against LLMs\nare continuously evolving. One significant method type of attack is the\njailbreak attack, which designed to evade model safety mechanisms and induce\nthe generation of inappropriate content. Existing jailbreak attacks primarily\nrely on crafting inducement prompts for direct jailbreaks, which are less\neffective against large models with robust filtering and high comprehension\nabilities. Given the increasing demand for real-time capabilities in large\nlanguage models, real-time updates and iterations of new knowledge have become\nessential. Retrieval-Augmented Generation (RAG), an advanced technique to\ncompensate for the model's lack of new knowledge, is gradually becoming\nmainstream. As RAG enables the model to utilize external knowledge bases, it\nprovides a new avenue for jailbreak attacks.\n  In this paper, we conduct the first work to propose the concept of indirect\njailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on\nthis, we further design a novel method of indirect jailbreak attack, termed\nPoisoned-LangChain (PLC), which leverages a poisoned external knowledge base to\ninteract with large language models, thereby causing the large models to\ngenerate malicious non-compliant dialogues.We tested this method on six\ndifferent large language models across three major categories of jailbreak\nissues. The experiments demonstrate that PLC successfully implemented indirect\njailbreak attacks under three different scenarios, achieving success rates of\n88.56%, 79.04%, and 82.69% respectively.\n","authors":["Ziqiu Wang","Jun Liu","Shengkai Zhang","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.18122v1.pdf","comment":"6 pages,2 figures,This paper is a submission to ACM TURC. It has been\n  accepted by the editor of the organizer"},{"id":"http://arxiv.org/abs/2406.18120v1","updated":"2024-06-26T07:19:51Z","published":"2024-06-26T07:19:51Z","title":"ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech\n  Recognition Using LLMs","summary":"  Motivated by the widespread increase in the phenomenon of code-switching\nbetween Egyptian Arabic and English in recent times, this paper explores the\nintricacies of machine translation (MT) and automatic speech recognition (ASR)\nsystems, focusing on translating code-switched Egyptian Arabic-English to\neither English or Egyptian Arabic. Our goal is to present the methodologies\nemployed in developing these systems, utilizing large language models such as\nLLama and Gemma. In the field of ASR, we explore the utilization of the Whisper\nmodel for code-switched Egyptian Arabic recognition, detailing our experimental\nprocedures including data preprocessing and training techniques. Through the\nimplementation of a consecutive speech-to-text translation system that\nintegrates ASR with MT, we aim to overcome challenges posed by limited\nresources and the unique characteristics of the Egyptian Arabic dialect.\nEvaluation against established metrics showcases promising results, with our\nmethodologies yielding a significant improvement of $56\\%$ in English\ntranslation over the state-of-the-art and $9.3\\%$ in Arabic translation. Since\ncode-switching is deeply inherent in spoken languages, it is crucial that ASR\nsystems can effectively handle this phenomenon. This capability is crucial for\nenabling seamless interaction in various domains, including business\nnegotiations, cultural exchanges, and academic discourse. Our models and code\nare available as open-source resources. Code:\n\\url{http://github.com/ahmedheakl/arazn-llm}}, Models:\n\\url{http://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e}.\n","authors":["Ahmed Heakl","Youssef Zaghloul","Mennatullah Ali","Rania Hossam","Walid Gomaa"],"pdf_url":"https://arxiv.org/pdf/2406.18120v1.pdf","comment":"9 pages, 4 figures, 5 tables, 6th International Conference on AI in\n  Computational Linguistics"},{"id":"http://arxiv.org/abs/2405.11966v4","updated":"2024-06-26T07:16:42Z","published":"2024-05-20T11:47:13Z","title":"Multiple-Choice Questions are Efficient and Robust LLM Evaluators","summary":"  We present GSM-MC, a multiple-choice (MC) dataset constructed by collecting\nanswers and incorrect predictions on GSM8K from 60 open-source models. Through\nextensive experiments, we show that LLMs' performance on the MC version of this\npopular benchmark is strongly correlated with their performance on the original\nversion and is quite robust to distractor choices and option orders, while the\nevaluation time is reduced by a factor of up to 30. Following similar\nprocedures, we introduce MATH-MC, constructed from MATH, and PythonIO, a new\nprogram reasoning MC dataset constructed from HumanEval and MBPP. Experimental\nresults indicate that LLMs' performance on these MC benchmarks leaves much room\nfor improvement. Our data and code are available at\nhttps://github.com/Geralt-Targaryen/MC-Evaluation.\n","authors":["Ziyin Zhang","Zhaokun Jiang","Lizhen Xu","Hongkun Hao","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2405.11966v4.pdf","comment":"data at https://github.com/Geralt-Targaryen/MC-Evaluation"},{"id":"http://arxiv.org/abs/2406.18118v1","updated":"2024-06-26T07:15:44Z","published":"2024-06-26T07:15:44Z","title":"SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance","summary":"  As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality.\n","authors":["Caishuang Huang","Wanxu Zhao","Rui Zheng","Huijie Lv","Shihan Dou","Sixian Li","Xiao Wang","Enyu Zhou","Junjie Ye","Yuming Yang","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.18118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07989v7","updated":"2024-06-26T07:11:00Z","published":"2023-11-14T08:34:26Z","title":"Unifying the Perspectives of NLP and Software Engineering: A Survey on\n  Language Models for Code","summary":"  In this work we systematically review the recent advancements in software\nengineering with language models, covering 70+ models, 40+ evaluation tasks,\n180+ datasets, and 900 related works. Unlike previous works, we integrate\nsoftware engineering (SE) with natural language processing (NLP) by discussing\nthe perspectives of both sides: SE applies language models for development\nautomation, while NLP adopts SE tasks for language model evaluation. We break\ndown code processing models into general language models represented by the GPT\nfamily and specialized models that are specifically pretrained on code, often\nwith tailored objectives. We discuss the relations and differences between\nthese models, and highlight the historical transition of code modeling from\nstatistical models and RNNs to pretrained Transformers and LLMs, which is\nexactly the same course that had been taken by NLP. We also go beyond\nprogramming and review LLMs' application in other software engineering\nactivities including requirement engineering, testing, deployment, and\noperations in an endeavor to provide a global view of NLP in SE, and identify\nkey challenges and potential future directions in this domain. We keep the\nsurvey open and updated on GitHub at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.\n","authors":["Ziyin Zhang","Chaoyu Chen","Bingchang Liu","Cong Liao","Zi Gong","Hang Yu","Jianguo Li","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2311.07989v7.pdf","comment":"Repo: https://github.com/codefuse-ai/Awesome-Code-LLM. 9 figures, 18\n  tables, and 902 references. Under review"},{"id":"http://arxiv.org/abs/2406.18116v1","updated":"2024-06-26T07:07:52Z","published":"2024-06-26T07:07:52Z","title":"BADGE: BADminton report Generation and Evaluation with LLM","summary":"  Badminton enjoys widespread popularity, and reports on matches generally\ninclude details such as player names, game scores, and ball types, providing\naudiences with a comprehensive view of the games. However, writing these\nreports can be a time-consuming task. This challenge led us to explore whether\na Large Language Model (LLM) could automate the generation and evaluation of\nbadminton reports. We introduce a novel framework named BADGE, designed for\nthis purpose using LLM. Our method consists of two main phases: Report\nGeneration and Report Evaluation. Initially, badminton-related data is\nprocessed by the LLM, which then generates a detailed report of the match. We\ntested different Input Data Types, In-Context Learning (ICL), and LLM, finding\nthat GPT-4 performs best when using CSV data type and the Chain of Thought\nprompting. Following report generation, the LLM evaluates and scores the\nreports to assess their quality. Our comparisons between the scores evaluated\nby GPT-4 and human judges show a tendency to prefer GPT-4 generated reports.\nSince the application of LLM in badminton reporting remains largely unexplored,\nour research serves as a foundational step for future advancements in this\narea. Moreover, our method can be extended to other sports games, thereby\nenhancing sports promotion. For more details, please refer to\nhttps://github.com/AndyChiangSH/BADGE.\n","authors":["Shang-Hsuan Chiang","Lin-Wei Chao","Kuang-Da Wang","Chih-Chuan Wang","Wen-Chih Peng"],"pdf_url":"https://arxiv.org/pdf/2406.18116v1.pdf","comment":"Accepted by IJCAI 2024 Workshop: The 2nd International Workshop on\n  Intelligent Technologies for Precision Sports Science (IT4PSS)"},{"id":"http://arxiv.org/abs/2402.13561v2","updated":"2024-06-26T07:05:21Z","published":"2024-02-21T06:34:46Z","title":"Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension\n  with Enhanced Visual Knowledge Alignment","summary":"  Evaluating and Rethinking the current landscape of Large Multimodal Models\n(LMMs), we observe that widely-used visual-language projection approaches\n(e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet\nignore the visual knowledge-dimension alignment, i.e., connecting visuals to\ntheir relevant knowledge. Visual knowledge plays a significant role in\nanalyzing, inferring, and interpreting information from visuals, helping\nimprove the accuracy of answers to knowledge-based visual questions. In this\npaper, we mainly explore improving LMMs with visual-language knowledge\nalignment, especially aimed at challenging knowledge-based visual question\nanswering (VQA). To this end, we present a Cognitive Visual-Language Mapper\n(CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a\nFine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning\nstage. Specifically, we design the VKA based on the interaction between a small\nlanguage model and a visual encoder, training it on collected image-knowledge\npairs to achieve visual knowledge acquisition and projection. FKA is employed\nto distill the fine-grained visual knowledge of an image and inject it into\nLarge Language Models (LLMs). We conduct extensive experiments on\nknowledge-based VQA benchmarks and experimental results show that CVLM\nsignificantly improves the performance of LMMs on knowledge-based VQA (average\ngain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA,\nrespectively. The codes are available at\nhttps://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper\n","authors":["Yunxin Li","Xinyu Chen","Baotian Hu","Haoyuan Shi","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.13561v2.pdf","comment":"12 pages,4 figures; Accepted by ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.10663v3","updated":"2024-06-26T06:54:35Z","published":"2024-02-16T13:13:18Z","title":"Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL","summary":"  Currently, the in-context learning method based on large language models\n(LLMs) has become the mainstream of text-to-SQL research. Previous works have\ndiscussed how to select demonstrations related to the user question from a\nhuman-labeled demonstration pool. However, human labeling suffers from the\nlimitations of insufficient diversity and high labeling overhead. Therefore, in\nthis paper, we discuss how to measure and improve the diversity of the\ndemonstrations for text-to-SQL. We present a metric to measure the diversity of\nthe demonstrations and analyze the insufficient of the existing labeled data by\nexperiments. Based on the above discovery, we propose fusing iteratively for\ndemonstrations (Fused) to build a high-diversity demonstration pool through\nhuman-free multiple-iteration synthesis, improving diversity and lowering label\ncost. Our method achieves an average improvement of 3.2% and 5.0% with and\nwithout human labeling on several mainstream datasets, which proves the\neffectiveness of Fused.\n","authors":["Dingzirui Wang","Longxu Dou","Xuanliang Zhang","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2402.10663v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18108v1","updated":"2024-06-26T06:48:11Z","published":"2024-06-26T06:48:11Z","title":"Token-Weighted RNN-T for Learning from Flawed Data","summary":"  ASR models are commonly trained with the cross-entropy criterion to increase\nthe probability of a target token sequence. While optimizing the probability of\nall tokens in the target sequence is sensible, one may want to de-emphasize\ntokens that reflect transcription errors. In this work, we propose a novel\ntoken-weighted RNN-T criterion that augments the RNN-T objective with\ntoken-specific weights. The new objective is used for mitigating accuracy loss\nfrom transcriptions errors in the training data, which naturally appear in two\nsettings: pseudo-labeling and human annotation errors. Experiments results show\nthat using our method for semi-supervised learning with pseudo-labels leads to\na consistent accuracy improvement, up to 38% relative. We also analyze the\naccuracy degradation resulting from different levels of WER in the reference\ntranscription, and show that token-weighted RNN-T is suitable for overcoming\nthis degradation, recovering 64%-99% of the accuracy loss.\n","authors":["Gil Keren","Wei Zhou","Ozlem Kalinli"],"pdf_url":"https://arxiv.org/pdf/2406.18108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02856v4","updated":"2024-06-26T06:28:45Z","published":"2024-06-05T02:12:06Z","title":"Xmodel-LM Technical Report","summary":"  We introduce Xmodel-LM, a compact and efficient 1.1B language model\npre-trained on around 2 trillion tokens. Trained on our self-built dataset\n(Xdata), which balances Chinese and English corpora based on downstream task\noptimization, Xmodel-LM exhibits remarkable performance despite its smaller\nsize. It notably surpasses existing open-source language models of similar\nscale. Our model checkpoints and code are publicly accessible on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM.\n","authors":["Yichuan Wang","Yang Liu","Yu Yan","Qun Wang","Xucheng Huang","Ling Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.02856v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18094v1","updated":"2024-06-26T06:10:20Z","published":"2024-06-26T06:10:20Z","title":"Shimo Lab at \"Discharge Me!\": Discharge Summarization by Prompt-Driven\n  Concatenation of Electronic Health Record Sections","summary":"  In this paper, we present our approach to the shared task \"Discharge Me!\" at\nthe BioNLP Workshop 2024. The primary goal of this task is to reduce the time\nand effort clinicians spend on writing detailed notes in the electronic health\nrecord (EHR). Participants develop a pipeline to generate the \"Brief Hospital\nCourse\" and \"Discharge Instructions\" sections from the EHR. Our approach\ninvolves a first step of extracting the relevant sections from the EHR. We then\nadd explanatory prompts to these sections and concatenate them with separate\ntokens to create the input text. To train a text generation model, we perform\nLoRA fine-tuning on the ClinicalT5-large model. On the final test data, our\napproach achieved a ROUGE-1 score of $0.394$, which is comparable to the top\nsolutions.\n","authors":["Yunzhen He","Hiroaki Yamagiwa","Hidetoshi Shimodaira"],"pdf_url":"https://arxiv.org/pdf/2406.18094v1.pdf","comment":"BioNLP @ ACL2024"},{"id":"http://arxiv.org/abs/2404.10237v2","updated":"2024-06-26T06:04:51Z","published":"2024-04-16T02:35:17Z","title":"Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical\n  Vision-Language Models","summary":"  Recent advancements in general-purpose or domain-specific multimodal large\nlanguage models (LLMs) have witnessed remarkable progress for medical\ndecision-making. However, they are designated for specific classification or\ngenerative tasks, and require model training or finetuning on large-scale\ndatasets with sizeable parameters and tremendous computing, hindering their\nclinical utility across diverse resource-constrained scenarios in practice. In\nthis paper, we propose a novel and lightweight framework Med-MoE\n(Mixture-of-Experts) that tackles both discriminative and generative multimodal\nmedical tasks. The learning of Med-MoE consists of three steps: multimodal\nmedical alignment, instruction tuning and routing, and domain-specific MoE\ntuning. After aligning multimodal medical images with LLM tokens, we then\nenable the model for different multimodal medical tasks with instruction\ntuning, together with a trainable router tailored for expert selection across\ninput modalities. Finally, the model is tuned by integrating the router with\nmultiple domain-specific experts, which are selectively activated and further\nempowered by meta expert. Comprehensive experiments on both open- and close-end\nmedical question answering (Med-VQA) and image classification tasks across\ndatasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can\nachieve performance superior to or on par with state-of-the-art baselines,\nwhile only requiring approximately 30\\%-50\\% of activated model parameters.\nExtensive analysis and ablations corroborate the effectiveness and practical\nutility of our method.\n","authors":["Songtao Jiang","Tuo Zheng","Yan Zhang","Yeying Jin","Li Yuan","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2404.10237v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18088v1","updated":"2024-06-26T05:52:47Z","published":"2024-06-26T05:52:47Z","title":"LLM-Driven Multimodal Opinion Expression Identification","summary":"  Opinion Expression Identification (OEI) is essential in NLP for applications\nranging from voice assistants to depression diagnosis. This study extends OEI\nto encompass multimodal inputs, underlining the significance of auditory cues\nin delivering emotional subtleties beyond the capabilities of text. We\nintroduce a novel multimodal OEI (MOEI) task, integrating text and speech to\nmirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we\nconstruct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is\napplied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template\nfor the OEI task to take full advantage of the generative power of large\nlanguage models (LLMs). Advancing further, we propose an LLM-driven method\nSTOEI, which combines speech and text modal to identify opinion expressions.\nOur experiments demonstrate that MOEI significantly improves the performance\nwhile our method outperforms existing methods by 9.20\\% and obtains SOTA\nresults.\n","authors":["Bonian Jia","Huiyao Chen","Yueheng Sun","Meishan Zhang","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18088v1.pdf","comment":"6 pages, 3 Figures"},{"id":"http://arxiv.org/abs/2406.18087v1","updated":"2024-06-26T05:51:08Z","published":"2024-06-26T05:51:08Z","title":"EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction\n  Using Large Language Multimodal Models","summary":"  Traditional diagnosis of chronic diseases involves in-person consultations\nwith physicians to identify the disease. However, there is a lack of research\nfocused on predicting and developing application systems using clinical notes\nand blood test values. We collected five years of Electronic Health Records\n(EHRs) from Taiwan's hospital database between 2017 and 2021 as an AI database.\nFurthermore, we developed an EHR-based chronic disease prediction platform\nutilizing Large Language Multimodal Models (LLMMs), successfully integrating\nwith frontend web and mobile applications for prediction. This prediction\nplatform can also connect to the hospital's backend database, providing\nphysicians with real-time risk assessment diagnostics. The demonstration link\ncan be found at https://www.youtube.com/watch?v=oqmL9DEDFgA.\n","authors":["Chun-Chieh Liao","Wei-Ting Kuo","I-Hsuan Hu","Yen-Chen Shih","Jun-En Ding","Feng Liu","Fang-Ming Hung"],"pdf_url":"https://arxiv.org/pdf/2406.18087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06918v2","updated":"2024-06-26T05:47:52Z","published":"2024-02-10T09:51:03Z","title":"Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to\n  Searching for the Most Promising Intermediate Thought","summary":"  To improve the ability of the large language model (LLMs) to tackle complex\nreasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs\nto reason step-by-step, enabling problem solving from simple to complex.\nState-of-the-art methods for generating such a chain involve interactive\ncollaboration, where the learner generates candidate intermediate thoughts,\nevaluated by the LLM, guiding the generation of subsequent thoughts. However, a\nwidespread yet understudied problem is that the evaluation from the LLM is\ntypically noisy and unreliable, potentially misleading the generation process\nin selecting promising intermediate thoughts. In this paper, motivated by\nVapnik's principle, we use pairwise-comparison evaluation instead of point-wise\nscoring to search for promising intermediate thoughts with the noisy feedback\nfrom the LLM. In each round, we randomly pair intermediate thoughts and\ndirectly prompt the LLM to select the more promising one from each pair,\nallowing us to identify the most promising thoughts through an iterative\nprocess. To further alleviate the noise in the comparison, we incorporate\ntechniques from ensemble learning and dueling bandits, proposing two variants\nof the algorithm. Experiments on three real-world tasks demonstrate the\neffectiveness of our proposed algorithm and verify the rationale of the\npairwise comparison mechanism.\n","authors":["Zhen-Yu Zhang","Siwei Han","Huaxiu Yao","Gang Niu","Masashi Sugiyama"],"pdf_url":"https://arxiv.org/pdf/2402.06918v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.18085v1","updated":"2024-06-26T05:46:35Z","published":"2024-06-26T05:46:35Z","title":"Multilingual Knowledge Graph Completion from Pretrained Language Models\n  with Knowledge Constraints","summary":"  Multilingual Knowledge Graph Completion (mKGC) aim at solving queries like\n(h, r, ?) in different languages by reasoning a tail entity t thus improving\nmultilingual knowledge graphs. Previous studies leverage multilingual\npretrained language models (PLMs) and the generative paradigm to achieve mKGC.\nAlthough multilingual pretrained language models contain extensive knowledge of\ndifferent languages, its pretraining tasks cannot be directly aligned with the\nmKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit\na pronounced English-centric bias. This makes it difficult for mKGC to achieve\ngood results, particularly in the context of low-resource languages. To\novercome previous problems, this paper introduces global and local knowledge\nconstraints for mKGC. The former is used to constrain the reasoning of answer\nentities, while the latter is used to enhance the representation of query\ncontexts. The proposed method makes the pretrained model better adapt to the\nmKGC task. Experimental results on public datasets demonstrate that our method\noutperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32% and\n16.03%, which indicates that our proposed method has significant enhancement on\nmKGC.\n","authors":["Ran Song","Shizhu He","Shengxiang Gao","Li Cai","Kang Liu","Zhengtao Yu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.18085v1.pdf","comment":"11 pages, ACL 2023"},{"id":"http://arxiv.org/abs/2406.16464v2","updated":"2024-06-26T05:40:16Z","published":"2024-06-24T09:13:42Z","title":"InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection","summary":"  The prevalence of sarcasm in social media, conveyed through text-image\ncombinations, presents significant challenges for sentiment analysis and\nintention mining. Current multi-modal sarcasm detection methods have been\nproven to struggle with biases from spurious cues, leading to a superficial\nunderstanding of the complex interactions between text and image. To address\nthese issues, we propose InterCLIP-MEP, a robust framework for multi-modal\nsarcasm detection. InterCLIP-MEP introduces a refined variant of CLIP,\nInteractive CLIP (InterCLIP), as the backbone, enhancing sample representations\nby embedding cross-modality information in each encoder. Furthermore, a novel\ntraining strategy is designed to adapt InterCLIP for a Memory-Enhanced\nPredictor (MEP). MEP uses dynamic dual-channel memory to store valuable\nhistorical knowledge of test samples and then leverages this memory as a\nnon-parametric classifier to derive the final prediction. By using InterCLIP to\nencode text-image interactions more effectively and incorporating MEP,\nInterCLIP-MEP offers a more robust recognition of multi-modal sarcasm.\nExperiments demonstrate that InterCLIP-MEP achieves state-of-the-art\nperformance on the MMSD2.0 benchmark. Code and data are available at\nhttps://github.com/CoderChen01/InterCLIP-MEP.\n","authors":["Junjie Chen","Subin Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16464v2.pdf","comment":"8 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.18082v1","updated":"2024-06-26T05:40:10Z","published":"2024-06-26T05:40:10Z","title":"Octo-planner: On-device Language Model for Planner-Action Agents","summary":"  AI agents have become increasingly significant in various domains, enabling\nautonomous decision-making and problem-solving. To function effectively, these\nagents require a planning process that determines the best course of action and\nthen executes the planned actions. In this paper, we present an efficient\non-device Planner-Action framework that separates planning and action execution\ninto two distinct components: a planner agent based on Phi-3 Mini, a 3.8\nbillion parameter LLM optimized for edge devices, and an action agent using the\nOctopus model for function execution. The planner agent first responds to user\nqueries by decomposing tasks into a sequence of sub-steps, which are then\nexecuted by the action agent. To optimize performance on resource-constrained\ndevices, we employ model fine-tuning instead of in-context learning, reducing\ncomputational costs and energy consumption while improving response times. Our\napproach involves using GPT-4 to generate diverse planning queries and\nresponses based on available functions, with subsequent validations to ensure\ndata quality. We fine-tune the Phi-3 Mini model on this curated dataset,\nachieving a 97\\% success rate in our in-domain test environment. To address\nmulti-domain planning challenges, we developed a multi-LoRA training method\nthat merges weights from LoRAs trained on distinct function subsets. This\napproach enables flexible handling of complex, multi-domain queries while\nmaintaining computational efficiency on resource-constrained devices. To\nsupport further research, we have open-sourced our model weights at\n\\url{https://huggingface.co/NexaAIDev/octopus-planning}. For the demo, please\nrefer to \\url{https://www.nexa4ai.com/octo-planner}.\n","authors":["Wei Chen","Zhiyuan Li","Zhen Guo","Yikang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.18082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18078v1","updated":"2024-06-26T05:30:21Z","published":"2024-06-26T05:30:21Z","title":"Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad\n  Prediction","summary":"  Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect\nterm, aspect category, opinion term, sentiment polarity) for a given review,\nwhich is the most representative and challenging task in aspect-based sentiment\nanalysis. A key challenge in the ASQP task is the scarcity of labeled data,\nwhich limits the performance of existing methods. To tackle this issue, we\npropose a self-training framework with a pseudo-label scorer, wherein a scorer\nassesses the match between reviews and their pseudo-labels, aiming to filter\nout mismatches and thereby enhance the effectiveness of self-training. We\nhighlight two critical aspects to ensure the scorer's effectiveness and\nreliability: the quality of the training dataset and its model architecture. To\nthis end, we create a human-annotated comparison dataset and train a generative\nmodel on it using ranking-based objectives. Extensive experiments on public\nASQP datasets reveal that using our scorer can greatly and consistently improve\nthe effectiveness of self-training. Moreover, we explore the possibility of\nreplacing humans with large language models for comparison dataset annotation,\nand experiments demonstrate its feasibility. We release our code and data at\nhttps://github.com/HITSZ-HLT/ST-w-Scorer-ABSA .\n","authors":["Yice Zhang","Jie Zeng","Weiming Hu","Ziyi Wang","Shiwei Chen","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2406.18078v1.pdf","comment":"Accepted to ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2406.18069v1","updated":"2024-06-26T04:54:45Z","published":"2024-06-26T04:54:45Z","title":"Large Language Models for Cuffless Blood Pressure Measurement From\n  Wearable Biosignals","summary":"  Large language models (LLMs) have captured significant interest from both\nacademia and industry due to their impressive performance across various\ntextual tasks. However, the potential of LLMs to analyze physiological\ntime-series data remains an emerging research field. Particularly, there is a\nnotable gap in the utilization of LLMs for analyzing wearable biosignals to\nachieve cuffless blood pressure (BP) measurement, which is critical for the\nmanagement of cardiovascular diseases. This paper presents the first work to\nexplore the capacity of LLMs to perform cuffless BP estimation based on\nwearable biosignals. We extracted physiological features from electrocardiogram\n(ECG) and photoplethysmogram (PPG) signals and designed context-enhanced\nprompts by combining these features with BP domain knowledge and user\ninformation. Subsequently, we adapted LLMs to BP estimation tasks through\ninstruction tuning. To evaluate the proposed approach, we conducted assessments\nof ten advanced LLMs using a comprehensive public dataset of wearable\nbiosignals from 1,272 participants. The experimental results demonstrate that\nthe optimally fine-tuned LLM significantly surpasses conventional task-specific\nbaselines, achieving an estimation error of 0.00 $\\pm$ 9.25 mmHg for systolic\nBP and 1.29 $\\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies\nhighlight the benefits of our context enhancement strategy, leading to an 8.9%\nreduction in mean absolute error for systolic BP estimation. This paper\npioneers the exploration of LLMs for cuffless BP measurement, providing a\npotential solution to enhance the accuracy of cuffless BP measurement.\n","authors":["Zengding Liu","Chen Chen","Jiannong Cao","Minglei Pan","Jikui Liu","Nan Li","Fen Miao","Ye Li"],"pdf_url":"https://arxiv.org/pdf/2406.18069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09361v3","updated":"2024-06-26T04:54:14Z","published":"2023-06-12T16:40:07Z","title":"MFSN: Multi-perspective Fusion Search Network For Pre-training Knowledge\n  in Speech Emotion Recognition","summary":"  Speech Emotion Recognition (SER) is an important research topic in\nhuman-computer interaction. Many recent works focus on directly extracting\nemotional cues through pre-trained knowledge, frequently overlooking\nconsiderations of appropriateness and comprehensiveness. Therefore, we propose\na novel framework for pre-training knowledge in SER, called Multi-perspective\nFusion Search Network (MFSN). Considering comprehensiveness, we partition\nspeech knowledge into Textual-related Emotional Content (TEC) and\nSpeech-related Emotional Content (SEC), capturing cues from both semantic and\nacoustic perspectives, and we design a new architecture search space to fully\nleverage them. Considering appropriateness, we verify the efficacy of different\nmodeling approaches in capturing SEC and fills the gap in current research.\nExperimental results on multiple datasets demonstrate the superiority of MFSN.\n","authors":["Haiyang Sun","Fulin Zhang","Yingying Gao","Zheng Lian","Shilei Zhang","Junlan Feng"],"pdf_url":"https://arxiv.org/pdf/2306.09361v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18067v1","updated":"2024-06-26T04:52:48Z","published":"2024-06-26T04:52:48Z","title":"Exploring Energy-Based Models for Out-of-Distribution Detection in\n  Dialect Identification","summary":"  The diverse nature of dialects presents challenges for models trained on\nspecific linguistic patterns, rendering them susceptible to errors when\nconfronted with unseen or out-of-distribution (OOD) data. This study introduces\na novel margin-enhanced joint energy model (MEJEM) tailored specifically for\nOOD detection in dialects. By integrating a generative model and the energy\nmargin loss, our approach aims to enhance the robustness of dialect\nidentification systems. Furthermore, we explore two OOD scores for OOD dialect\ndetection, and our findings conclusively demonstrate that the energy score\noutperforms the softmax score. Leveraging Sharpness-Aware Minimization to\noptimize the training process of the joint model, we enhance model\ngeneralization by minimizing both loss and sharpness. Experiments conducted on\ndialect identification tasks validate the efficacy of Energy-Based Models and\nprovide valuable insights into their performance.\n","authors":["Yaqian Hao","Chenguang Hu","Yingying Gao","Shilei Zhang","Junlan Feng"],"pdf_url":"https://arxiv.org/pdf/2406.18067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13720v2","updated":"2024-06-26T04:52:02Z","published":"2024-02-21T11:31:28Z","title":"Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster\n  Speculative Decoding","summary":"  Speculative decoding is a widely used method that accelerates the generation\nprocess of large language models (LLMs) with no compromise in model\nperformance. It achieves this goal by using an existing smaller model for\ndrafting and then employing the target LLM to verify the draft in a low-cost\nparallel manner. Under such a drafting-verification framework, drafting\nefficiency has become a bottleneck in the final speedup of speculative\ndecoding. Therefore, generating longer drafts at less cost can lead to better\ndecoding speedup. To achieve this, we introduce Ouroboros, which can generate\ndraft phrases to parallelize the drafting process and meanwhile lengthen drafts\nin a training-free manner. The experimental results on various typical text\ngeneration tasks show that Ouroboros can achieve speedups of up to $2.4\\times$\nover speculative decoding and $3.9\\times$ over vanilla decoding, without\nfine-tuning draft and target models.\n","authors":["Weilin Zhao","Yuxiang Huang","Xu Han","Wang Xu","Chaojun Xiao","Xinrong Zhang","Yewei Fang","Kaihuo Zhang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2402.13720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18064v1","updated":"2024-06-26T04:49:41Z","published":"2024-06-26T04:49:41Z","title":"Evaluating Quality of Answers for Retrieval-Augmented Generation: A\n  Strong LLM Is All You Need","summary":"  We present a comprehensive evaluation of answer quality in\nRetrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel\ngrading system that is designed to assess correctness, completeness, and\nhonesty. We further map the grading of quality aspects aforementioned into a\nbinary score, indicating an accept or reject decision, mirroring the intuitive\n\"thumbs-up\" or \"thumbs-down\" gesture commonly used in chat applications. This\napproach suits factual business settings where a clear decision opinion is\nessential. Our assessment applies vRAG-Eval to two Large Language Models\n(LLMs), evaluating the quality of answers generated by a vanilla RAG\napplication. We compare these evaluations with human expert judgments and find\na substantial alignment between GPT-4's assessments and those of human experts,\nreaching 83% agreement on accept or reject decisions. This study highlights the\npotential of LLMs as reliable evaluators in closed-domain, closed-ended\nsettings, particularly when human evaluations require significant resources.\n","authors":["Yang Wang","Alberto Garcia Hernandez","Roman Kyslyi","Nicholas Kersting"],"pdf_url":"https://arxiv.org/pdf/2406.18064v1.pdf","comment":"12 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2312.09542v2","updated":"2024-06-26T04:48:11Z","published":"2023-12-15T05:30:14Z","title":"Marathon: A Race Through the Realm of Long Context with Large Language\n  Models","summary":"  With the advancement of large language models (LLMs) and the expansion of\ntheir context windows, existing long-context benchmarks fall short in\neffectively evaluating the models' comprehension and reasoning abilities in\nextended texts. Moreover, conventional benchmarks relying on F1 metrics often\ninaccurately score responses: they may undervalue correct answers that differ\nfrom the reference responses and overvalue incorrect ones that resemble the\nreference texts. In response to these limitations, we introduce Marathon, a\nnovel evaluation benchmark that adopts a multiple-choice question format. It is\nspecifically designed to overcome the constraints of previous benchmarks and\nprovide a rapid, precise, and unbiased appraisal of the long-context\ncomprehension skills of large language models. We conducted comprehensive\nevaluations on the Marathon benchmark with a range of state-of-the-art LLMs and\nassessed the effectiveness of various optimization strategies tailored for\nlong-context generation. We anticipate that the Marathon benchmark and its\nassociated leaderboard will enable a more precise and equitable evaluation of\nLLMs' capabilities in understanding and reasoning over extended contexts.\nMarathon is available at https://github.com/Hambaobao/Marathon.\n","authors":["Lei Zhang","Yunshui Li","Ziqiang Liu","Jiaxi yang","Junhao Liu","Longze Chen","Run Luo","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2312.09542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18060v1","updated":"2024-06-26T04:33:13Z","published":"2024-06-26T04:33:13Z","title":"AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for\n  Memory-Efficient Large Language Models Fine-Tuning","summary":"  Fine-tuning large language models (LLMs) has achieved remarkable performance\nacross various natural language processing tasks, yet it demands more and more\nmemory as model sizes keep growing. To address this issue, the recently\nproposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs\nusing only forward passes, thereby avoiding the need for a backpropagation\ngraph. However, significant performance drops and a high risk of divergence\nhave limited their widespread adoption. In this paper, we propose the Adaptive\nZeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed\nto improve the performance and convergence of the ZO methods. To enhance\ndimension-dependent ZO estimation accuracy, we introduce a fast-forward,\nlow-parameter tensorized adapter. To tackle the frequently observed divergence\nissue in large-scale ZO fine-tuning tasks, we propose an adaptive query number\nschedule that guarantees convergence. Detailed theoretical analysis and\nextensive experimental results on Roberta-Large and Llama-2-7B models\nsubstantiate the efficacy of our AdaZeta framework in terms of accuracy, memory\nefficiency, and convergence speed.\n","authors":["Yifan Yang","Kai Zhen","Ershad Banijamal","Athanasios Mouchtaris","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14898v2","updated":"2024-06-26T04:28:38Z","published":"2024-06-21T06:43:15Z","title":"Safely Learning with Private Data: A Federated Learning Framework for\n  Large Language Model","summary":"  Private data, being larger and quality-higher than public data, can greatly\nimprove large language models (LLM). However, due to privacy concerns, this\ndata is often dispersed in multiple silos, making its secure utilization for\nLLM training a challenge. Federated learning (FL) is an ideal solution for\ntraining models with distributed private data, but traditional frameworks like\nFedAvg are unsuitable for LLM due to their high computational demands on\nclients. An alternative, split learning, offloads most training parameters to\nthe server while training embedding and output layers locally, making it more\nsuitable for LLM. Nonetheless, it faces significant challenges in security and\nefficiency. Firstly, the gradients of embeddings are prone to attacks, leading\nto potential reverse engineering of private data. Furthermore, the server's\nlimitation of handle only one client's training request at a time hinders\nparallel training, severely impacting training efficiency. In this paper, we\npropose a Federated Learning framework for LLM, named FL-GLM, which prevents\ndata leakage caused by both server-side and peer-client attacks while improving\ntraining efficiency. Specifically, we first place the input block and output\nblock on local client to prevent embedding gradient attacks from server.\nSecondly, we employ key-encryption during client-server communication to\nprevent reverse engineering attacks from peer-clients. Lastly, we employ\noptimization methods like client-batching or server-hierarchical, adopting\ndifferent acceleration methods based on the actual computational capabilities\nof the server. Experimental results on NLU and generation tasks demonstrate\nthat FL-GLM achieves comparable metrics to centralized chatGLM model,\nvalidating the effectiveness of our federated learning framework.\n","authors":["JiaYing Zheng","HaiNan Zhang","LingXiang Wang","WangJie Qiu","HongWei Zheng","ZhiMing Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.14898v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18049v1","updated":"2024-06-26T03:56:21Z","published":"2024-06-26T03:56:21Z","title":"Improving Entity Recognition Using Ensembles of Deep Learning and\n  Fine-tuned Large Language Models: A Case Study on Adverse Event Extraction\n  from Multiple Sources","summary":"  Adverse event (AE) extraction following COVID-19 vaccines from text data is\ncrucial for monitoring and analyzing the safety profiles of immunizations.\nTraditional deep learning models are adept at learning intricate feature\nrepresentations and dependencies in sequential data, but often require\nextensive labeled data. In contrast, large language models (LLMs) excel in\nunderstanding contextual information, but exhibit unstable performance on named\nentity recognition tasks, possibly due to their broad but unspecific training.\nThis study aims to evaluate the effectiveness of LLMs and traditional deep\nlearning models in AE extraction, and to assess the impact of ensembling these\nmodels on performance. In this study, we utilized reports and posts from the\nVAERS (n=621), Twitter (n=9,133), and Reddit (n=131) as our corpora. Our goal\nwas to extract three types of entities: \"vaccine\", \"shot\", and \"ae\". We\nexplored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5,\nGPT-4, and Llama-2, as well as traditional deep learning models like RNN and\nBioBERT. To enhance performance, we created ensembles of the three models with\nthe best performance. For evaluation, we used strict and relaxed F1 scores to\nevaluate the performance for each entity type, and micro-average F1 was used to\nassess the overall performance. The ensemble model achieved the highest\nperformance in \"vaccine\", \"shot\", and \"ae\" with strict F1-scores of 0.878,\n0.930, and 0.925, respectively, along with a micro-average score of 0.903. In\nconclusion, this study demonstrates the effectiveness and robustness of\nensembling fine-tuned traditional deep learning models and LLMs, for extracting\nAE-related information. This study contributes to the advancement of biomedical\nnatural language processing, providing valuable insights into improving AE\nextraction from text data for pharmacovigilance and public health surveillance.\n","authors":["Yiming Li","Deepthi Viswaroopan","William He","Jianfu Li","Xu Zuo","Hua Xu","Cui Tao"],"pdf_url":"https://arxiv.org/pdf/2406.18049v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14355v3","updated":"2024-06-26T03:52:24Z","published":"2024-04-22T17:07:25Z","title":"Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language\n  Models","summary":"  Quantitative and numerical comprehension in language is an important task in\nmany fields like education and finance, but still remains a challenging task\nfor language models. While tool and calculator usage has shown to be helpful to\nimprove mathematical reasoning in large pretrained decoder-only language\nmodels, this remains unexplored for smaller language models with encoders. In\nthis paper, we propose Pre-Calc, a simple pre-finetuning objective of learning\nto use the calculator for both encoder-only and encoder-decoder architectures,\nformulated as a discriminative and generative task respectively. We pre-train\nBERT and RoBERTa for discriminative calculator use and Flan-T5 for generative\ncalculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves\nperformance on downstream tasks that require numerical understanding. Our code\nand data are available at https://github.com/calc-cmu/pre-calc.\n","authors":["Vishruth Veerendranath","Vishwa Shah","Kshitish Ghate"],"pdf_url":"https://arxiv.org/pdf/2404.14355v3.pdf","comment":"AI4Math workshop, ICML 2024"},{"id":"http://arxiv.org/abs/2406.18045v1","updated":"2024-06-26T03:43:09Z","published":"2024-06-26T03:43:09Z","title":"PharmGPT: Domain-Specific Large Language Models for Bio-Pharmaceutical\n  and Chemistry","summary":"  Large language models (LLMs) have revolutionized Natural Language Processing\n(NLP) by by minimizing the need for complex feature engineering. However, the\napplication of LLMs in specialized domains like biopharmaceuticals and\nchemistry remains largely unexplored. These fields are characterized by\nintricate terminologies, specialized knowledge, and a high demand for precision\nareas where general purpose LLMs often fall short. In this study, we introduce\nPharmGPT, a suite of multilingual LLMs with 13 billion and 70 billion\nparameters, specifically trained on a comprehensive corpus of hundreds of\nbillions of tokens tailored to the Bio-Pharmaceutical and Chemical sectors. Our\nevaluation shows that PharmGPT matches or surpasses existing general models on\nkey benchmarks, such as NAPLEX, demonstrating its exceptional capability in\ndomain-specific tasks. This advancement establishes a new benchmark for LLMs in\nthe Bio-Pharmaceutical and Chemical fields, addressing the existing gap in\nspecialized language modeling. Furthermore, this suggests a promising path for\nenhanced research and development in these specialized areas, paving the way\nfor more precise and effective applications of NLP in specialized domains.\n","authors":["Linqing Chen","Weilei Wang","Zilong Bai","Peng Xu","Yan Fang","Jie Fang","Wentao Wu","Lizhi Zhou","Ruiji Zhang","Yubin Xia","Chaobo Xu","Ran Hu","Licong Xu","Qijun Cai","Haoran Hua","Jing Sun","Jin Liu","Tian Qiu","Haowen Liu","Meng Hu","Xiuwen Li","Fei Gao","Yufu Wang","Lin Tie","Chaochao Wang","Jianping Lu","Cheng Sun","Yixin Wang","Shengjie Yang","Yuancheng Li","Lu Jin","Lisha Zhang","Fu Bian","Changyang Tu"],"pdf_url":"https://arxiv.org/pdf/2406.18045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18034v1","updated":"2024-06-26T03:08:24Z","published":"2024-06-26T03:08:24Z","title":"LLMs for Doctors: Leveraging Medical LLMs to Assist Doctors, Not Replace\n  Them","summary":"  The recent success of Large Language Models (LLMs) has had a significant\nimpact on the healthcare field, providing patients with medical advice,\ndiagnostic information, and more. However, due to a lack of professional\nmedical knowledge, patients are easily misled by generated erroneous\ninformation from LLMs, which may result in serious medical problems. To address\nthis issue, we focus on tuning the LLMs to be medical assistants who\ncollaborate with more experienced doctors. We first conduct a two-stage survey\nby inspiration-feedback to gain a broad understanding of the real needs of\ndoctors for medical assistants. Based on this, we construct a Chinese medical\ndataset called DoctorFLAN to support the entire workflow of doctors, which\nincludes 92K Q\\&A samples from 22 tasks and 27 specialists. Moreover, we\nevaluate LLMs in doctor-oriented scenarios by constructing the\nDoctorFLAN-\\textit{test} containing 550 single-turn Q\\&A and DotaBench\ncontaining 74 multi-turn conversations. The evaluation results indicate that\nbeing a medical assistant still poses challenges for existing open-source\nmodels, but DoctorFLAN can help them significantly. It demonstrates that the\ndoctor-oriented dataset and benchmarks we construct can complement existing\npatient-oriented work and better promote medical LLMs research.\n","authors":["Wenya Xie","Qingying Xiao","Yu Zheng","Xidong Wang","Junying Chen","Ke Ji","Anningzhe Gao","Xiang Wan","Feng Jiang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18027v1","updated":"2024-06-26T02:49:28Z","published":"2024-06-26T02:49:28Z","title":"Automated Clinical Data Extraction with Knowledge Conditioned LLMs","summary":"  The extraction of lung lesion information from clinical and medical imaging\nreports is crucial for research on and clinical care of lung-related diseases.\nLarge language models (LLMs) can be effective at interpreting unstructured text\nin reports, but they often hallucinate due to a lack of domain-specific\nknowledge, leading to reduced accuracy and posing challenges for use in\nclinical settings. To address this, we propose a novel framework that aligns\ngenerated internal knowledge with external knowledge through in-context\nlearning (ICL). Our framework employs a retriever to identify relevant units of\ninternal or external knowledge and a grader to evaluate the truthfulness and\nhelpfulness of the retrieved internal-knowledge rules, to align and update the\nknowledge bases. Our knowledge-conditioned approach also improves the accuracy\nand reliability of LLM outputs by addressing the extraction task in two stages:\n(i) lung lesion finding detection and primary structured field parsing,\nfollowed by (ii) further parsing of lesion description text into additional\nstructured fields. Experiments with expert-curated test datasets demonstrate\nthat this ICL approach can increase the F1 score for key fields (lesion size,\nmargin and solidity) by an average of 12.9% over existing ICL methods.\n","authors":["Diya Li","Asim Kadav","Aijing Gao","Rui Li","Richard Bourgon"],"pdf_url":"https://arxiv.org/pdf/2406.18027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04625v2","updated":"2024-06-26T02:22:11Z","published":"2024-06-07T04:19:01Z","title":"Key-Element-Informed sLLM Tuning for Document Summarization","summary":"  Remarkable advances in large language models (LLMs) have enabled high-quality\ntext summarization. However, this capability is currently accessible only\nthrough LLMs of substantial size or proprietary LLMs with usage fees. In\nresponse, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have\nbeen extensively studied, yet they often suffer from missing key information\nand entities, i.e., low relevance, in particular, when input documents are\nlong. We hence propose a key-element-informed instruction tuning for\nsummarization, so-called KEITSum, which identifies key elements in documents\nand instructs sLLM to generate summaries capturing these key elements.\nExperimental results on dialogue and news datasets demonstrate that sLLM with\nKEITSum indeed provides high-quality summarization with higher relevance and\nless hallucinations, competitive to proprietary LLM.\n","authors":["Sangwon Ryu","Heejin Do","Yunsu Kim","Gary Geunbae Lee","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2406.04625v2.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.16746v2","updated":"2024-06-26T02:19:01Z","published":"2024-06-24T15:55:49Z","title":"The Responsible Foundation Model Development Cheatsheet: A Review of\n  Tools & Resources","summary":"  Foundation model development attracts a rapidly expanding body of\ncontributors, scientists, and applications. To help shape responsible\ndevelopment practices, we introduce the Foundation Model Development\nCheatsheet: a growing collection of 250+ tools and resources spanning text,\nvision, and speech modalities. We draw on a large body of prior work to survey\nresources (e.g. software, documentation, frameworks, guides, and practical\ntools) that support informed data selection, processing, and understanding,\nprecise and limitation-aware artifact documentation, efficient model training,\nadvance awareness of the environmental impact from training, careful model\nevaluation of capabilities, risks, and claims, as well as responsible model\nrelease, licensing and deployment practices. We hope this curated collection of\nresources helps guide more responsible development. The process of curating\nthis list, enabled us to review the AI development ecosystem, revealing what\ntools are critically missing, misused, or over-used in existing practices. We\nfind that (i) tools for data sourcing, model evaluation, and monitoring are\ncritically under-serving ethical and real-world needs, (ii) evaluations for\nmodel safety, capabilities, and environmental impact all lack reproducibility\nand transparency, (iii) text and particularly English-centric analyses continue\nto dominate over multilingual and multi-modal analyses, and (iv) evaluation of\nsystems, rather than just models, is needed so that capabilities and impact are\nassessed in context.\n","authors":["Shayne Longpre","Stella Biderman","Alon Albalak","Hailey Schoelkopf","Daniel McDuff","Sayash Kapoor","Kevin Klyman","Kyle Lo","Gabriel Ilharco","Nay San","Maribeth Rauh","Aviya Skowron","Bertie Vidgen","Laura Weidinger","Arvind Narayanan","Victor Sanh","David Adelani","Percy Liang","Rishi Bommasani","Peter Henderson","Sasha Luccioni","Yacine Jernite","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2406.16746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11162v2","updated":"2024-06-26T01:43:15Z","published":"2024-06-17T03:02:04Z","title":"How Good are LLMs at Relation Extraction under Low-Resource Scenario?\n  Comprehensive Evaluation","summary":"  Relation Extraction (RE) serves as a crucial technology for transforming\nunstructured text into structured information, especially within the framework\nof Knowledge Graph development. Its importance is emphasized by its essential\nrole in various downstream tasks. Besides the conventional RE methods which are\nbased on neural networks and pre-trained language models, large language models\n(LLMs) are also utilized in the research field of RE. However, on low-resource\nlanguages (LRLs), both conventional RE methods and LLM-based methods perform\npoorly on RE due to the data scarcity issues. To this end, this paper\nconstructs low-resource relation extraction datasets in 10 LRLs in three\nregions (Central Asia, Southeast Asia and Middle East). The corpora are\nconstructed by translating the original publicly available English RE datasets\n(NYT10, FewRel and CrossRE) using an effective multilingual machine\ntranslation. Then, we use the language perplexity (PPL) to filter out the\nlow-quality data from the translated datasets. Finally, we conduct an empirical\nstudy and validate the performance of several open-source LLMs on these\ngenerated LRL RE datasets.\n","authors":["Dawulie Jinensibieke","Mieradilijiang Maimaiti","Wentao Xiao","Yuanhang Zheng","Xiaobo Wang"],"pdf_url":"https://arxiv.org/pdf/2406.11162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19733v3","updated":"2024-06-26T01:28:35Z","published":"2024-04-30T17:28:05Z","title":"Iterative Reasoning Preference Optimization","summary":"  Iterative preference optimization methods have recently been shown to perform\nwell for general instruction tuning tasks, but typically make little\nimprovement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this\nwork we develop an iterative approach that optimizes the preference between\ncompeting generated Chain-of-Thought (CoT) candidates by optimizing for winning\nvs. losing reasoning steps that lead to the correct answer. We train using a\nmodified DPO loss (Rafailov et al., 2023) with an additional negative\nlog-likelihood term, which we find to be crucial. We show reasoning improves\nacross repeated iterations of this scheme. While only relying on examples in\nthe training set, our approach results in increasing accuracy on GSM8K, MATH,\nand ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based\nmodels not relying on additionally sourced datasets. For example, we see a\nlarge improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with\nmajority voting out of 32 samples.\n","authors":["Richard Yuanzhe Pang","Weizhe Yuan","Kyunghyun Cho","He He","Sainbayar Sukhbaatar","Jason Weston"],"pdf_url":"https://arxiv.org/pdf/2404.19733v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18002v1","updated":"2024-06-26T01:16:12Z","published":"2024-06-26T01:16:12Z","title":"Decoding with Limited Teacher Supervision Requires Understanding When to\n  Trust the Teacher","summary":"  How can sLLMs efficiently utilize the supervision of LLMs to improve their\ngenerative quality? This question has been well studied in scenarios where\nthere is no restriction on the number of LLM supervisions one can use, giving\nbirth to many decoding algorithms that utilize supervision without further\ntraining. However, it is still unclear what is an effective strategy under the\nlimited supervision scenario, where we assume that no more than a few tokens\ncan be generated by LLMs. To this end, we develop an algorithm to effectively\naggregate the sLLM and LLM predictions on initial tokens so that the generated\ntokens can more accurately condition the subsequent token generation by sLLM\nonly. Critically, we find that it is essential to adaptively overtrust or\ndisregard the LLM prediction based on the confidence of the sLLM. Through our\nexperiments on a wide range of models and datasets, we demonstrate that our\nmethod provides a consistent improvement over conventional decoding strategies.\n","authors":["Hyunjong Ok","Jegwang Ryu","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2406.18002v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2405.11407v2","updated":"2024-06-26T01:12:11Z","published":"2024-05-18T22:43:44Z","title":"Can Public LLMs be used for Self-Diagnosis of Medical Conditions ?","summary":"  Advancements in deep learning have generated a large-scale interest in the\ndevelopment of foundational deep learning models. The development of Large\nLanguage Models (LLM) has evolved as a transformative paradigm in\nconversational tasks, which has led to its integration and extension even in\nthe critical domain of healthcare. With LLMs becoming widely popular and their\npublic access through open-source models and integration with other\napplications, there is a need to investigate their potential and limitations.\nOne such crucial task where LLMs are applied but require a deeper understanding\nis that of self-diagnosis of medical conditions based on bias-validating\nsymptoms in the interest of public health. The widespread integration of Gemini\nwith Google search and GPT-4.0 with Bing search has led to a shift in the trend\nof self-diagnosis using search engines to conversational LLM models. Owing to\nthe critical nature of the task, it is prudent to investigate and understand\nthe potential and limitations of public LLMs in the task of self-diagnosis. In\nthis study, we prepare a prompt engineered dataset of 10000 samples and test\nthe performance on the general task of self-diagnosis. We compared the\nperformance of both the state-of-the-art GPT-4.0 and the fee Gemini model on\nthe task of self-diagnosis and recorded contrasting accuracies of 63.07% and\n6.01%, respectively. We also discuss the challenges, limitations, and potential\nof both Gemini and GPT-4.0 for the task of self-diagnosis to facilitate future\nresearch and towards the broader impact of general public knowledge.\nFurthermore, we demonstrate the potential and improvement in performance for\nthe task of self-diagnosis using Retrieval Augmented Generation.\n","authors":["Nikil Sharan Prabahar Balasubramanian","Sagnik Dakshit"],"pdf_url":"https://arxiv.org/pdf/2405.11407v2.pdf","comment":"11 Pages, 4 figures, Submitted to ACM Transactions on Computing for\n  Healthcare"},{"id":"http://arxiv.org/abs/2406.17992v1","updated":"2024-06-26T00:21:39Z","published":"2024-06-26T00:21:39Z","title":"Catching Chameleons: Detecting Evolving Disinformation Generated using\n  Large Language Models","summary":"  Despite recent advancements in detecting disinformation generated by large\nlanguage models (LLMs), current efforts overlook the ever-evolving nature of\nthis disinformation. In this work, we investigate a challenging yet practical\nresearch problem of detecting evolving LLM-generated disinformation.\nDisinformation evolves constantly through the rapid development of LLMs and\ntheir variants. As a consequence, the detection model faces significant\nchallenges. First, it is inefficient to train separate models for each\ndisinformation generator. Second, the performance decreases in scenarios when\nevolving LLM-generated disinformation is encountered in sequential order. To\naddress this problem, we propose DELD (Detecting Evolving LLM-generated\nDisinformation), a parameter-efficient approach that jointly leverages the\ngeneral fact-checking capabilities of pre-trained language models (PLM) and the\nindependent disinformation generation characteristics of various LLMs. In\nparticular, the learned characteristics are concatenated sequentially to\nfacilitate knowledge accumulation and transformation. DELD addresses the issue\nof label scarcity by integrating the semantic embeddings of disinformation with\ntrainable soft prompts to elicit model-specific knowledge. Our experiments show\nthat \\textit{DELD} significantly outperforms state-of-the-art methods.\nMoreover, our method provides critical insights into the unique patterns of\ndisinformation generation across different LLMs, offering valuable perspectives\nin this line of research.\n","authors":["Bohan Jiang","Chengshuai Zhao","Zhen Tan","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2406.17992v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.17990v1","updated":"2024-06-26T00:12:08Z","published":"2024-06-26T00:12:08Z","title":"Explicit Diversity Conditions for Effective Question Answer Generation\n  with Large Language Models","summary":"  Question Answer Generation (QAG) is an effective data augmentation technique\nto improve the accuracy of question answering systems, especially in\nlow-resource domains. While recent pretrained and large language model-based\nQAG methods have made substantial progress, they face the critical issue of\nredundant QA pair generation, affecting downstream QA systems. Implicit\ndiversity techniques such as sampling and diverse beam search are proven\neffective solutions but often yield smaller diversity. We present explicit\ndiversity conditions for QAG, focusing on spatial aspects, question types, and\nentities, substantially increasing diversity in QA generation. Our work\nemphasizes the need of explicit diversity conditions for generating diverse\nquestion-answer synthetic data by showing significant improvements in\ndownstream QA task over existing widely adopted implicit diversity techniques.\nIn particular, generated QA pairs from explicit diversity conditions when used\nto train the downstream QA model results in an average 4.1% exact match and\n4.5% F1 improvement over QAG from implicit sampling techniques on SQuADDU. Our\nwork emphasizes the need for explicit diversity conditions even more in\nlow-resource datasets (SubjQA), where average downstream QA performance\nimprovements are around 12% EM.\n","authors":["Vikas Yadav","Hyuk Joon Kwon","Vijay Srinivasan","Hongxia Jin"],"pdf_url":"https://arxiv.org/pdf/2406.17990v1.pdf","comment":"Published at COLING 2024"},{"id":"http://arxiv.org/abs/2406.17987v1","updated":"2024-06-26T00:00:45Z","published":"2024-06-26T00:00:45Z","title":"Multi-step Knowledge Retrieval and Inference over Unstructured Data","summary":"  The advent of Large Language Models (LLMs) and Generative AI has\nrevolutionized natural language applications across various domains. However,\nhigh-stakes decision-making tasks in fields such as medical, legal and finance\nrequire a level of precision, comprehensiveness, and logical consistency that\npure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to\ndeliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI\nplatform to tackle these problems. The platform integrates fine-tuned LLMs for\nknowledge extraction and alignment with a robust symbolic reasoning engine for\nlogical inference, planning and interactive constraint solving. We describe\nCora, a Collaborative Research Assistant built on this platform, that is\ndesigned to perform complex research and discovery tasks in high-stakes\ndomains. This paper discusses the multi-step inference challenges inherent in\nsuch domains, critiques the limitations of existing LLM-based methods, and\ndemonstrates how Cora's neuro-symbolic approach effectively addresses these\nissues. We provide an overview of the system architecture, key algorithms for\nknowledge extraction and formal reasoning, and present preliminary evaluation\nresults that highlight Cora's superior performance compared to well-known LLM\nand RAG baselines.\n","authors":["Aditya Kalyanpur","Kailash Saravanakumar","Victor Barres","CJ McFate","Lori Moon","Nati Seifu","Maksim Eremeev","Jose Barrera","Eric Brown","David Ferrucci"],"pdf_url":"https://arxiv.org/pdf/2406.17987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04931v2","updated":"2024-06-26T23:44:48Z","published":"2024-03-07T22:37:49Z","title":"A Survey on Human-AI Teaming with Large Pre-Trained Models","summary":"  In the rapidly evolving landscape of artificial intelligence (AI), the\ncollaboration between human intelligence and AI systems, known as Human-AI\n(HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and\ndecision-making processes. The advent of Large Pre-trained Models (LPtM) has\nsignificantly transformed this landscape, offering unprecedented capabilities\nby leveraging vast amounts of data to understand and predict complex patterns.\nThis paper surveys the pivotal integration of LPtMs with HAI, emphasizing how\nthese models enhance collaborative intelligence beyond traditional approaches.\nIt examines the potential of LPtMs in augmenting human capabilities, discussing\nthis collaboration for AI model improvements, effective teaming, ethical\nconsiderations, and their broad applied implications in various sectors.\nThrough this exploration, the study sheds light on the transformative impact of\nLPtM-enhanced HAI Teaming, providing insights for future research, policy\ndevelopment, and strategic implementations aimed at harnessing the full\npotential of this collaboration for research and societal benefit.\n","authors":["Vanshika Vats","Marzia Binta Nizam","Minghao Liu","Ziyuan Wang","Richard Ho","Mohnish Sai Prasad","Vincent Titterton","Sai Venkat Malreddy","Riya Aggarwal","Yanwen Xu","Lei Ding","Jay Mehta","Nathan Grinnell","Li Liu","Sijia Zhong","Devanathan Nallur Gandamani","Xinyi Tang","Rohan Ghosalkar","Celeste Shen","Rachel Shen","Nafisa Hussain","Kesav Ravichandran","James Davis"],"pdf_url":"https://arxiv.org/pdf/2403.04931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18783v1","updated":"2024-06-26T23:04:52Z","published":"2024-06-26T23:04:52Z","title":"Psychological Profiling in Cybersecurity: A Look at LLMs and\n  Psycholinguistic Features","summary":"  The increasing sophistication of cyber threats necessitates innovative\napproaches to cybersecurity. In this paper, we explore the potential of\npsychological profiling techniques, particularly focusing on the utilization of\nLarge Language Models (LLMs) and psycholinguistic features. We investigate the\nintersection of psychology and cybersecurity, discussing how LLMs can be\nemployed to analyze textual data for identifying psychological traits of threat\nactors. We explore the incorporation of psycholinguistic features, such as\nlinguistic patterns and emotional cues, into cybersecurity frameworks. \\iffalse\nThrough case studies and experiments, we discuss the effectiveness of these\nmethods in enhancing threat detection and mitigation strategies.\\fi Our\nresearch underscores the importance of integrating psychological perspectives\ninto cybersecurity practices to bolster defense mechanisms against evolving\nthreats.\n","authors":["Jean Marie Tshimula","D'Jeff K. Nkashama","Jean Tshibangu Muabila","René Manassé Galekwa","Hugues Kanda","Maximilien V. Dialufuma","Mbuyi Mukendi Didier","Kalala Kalonji","Serge Mundele","Patience Kinshie Lenye","Tighana Wenge Basele","Aristarque Ilunga","Christian N. Mayemba","Nathanaël M. Kasoro","Selain K. Kasereka","Hardy Mikese","Pierre-Martin Tardif","Marc Frappier","Froduald Kabanza","Belkacem Chikhaoui","Shengrui Wang","Ali Mulenda Sumbu","Xavier Ndona","Raoul Kienge-Kienge Intudi"],"pdf_url":"https://arxiv.org/pdf/2406.18783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10944v4","updated":"2024-06-26T22:45:32Z","published":"2023-11-18T02:44:33Z","title":"Deception Detection from Linguistic and Physiological Data Streams Using\n  Bimodal Convolutional Neural Networks","summary":"  Deception detection is gaining increasing interest due to ethical and\nsecurity concerns. This paper explores the application of convolutional neural\nnetworks for the purpose of multimodal deception detection. We use a dataset\nbuilt by interviewing 104 subjects about two topics, with one truthful and one\nfalsified response from each subject about each topic. In particular, we make\nthree main contributions. First, we extract linguistic and physiological\nfeatures from this data to train and construct the neural network models.\nSecond, we propose a fused convolutional neural network model using both\nmodalities in order to achieve an improved overall performance. Third, we\ncompare our new approach with earlier methods designed for multimodal deception\ndetection. We find that our system outperforms regular classification methods;\nour results indicate the feasibility of using neural networks for deception\ndetection even in the presence of limited amounts of data.\n","authors":["Panfeng Li","Mohamed Abouelenien","Rada Mihalcea","Zhicheng Ding","Qikai Yang","Yiming Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.10944v4.pdf","comment":"Accepted by 2024 5th International Conference on Information Science,\n  Parallel and Distributed Systems"},{"id":"http://arxiv.org/abs/2406.18776v1","updated":"2024-06-26T22:10:15Z","published":"2024-06-26T22:10:15Z","title":"Implicit Discourse Relation Classification For Nigerian Pidgin","summary":"  Despite attempts to make Large Language Models multi-lingual, many of the\nworld's languages are still severely under-resourced. This widens the\nperformance gap between NLP and AI applications aimed at well-financed, and\nthose aimed at less-resourced languages. In this paper, we focus on Nigerian\nPidgin (NP), which is spoken by nearly 100 million people, but has\ncomparatively very few NLP resources and corpora. We address the task of\nImplicit Discourse Relation Classification (IDRC) and systematically compare an\napproach translating NP data to English and then using a well-resourced IDRC\ntool and back-projecting the labels versus creating a synthetic discourse\ncorpus for NP, in which we translate PDTB and project PDTB labels, and then\ntrain an NP IDR classifier. The latter approach of learning a \"native\" NP\nclassifier outperforms our baseline by 13.27\\% and 33.98\\% in f$_{1}$ score for\n4-way and 11-way classification, respectively.\n","authors":["Muhammed Saeed","Peter Bourgonje","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2406.18776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18762v1","updated":"2024-06-26T21:17:20Z","published":"2024-06-26T21:17:20Z","title":"Categorical Syllogisms Revisited: A Review of the Logical Reasoning\n  Abilities of LLMs for Analyzing Categorical Syllogism","summary":"  There have been a huge number of benchmarks proposed to evaluate how large\nlanguage models (LLMs) behave for logic inference tasks. However, it remains an\nopen question how to properly evaluate this ability. In this paper, we provide\na systematic overview of prior works on the logical reasoning ability of LLMs\nfor analyzing categorical syllogisms. We first investigate all the possible\nvariations for the categorical syllogisms from a purely logical perspective and\nthen examine the underlying configurations (i.e., mood and figure) tested by\nthe existing datasets. Our results indicate that compared to template-based\nsynthetic datasets, crowdsourcing approaches normally sacrifice the coverage of\nconfigurations (i.e., mood and figure) of categorical syllogisms for more\nlanguage variations, thus bringing challenges to fully testing LLMs under\ndifferent situations. We then proceed to summarize the findings and\nobservations for the performances of LLMs to infer the validity of syllogisms\nfrom the current literature. The error rate breakdown analyses suggest that the\ninterpretation of the quantifiers seems to be the current bottleneck that\nlimits the performances of the LLMs and is thus worth more attention. Finally,\nwe discuss several points that might be worth considering when researchers plan\non the future release of categorical syllogism datasets. We hope our work will\nnot only provide a timely review of the current literature regarding\ncategorical syllogisms, but also motivate more interdisciplinary research\nbetween communities, specifically computational linguists and logicians.\n","authors":["Shi Zong","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2406.18762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05750v3","updated":"2024-06-26T20:49:32Z","published":"2024-03-09T01:13:54Z","title":"Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated\n  Text","summary":"  Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Generation (NLG) by demonstrating an impressive ability to generate\nhuman-like text. However, their widespread usage introduces challenges that\nnecessitate thoughtful examination, ethical scrutiny, and responsible\npractices. In this study, we delve into these challenges, explore existing\nstrategies for mitigating them, with a particular emphasis on identifying\nAI-generated text as the ultimate solution. Additionally, we assess the\nfeasibility of detection from a theoretical perspective and propose novel\nresearch directions to address the current limitations in this domain.\n","authors":["Sara Abdali","Richard Anarfi","CJ Barberan","Jia He"],"pdf_url":"https://arxiv.org/pdf/2403.05750v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00253v2","updated":"2024-06-26T20:28:36Z","published":"2024-04-30T23:56:38Z","title":"CodeHalu: Code Hallucinations in LLMs Driven by Execution-based\n  Verification","summary":"  Large Language Models (LLMs) have made significant progress in code\ngeneration, providing developers with unprecedented automated programming\nsupport. However, LLMs often generate code that is syntactically correct and\neven semantically plausible but may not execute as expected or meet specified\nrequirements. This phenomenon of hallucinations in the code domain has not been\nsystematically explored. To enhance the community's understanding and research\non this issue, we introduce the concept of code hallucinations and propose a\nclassification method for code hallucination based on execution verification.\nWe classify code hallucinations into four main types: mapping, naming,\nresource, and logic hallucinations, with each category further divided into\ndifferent subcategories to understand and address the unique challenges faced\nby LLMs in code generation with finer granularity. Additionally, we develop a\ndynamic detection algorithm named CodeHalu to quantify code hallucinations and\nestablish the CodeHaluEval benchmark, which includes 8,883 samples from 699\ntasks to systematically and quantitatively evaluate code hallucinations. By\nevaluating 17 popular LLMs on this benchmark, we reveal significant differences\nin their accuracy and reliability in code generation and provide detailed\ninsights for further improving the code generation capabilities of LLMs. The\nCodeHalu benchmark and code are publicly available at\nhttps://github.com/yuchen814/CodeHalu.\n","authors":["Yuchen Tian","Weixiang Yan","Qian Yang","Qian Chen","Wen Wang","Ziyang Luo","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2405.00253v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10443v2","updated":"2024-06-26T20:22:31Z","published":"2024-05-16T21:07:42Z","title":"Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation","summary":"  Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.\n","authors":["Matthew Raffel","Victor Agostinelli","Lizhong Chen"],"pdf_url":"https://arxiv.org/pdf/2405.10443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10701v3","updated":"2024-06-26T20:15:34Z","published":"2023-10-16T07:51:19Z","title":"Theory of Mind for Multi-Agent Collaboration via Large Language Models","summary":"  While Large Language Models (LLMs) have demonstrated impressive\naccomplishments in both reasoning and planning, their abilities in multi-agent\ncollaborations remains largely unexplored. This study evaluates LLM-based\nagents in a multi-agent cooperative text game with Theory of Mind (ToM)\ninference tasks, comparing their performance with Multi-Agent Reinforcement\nLearning (MARL) and planning-based baselines. We observed evidence of emergent\ncollaborative behaviors and high-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limitations in LLM-based agents' planning\noptimization due to systematic failures in managing long-horizon contexts and\nhallucination about the task state. We explore the use of explicit belief state\nrepresentations to mitigate these issues, finding that it enhances task\nperformance and the accuracy of ToM inferences for LLM-based agents.\n","authors":["Huao Li","Yu Quan Chong","Simon Stepputtis","Joseph Campbell","Dana Hughes","Michael Lewis","Katia Sycara"],"pdf_url":"https://arxiv.org/pdf/2310.10701v3.pdf","comment":"Accepted to EMNLP 2023 (Main Conference). Code available at\n  https://github.com/romanlee6/multi_LLM_comm"},{"id":"http://arxiv.org/abs/2406.18740v1","updated":"2024-06-26T20:12:24Z","published":"2024-06-26T20:12:24Z","title":"Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with\n  Large Language Models","summary":"  Large Language Models (LLMs) have been revolutionizing a myriad of natural\nlanguage processing tasks with their diverse zero-shot capabilities. Indeed,\nexisting work has shown that LLMs can be used to great effect for many tasks,\nsuch as information retrieval (IR), and passage ranking. However, current\nstate-of-the-art results heavily lean on the capabilities of the LLM being\nused. Currently, proprietary, and very large LLMs such as GPT-4 are the highest\nperforming passage re-rankers. Hence, users without the resources to leverage\ntop of the line LLMs, or ones that are closed source, are at a disadvantage. In\nthis paper, we investigate the use of a pre-filtering step before passage\nre-ranking in IR. Our experiments show that by using a small number of human\ngenerated relevance scores, coupled with LLM relevance scoring, it is\neffectively possible to filter out irrelevant passages before re-ranking. Our\nexperiments also show that this pre-filtering then allows the LLM to perform\nsignificantly better at the re-ranking task. Indeed, our results show that\nsmaller models such as Mixtral can become competitive with much larger\nproprietary models (e.g., ChatGPT and GPT-4).\n","authors":["Baharan Nouriinanloo","Maxime Lamothe"],"pdf_url":"https://arxiv.org/pdf/2406.18740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18731v1","updated":"2024-06-26T19:59:21Z","published":"2024-06-26T19:59:21Z","title":"WavRx: a Disease-Agnostic, Generalizable, and Privacy-Preserving Speech\n  Health Diagnostic Model","summary":"  Speech is known to carry health-related attributes, which has emerged as a\nnovel venue for remote and long-term health monitoring. However, existing\nmodels are usually tailored for a specific type of disease, and have been shown\nto lack generalizability across datasets. Furthermore, concerns have been\nraised recently towards the leakage of speaker identity from health embeddings.\nTo mitigate these limitations, we propose WavRx, a speech health diagnostics\nmodel that captures the respiration and articulation related dynamics from a\nuniversal speech representation. Our in-domain and cross-domain experiments on\nsix pathological speech datasets demonstrate WavRx as a new state-of-the-art\nhealth diagnostic model. Furthermore, we show that the amount of speaker\nidentity entailed in the WavRx health embeddings is significantly reduced\nwithout extra guidance during training. An in-depth analysis of the model was\nperformed, thus providing physiological interpretation of its improved\ngeneralizability and privacy-preserving ability.\n","authors":["Yi Zhu","Tiago Falk"],"pdf_url":"https://arxiv.org/pdf/2406.18731v1.pdf","comment":"Under review; Model script available at\n  https://github.com/zhu00121/WavRx"},{"id":"http://arxiv.org/abs/2401.08574v2","updated":"2024-06-26T19:52:35Z","published":"2024-01-16T18:58:37Z","title":"Deductive Closure Training of Language Models for Coherence, Accuracy,\n  and Updatability","summary":"  While language models (LMs) can sometimes generate factually correct text and\nestimate truth values of individual claims, these generally do not reflect a\nglobally coherent, manipulable model of the world. As a consequence, current\nLMs also generate incorrect or nonsensical content, and are difficult to edit\nand bring up to date. We present a method called Deductive Closure Training\n(DCT) that uses LMs themselves to identify implications of (and contradictions\nwithin) the text that they generate, yielding an efficient self-supervised\nprocedure for improving LM factuality. Given a collection of seed documents,\nDCT prompts LMs to generate additional text implied by these documents, reason\nglobally about the correctness of this generated text, and finally fine-tune on\ntext inferred to be correct. Given seed documents from a trusted source, DCT\nprovides a tool for supervised model updating; if seed documents are sampled\nfrom the LM itself, DCT enables fully unsupervised fine-tuning for improved\ncoherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets,\nsupervised DCT improves LM fact verification and text generation accuracy by\n3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%.\nThese results show that LMs' reasoning capabilities during inference can be\nleveraged during training to improve their reliability.\n","authors":["Afra Feyza Akyürek","Ekin Akyürek","Leshem Choshen","Derry Wijaya","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2401.08574v2.pdf","comment":"ACL Findings"},{"id":"http://arxiv.org/abs/2406.18725v1","updated":"2024-06-26T19:48:48Z","published":"2024-06-26T19:48:48Z","title":"Jailbreaking LLMs with Arabic Transliteration and Arabizi","summary":"  This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms.\n","authors":["Mansour Al Ghanim","Saleh Almohaimeed","Mengxin Zheng","Yan Solihin","Qian Lou"],"pdf_url":"https://arxiv.org/pdf/2406.18725v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.18708v1","updated":"2024-06-26T19:18:28Z","published":"2024-06-26T19:18:28Z","title":"Learn it or Leave it: Module Composition and Pruning for Continual\n  Learning","summary":"  In real-world environments, continual learning is essential for machine\nlearning models, as they need to acquire new knowledge incrementally without\nforgetting what they have already learned. While pretrained language models\nhave shown impressive capabilities on various static tasks, applying them to\ncontinual learning poses significant challenges, including avoiding\ncatastrophic forgetting, facilitating knowledge transfer, and maintaining\nparameter efficiency. In this paper, we introduce MoCL-P, a novel lightweight\ncontinual learning method that addresses these challenges simultaneously.\nUnlike traditional approaches that continuously expand parameters for newly\narriving tasks, MoCL-P integrates task representation-guided module composition\nwith adaptive pruning, effectively balancing knowledge integration and\ncomputational overhead. Our evaluation across three continual learning\nbenchmarks with up to 176 tasks shows that MoCL-P achieves state-of-the-art\nperformance and improves parameter efficiency by up to three times,\ndemonstrating its potential for practical applications where resource\nrequirements are constrained.\n","authors":["Mingyang Wang","Heike Adel","Lukas Lange","Jannik Strötgen","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2406.18708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18702v1","updated":"2024-06-26T19:10:51Z","published":"2024-06-26T19:10:51Z","title":"Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling\n  Legislative Behavior and Bipartisanship","summary":"  This study introduces a novel approach to simulating legislative processes\nusing LLM-driven virtual agents, focusing on the U.S. Senate Intelligence\nCommittee. We developed agents representing individual senators and placed them\nin simulated committee discussions. The agents demonstrated the ability to\nengage in realistic debate, provide thoughtful reflections, and find bipartisan\nsolutions under certain conditions. Notably, the simulation also showed promise\nin modeling shifts towards bipartisanship in response to external\nperturbations. Our results indicate that this LLM-driven approach could become\na valuable tool for understanding and potentially improving legislative\nprocesses, supporting a broader pattern of findings highlighting how LLM-based\nagents can usefully model real-world phenomena. Future works will focus on\nenhancing agent complexity, expanding the simulation scope, and exploring\napplications in policy testing and negotiation.\n","authors":["Zachary R. Baker","Zarif L. Azher"],"pdf_url":"https://arxiv.org/pdf/2406.18702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18696v1","updated":"2024-06-26T18:58:23Z","published":"2024-06-26T18:58:23Z","title":"Sequence Graph Network for Online Debate Analysis","summary":"  Online debates involve a dynamic exchange of ideas over time, where\nparticipants need to actively consider their opponents' arguments, respond with\ncounterarguments, reinforce their own points, and introduce more compelling\narguments as the discussion unfolds. Modeling such a complex process is not a\nsimple task, as it necessitates the incorporation of both sequential\ncharacteristics and the capability to capture interactions effectively. To\naddress this challenge, we employ a sequence-graph approach. Building the\nconversation as a graph allows us to effectively model interactions between\nparticipants through directed edges. Simultaneously, the propagation of\ninformation along these edges in a sequential manner enables us to capture a\nmore comprehensive representation of context. We also introduce a Sequence\nGraph Attention layer to illustrate the proposed information update scheme. The\nexperimental results show that sequence graph networks achieve superior results\nto existing methods in online debates.\n","authors":["Quan Mai","Susan Gauch","Douglas Adams","Miaoqing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.18696v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.18695v1","updated":"2024-06-26T18:57:32Z","published":"2024-06-26T18:57:32Z","title":"Learning to Correct for QA Reasoning with Black-box LLMs","summary":"  An open challenge in recent machine learning is about how to improve the\nreasoning capability of large language models (LLMs) in a black-box setting,\ni.e., without access to detailed information such as output token\nprobabilities. Existing approaches either rely on accessibility (which is often\nunrealistic) or involve significantly increased train- and inference-time\ncosts. This paper addresses those limitations or shortcomings by proposing a\nnovel approach, namely CoBB (Correct for improving QA reasoning of Black-Box\nLLMs). It uses a trained adaptation model to perform a seq2seq mapping from the\noften-imperfect reasonings of the original black-box LLM to the correct or\nimproved reasonings. Specifically, the adaptation model is initialized with a\nrelatively small open-source LLM and adapted over a collection of sub-sampled\ntraining pairs. To select the representative pairs of correct and incorrect\nreasonings, we formulated the dataset construction as an optimization problem\nthat minimizes the statistical divergence between the sampled subset and the\nentire collection, and solved it via a genetic algorithm. We then train the\nadaptation model over the sampled pairs by contrasting the likelihoods of\ncorrect and incorrect reasonings. Our experimental results demonstrate that\nCoBB significantly improves reasoning accuracy across various QA benchmarks,\ncompared to the best-performing adaptation baselines.\n","authors":["Jaehyung Kim","Dongyoung Kim","Yiming Yang"],"pdf_url":"https://arxiv.org/pdf/2406.18695v1.pdf","comment":"preprint, 18 pages"},{"id":"http://arxiv.org/abs/2406.18682v1","updated":"2024-06-26T18:39:08Z","published":"2024-06-26T18:39:08Z","title":"The Multilingual Alignment Prism: Aligning Global and Local Preferences\n  to Reduce Harm","summary":"  A key concern with the concept of \"alignment\" is the implicit question of\n\"alignment to what?\". AI systems are increasingly used across the world, yet\nsafety alignment is often focused on homogeneous monolingual settings.\nAdditionally, preference training and safety measures often overfit to harms\ncommon in Western-centric datasets. Here, we explore the viability of different\nalignment approaches when balancing dual objectives: addressing and optimizing\nfor a non-homogeneous set of languages and cultural preferences while\nminimizing both global and local harms. We collect the first set of human\nannotated red-teaming prompts in different languages distinguishing between\nglobal and local harm, which serve as a laboratory for understanding the\nreliability of alignment techniques when faced with preference distributions\nthat are non-stationary across geographies and languages. While this setting is\nseldom covered by the literature to date, which primarily centers on English\nharm mitigation, it captures real-world interactions with AI systems around the\nworld. We establish a new precedent for state-of-the-art alignment techniques\nacross 6 languages with minimal degradation in general performance. Our work\nprovides important insights into cross-lingual transfer and novel optimization\napproaches to safeguard AI systems designed to serve global populations.\n","authors":[" Aakanksha","Arash Ahmadian","Beyza Ermis","Seraphina Goldfarb-Tarrant","Julia Kreutzer","Marzieh Fadaee","Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2406.18682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18679v1","updated":"2024-06-26T18:32:16Z","published":"2024-06-26T18:32:16Z","title":"Speakers Unembedded: Embedding-free Approach to Long-form Neural\n  Diarization","summary":"  End-to-end neural diarization (EEND) models offer significant improvements\nover traditional embedding-based Speaker Diarization (SD) approaches but falls\nshort on generalizing to long-form audio with large number of speakers.\nEEND-vector-clustering method mitigates this by combining local EEND with\nglobal clustering of speaker embeddings from local windows, but this requires\nan additional speaker embedding framework alongside the EEND module. In this\npaper, we propose a novel framework applying EEND both locally and globally for\nlong-form audio without separate speaker embeddings. This approach achieves\nsignificant relative DER reduction of 13% and 10% over the conventional 1-pass\nEEND on Callhome American English and RT03-CTS datasets respectively and\nmarginal improvements over EEND-vector-clustering without the need for\nadditional speaker embeddings. Furthermore, we discuss the computational\ncomplexity of our proposed framework and explore strategies for reducing\nprocessing times.\n","authors":["Xiang Li","Vivek Govindan","Rohit Paturi","Sundararajan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2406.18679v1.pdf","comment":"Accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2406.18678v1","updated":"2024-06-26T18:29:12Z","published":"2024-06-26T18:29:12Z","title":"Few-shot Personalization of LLMs with Mis-aligned Responses","summary":"  As the diversity of users increases, the capability of providing personalized\nresponses by large language models (LLMs) has become increasingly important.\nExisting approaches have only limited successes in LLM personalization, due to\nthe absence of personalized learning or the reliance on shared personal data.\nThis paper proposes a new approach for a few-shot personalization of LLMs with\ntheir mis-aligned responses (Fermi). Our key idea is to learn a set of\npersonalized prompts for each user by progressively improving the prompts using\nLLMs, based on user profile (e.g., demographic information) and a few examples\nof previous opinions. During an iterative process of prompt improvement, we\nincorporate the contexts of mis-aligned responses by LLMs, which are especially\ncrucial for the effective personalization of LLMs. In addition, we develop an\neffective inference method to further leverage the context of the test query\nand the personalized prompts. Our experimental results demonstrate that Fermi\nsignificantly improves performance across various benchmarks, compared to the\nbest-performing baselines.\n","authors":["Jaehyung Kim","Yiming Yang"],"pdf_url":"https://arxiv.org/pdf/2406.18678v1.pdf","comment":"preprint, 30 pages"},{"id":"http://arxiv.org/abs/2406.18676v1","updated":"2024-06-26T18:26:53Z","published":"2024-06-26T18:26:53Z","title":"Understand What LLM Needs: Dual Preference Alignment for\n  Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) has demonstrated effectiveness in\nmitigating the hallucination problem of large language models (LLMs). However,\nthe difficulty of aligning the retriever with the diverse LLMs' knowledge\npreferences inevitably poses an inevitable challenge in developing a reliable\nRAG system. To address this issue, we propose DPA-RAG, a universal framework\ndesigned to align diverse knowledge preferences within RAG systems.\nSpecifically, we initially introduce a preference knowledge construction\npipline and incorporate five novel query augmentation strategies to alleviate\npreference data scarcity. Based on preference data, DPA-RAG accomplishes both\nexternal and internal preference alignment: 1) It jointly integrate pair-wise,\npoint-wise, and contrastive preference alignment abilities into the reranker,\nachieving external preference alignment among RAG components. 2) It further\nintroduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),\nenabling LLMs to implicitly capture knowledge aligned with their reasoning\npreferences, achieving LLMs' internal alignment. Experimental results across\nfour knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all\nbaselines and seamlessly integrates both black-box and open-sourced LLM\nreaders. Further qualitative analysis and discussions also provide empirical\nguidance for achieving reliable RAG systems. Our code is publicly available at\nhttps://github.com/dongguanting/DPA-RAG.\n","authors":["Guanting Dong","Yutao Zhu","Chenghao Zhang","Zechen Wang","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.18676v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2406.18675v1","updated":"2024-06-26T18:25:06Z","published":"2024-06-26T18:25:06Z","title":"Human-AI Collaborative Taxonomy Construction: A Case Study in\n  Profession-Specific Writing Assistants","summary":"  Large Language Models (LLMs) have assisted humans in several writing tasks,\nincluding text revision and story generation. However, their effectiveness in\nsupporting domain-specific writing, particularly in business contexts, is\nrelatively less explored. Our formative study with industry professionals\nrevealed the limitations in current LLMs' understanding of the nuances in such\ndomain-specific writing. To address this gap, we propose an approach of\nhuman-AI collaborative taxonomy development to perform as a guideline for\ndomain-specific writing assistants. This method integrates iterative feedback\nfrom domain experts and multiple interactions between these experts and LLMs to\nrefine the taxonomy. Through larger-scale experiments, we aim to validate this\nmethodology and thus improve LLM-powered writing assistance, tailoring it to\nmeet the unique requirements of different stakeholder needs.\n","authors":["Minhwa Lee","Zae Myung Kim","Vivek A. Khetan","Dongyeop Kang"],"pdf_url":"https://arxiv.org/pdf/2406.18675v1.pdf","comment":"Accepted to CHI 2024 In2Writing Workshop"},{"id":"http://arxiv.org/abs/2406.18665v1","updated":"2024-06-26T18:10:22Z","published":"2024-06-26T18:10:22Z","title":"RouteLLM: Learning to Route LLMs with Preference Data","summary":"  Large language models (LLMs) exhibit impressive capabilities across a wide\nrange of tasks, yet the choice of which model to use often involves a trade-off\nbetween performance and cost. More powerful models, though effective, come with\nhigher expenses, while less capable models are more cost-effective. To address\nthis dilemma, we propose several efficient router models that dynamically\nselect between a stronger and a weaker LLM during inference, aiming to optimize\nthe balance between cost and response quality. We develop a training framework\nfor these routers leveraging human preference data and data augmentation\ntechniques to enhance performance. Our evaluation on widely-recognized\nbenchmarks shows that our approach significantly reduces costs-by over 2 times\nin certain cases-without compromising the quality of responses. Interestingly,\nour router models also demonstrate significant transfer learning capabilities,\nmaintaining their performance even when the strong and weak models are changed\nat test time. This highlights the potential of these routers to provide a\ncost-effective yet high-performance solution for deploying LLMs.\n","authors":["Isaac Ong","Amjad Almahairi","Vincent Wu","Wei-Lin Chiang","Tianhao Wu","Joseph E. Gonzalez","M Waleed Kadous","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2406.18665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18664v1","updated":"2024-06-26T18:09:46Z","published":"2024-06-26T18:09:46Z","title":"Evaluating Copyright Takedown Methods for Language Models","summary":"  Language models (LMs) derive their capabilities from extensive training on\ndiverse data, including potentially copyrighted material. These models can\nmemorize and generate content similar to their training data, posing potential\nconcerns. Therefore, model creators are motivated to develop mitigation methods\nthat prevent generating protected content. We term this procedure as copyright\ntakedowns for LMs, noting the conceptual similarity to (but legal distinction\nfrom) the DMCA takedown This paper introduces the first evaluation of the\nfeasibility and side effects of copyright takedowns for LMs. We propose\nCoTaEval, an evaluation framework to assess the effectiveness of copyright\ntakedown methods, the impact on the model's ability to retain uncopyrightable\nfactual knowledge from the training data whose recitation is embargoed, and how\nwell the model maintains its general utility and efficiency. We examine several\nstrategies, including adding system prompts, decoding-time filtering\ninterventions, and unlearning approaches. Our findings indicate that no tested\nmethod excels across all metrics, showing significant room for research in this\nunique problem setting and indicating potential unresolved challenges for live\npolicy proposals.\n","authors":["Boyi Wei","Weijia Shi","Yangsibo Huang","Noah A. Smith","Chiyuan Zhang","Luke Zettlemoyer","Kai Li","Peter Henderson"],"pdf_url":"https://arxiv.org/pdf/2406.18664v1.pdf","comment":"31 pages, 9 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.18629v1","updated":"2024-06-26T17:43:06Z","published":"2024-06-26T17:43:06Z","title":"Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of\n  LLMs","summary":"  Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs) due to the extensive and precise chain of reasoning required for\naccuracy. Ensuring the correctness of each reasoning step is critical. To\naddress this, we aim to enhance the robustness and factuality of LLMs by\nlearning from human feedback. However, Direct Preference Optimization (DPO) has\nshown limited benefits for long-chain mathematical reasoning, as models\nemploying DPO struggle to identify detailed errors in incorrect answers. This\nlimitation stems from a lack of fine-grained process supervision. We propose a\nsimple, effective, and data-efficient method called Step-DPO, which treats\nindividual reasoning steps as units for preference optimization rather than\nevaluating answers holistically. Additionally, we have developed a data\nconstruction pipeline for Step-DPO, enabling the creation of a high-quality\ndataset containing 10K step-wise preference pairs. We also observe that in DPO,\nself-generated data is more effective than data generated by humans or GPT-4,\ndue to the latter's out-of-distribution nature. Our findings demonstrate that\nas few as 10K preference data pairs and fewer than 500 Step-DPO training steps\ncan yield a nearly 3% gain in accuracy on MATH for models with over 70B\nparameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves\nscores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively,\nsurpassing a series of closed-source models, including GPT-4-1106,\nClaude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at\nhttps://github.com/dvlab-research/Step-DPO.\n","authors":["Xin Lai","Zhuotao Tian","Yukang Chen","Senqiao Yang","Xiangru Peng","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2406.18629v1.pdf","comment":"Code, data, and models are available at\n  https://github.com/dvlab-research/Step-DPO"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.18470v1","updated":"2024-06-26T16:28:24Z","published":"2024-06-26T16:28:24Z","title":"UniRec: A Dual Enhancement of Uniformity and Frequency in Sequential\n  Recommendations","summary":"  Representation learning in sequential recommendation is critical for\naccurately modeling user interaction patterns and improving recommendation\nprecision. However, existing approaches predominantly emphasize item-to-item\ntransitions, often neglecting the time intervals between interactions, which\nare closely related to behavior pattern changes. Additionally, broader\ninteraction attributes, such as item frequency, are frequently overlooked. We\nfound that both sequences with more uniform time intervals and items with\nhigher frequency yield better prediction performance. Conversely, non-uniform\nsequences exacerbate user interest drift and less-frequent items are difficult\nto model due to sparse sampling, presenting unique challenges inadequately\naddressed by current methods. In this paper, we propose UniRec, a novel\nbidirectional enhancement sequential recommendation method. UniRec leverages\nsequence uniformity and item frequency to enhance performance, particularly\nimproving the representation of non-uniform sequences and less-frequent items.\nThese two branches mutually reinforce each other, driving comprehensive\nperformance optimization in complex sequential recommendation scenarios.\nAdditionally, we present a multidimensional time module to further enhance\nadaptability. To the best of our knowledge, UniRec is the first method to\nutilize the characteristics of uniformity and frequency for feature\naugmentation. Comparing with eleven advanced models across four datasets, we\ndemonstrate that UniRec outperforms SOTA models significantly. The code is\navailable at https://github.com/Linxi000/UniRec.\n","authors":["Yang Liu","Yitong Wang","Chenyue Feng"],"pdf_url":"https://arxiv.org/pdf/2406.18470v1.pdf","comment":"15 pages, 8 figures, for source code, see\n  https://github.com/Linxi000/UniRec"},{"id":"http://arxiv.org/abs/2403.19302v2","updated":"2024-06-26T14:49:07Z","published":"2024-03-28T10:40:22Z","title":"Generate then Retrieve: Conversational Response Retrieval Using LLMs as\n  Answer and Query Generators","summary":"  CIS is a prominent area in IR which focuses on developing interactive\nknowledge assistants. These systems must adeptly comprehend the user's\ninformation requirements within the conversational context and retrieve the\nrelevant information. To this aim, the existing approaches model the user's\ninformation needs by generating a single query rewrite or a single\nrepresentation of the query in the query space embedding. However, to answer\ncomplex questions, a single query rewrite or representation is often\nineffective. To address this, a system needs to do reasoning over multiple\npassages. In this work, we propose using a generate-then-retrieve approach to\nimprove the passage retrieval performance for complex user queries. In this\napproach, we utilize large language models (LLMs) to (i) generate an initial\nanswer to the user's information need by doing reasoning over the context of\nthe conversation, and (ii) ground this answer to the collection. Based on the\nexperiments, our proposed approach significantly improves the retrieval\nperformance on TREC iKAT 23, TREC CAsT 20 and 22 datasets, under various\nsetups. Also, we show that grounding the LLM's answer requires more than one\nsearchable query, where an average of 3 queries outperforms human rewrites.\n","authors":["Zahra Abbasiantaeb","Mohammad Aliannejadi"],"pdf_url":"https://arxiv.org/pdf/2403.19302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18320v1","updated":"2024-06-26T13:01:52Z","published":"2024-06-26T13:01:52Z","title":"The Effects of Data Split Strategies on the Offline Experiments for CTR\n  Prediction","summary":"  Click-through rate (CTR) prediction is a crucial task in online advertising\nto recommend products that users are likely to be interested in. To identify\nthe best-performing models, rigorous model evaluation is necessary. Offline\nexperimentation plays a significant role in selecting models for live user-item\ninteractions, despite the value of online experimentation like A/B testing,\nwhich has its own limitations and risks. Often, the correlation between offline\nperformance metrics and actual online model performance is inadequate. One main\nreason for this discrepancy is the common practice of using random splits to\ncreate training, validation, and test datasets in CTR prediction. In contrast,\nreal-world CTR prediction follows a temporal order. Therefore, the methodology\nused in offline evaluation, particularly the data splitting strategy, is\ncrucial. This study aims to address the inconsistency between current offline\nevaluation methods and real-world use cases, by focusing on data splitting\nstrategies. To examine the impact of different data split strategies on offline\nperformance, we conduct extensive experiments using both random and temporal\nsplits on a large open benchmark dataset, Criteo.\n","authors":["Ramazan Tarik Turksoy","Beyza Turkmen"],"pdf_url":"https://arxiv.org/pdf/2406.18320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05666v4","updated":"2024-06-26T12:32:28Z","published":"2024-06-09T06:49:22Z","title":"General Distribution Learning: A theoretical framework for Deep Learning","summary":"  There remain numerous unanswered research questions on deep learning (DL)\nwithin the classical learning theory framework. These include the remarkable\ngeneralization capabilities of overparametrized neural networks (NNs), the\nefficient optimization performance despite non-convexity of objectives, the\nmechanism of flat minima for generalization, and the exceptional performance of\ndeep architectures in solving physical problems. This paper introduces General\nDistribution Learning (GD Learning), a novel theoretical learning framework\ndesigned to address a comprehensive range of machine learning and statistical\ntasks, including classification, regression and parameter estimation. Departing\nfrom traditional statistical machine learning, GD Learning focuses on the true\nunderlying distribution. In GD Learning, learning error, corresponding to the\nexpected error in classical statistical learning framework, is divided into\nfitting errors due to models and algorithms, as well as sampling errors\nintroduced by limited sampling data. The framework significantly incorporates\nprior knowledge, especially in scenarios characterized by data scarcity,\nthereby enhancing performance. Within the GD Learning framework, we demonstrate\nthat the global optimal solutions in non-convex optimization can be approached\nby minimizing the gradient norm and the non-uniformity of the eigenvalues of\nthe model's Jacobian matrix. This insight leads to the development of the\ngradient structure control algorithm. GD Learning also offers fresh insights\ninto the questions on deep learning, including overparameterization and\nnon-convex optimization, bias-variance trade-off, and the mechanism of flat\nminima.\n","authors":["Binchuan Qi","Li Li","Wei Gong"],"pdf_url":"https://arxiv.org/pdf/2406.05666v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2405.20204v2","updated":"2024-06-26T12:31:48Z","published":"2024-05-30T16:07:54Z","title":"Jina CLIP: Your CLIP Model Is Also Your Text Retriever","summary":"  Contrastive Language-Image Pretraining (CLIP) is widely used to train models\nto align images and texts in a common embedding space by mapping them to\nfixed-sized vectors. These models are key to multimodal information retrieval\nand related tasks. However, CLIP models generally underperform in text-only\ntasks compared to specialized text models. This creates inefficiencies for\ninformation retrieval systems that keep separate embeddings and models for\ntext-only and multimodal tasks. We propose a novel, multi-task contrastive\ntraining method to address this issue, which we use to train the jina-clip-v1\nmodel to achieve the state-of-the-art performance on both text-image and\ntext-text retrieval tasks.\n","authors":["Andreas Koukounas","Georgios Mastrapas","Michael Günther","Bo Wang","Scott Martens","Isabelle Mohr","Saba Sturua","Mohammad Kalim Akram","Joan Fontanals Martínez","Saahil Ognawala","Susana Guzman","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2405.20204v2.pdf","comment":"4 pages, MFM-EAI@ICML2024"},{"id":"http://arxiv.org/abs/2406.18286v1","updated":"2024-06-26T12:14:10Z","published":"2024-06-26T12:14:10Z","title":"Effects of Using Synthetic Data on Deep Recommender Models' Performance","summary":"  Recommender systems are essential for enhancing user experiences by\nsuggesting items based on individual preferences. However, these systems\nfrequently face the challenge of data imbalance, characterized by a\npredominance of negative interactions over positive ones. This imbalance can\nresult in biased recommendations favoring popular items. This study\ninvestigates the effectiveness of synthetic data generation in addressing data\nimbalances within recommender systems. Six different methods were used to\ngenerate synthetic data. Our experimental approach involved generating\nsynthetic data using these methods and integrating the generated samples into\nthe original dataset. Our results show that the inclusion of generated negative\nsamples consistently improves the Area Under the Curve (AUC) scores. The\nsignificant impact of synthetic negative samples highlights the potential of\ndata augmentation strategies to address issues of data sparsity and imbalance,\nultimately leading to improved performance of recommender systems.\n","authors":["Fatih Cihan Taskin","Ilknur Akcay","Muhammed Pesen","Said Aldemir","Ipek Iraz Esin","Furkan Durmus"],"pdf_url":"https://arxiv.org/pdf/2406.18286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18254v1","updated":"2024-06-26T11:04:25Z","published":"2024-06-26T11:04:25Z","title":"Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with\n  1-to-K Contrastive Learning","summary":"  Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search,\nwhich aims to break the barriers between modality and language simultaneously\nand achieves image-text retrieval in the multi-lingual scenario with a single\nmodel. In recent years, excellent progress has been made based on cross-lingual\ncross-modal pre-training; particularly, the methods based on contrastive\nlearning on large-scale data have significantly improved retrieval tasks.\nHowever, these methods directly follow the existing pre-training methods in the\ncross-lingual or cross-modal domain, leading to two problems of inconsistency\nin CCR: The methods with cross-lingual style suffer from the intra-modal error\npropagation, resulting in inconsistent recall performance across languages in\nthe whole dataset. The methods with cross-modal style suffer from the\ninter-modal optimization direction bias, resulting in inconsistent rank across\nlanguages within each instance, which cannot be reflected by Recall@K. To solve\nthese problems, we propose a simple but effective 1-to-K contrastive learning\nmethod, which treats each language equally and eliminates error propagation and\noptimization bias. In addition, we propose a new evaluation metric, Mean Rank\nVariance (MRV), to reflect the rank inconsistency across languages within each\ninstance. Extensive experiments on four CCR datasets show that our method\nimproves both recall rates and MRV with smaller-scale pre-trained data,\nachieving the new state-of-art.\n","authors":["Zhijie Nie","Richong Zhang","Zhangchi Feng","Hailang Huang","Xudong Liu"],"pdf_url":"https://arxiv.org/pdf/2406.18254v1.pdf","comment":"Accepted by KDD 2024 Research Track"},{"id":"http://arxiv.org/abs/2406.18240v1","updated":"2024-06-26T10:44:48Z","published":"2024-06-26T10:44:48Z","title":"Concordance in basal cell carcinoma diagnosis. Building a proper ground\n  truth to train Artificial Intelligence tools","summary":"  Background: The existence of different basal cell carcinoma (BCC) clinical\ncriteria cannot be objectively validated. An adequate ground-truth is needed to\ntrain an artificial intelligence (AI) tool that explains the BCC diagnosis by\nproviding its dermoscopic features. Objectives: To determine the consensus\namong dermatologists on dermoscopic criteria of 204 BCC. To analyze the\nperformance of an AI tool when the ground-truth is inferred. Methods: A single\ncenter, diagnostic and prospective study was conducted to analyze the agreement\nin dermoscopic criteria by four dermatologists and then derive a reference\nstandard. 1434 dermoscopic images have been used, that were taken by a primary\nhealth physician, sent via teledermatology, and diagnosed by a dermatologist.\nThey were randomly selected from the teledermatology platform (2019-2021). 204\nof them were tested with an AI tool; the remainder trained it. The performance\nof the AI tool trained using the ground-truth of one dermatologist versus the\nground-truth statistically inferred from the consensus of four dermatologists\nwas analyzed using McNemar's test and Hamming distance. Results: Dermatologists\nachieve perfect agreement in the diagnosis of BCC (Fleiss-Kappa=0.9079), and a\nhigh correlation with the biopsy (PPV=0.9670). However, there is low agreement\nin detecting some dermoscopic criteria. Statistical differences were found in\nthe performance of the AI tool trained using the ground-truth of one\ndermatologist versus the ground-truth statistically inferred from the consensus\nof four dermatologists. Conclusions: Care should be taken when training an AI\ntool to determine the BCC patterns present in a lesion. Ground-truth should be\nestablished from multiple dermatologists.\n","authors":["Francisca Silva-Clavería","Carmen Serrano","Iván Matas","Amalia Serrano","Tomás Toledo-Pastrana","David Moreno-Ramírez","Begoña Acha"],"pdf_url":"https://arxiv.org/pdf/2406.18240v1.pdf","comment":"Manuscript word count: 3000, Number of figures: 2, Number of tables:\n  3"},{"id":"http://arxiv.org/abs/2310.09234v5","updated":"2024-06-26T08:59:47Z","published":"2023-10-13T16:37:53Z","title":"ClickPrompt: CTR Models are Strong Prompt Generators for Adapting\n  Language Models to CTR Prediction","summary":"  Click-through rate (CTR) prediction has become increasingly indispensable for\nvarious Internet applications. Traditional CTR models convert the multi-field\ncategorical data into ID features via one-hot encoding, and extract the\ncollaborative signals among features. Such a paradigm suffers from the problem\nof semantic information loss. Another line of research explores the potential\nof pretrained language models (PLMs) for CTR prediction by converting input\ndata into textual sentences through hard prompt templates. Although semantic\nsignals are preserved, they generally fail to capture the collaborative\ninformation (e.g., feature interactions, pure ID features), not to mention the\nunacceptable inference overhead brought by the huge model size. In this paper,\nwe aim to model both the semantic knowledge and collaborative knowledge for\naccurate CTR estimation, and meanwhile address the inference inefficiency\nissue. To benefit from both worlds and close their gaps, we propose a novel\nmodel-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models\nto generate interaction-aware soft prompts for PLMs. We design a\nprompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM\nhas to recover the masked tokens based on the language context, as well as the\nsoft prompts generated by CTR model. The collaborative and semantic knowledge\nfrom ID and textual features would be explicitly aligned and interacted via the\nprompt interface. Then, we can either tune the CTR model with PLM for superior\nperformance, or solely tune the CTR model without PLM for inference efficiency.\nExperiments on four real-world datasets validate the effectiveness of\nClickPrompt compared with existing baselines.\n","authors":["Jianghao Lin","Bo Chen","Hangyu Wang","Yunjia Xi","Yanru Qu","Xinyi Dai","Kangning Zhang","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.09234v5.pdf","comment":"Accepted by WWW 2024"},{"id":"http://arxiv.org/abs/2406.18114v1","updated":"2024-06-26T07:02:49Z","published":"2024-06-26T07:02:49Z","title":"Knowledge Graph Enhanced Retrieval-Augmented Generation for Failure Mode\n  and Effects Analysis","summary":"  Failure mode and effects analysis (FMEA) is a critical tool for mitigating\npotential failures, particular during ramp-up phases of new products. However,\nits effectiveness is often limited by the missing reasoning capabilities of the\nFMEA tools, which are usually tabular structured. Meanwhile, large language\nmodels (LLMs) offer novel prospects for fine-tuning on custom datasets for\nreasoning within FMEA contexts. However, LLMs face challenges in tasks that\nrequire factual knowledge, a gap that retrieval-augmented generation (RAG)\napproaches aim to fill. RAG retrieves information from a non-parametric data\nstore and uses a language model to generate responses. Building on this idea,\nwe propose to advance the non-parametric data store with a knowledge graph\n(KG). By enhancing the RAG framework with a KG, our objective is to leverage\nanalytical and semantic question-answering capabilities on FMEA data. This\npaper contributes by presenting a new ontology for FMEA observations, an\nalgorithm for creating vector embeddings from the FMEA KG, and a KG enhanced\nRAG framework. Our approach is validated through a human study and we measure\nthe performance of the context retrieval recall and precision.\n","authors":["Lukas Bahr","Christoph Wehner","Judith Wewerka","José Bittencourt","Ute Schmid","Rüdiger Daub"],"pdf_url":"https://arxiv.org/pdf/2406.18114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19046v4","updated":"2024-06-26T00:21:43Z","published":"2024-05-29T12:43:39Z","title":"Continual Collaborative Distillation for Recommender System","summary":"  Knowledge distillation (KD) has emerged as a promising technique for\naddressing the computational challenges associated with deploying large-scale\nrecommender systems. KD transfers the knowledge of a massive teacher system to\na compact student model, to reduce the huge computational burdens for inference\nwhile retaining high accuracy. The existing KD studies primarily focus on\none-time distillation in static environments, leaving a substantial gap in\ntheir applicability to real-world scenarios dealing with continuously incoming\nusers, items, and their interactions. In this work, we delve into a systematic\napproach to operating the teacher-student KD in a non-stationary data stream.\nOur goal is to enable efficient deployment through a compact student, which\npreserves the high performance of the massive teacher, while effectively\nadapting to continuously incoming data. We propose Continual Collaborative\nDistillation (CCD) framework, where both the teacher and the student\ncontinually and collaboratively evolve along the data stream. CCD facilitates\nthe student in effectively adapting to new data, while also enabling the\nteacher to fully leverage accumulated knowledge. We validate the effectiveness\nof CCD through extensive quantitative, ablative, and exploratory experiments on\ntwo real-world datasets. We expect this research direction to contribute to\nnarrowing the gap between existing KD studies and practical applications,\nthereby enhancing the applicability of KD in real-world systems.\n","authors":["Gyuseok Lee","SeongKu Kang","Wonbin Kweon","Hwanjo Yu"],"pdf_url":"https://arxiv.org/pdf/2405.19046v4.pdf","comment":"Accepted by KDD 2024 research track. 9 main pages + 1 appendix page,\n  5 figures"},{"id":"http://arxiv.org/abs/2406.15241v2","updated":"2024-06-26T22:04:30Z","published":"2024-06-21T15:28:50Z","title":"Retrieval Augmented Zero-Shot Text Classification","summary":"  Zero-shot text learning enables text classifiers to handle unseen classes\nefficiently, alleviating the need for task-specific training data. A simple\napproach often relies on comparing embeddings of query (text) to those of\npotential classes. However, the embeddings of a simple query sometimes lack\nrich contextual information, which hinders the classification performance.\nTraditionally, this has been addressed by improving the embedding model with\nexpensive training. We introduce QZero, a novel training-free knowledge\naugmentation approach that reformulates queries by retrieving supporting\ncategories from Wikipedia to improve zero-shot text classification performance.\nOur experiments across six diverse datasets demonstrate that QZero enhances\nperformance for state-of-the-art static and contextual embedding models without\nthe need for retraining. Notably, in News and medical topic classification\ntasks, QZero improves the performance of even the largest OpenAI embedding\nmodel by at least 5% and 3%, respectively. Acting as a knowledge amplifier,\nQZero enables small word embedding models to achieve performance levels\ncomparable to those of larger contextual models, offering the potential for\nsignificant computational savings. Additionally, QZero offers meaningful\ninsights that illuminate query context and verify topic relevance, aiding in\nunderstanding model predictions. Overall, QZero improves embedding-based\nzero-shot classifiers while maintaining their simplicity. This makes it\nparticularly valuable for resource-constrained environments and domains with\nconstantly evolving information.\n","authors":["Tassallah Abdullahi","Ritambhara Singh","Carsten Eickhoff"],"pdf_url":"https://arxiv.org/pdf/2406.15241v2.pdf","comment":"Proceedings of the 2024 ACM SIGIR International Conference on the\n  Theory of Information Retrieval (ICTIR '24), July 13, 2024, Washington DC,\n  DC, USA"},{"id":"http://arxiv.org/abs/2406.18747v1","updated":"2024-06-26T20:25:53Z","published":"2024-06-26T20:25:53Z","title":"A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond\n  Four Stems","summary":"  Despite significant recent progress across multiple subtasks of audio source\nseparation, few music source separation systems support separation beyond the\nfour-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current\nsystems that support source separation beyond this setup, most continue to rely\non an inflexible decoder setup that can only support a fixed pre-defined set of\nstems. Increasing stem support in these inflexible systems correspondingly\nrequires increasing computational complexity, rendering extensions of these\nsystems computationally infeasible for long-tail instruments. In this work, we\npropose Banquet, a system that allows source separation of multiple stems using\njust one decoder. A bandsplit source separation model is extended to work in a\nquery-based setup in tandem with a music instrument recognition PaSST model. On\nthe MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached\nthe performance level of the significantly more complex 6-stem Hybrid\nTransformer Demucs on VDBO stems and outperformed it on guitar and piano. The\nquery-based setup allows for the separation of narrow instrument classes such\nas clean acoustic guitars, and can be successfully applied to the extraction of\nless common stems such as reeds and organs. Implementation is available at\nhttps://github.com/kwatcharasupat/query-bandit.\n","authors":["Karn N. Watcharasupat","Alexander Lerch"],"pdf_url":"https://arxiv.org/pdf/2406.18747v1.pdf","comment":"Submitted to the 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)"},{"id":"http://arxiv.org/abs/2406.18740v1","updated":"2024-06-26T20:12:24Z","published":"2024-06-26T20:12:24Z","title":"Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with\n  Large Language Models","summary":"  Large Language Models (LLMs) have been revolutionizing a myriad of natural\nlanguage processing tasks with their diverse zero-shot capabilities. Indeed,\nexisting work has shown that LLMs can be used to great effect for many tasks,\nsuch as information retrieval (IR), and passage ranking. However, current\nstate-of-the-art results heavily lean on the capabilities of the LLM being\nused. Currently, proprietary, and very large LLMs such as GPT-4 are the highest\nperforming passage re-rankers. Hence, users without the resources to leverage\ntop of the line LLMs, or ones that are closed source, are at a disadvantage. In\nthis paper, we investigate the use of a pre-filtering step before passage\nre-ranking in IR. Our experiments show that by using a small number of human\ngenerated relevance scores, coupled with LLM relevance scoring, it is\neffectively possible to filter out irrelevant passages before re-ranking. Our\nexperiments also show that this pre-filtering then allows the LLM to perform\nsignificantly better at the re-ranking task. Indeed, our results show that\nsmaller models such as Mixtral can become competitive with much larger\nproprietary models (e.g., ChatGPT and GPT-4).\n","authors":["Baharan Nouriinanloo","Maxime Lamothe"],"pdf_url":"https://arxiv.org/pdf/2406.18740v1.pdf","comment":null}]},"2024-06-27T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.19388v1","updated":"2024-06-27T17:58:54Z","published":"2024-06-27T17:58:54Z","title":"Taming Data and Transformers for Audio Generation","summary":"  Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Guha Balakrishnan","Sergey Tulyakov","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.19388v1.pdf","comment":"Project Webpage: https://snap-research.github.io/GenAU/"},{"id":"http://arxiv.org/abs/2406.19384v1","updated":"2024-06-27T17:57:03Z","published":"2024-06-27T17:57:03Z","title":"The Remarkable Robustness of LLMs: Stages of Inference?","summary":"  We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.\n","authors":["Vedang Lad","Wes Gurnee","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2406.19384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19371v1","updated":"2024-06-27T17:50:35Z","published":"2024-06-27T17:50:35Z","title":"Suri: Multi-constraint Instruction Following for Long-form Text\n  Generation","summary":"  Existing research on instruction following largely focuses on tasks with\nsimple instructions and short responses. In this work, we explore\nmulti-constraint instruction following for generating long-form text. We create\nSuri, a dataset with 20K human-written long-form texts paired with\nLLM-generated backtranslated instructions that contain multiple complex\nconstraints. Because of prohibitive challenges associated with collecting human\npreference judgments on long-form texts, preference-tuning algorithms such as\nDPO are infeasible in our setting; thus, we propose Instructional ORPO\n(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving\nnegative feedback from dispreferred responses, I-ORPO obtains negative feedback\nfrom synthetically corrupted instructions generated by an LLM. Using Suri, we\nperform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The\nresulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts\n(~5K tokens) than base models without significant quality deterioration. Our\nhuman evaluation shows that while both SFT and I-ORPO models satisfy most\nconstraints, Suri-I-ORPO generations are generally preferred for their coherent\nand informative incorporation of the constraints. We release our code at\nhttps://github.com/chtmp223/suri.\n","authors":["Chau Minh Pham","Simeng Sun","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2406.19371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19358v1","updated":"2024-06-27T17:38:45Z","published":"2024-06-27T17:38:45Z","title":"The Model Arena for Cross-lingual Sentiment Analysis: A Comparative\n  Study in the Era of Large Language Models","summary":"  Sentiment analysis serves as a pivotal component in Natural Language\nProcessing (NLP). Advancements in multilingual pre-trained models such as XLM-R\nand mT5 have contributed to the increasing interest in cross-lingual sentiment\nanalysis. The recent emergence in Large Language Models (LLM) has significantly\nadvanced general NLP tasks, however, the capability of such LLMs in\ncross-lingual sentiment analysis has not been fully studied. This work\nundertakes an empirical analysis to compare the cross-lingual transfer\ncapability of public Small Multilingual Language Models (SMLM) like XLM-R,\nagainst English-centric LLMs such as Llama-3, in the context of sentiment\nanalysis across English, Spanish, French and Chinese. Our findings reveal that\namong public models, SMLMs exhibit superior zero-shot cross-lingual performance\nrelative to LLMs. However, in few-shot cross-lingual settings, public LLMs\ndemonstrate an enhanced adaptive potential. In addition, we observe that\nproprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but\nare outpaced by public models in few-shot scenarios.\n","authors":["Xiliang Zhu","Shayna Gardiner","Tere Roldán","David Rossouw"],"pdf_url":"https://arxiv.org/pdf/2406.19358v1.pdf","comment":"Accepted to WASSA workshop at ACL2024"},{"id":"http://arxiv.org/abs/2406.19356v1","updated":"2024-06-27T17:37:31Z","published":"2024-06-27T17:37:31Z","title":"DiVERT: Distractor Generation with Variational Errors Represented as\n  Text for Math Multiple-choice Questions","summary":"  High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.\n","authors":["Nigel Fernandez","Alexander Scarlatos","Simon Woodhead","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2406.19356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19354v1","updated":"2024-06-27T17:33:03Z","published":"2024-06-27T17:33:03Z","title":"Fundamental Problems With Model Editing: How Should Rational Belief\n  Revision Work in LLMs?","summary":"  The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision\n","authors":["Peter Hase","Thomas Hofweber","Xiang Zhou","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2406.19354v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.19349v1","updated":"2024-06-27T17:26:38Z","published":"2024-06-27T17:26:38Z","title":"IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and\n  Toxicity Types for Indonesian Language","summary":"  Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.\n","authors":["Lucky Susanto","Musa Izzanardi Wijanarko","Prasetia Anugrah Pratama","Traci Hong","Ika Idris","Alham Fikri Aji","Derry Wijaya"],"pdf_url":"https://arxiv.org/pdf/2406.19349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05162v2","updated":"2024-06-27T17:23:58Z","published":"2024-02-07T18:34:38Z","title":"Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank\n  Modifications","summary":"  Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.\n","authors":["Boyi Wei","Kaixuan Huang","Yangsibo Huang","Tinghao Xie","Xiangyu Qi","Mengzhou Xia","Prateek Mittal","Mengdi Wang","Peter Henderson"],"pdf_url":"https://arxiv.org/pdf/2402.05162v2.pdf","comment":"22 pages, 9 figures. Project page is available at\n  https://boyiwei.com/alignment-attribution/"},{"id":"http://arxiv.org/abs/2406.13444v2","updated":"2024-06-27T17:09:24Z","published":"2024-06-19T11:09:16Z","title":"VDebugger: Harnessing Execution Feedback for Debugging Visual Programs","summary":"  Visual programs are executable code generated by large language models to\naddress visual reasoning problems. They decompose complex questions into\nmultiple reasoning steps and invoke specialized models for each step to solve\nthe problems. However, these programs are prone to logic errors, with our\npreliminary evaluation showing that 58% of the total errors are caused by\nprogram logic errors. Debugging complex visual programs remains a major\nbottleneck for visual reasoning. To address this, we introduce VDebugger, a\nnovel critic-refiner framework trained to localize and debug visual programs by\ntracking execution step by step. VDebugger identifies and corrects program\nerrors leveraging detailed execution feedback, improving interpretability and\naccuracy. The training data is generated through an automated pipeline that\ninjects errors into correct visual programs using a novel mask-best decoding\ntechnique. Evaluations on six datasets demonstrate VDebugger's effectiveness,\nshowing performance improvements of up to 3.2% in downstream task accuracy.\nFurther studies show VDebugger's ability to generalize to unseen tasks,\nbringing a notable improvement of 2.3% on the unseen COVR task. Code, data and\nmodels are made publicly available at https://github.com/shirley-wu/vdebugger/\n","authors":["Xueqing Wu","Zongyu Lin","Songyan Zhao","Te-Lin Wu","Pan Lu","Nanyun Peng","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2406.13444v2.pdf","comment":"update reference"},{"id":"http://arxiv.org/abs/2406.12373v2","updated":"2024-06-27T16:56:13Z","published":"2024-06-18T07:58:33Z","title":"WebCanvas: Benchmarking Web Agents in Online Environments","summary":"  For web agents to be practically useful, they must adapt to the continuously\nevolving web environment characterized by frequent updates to user interfaces\nand content. However, most existing benchmarks only capture the static aspects\nof the web. To bridge this gap, we introduce WebCanvas, an innovative online\nevaluation framework for web agents that effectively addresses the dynamic\nnature of web interactions. WebCanvas contains three main components to\nfacilitate realistic assessments: (1) A novel evaluation metric which reliably\ncapture critical intermediate actions or states necessary for task completions\nwhile disregarding noise caused by insignificant events or changed\nweb-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version\nof original Mind2Web static dataset containing 542 tasks with 2439 intermediate\nevaluation states; (3) Lightweight and generalizable annotation tools and\ntesting pipelines that enables the community to collect and maintain the\nhigh-quality, up-to-date dataset. Building on WebCanvas, we open-source an\nagent framework with extensible modules for reasoning, providing a foundation\nfor the community to conduct online inference and evaluations. Our\nbest-performing agent achieves a task success rate of 23.1% and a task\ncompletion rate of 48.8% on the Mind2Web-Live test set. Additionally, we\nanalyze the performance discrepancies across various websites, domains, and\nexperimental environments. We encourage the community to contribute further\ninsights on online agent evaluation, thereby advancing this field of research.\n","authors":["Yichen Pan","Dehan Kong","Sida Zhou","Cheng Cui","Yifei Leng","Bing Jiang","Hangyu Liu","Yanyi Shang","Shuyan Zhou","Tongshuang Wu","Zhengyang Wu"],"pdf_url":"https://arxiv.org/pdf/2406.12373v2.pdf","comment":"Our platform, tool and dataset are publically available at\n  https://www.imean.ai/web-canvas/ and\n  https://huggingface.co/datasets/iMeanAI/Mind2Web-Live/"},{"id":"http://arxiv.org/abs/2406.19317v1","updated":"2024-06-27T16:52:19Z","published":"2024-06-27T16:52:19Z","title":"Jump Starting Bandits with LLM-Generated Prior Knowledge","summary":"  We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.\n","authors":["Parand A. Alamdari","Yanshuai Cao","Kevin H. Wilson"],"pdf_url":"https://arxiv.org/pdf/2406.19317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19314v1","updated":"2024-06-27T16:47:42Z","published":"2024-06-27T16:47:42Z","title":"LiveBench: A Challenging, Contamination-Free LLM Benchmark","summary":"  Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.\n","authors":["Colin White","Samuel Dooley","Manley Roberts","Arka Pal","Ben Feuer","Siddhartha Jain","Ravid Shwartz-Ziv","Neel Jain","Khalid Saifullah","Siddartha Naidu","Chinmay Hegde","Yann LeCun","Tom Goldstein","Willie Neiswanger","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2406.19314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07610v3","updated":"2024-06-27T16:38:35Z","published":"2024-02-12T12:30:42Z","title":"Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping","summary":"  Self-alignment is an effective way to reduce the cost of human annotation\nwhile ensuring promising model capability. However, most current methods\ncomplete the data collection and training steps in a single round, which may\noverlook the continuously improving ability of self-aligned models. This gives\nrise to a key query: What if we do multi-time bootstrapping self-alignment?\nDoes this strategy enhance model performance or lead to rapid degradation? In\nthis paper, our pioneering exploration delves into the impact of bootstrapping\nself-alignment on large language models. Our findings reveal that bootstrapping\nself-alignment markedly surpasses the single-round approach, by guaranteeing\ndata diversity from in-context learning. To further exploit the capabilities of\nbootstrapping, we investigate and adjust the training order of data, which\nyields improved performance of the model. Drawing on these findings, we propose\nStep-On-Feet Tuning (SOFT) which leverages model's continuously enhanced\nfew-shot ability to boost zero or one-shot performance. Based on easy-to-hard\ntraining recipe, we propose SOFT+ which further boost self-alignment's\nperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across\nvarious classification and generation tasks, highlighting the potential of\nbootstrapping self-alignment on continually enhancing model alignment\nperformance.\n","authors":["Haoyu Wang","Guozheng Ma","Ziqiao Meng","Zeyu Qin","Li Shen","Zhong Zhang","Bingzhe Wu","Liu Liu","Yatao Bian","Tingyang Xu","Xueqian Wang","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.07610v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19307v1","updated":"2024-06-27T16:30:50Z","published":"2024-06-27T16:30:50Z","title":"The Odyssey of Commonsense Causality: From Foundational Benchmarks to\n  Cutting-Edge Reasoning","summary":"  Understanding commonsense causality is a unique mark of intelligence for\nhumans. It helps people understand the principles of the real world better and\nbenefits the decision-making process related to causation. For instance,\ncommonsense causality is crucial in judging whether a defendant's action causes\nthe plaintiff's loss in determining legal liability. Despite its significance,\na systematic exploration of this topic is notably lacking. Our comprehensive\nsurvey bridges this gap by focusing on taxonomies, benchmarks, acquisition\nmethods, qualitative reasoning, and quantitative measurements in commonsense\ncausality, synthesizing insights from over 200 representative articles. Our\nwork aims to provide a systematic overview, update scholars on recent\nadvancements, provide a pragmatic guide for beginners, and highlight promising\nfuture research directions in this vital field.\n","authors":["Shaobo Cui","Zhijing Jin","Bernhard Schölkopf","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2406.19307v1.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2403.08819v2","updated":"2024-06-27T16:30:32Z","published":"2024-02-20T04:13:48Z","title":"Thermometer: Towards Universal Calibration for Large Language Models","summary":"  We consider the issue of calibration in large language models (LLM). Recent\nstudies have found that common interventions such as instruction tuning often\nresult in poorly calibrated LLMs. Although calibration is well-explored in\ntraditional applications, calibrating LLMs is uniquely challenging. These\nchallenges stem as much from the severe computational requirements of LLMs as\nfrom their versatility, which allows them to be applied to diverse tasks.\nAddressing these challenges, we propose THERMOMETER, a calibration approach\ntailored to LLMs. THERMOMETER learns an auxiliary model, given data from\nmultiple tasks, for calibrating a LLM. It is computationally efficient,\npreserves the accuracy of the LLM, and produces better-calibrated responses for\nnew tasks. Extensive empirical evaluations across various benchmarks\ndemonstrate the effectiveness of the proposed method.\n","authors":["Maohao Shen","Subhro Das","Kristjan Greenewald","Prasanna Sattigeri","Gregory Wornell","Soumya Ghosh"],"pdf_url":"https://arxiv.org/pdf/2403.08819v2.pdf","comment":"Camera ready version for ICML 2024"},{"id":"http://arxiv.org/abs/2401.05060v2","updated":"2024-06-27T16:05:35Z","published":"2024-01-10T10:37:45Z","title":"MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot\n  Detector","summary":"  Research in toxicity detection in natural language processing for the speech\nmodality (audio-based) is quite limited, particularly for languages other than\nEnglish. To address these limitations and lay the groundwork for truly\nmultilingual audio-based toxicity detection, we introduce MuTox, the first\nhighly multilingual audio-based dataset with toxicity labels. The dataset\ncomprises 20,000 audio utterances for English and Spanish, and 4,000 for the\nother 19 languages. To demonstrate the quality of this dataset, we trained the\nMuTox audio-based toxicity classifier, which enables zero-shot toxicity\ndetection across a wide range of languages. This classifier outperforms\nexisting text-based trainable classifiers by more than 1% AUC, while expanding\nthe language coverage more than tenfold. When compared to a wordlist-based\nclassifier that covers a similar number of languages, MuTox improves precision\nand recall by approximately 2.5 times. This significant improvement underscores\nthe potential of MuTox in advancing the field of audio-based toxicity\ndetection.\n","authors":["Marta R. Costa-jussà","Mariano Coria Meglioli","Pierre Andrews","David Dale","Prangthip Hansanti","Elahe Kalbassi","Alex Mourachko","Christophe Ropers","Carleigh Wood"],"pdf_url":"https://arxiv.org/pdf/2401.05060v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19292v1","updated":"2024-06-27T16:05:13Z","published":"2024-06-27T16:05:13Z","title":"From Artificial Needles to Real Haystacks: Improving Retrieval\n  Capabilities in LLMs by Finetuning on Synthetic Data","summary":"  Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.\n","authors":["Zheyang Xiong","Vasilis Papageorgiou","Kangwook Lee","Dimitris Papailiopoulos"],"pdf_url":"https://arxiv.org/pdf/2406.19292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11385v2","updated":"2024-06-27T16:01:28Z","published":"2024-06-17T10:12:45Z","title":"MetaGPT: Merging Large Language Models Using Model Exclusive Task\n  Arithmetic","summary":"  The advent of large language models (LLMs) like GPT-4 has catalyzed the\nexploration of multi-task learning (MTL), in which a single model demonstrates\nproficiency across diverse tasks. Task arithmetic has emerged as a\ncost-effective approach for MTL. It enables performance enhancement across\nmultiple tasks by adding their corresponding task vectors to a pre-trained\nmodel. However, the current lack of a method that can simultaneously achieve\noptimal performance, computational efficiency, and data privacy limits their\napplication to LLMs. In this paper, we propose \\textbf{M}odel\n\\textbf{E}xclusive \\textbf{T}ask \\textbf{A}rithmetic for merging\n\\textbf{GPT}-scale models, which formalizes the objective of model merging into\na multi-task learning framework, aiming to minimize the average loss difference\nbetween the merged model and each individual task model. Since data privacy\nlimits the use of multi-task training data, we leverage LLMs' local linearity\nand task vectors' orthogonality to separate the data term and scaling\ncoefficients term and derive a model-exclusive task arithmetic method. Our\nproposed MetaGPT is data-agnostic and bypasses the heavy search process, making\nit cost-effective and easy to implement for LLMs.Extensive experiments\ndemonstrate that MetaGPT leads to improvements in task arithmetic and achieves\nstate-of-the-art performance on multiple tasks.\n","authors":["Yuyan Zhou","Liang Song","Bingning Wang","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.11385v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2406.17186v2","updated":"2024-06-27T15:55:57Z","published":"2024-06-24T23:57:57Z","title":"CLERC: A Dataset for Legal Case Retrieval and Retrieval-Augmented\n  Analysis Generation","summary":"  Legal professionals need to write analyses that rely on citations to relevant\nprecedents, i.e., previous case decisions. Intelligent systems assisting legal\nprofessionals in writing such documents provide great benefits but are\nchallenging to design. Such systems need to help locate, summarize, and reason\nover salient precedents in order to be useful. To enable systems for such\ntasks, we work with legal professionals to transform a large open-source legal\ncorpus into a dataset supporting two important backbone tasks: information\nretrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC\n(Case Law Evaluation Retrieval Corpus), is constructed for training and\nevaluating models on their ability to (1) find corresponding citations for a\ngiven piece of legal analysis and to (2) compile the text of these citations\n(as well as previous context) into a cogent analysis that supports a reasoning\ngoal. We benchmark state-of-the-art models on CLERC, showing that current\napproaches still struggle: GPT-4o generates analyses with the highest ROUGE\nF-scores but hallucinates the most, while zero-shot IR models only achieve\n48.3% recall@1000.\n","authors":["Abe Bohan Hou","Orion Weller","Guanghui Qin","Eugene Yang","Dawn Lawrie","Nils Holzenberger","Andrew Blair-Stanek","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2406.17186v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07683v3","updated":"2024-06-27T15:54:58Z","published":"2023-09-14T12:58:30Z","title":"Assessing the nature of large language models: A caution against\n  anthropocentrism","summary":"  Generative AI models garnered a large amount of public attention and\nspeculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion\ncamps exist: one excited about possibilities these models offer for fundamental\nchanges to human tasks, and another highly concerned about power these models\nseem to have. To address these concerns, we assessed several LLMs, primarily\nGPT 3.5, using standard, normed, and validated cognitive and personality\nmeasures. For this seedling project, we developed a battery of tests that\nallowed us to estimate the boundaries of some of these models capabilities, how\nstable those capabilities are over a short period of time, and how they compare\nto humans. Our results indicate that LLMs are unlikely to have developed\nsentience, although its ability to respond to personality inventories is\ninteresting. GPT3.5 did display large variability in both cognitive and\npersonality measures over repeated observations, which is not expected if it\nhad a human-like personality. Variability notwithstanding, LLMs display what in\na human would be considered poor mental health, including low self-esteem,\nmarked dissociation from reality, and in some cases narcissism and psychopathy,\ndespite upbeat and helpful responses.\n","authors":["Ann Speed"],"pdf_url":"https://arxiv.org/pdf/2309.07683v3.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.19280v1","updated":"2024-06-27T15:50:41Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19276v1","updated":"2024-06-27T15:43:18Z","published":"2024-06-27T15:43:18Z","title":"VERISCORE: Evaluating the factuality of verifiable claims in long-form\n  text generation","summary":"  Existing metrics for evaluating the factuality of long-form text, such as\nFACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input\ntext into \"atomic claims\" and verify each against a knowledge base like\nWikipedia. These metrics are not suitable for most generation tasks because\nthey assume that every claim is verifiable (i.e., can plausibly be proven true\nor false). We address this issue with VERISCORE, a metric for diverse long-form\ngeneration tasks that contain both verifiable and unverifiable content.\nVERISCORE can be effectively implemented with either closed or fine-tuned\nopen-weight language models, and human evaluation confirms that VERISCORE's\nextracted claims are more sensible than those from competing methods across\neight different long-form tasks. We use VERISCORE to evaluate generations from\n16 different models across multiple long-form tasks and find that while GPT-4o\nis the best-performing model overall, open-weight models such as Mixtral-8x22\nare closing the gap. We show that an LM's VERISCORE on one task (e.g.,\nbiography generation) does not necessarily correlate to its VERISCORE on a\ndifferent task (e.g., long-form QA), highlighting the need for expanding\nfactuality evaluation across tasks with varying fact density.\n","authors":["Yixiao Song","Yekyung Kim","Mohit Iyyer"],"pdf_url":"https://arxiv.org/pdf/2406.19276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09742v3","updated":"2024-06-27T15:40:53Z","published":"2024-02-15T06:46:48Z","title":"AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical\n  Interaction Simulator","summary":"  Artificial intelligence has significantly advanced healthcare, particularly\nthrough large language models (LLMs) that excel in medical question answering\nbenchmarks. However, their real-world clinical application remains limited due\nto the complexities of doctor-patient interactions. To address this, we\nintroduce \\textbf{AI Hospital}, a multi-agent framework simulating dynamic\nmedical interactions between \\emph{Doctor} as player and NPCs including\n\\emph{Patient}, \\emph{Examiner}, \\emph{Chief Physician}. This setup allows for\nrealistic assessments of LLMs in clinical scenarios. We develop the Multi-View\nMedical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical\nrecords and NPCs to evaluate LLMs' performance in symptom collection,\nexamination recommendations, and diagnoses. Additionally, a dispute resolution\ncollaborative mechanism is proposed to enhance diagnostic accuracy through\niterative discussions. Despite improvements, current LLMs exhibit significant\nperformance gaps in multi-turn interactions compared to one-step approaches.\nOur findings highlight the need for further research to bridge these gaps and\nimprove LLMs' clinical diagnostic capabilities. Our data, code, and\nexperimental results are all open-sourced at\n\\url{https://github.com/LibertFan/AI_Hospital}.\n","authors":["Zhihao Fan","Jialong Tang","Wei Chen","Siyuan Wang","Zhongyu Wei","Jun Xi","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.09742v3.pdf","comment":"https://github.com/LibertFan/AI_Hospital"},{"id":"http://arxiv.org/abs/2401.15847v3","updated":"2024-06-27T15:38:17Z","published":"2024-01-29T02:43:40Z","title":"Muffin or Chihuahua? Challenging Multimodal Large Language Models with\n  Multipanel VQA","summary":"  Multipanel images, commonly seen as web screenshots, posters, etc., pervade\nour daily lives. These images, characterized by their composition of multiple\nsubfigures in distinct layouts, effectively convey information to people.\nToward building advanced multimodal AI applications, such as agents that\nunderstand complex scenes and navigate through webpages, the skill of\nmultipanel visual reasoning is essential, and a comprehensive evaluation of\nmodels in this regard is important. Therefore, we introduce Multipanel Visual\nQuestion Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets\nof questions, answers, and multipanel images that specifically challenge models\nin comprehending multipanel images. Our evaluation shows that questions in the\nMultipanelVQA benchmark pose significant challenges to the state-of-the-art\nMultimodal Large Language Models (MLLMs) tested, even though humans can attain\napproximately 99% accuracy on these questions. Distinctively, the MultipanelVQA\nbenchmark features synthetically generated multipanel images specifically\ncrafted to isolate and assess the impact of various factors, such as the\nlayout, on MLLMs' multipanel image comprehension abilities. As a result, in\naddition to benchmarking the capabilities of MLLMs in understanding multipanel\nimages, we analyze various factors of the multipanel image that affect MLLMs'\nperformance with synthetic data and offer insights for enhancement. Code and\ndata are released at https://sites.google.com/view/multipanelvqa/home.\n","authors":["Yue Fan","Jing Gu","Kaiwen Zhou","Qianqi Yan","Shan Jiang","Ching-Chen Kuo","Xinze Guan","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2401.15847v3.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.19271v1","updated":"2024-06-27T15:37:57Z","published":"2024-06-27T15:37:57Z","title":"AutoPureData: Automated Filtering of Web Data for LLM Fine-tuning","summary":"  Up-to-date and reliable Large Language Models (LLMs) are consistently sought\nafter. Typically, LLMs are trained on a fixed dataset and then deployed.\nHowever, the training data continually becomes outdated. Enable automatic\ntraining of AI using web data involves significant concerns regarding data\nquality and safety due to bias, spam, and other unsafe or unwanted text. Pure\ndata is essential for producing reliable models. Training a model on impure\ndata may result in undesirable outcomes. This research proposes a system that\ncollects web data and automatically filters out unwanted text with the\nassistance of existing trusted AI models. In the experiment, a small sample of\nweb data was collected and filtered, demonstrating the system's effectiveness\nin purifying the data.\n","authors":["Praneeth Vadlapati"],"pdf_url":"https://arxiv.org/pdf/2406.19271v1.pdf","comment":"Initial version"},{"id":"http://arxiv.org/abs/2406.12534v3","updated":"2024-06-27T15:37:15Z","published":"2024-06-18T12:09:02Z","title":"Unified Active Retrieval for Retrieval Augmented Generation","summary":"  In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and\napplying it to every instruction is sub-optimal. Therefore, determining whether\nto retrieve is crucial for RAG, which is usually referred to as Active\nRetrieval. However, existing active retrieval methods face two challenges: 1.\nThey usually rely on a single criterion, which struggles with handling various\ntypes of instructions. 2. They depend on specialized and highly differentiated\nprocedures, and thus combining them makes the RAG system more complicated and\nleads to higher response latency. To address these challenges, we propose\nUnified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts\nthem into plug-and-play classification tasks, which achieves multifaceted\nretrieval timing judgements with negligible extra inference cost. We further\nintroduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to\nprocess diverse active retrieval scenarios through a standardized procedure.\nExperiments on four representative types of user instructions show that UAR\nsignificantly outperforms existing work on the retrieval timing judgement and\nthe performance of downstream tasks, which shows the effectiveness of UAR and\nits helpfulness to downstream tasks.\n","authors":["Qinyuan Cheng","Xiaonan Li","Shimin Li","Qin Zhu","Zhangyue Yin","Yunfan Shao","Linyang Li","Tianxiang Sun","Hang Yan","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2406.12534v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19263v1","updated":"2024-06-27T15:34:16Z","published":"2024-06-27T15:34:16Z","title":"Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens\n  Grounding","summary":"  Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io\n","authors":["Yue Fan","Lei Ding","Ching-Chen Kuo","Shan Jiang","Yang Zhao","Xinze Guan","Jie Yang","Yi Zhang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08967v2","updated":"2024-06-27T15:29:15Z","published":"2024-01-17T04:43:21Z","title":"ReFT: Reasoning with Reinforced Fine-Tuning","summary":"  One way to enhance the reasoning capability of Large Language Models (LLMs)\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\nannotations. This approach does not show sufficiently strong generalization\nability, however, because the training only relies on the given CoT data. In\nmath problem-solving, for example, there is usually only one annotated\nreasoning path for each question in the training data. Intuitively, it would be\nbetter for the algorithm to learn from multiple annotated reasoning paths given\na question. To address this issue, we propose a simple yet effective approach\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\nfirst warmups the model with SFT, and then employs on-line reinforcement\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\nthe model, where an abundance of reasoning paths are automatically sampled\ngiven the question and the rewards are naturally derived from the ground-truth\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\nReFT significantly outperforms SFT, and the performance can be potentially\nfurther boosted by combining inference-time strategies such as majority voting\nand re-ranking. Note that ReFT obtains the improvement by learning from the\nsame training questions as SFT, without relying on extra or augmented training\nquestions. This indicates a superior generalization ability for ReFT.\n","authors":["Trung Quoc Luong","Xinbo Zhang","Zhanming Jie","Peng Sun","Xiaoran Jin","Hang Li"],"pdf_url":"https://arxiv.org/pdf/2401.08967v2.pdf","comment":"ACL 2024 main conference; adjust with reviewer comments; 13 pages"},{"id":"http://arxiv.org/abs/2404.11999v4","updated":"2024-06-27T15:27:41Z","published":"2024-04-18T08:49:38Z","title":"Token-level Direct Preference Optimization","summary":"  Fine-tuning pre-trained Large Language Models (LLMs) is essential to align\nthem with human values and intentions. This process often utilizes methods like\npairwise comparisons and KL divergence against a reference LLM, focusing on the\nevaluation of full answers generated by the models. However, the generation of\nthese responses occurs in a token level, following a sequential,\nauto-regressive fashion. In this paper, we introduce Token-level Direct\nPreference Optimization (TDPO), a novel approach to align LLMs with human\npreferences by optimizing policy at the token level. Unlike previous methods,\nwhich face challenges in divergence efficiency, TDPO incorporates forward KL\ndivergence constraints for each token, improving alignment and diversity.\nUtilizing the Bradley-Terry model for a token-based reward system, TDPO\nenhances the regulation of KL divergence, while preserving simplicity without\nthe need for explicit reward modeling. Experimental results across various text\ntasks demonstrate TDPO's superior performance in balancing alignment with\ngeneration diversity. Notably, fine-tuning with TDPO strikes a better balance\nthan DPO in the controlled sentiment generation and single-turn dialogue\ndatasets, and significantly improves the quality of generated responses\ncompared to both DPO and PPO-based RLHF methods. Our code is open-sourced at\nhttps://github.com/Vance0124/Token-level-Direct-Preference-Optimization.\n","authors":["Yongcheng Zeng","Guoqing Liu","Weiyu Ma","Ning Yang","Haifeng Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2404.11999v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12036v3","updated":"2024-06-27T15:25:25Z","published":"2024-06-17T19:07:21Z","title":"MedCalc-Bench: Evaluating Large Language Models for Medical Calculations","summary":"  As opposed to evaluating computation and logic-based reasoning, current\nbenchmarks for evaluating large language models (LLMs) in medicine are\nprimarily focused on question-answering involving domain knowledge and\ndescriptive reasoning. While such qualitative capabilities are vital to medical\ndiagnosis, in real-world scenarios, doctors frequently use clinical calculators\nthat follow quantitative equations and rule-based reasoning paradigms for\nevidence-based decision support. To this end, we propose MedCalc-Bench, a\nfirst-of-its-kind dataset focused on evaluating the medical calculation\ncapability of LLMs. MedCalc-Bench contains an evaluation set of over 1000\nmanually reviewed instances from 55 different medical calculation tasks. Each\ninstance in MedCalc-Bench consists of a patient note, a question requesting to\ncompute a specific medical value, a ground truth answer, and a step-by-step\nexplanation showing how the answer is obtained. While our evaluation results\nshow the potential of LLMs in this area, none of them are effective enough for\nclinical settings. Common issues include extracting the incorrect entities, not\nusing the correct equation or rules for a calculation task, or incorrectly\nperforming the arithmetic for the computation. We hope our study highlights the\nquantitative knowledge and reasoning gaps in LLMs within medical settings,\nencouraging future improvements of LLMs for various clinical calculation tasks.\n","authors":["Nikhil Khandekar","Qiao Jin","Guangzhi Xiong","Soren Dunn","Serina S Applebaum","Zain Anwar","Maame Sarfo-Gyamfi","Conrad W Safranek","Abid A Anwar","Andrew Zhang","Aidan Gilson","Maxwell B Singer","Amisha Dave","Andrew Taylor","Aidong Zhang","Qingyu Chen","Zhiyong Lu"],"pdf_url":"https://arxiv.org/pdf/2406.12036v3.pdf","comment":"Github link: https://github.com/ncbi-nlp/MedCalc-Bench HuggingFace\n  link: https://huggingface.co/datasets/nsk7153/MedCalc-Bench"},{"id":"http://arxiv.org/abs/2406.19255v1","updated":"2024-06-27T15:23:36Z","published":"2024-06-27T15:23:36Z","title":"Enhancing Video-Language Representations with Structural Spatio-Temporal\n  Alignment","summary":"  While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.\n","authors":["Hao Fei","Shengqiong Wu","Meishan Zhang","Min Zhang","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19255v1.pdf","comment":"Accepted by IEEE TPAMI 2024"},{"id":"http://arxiv.org/abs/2406.19251v1","updated":"2024-06-27T15:18:21Z","published":"2024-06-27T15:18:21Z","title":"AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for\n  Retrieval-Augmented Generation","summary":"  Recent advancements in Large Language Models have transformed ML/AI\ndevelopment, necessitating a reevaluation of AutoML principles for the\nRetrieval-Augmented Generation (RAG) systems. To address the challenges of\nhyper-parameter optimization and online adaptation in RAG, we propose the\nAutoRAG-HP framework, which formulates the hyper-parameter tuning as an online\nmulti-armed bandit (MAB) problem and introduces a novel two-level Hierarchical\nMAB (Hier-MAB) method for efficient exploration of large search spaces. We\nconduct extensive experiments on tuning hyper-parameters, such as top-k\nretrieved documents, prompt compression ratio, and embedding methods, using the\nALCE-ASQA and Natural Questions datasets. Our evaluation from jointly\noptimization all three hyper-parameters demonstrate that MAB-based online\nlearning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with\nprominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls\nrequired by the Grid Search approach. Additionally, the proposed Hier-MAB\napproach outperforms other baselines in more challenging optimization\nscenarios. The code will be made available at https://aka.ms/autorag.\n","authors":["Jia Fu","Xiaoting Qin","Fangkai Yang","Lu Wang","Jue Zhang","Qingwei Lin","Yubo Chen","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.19251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14523v2","updated":"2024-06-27T15:14:58Z","published":"2024-02-22T13:15:49Z","title":"Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding\n  Decomposition","summary":"  We often verbally express emotions in a multifaceted manner, they may vary in\ntheir intensities and may be expressed not just as a single but as a mixture of\nemotions. This wide spectrum of emotions is well-studied in the structural\nmodel of emotions, which represents variety of emotions as derivative products\nof primary emotions with varying degrees of intensity. In this paper, we\npropose an emotional text-to-speech design to simulate a wider spectrum of\nemotions grounded on the structural model. Our proposed design, Daisy-TTS,\nincorporates a prosody encoder to learn emotionally-separable prosody embedding\nas a proxy for emotion. This emotion representation allows the model to\nsimulate: (1) Primary emotions, as learned from the training samples, (2)\nSecondary emotions, as a mixture of primary emotions, (3) Intensity-level, by\nscaling the emotion embedding, and (4) Emotions polarity, by negating the\nemotion embedding. Through a series of perceptual evaluations, Daisy-TTS\ndemonstrated overall higher emotional speech naturalness and emotion\nperceiveability compared to the baseline.\n","authors":["Rendi Chevi","Alham Fikri Aji"],"pdf_url":"https://arxiv.org/pdf/2402.14523v2.pdf","comment":"Project Page: https://rendchevi.github.io/daisy-tts; Updates: (1)\n  Fixed typos, missing references, and layout, (2) Revise explanation on\n  emotion classifier or discriminator"},{"id":"http://arxiv.org/abs/2406.19238v1","updated":"2024-06-27T15:01:53Z","published":"2024-06-27T15:01:53Z","title":"Revealing Fine-Grained Values and Opinions in Large Language Models","summary":"  Uncovering latent values and opinions in large language models (LLMs) can\nhelp identify biases and mitigate potential harm. Recently, this has been\napproached by presenting LLMs with survey questions and quantifying their\nstances towards morally and politically charged statements. However, the\nstances generated by LLMs can vary greatly depending on how they are prompted,\nand there are many ways to argue for or against a given position. In this work,\nwe propose to address this by analysing a large and robust dataset of 156k LLM\nresponses to the 62 propositions of the Political Compass Test (PCT) generated\nby 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of\ntheir generated stances and fine-grained analysis of the plain text\njustifications for those stances. For fine-grained analysis, we propose to\nidentify tropes in the responses: semantically similar phrases that are\nrecurrent and consistent across different prompts, revealing patterns in the\ntext that a given LLM is prone to produce. We find that demographic features\nadded to prompts significantly affect outcomes on the PCT, reflecting bias, as\nwell as disparities between the results of tests when eliciting closed-form vs.\nopen domain responses. Additionally, patterns in the plain text rationales via\ntropes show that similar justifications are repeatedly generated across models\nand prompts even with disparate stances.\n","authors":["Dustin Wright","Arnav Arora","Nadav Borenstein","Srishti Yadav","Serge Belongie","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2406.19238v1.pdf","comment":"28 pages, 20 figures, 7 tables"},{"id":"http://arxiv.org/abs/2406.19237v1","updated":"2024-06-27T15:01:48Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19232v1","updated":"2024-06-27T14:55:19Z","published":"2024-06-27T14:55:19Z","title":"RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs","summary":"  Minimal pairs are a well-established approach to evaluating the grammatical\nknowledge of language models. However, existing resources for minimal pairs\naddress a limited number of languages and lack diversity of language-specific\ngrammatical phenomena. This paper introduces the Russian Benchmark of\nLinguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that\ndiffer in grammaticality and isolate a morphological, syntactic, or semantic\nphenomenon. In contrast to existing benchmarks of linguistic minimal pairs,\nRuBLiMP is created by applying linguistic perturbations to automatically\nannotated sentences from open text corpora and carefully curating test data. We\ndescribe the data collection protocol and present the results of evaluating 25\nlanguage models in various scenarios. We find that the widely used language\nmodels for Russian are sensitive to morphological and agreement-oriented\ncontrasts but fall behind humans on phenomena requiring understanding of\nstructural relations, negation, transitivity, and tense. RuBLiMP, the codebase,\nand other materials are publicly available.\n","authors":["Ekaterina Taktasheva","Maxim Bazhukov","Kirill Koncha","Alena Fenogenova","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2406.19232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19230v1","updated":"2024-06-27T14:54:27Z","published":"2024-06-27T14:54:27Z","title":"Spiking Convolutional Neural Networks for Text Classification","summary":"  Spiking neural networks (SNNs) offer a promising pathway to implement deep\nneural networks (DNNs) in a more energy-efficient manner since their neurons\nare sparsely activated and inferences are event-driven. However, there have\nbeen very few works that have demonstrated the efficacy of SNNs in language\ntasks partially because it is non-trivial to represent words in the forms of\nspikes and to deal with variable-length texts by SNNs. This work presents a\n\"conversion + fine-tuning\" two-step method for training SNNs for text\nclassification and proposes a simple but effective way to encode pre-trained\nword embeddings as spike trains. We show empirically that after fine-tuning\nwith surrogate gradients, the converted SNNs achieve comparable results to\ntheir DNN counterparts with much less energy consumption across multiple\ndatasets for both English and Chinese. We also show that such SNNs are more\nrobust to adversarial attacks than DNNs.\n","authors":["Changze Lv","Jianhan Xu","Xiaoqing Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.19230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19228v1","updated":"2024-06-27T14:52:34Z","published":"2024-06-27T14:52:34Z","title":"Tools Fail: Detecting Silent Errors in Faulty Tools","summary":"  Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not\nin their weights, to perform tasks on the web, and even to control robots.\nHowever, most ontologies and surveys of tool-use have assumed the core\nchallenge for LLMs is choosing the tool. Instead, we introduce a framework for\ntools more broadly which guides us to explore a model's ability to detect\n\"silent\" tool errors, and reflect on how to plan. This more directly aligns\nwith the increasingly popular use of models as tools. We provide an initial\napproach to failure recovery with promising results both on a controlled\ncalculator setting and embodied agent planning.\n","authors":["Jimin Sun","So Yeon Min","Yingshan Chang","Yonatan Bisk"],"pdf_url":"https://arxiv.org/pdf/2406.19228v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.19227v1","updated":"2024-06-27T14:51:17Z","published":"2024-06-27T14:51:17Z","title":"Aligning Teacher with Student Preferences for Tailored Training Data\n  Generation","summary":"  Large Language Models (LLMs) have shown significant promise as copilots in\nvarious tasks. Local deployment of LLMs on edge devices is necessary when\nhandling privacy-sensitive data or latency-sensitive tasks. The computational\nconstraints of such devices make direct deployment of powerful large-scale LLMs\nimpractical, necessitating the Knowledge Distillation from large-scale models\nto lightweight models. Lots of work has been done to elicit diversity and\nquality training examples from LLMs, but little attention has been paid to\naligning teacher instructional content based on student preferences, akin to\n\"responsive teaching\" in pedagogy. Thus, we propose ARTE, dubbed Aligning\nTeacheR with StudenT PreferencEs, a framework that aligns the teacher model\nwith student preferences to generate tailored training examples for Knowledge\nDistillation. Specifically, we elicit draft questions and rationales from the\nteacher model, then collect student preferences on these questions and\nrationales using students' performance with in-context learning as a proxy, and\nfinally align the teacher model with student preferences. In the end, we repeat\nthe first step with the aligned teacher model to elicit tailored training\nexamples for the student model on the target task. Extensive experiments on\nacademic benchmarks demonstrate the superiority of ARTE over existing\ninstruction-tuning datasets distilled from powerful LLMs. Moreover, we\nthoroughly investigate the generalization of ARTE, including the generalization\nof fine-tuned student models in reasoning ability and the generalization of\naligned teacher models to generate tailored training data across tasks and\nstudents. In summary, our contributions lie in proposing a novel framework for\ntailored training example generation, demonstrating its efficacy in\nexperiments, and investigating the generalization of both student & aligned\nteacher models in ARTE.\n","authors":["Yantao Liu","Zhao Zhang","Zijun Yao","Shulin Cao","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19226v1","updated":"2024-06-27T14:51:07Z","published":"2024-06-27T14:51:07Z","title":"Simulating Classroom Education with LLM-Empowered Agents","summary":"  Large language models (LLMs) have been employed in various intelligent\neducational tasks to assist teaching. While preliminary explorations have\nfocused on independent LLM-empowered agents for specific educational tasks, the\npotential for LLMs within a multi-agent collaborative framework to simulate a\nclassroom with real user participation remains unexplored. In this work, we\npropose SimClass, a multi-agent classroom simulation framework involving user\nparticipation. We recognize representative class roles and introduce a novel\nclass control mechanism for automatic classroom teaching, and conduct user\nexperiments in two real-world courses. Utilizing the Flanders Interactive\nAnalysis System and Community of Inquiry theoretical frame works from\neducational analysis, we demonstrate that LLMs can simulate traditional\nclassroom interaction patterns effectively while enhancing user's experience.\nWe also observe emergent group behaviors among agents in SimClass, where agents\ncollaborate to create enlivening interactions in classrooms to improve user\nlearning process. We hope this work pioneers the application of LLM-empowered\nmulti-agent systems in virtual classroom teaching.\n","authors":["Zheyuan Zhang","Daniel Zhang-Li","Jifan Yu","Linlu Gong","Jinchang Zhou","Zhiyuan Liu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19223v1","updated":"2024-06-27T14:49:08Z","published":"2024-06-27T14:49:08Z","title":"T-FREE: Tokenizer-Free Generative LLMs via Sparse Representations for\n  Memory-Efficient Embeddings","summary":"  Tokenizers are crucial for encoding information in Large Language Models, but\ntheir development has recently stagnated, and they contain inherent weaknesses.\nMajor limitations include computational overhead, ineffective vocabulary use,\nand unnecessarily large embedding and head layers. Additionally, their\nperformance is biased towards a reference corpus, leading to reduced\neffectiveness for underrepresented languages.\n  To remedy these issues, we propose T-FREE, which directly embeds words\nthrough sparse activation patterns over character triplets, and does not\nrequire a reference corpus. T-FREE inherently exploits morphological\nsimilarities and allows for strong compression of embedding layers. In our\nexhaustive experimental evaluation, we achieve competitive downstream\nperformance with a parameter reduction of more than 85% on these layers.\nFurther, T-FREE shows significant improvements in cross-lingual transfer\nlearning.\n","authors":["Björn Deiseroth","Manuel Brack","Patrick Schramowski","Kristian Kersting","Samuel Weinbach"],"pdf_url":"https://arxiv.org/pdf/2406.19223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16732v2","updated":"2024-06-27T14:44:51Z","published":"2024-06-24T15:36:00Z","title":"CLIMATELI: Evaluating Entity Linking on Climate Change Data","summary":"  Climate Change (CC) is a pressing topic of global importance, attracting\nincreasing attention across research fields, from social sciences to Natural\nLanguage Processing (NLP). CC is also discussed in various settings and\ncommunication platforms, from academic publications to social media forums.\nUnderstanding who and what is mentioned in such data is a first critical step\nto gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),\nthe first manually annotated CC dataset that links 3,087 entity spans to\nWikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing\nentity linking (EL) systems on the CC topic across various genres and propose\nautomated filtering methods for CC entities. We find that the performance of EL\nmodels notably lags behind humans at both token and entity levels. Testing\nwithin the scope of retaining or excluding non-nominal and/or non-CC entities\nparticularly impacts the models' performances.\n","authors":["Shijia Zhou","Siyao Peng","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2406.16732v2.pdf","comment":"8 pages, accepted at ClimateNLP 2024 workshop @ ACL 2024"},{"id":"http://arxiv.org/abs/2406.19215v1","updated":"2024-06-27T14:38:33Z","published":"2024-06-27T14:38:33Z","title":"SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented\n  Generation","summary":"  This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel\nadaptive RAG model that extracts self-aware uncertainty of LLMs from their\ninternal states. SeaKR activates retrieval when the LLMs present high\nself-aware uncertainty for generation. To effectively integrate retrieved\nknowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty\nto preserve the snippet that reduces their uncertainty to the utmost. To\nfacilitate solving complex tasks that require multiple retrievals, SeaKR\nutilizes their self-aware uncertainty to choose among different reasoning\nstrategies. Our experiments on both complex and simple Question Answering\ndatasets show that SeaKR outperforms existing adaptive RAG methods. We release\nour code at https://github.com/THU-KEG/SeaKR.\n","authors":["Zijun Yao","Weijian Qi","Liangming Pan","Shulin Cao","Linmei Hu","Weichuan Liu","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12644v2","updated":"2024-06-27T14:32:07Z","published":"2024-06-18T14:12:27Z","title":"Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for\n  Large Language Models","summary":"  Assessing the effectiveness of large language models (LLMs) in addressing\ndiverse tasks is essential for comprehending their strengths and weaknesses.\nConventional evaluation techniques typically apply a single prompting strategy\nuniformly across datasets, not considering the varying degrees of task\ncomplexity. We introduce the Hierarchical Prompting Taxonomy (HPT), a taxonomy\nthat employs a Hierarchical Prompt Framework (HPF) composed of five unique\nprompting strategies, arranged from the simplest to the most complex, to assess\nLLMs more precisely and to offer a clearer perspective. This taxonomy assigns a\nscore, called the Hierarchical Prompting Score (HP-Score), to datasets as well\nas LLMs based on the rules of the taxonomy, providing a nuanced understanding\nof their ability to solve diverse tasks and offering a universal measure of\ntask complexity. Additionally, we introduce the Adaptive Hierarchical Prompt\nframework, which automates the selection of appropriate prompting strategies\nfor each task. This study compares manual and adaptive hierarchical prompt\nframeworks using four instruction-tuned LLMs, namely Llama 3 8B, Phi 3 3.8B,\nMistral 7B, and Gemma 7B, across four datasets: BoolQ, CommonSenseQA (CSQA),\nIWSLT-2017 en-fr (IWSLT), and SamSum. Experiments demonstrate the effectiveness\nof HPT, providing a reliable way to compare different tasks and LLM\ncapabilities. This paper leads to the development of a universal evaluation\nmetric that can be used to evaluate both the complexity of the datasets and the\ncapabilities of LLMs. The implementation of both manual HPF and adaptive HPF is\npublicly available.\n","authors":["Devichand Budagam","Sankalp KJ","Ashutosh Kumar","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2406.12644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00041v2","updated":"2024-06-27T14:31:22Z","published":"2024-05-27T17:55:36Z","title":"QUB-Cirdan at \"Discharge Me!\": Zero shot discharge letter generation by\n  open-source LLM","summary":"  The BioNLP ACL'24 Shared Task on Streamlining Discharge Documentation aims to\nreduce the administrative burden on clinicians by automating the creation of\ncritical sections of patient discharge letters. This paper presents our\napproach using the Llama3 8B quantized model to generate the \"Brief Hospital\nCourse\" and \"Discharge Instructions\" sections. We employ a zero-shot method\ncombined with Retrieval-Augmented Generation (RAG) to produce concise,\ncontextually accurate summaries. Our contributions include the development of a\ncurated template-based approach to ensure reliability and consistency, as well\nas the integration of RAG for word count prediction. We also describe several\nunsuccessful experiments to provide insights into our pathway for the\ncompetition. Our results demonstrate the effectiveness and efficiency of our\napproach, achieving high scores across multiple evaluation metrics.\n","authors":["Rui Guo","Greg Farnan","Niall McLaughlin","Barry Devereux"],"pdf_url":"https://arxiv.org/pdf/2406.00041v2.pdf","comment":"BioNLP 2024 workshop"},{"id":"http://arxiv.org/abs/2405.06196v2","updated":"2024-06-27T14:19:56Z","published":"2024-05-10T02:23:56Z","title":"VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with\n  Lightweight Blocks","summary":"  Foundation Vision-Language Models (VLMs) trained using large-scale\nopen-domain images and text pairs have recently been adapted to develop\nVision-Language Segmentation Models (VLSMs) that allow providing text prompts\nduring inference to guide image segmentation. If robust and powerful VLSMs can\nbe built for medical images, it could aid medical professionals in many\nclinical tasks where they must spend substantial time delineating the target\nstructure of interest. VLSMs for medical images resort to fine-tuning base VLM\nor VLSM pretrained on open-domain natural image datasets due to fewer annotated\nmedical image datasets; this fine-tuning is resource-consuming and expensive as\nit usually requires updating all or a significant fraction of the pretrained\nparameters. Recently, lightweight blocks called adapters have been proposed in\nVLMs that keep the pretrained model frozen and only train adapters during\nfine-tuning, substantially reducing the computing resources required. We\nintroduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained\nvision-language segmentation models using transformer encoders. Our experiments\nin widely used CLIP-based segmentation models show that with only 3 million\ntrainable parameters, the VLSM-Adapter outperforms state-of-the-art and is\ncomparable to the upper bound end-to-end fine-tuning. The source code is\navailable at: https://github.com/naamiinepal/vlsm-adapter.\n","authors":["Manish Dhakal","Rabin Adhikari","Safal Thapaliya","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2405.06196v2.pdf","comment":"Accepted at MICCAI 2024, the 27th International Conference on Medical\n  Image Computing and Computer Assisted Intervention"},{"id":"http://arxiv.org/abs/2309.08316v3","updated":"2024-06-27T14:02:44Z","published":"2023-09-15T11:15:47Z","title":"How to Handle Different Types of Out-of-Distribution Scenarios in\n  Computational Argumentation? A Comprehensive and Fine-Grained Field Study","summary":"  The advent of pre-trained Language Models (LMs) has markedly advanced natural\nlanguage processing, but their efficacy in out-of-distribution (OOD) scenarios\nremains a significant challenge. Computational argumentation (CA), modeling\nhuman argumentation processes, is a field notably impacted by these challenges\nbecause complex annotation schemes and high annotation costs naturally lead to\nresources barely covering the multiplicity of available text sources and\ntopics. Due to this data scarcity, generalization to data from uncovered\ncovariant distributions is a common challenge for CA tasks like stance\ndetection or argument classification. This work systematically assesses LMs'\ncapabilities for such OOD scenarios. While previous work targets specific OOD\ntypes like topic shifts or OOD uniformly, we address three prevalent OOD\nscenarios in CA: topic shift, domain shift, and language shift. Our findings\nchallenge the previously asserted general superiority of in-context learning\n(ICL) for OOD. We find that the efficacy of such learning paradigms varies with\nthe type of OOD. Specifically, while ICL excels for domain shifts, prompt-based\nfine-tuning surpasses for topic shifts. To sum up, we navigate the\nheterogeneity of OOD scenarios in CA and empirically underscore the potential\nof base-sized LMs in overcoming these challenges.\n","authors":["Andreas Waldis","Yufang Hou","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2309.08316v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08426v2","updated":"2024-06-27T13:51:30Z","published":"2024-06-12T17:13:17Z","title":"Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL","summary":"  Generating accurate SQL according to natural language questions (text-to-SQL)\nis a long-standing challenge due to the complexities involved in user question\nunderstanding, database schema comprehension, and SQL generation. Conventional\ntext-to-SQL systems, comprising human engineering and deep neural networks,\nhave made substantial progress. Subsequently, pre-trained language models\n(PLMs) have been developed and utilized for text-to-SQL tasks, achieving\npromising performance. As modern databases become more complex, the\ncorresponding user questions also grow more challenging, leading PLMs with\nlimited comprehension capabilities to produce incorrect SQL. This necessitates\nmore sophisticated and tailored optimization methods for PLMs, which, in turn,\nrestricts the applications of PLM-based systems. Most recently, large language\nmodels (LLMs) have demonstrated significant capabilities in natural language\nunderstanding as the model scale remains increasing. Therefore, integrating the\nLLM-based implementation can bring unique opportunities, improvements, and\nsolutions to text-to-SQL research. In this survey, we present a comprehensive\nreview of LLM-based text-to-SQL. Specifically, we propose a brief overview of\nthe technical challenges and the evolutionary process of text-to-SQL. Then, we\nprovide a detailed introduction to the datasets and metrics designed to\nevaluate text-to-SQL systems. After that, we present a systematic analysis of\nrecent advances in LLM-based text-to-SQL. Finally, we discuss the remaining\nchallenges in this field and propose expectations for future research\ndirections.\n","authors":["Zijin Hong","Zheng Yuan","Qinggang Zhang","Hao Chen","Junnan Dong","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.08426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19172v1","updated":"2024-06-27T13:48:46Z","published":"2024-06-27T13:48:46Z","title":"Annotation Errors and NER: A Study with OntoNotes 5.0","summary":"  Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.\n","authors":["Gabriel Bernier-Colborne","Sowmya Vajjala"],"pdf_url":"https://arxiv.org/pdf/2406.19172v1.pdf","comment":"Unpublished report. Originally submitted to LREC 2022"},{"id":"http://arxiv.org/abs/2406.19170v1","updated":"2024-06-27T13:44:03Z","published":"2024-06-27T13:44:03Z","title":"The Illusion of Competence: Evaluating the Effect of Explanations on\n  Users' Mental Models of Visual Question Answering Systems","summary":"  We examine how users perceive the limitations of an AI system when it\nencounters a task that it cannot perform perfectly and whether providing\nexplanations alongside its answers aids users in constructing an appropriate\nmental model of the system's capabilities and limitations. We employ a visual\nquestion answer and explanation task where we control the AI system's\nlimitations by manipulating the visual inputs: during inference, the system\neither processes full-color or grayscale images. Our goal is to determine\nwhether participants can perceive the limitations of the system. We hypothesize\nthat explanations will make limited AI capabilities more transparent to users.\nHowever, our results show that explanations do not have this effect. Instead of\nallowing users to more accurately assess the limitations of the AI system,\nexplanations generally increase users' perceptions of the system's competence -\nregardless of its actual performance.\n","authors":["Judith Sieker","Simeon Junker","Ronja Utescher","Nazia Attari","Heiko Wersing","Hendrik Buschmeier","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2406.19170v1.pdf","comment":"16 pages (including Appendix); under review"},{"id":"http://arxiv.org/abs/2406.19146v1","updated":"2024-06-27T13:02:43Z","published":"2024-06-27T13:02:43Z","title":"Resolving Discrepancies in Compute-Optimal Scaling of Language Models","summary":"  Kaplan et al. and Hoffmann et al. developed influential scaling laws for the\noptimal model size as a function of the compute budget, but these laws yield\nsubstantially different predictions. We explain the discrepancy by reproducing\nthe Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and\nidentifying three factors causing the difference: last layer computational\ncost, warmup duration, and scale-dependent optimizer tuning. With these factors\ncorrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\n\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find\nthat careful learning rate decay is not essential for the validity of their\nscaling law. As a secondary result, we derive scaling laws for the optimal\nlearning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter\nis essential at lower batch sizes.\n","authors":["Tomer Porian","Mitchell Wortsman","Jenia Jitsev","Ludwig Schmidt","Yair Carmon"],"pdf_url":"https://arxiv.org/pdf/2406.19146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16480v4","updated":"2024-06-27T12:38:12Z","published":"2023-11-27T05:05:41Z","title":"WsiCaption: Multiple Instance Generation of Pathology Reports for\n  Gigapixel Whole-Slide Images","summary":"  Whole slide images are the foundation of digital pathology for the diagnosis\nand treatment of carcinomas. Writing pathology reports is laborious and\nerror-prone for inexperienced pathologists. To reduce the workload and improve\nclinical automation, we investigate how to generate pathology reports given\nwhole slide images. On the data end, we curated the largest WSI-text dataset\n(PathText). In specific, we collected nearly 10000 high-quality WSI-text pairs\nfor visual-language models by recognizing and cleaning pathology reports which\nnarrate diagnostic slides in TCGA. On the model end, we propose the multiple\ninstance generative model (MI-Gen) which can produce pathology reports for\ngigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText.\nExperimental results show our model can generate pathology reports which\ncontain multiple clinical clues and achieve competitive performance on certain\nslide-level tasks. We observe that simple semantic extraction from the\npathology reports can achieve the best performance (0.838 of F1 score) on BRCA\nsubtyping surpassing previous state-of-the-art approaches. Our collected\ndataset and related code are available.\n","authors":["Pingyi Chen","Honglin Li","Chenglu Zhu","Sunyi Zheng","Zhongyi Shui","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2311.16480v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12416v2","updated":"2024-06-27T12:07:55Z","published":"2024-06-18T09:07:30Z","title":"Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for\n  Large Language Models","summary":"  Large language models (LLMs) have achieved remarkable success but still tend\nto generate factually erroneous responses, a phenomenon known as hallucination.\nA recent trend is to use preference learning to fine-tune models to align with\nfactuality. However, existing work primarily evaluates fine-tuned models on\nin-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets\nremains underexplored. In this paper, we conduct a comprehensive evaluation of\nthe factuality of different models tuned by various preference learning\nalgorithms and demonstrate that their performance on OOD datasets either\nincreases minimally or decreases. Subsequently, we reveal that the main cause\nof model's failure to uphold factuality under a distribution shift is\n\\textbf{under-alignment}, rather than \\textbf{over-alignment}, by analyzing the\ntoken distribution shift of the models before and after tuning. Finally, we\npropose \\textbf{APEFT} (\\textbf{A}tomic \\textbf{P}reference \\textbf{E}nhanced\n\\textbf{F}actuality \\textbf{T}uning), a framework that enhances model's\nawareness of factuality at the granularity of individual facts. Extensive\nexperiments demonstrate that APEFT improves model performance by an average of\n$\\boldsymbol{3.45\\%}$ on both ID and OOD datasets, which is highly effective.\n","authors":["Hongbang Yuan","Yubo Chen","Pengfei Cao","Zhuoran Jin","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.12416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19116v1","updated":"2024-06-27T11:53:15Z","published":"2024-06-27T11:53:15Z","title":"CHEW: A Dataset of CHanging Events in Wikipedia","summary":"  We introduce CHEW, a novel dataset of changing events in Wikipedia expressed\nin naturally occurring text. We use CHEW for probing LLMs for their timeline\nunderstanding of Wikipedia entities and events in generative and classification\nexperiments. Our results suggest that LLMs, despite having temporal information\navailable, struggle to construct accurate timelines. We further show the\nusefulness of CHEW-derived embeddings for identifying meaning shift.\n","authors":["Hsuvas Borkakoty","Luis Espinosa-Anke"],"pdf_url":"https://arxiv.org/pdf/2406.19116v1.pdf","comment":"Short Paper"},{"id":"http://arxiv.org/abs/2406.19102v1","updated":"2024-06-27T11:28:50Z","published":"2024-06-27T11:28:50Z","title":"Statements: Universal Information Extraction from Tables with Large\n  Language Models for ESG KPIs","summary":"  Environment, Social, and Governance (ESG) KPIs assess an organization's\nperformance on issues such as climate change, greenhouse gas emissions, water\nconsumption, waste management, human rights, diversity, and policies. ESG\nreports convey this valuable quantitative information through tables.\nUnfortunately, extracting this information is difficult due to high variability\nin the table structure as well as content. We propose Statements, a novel\ndomain agnostic data structure for extracting quantitative facts and related\ninformation. We propose translating tables to statements as a new supervised\ndeep-learning universal information extraction task. We introduce SemTabNet - a\ndataset of over 100K annotated tables. Investigating a family of T5-based\nStatement Extraction Models, our best model generates statements which are 82%\nsimilar to the ground-truth (compared to baseline of 21%). We demonstrate the\nadvantages of statements by applying our model to over 2700 tables from ESG\nreports. The homogeneous nature of statements permits exploratory data analysis\non expansive information found in large collections of ESG reports.\n","authors":["Lokesh Mishra","Sohayl Dhibi","Yusik Kim","Cesar Berrospi Ramis","Shubham Gupta","Michele Dolfi","Peter Staar"],"pdf_url":"https://arxiv.org/pdf/2406.19102v1.pdf","comment":"Accepted at the NLP4Climate workshop in the 62nd Annual Meeting of\n  the Association for Computational Linguistics (ACL 2024)"},{"id":"http://arxiv.org/abs/2406.19097v1","updated":"2024-06-27T11:26:17Z","published":"2024-06-27T11:26:17Z","title":"Fairness and Bias in Multimodal AI: A Survey","summary":"  The importance of addressing fairness and bias in artificial intelligence\n(AI) systems cannot be over-emphasized. Mainstream media has been awashed with\nnews of incidents around stereotypes and bias in many of these systems in\nrecent years. In this survey, we fill a gap with regards to the minimal study\nof fairness and bias in Large Multimodal Models (LMMs) compared to Large\nLanguage Models (LLMs), providing 50 examples of datasets and models along with\nthe challenges affecting them; we identify a new category of quantifying bias\n(preuse), in addition to the two well-known ones in the literature: intrinsic\nand extrinsic; we critically discuss the various ways researchers are\naddressing these challenges. Our method involved two slightly different search\nqueries on Google Scholar, which revealed that 33,400 and 538,000 links are the\nresults for the terms \"Fairness and bias in Large Multimodal Models\" and\n\"Fairness and bias in Large Language Models\", respectively. We believe this\nwork contributes to filling this gap and providing insight to researchers and\nother stakeholders on ways to address the challenge of fairness and bias in\nmultimodal A!.\n","authors":["Tosin Adewumi","Lama Alkhaled","Namrata Gurung","Goya van Boven","Irene Pagliai"],"pdf_url":"https://arxiv.org/pdf/2406.19097v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2406.19073v1","updated":"2024-06-27T10:43:04Z","published":"2024-06-27T10:43:04Z","title":"AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database\n  Queries","summary":"  Practical semantic parsers are expected to understand user utterances and map\nthem to executable programs, even when these are ambiguous. We introduce a new\nbenchmark, AMBROSIA, which we hope will inform and inspire the development of\ntext-to-SQL parsers capable of recognizing and interpreting ambiguous requests.\nOur dataset contains questions showcasing three different types of ambiguity\n(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,\nand corresponding SQL queries. In each case, the ambiguity persists even when\nthe database context is provided. This is achieved through a novel approach\nthat involves controlled generation of databases from scratch. We benchmark\nvarious LLMs on AMBROSIA, revealing that even the most advanced models struggle\nto identify and interpret ambiguity in questions.\n","authors":["Irina Saparina","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2406.19073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19071v1","updated":"2024-06-27T10:41:22Z","published":"2024-06-27T10:41:22Z","title":"EmPO: Theory-Driven Dataset Construction for Empathetic Response\n  Generation through Preference Optimization","summary":"  Empathetic response generation is a desirable aspect of conversational\nagents, crucial for facilitating engaging and emotionally intelligent\nmulti-turn conversations between humans and machines. Leveraging large language\nmodels for this task has shown promising results, yet challenges persist in\nensuring both the empathetic quality of the responses and retention of the\ngeneralization performance of the models. In this paper, we propose a novel\napproach where we construct theory-driven preference datasets and use them to\nalign LLMs with preference optimization algorithms to address these challenges.\nTo measure empathetic response generation, we employ the EmpatheticDialogues\ndataset, assessing empathy with the diff-EPITOME and BERTscore metrics, and\nevaluate the generalization performance on the MMLU benchmark. We make all\ndatasets, source code, and models publicly available.\n","authors":["Ondrej Sotolar"],"pdf_url":"https://arxiv.org/pdf/2406.19071v1.pdf","comment":"v01, 4 pages short paper, ACL style"},{"id":"http://arxiv.org/abs/2406.19065v1","updated":"2024-06-27T10:34:02Z","published":"2024-06-27T10:34:02Z","title":"STBench: Assessing the Ability of Large Language Models in\n  Spatio-Temporal Analysis","summary":"  The rapid evolution of large language models (LLMs) holds promise for\nreforming the methodology of spatio-temporal data mining. However, current\nworks for evaluating the spatio-temporal understanding capability of LLMs are\nsomewhat limited and biased. These works either fail to incorporate the latest\nlanguage models or only focus on assessing the memorized spatio-temporal\nknowledge. To address this gap, this paper dissects LLMs' capability of\nspatio-temporal data into four distinct dimensions: knowledge comprehension,\nspatio-temporal reasoning, accurate computation, and downstream applications.\nWe curate several natural language question-answer tasks for each category and\nbuild the benchmark dataset, namely STBench, containing 13 distinct tasks and\nover 60,000 QA pairs. Moreover, we have assessed the capabilities of 13 LLMs,\nsuch as GPT-4o, Gemma and Mistral. Experimental results reveal that existing\nLLMs show remarkable performance on knowledge comprehension and spatio-temporal\nreasoning tasks, with potential for further enhancement on other tasks through\nin-context learning, chain-of-though prompting, and fine-tuning. The code and\ndatasets of STBench are released on https://github.com/LwbXc/STBench.\n","authors":["Wenbin Li","Di Yao","Ruibo Zhao","Wenjie Chen","Zijie Xu","Chengxue Luo","Chang Gong","Quanliang Jing","Haining Tan","Jingping Bi"],"pdf_url":"https://arxiv.org/pdf/2406.19065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18245v2","updated":"2024-06-27T10:19:55Z","published":"2024-06-26T10:48:14Z","title":"Weak Reward Model Transforms Generative Models into Robust Causal Event\n  Extraction Systems","summary":"  The inherent ambiguity of cause and effect boundaries poses a challenge in\nevaluating causal event extraction tasks. Traditional metrics like Exact Match\nand BertScore poorly reflect model performance, so we trained evaluation models\nto approximate human evaluation, achieving high agreement. We used them to\nperform Reinforcement Learning with extraction models to align them with human\npreference, prioritising semantic understanding. We successfully explored our\napproach through multiple datasets, including transferring an evaluator trained\non one dataset to another as a way to decrease the reliance on human-annotated\ndata. In that vein, we also propose a weak-to-strong supervision method that\nuses a fraction of the annotated data to train an evaluation model while still\nachieving high performance in training an RL model. Our code is available at\nhttps://github.com/oyarsa/event_extraction/tree/causal-event-extraction.\n","authors":["Italo Luis da Silva","Hanqi Yan","Lin Gui","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2406.18245v2.pdf","comment":"13 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.11497v2","updated":"2024-06-27T10:18:53Z","published":"2024-06-17T13:01:12Z","title":"CrAM: Credibility-Aware Attention Modification in LLMs for Combating\n  Misinformation in RAG","summary":"  Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large\nLanguage Models (LLMs) by referencing external documents. However, the\nmisinformation in external documents may mislead LLMs' generation. To address\nthis issue, we explore the task of \"credibility-aware RAG\", in which LLMs\nautomatically adjust the influence of retrieved documents based on their\ncredibility scores to counteract misinformation. To this end, we introduce a\nplug-and-play method named $\\textbf{Cr}$edibility-aware $\\textbf{A}$ttention\n$\\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in\nLLMs and adjusts their attention weights based on the credibility of the\ndocuments, thereby reducing the impact of low-credibility documents.\nExperiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and\nQwen-7B show that CrAM improves the RAG performance of LLMs against\nmisinformation pollution by over 20%, even surpassing supervised fine-tuning\nmethods.\n","authors":["Boyi Deng","Wenjie Wang","Fengbin Zhu","Qifan Wang","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2406.11497v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.03183v2","updated":"2024-06-27T10:08:05Z","published":"2024-01-06T10:08:33Z","title":"Exploring Defeasibility in Causal Reasoning","summary":"  Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present $\\delta$-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n$\\delta$-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin $\\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by $\\delta$-CAUSAL.\n","authors":["Shaobo Cui","Lazar Milikic","Yiyang Feng","Mete Ismayilzada","Debjit Paul","Antoine Bosselut","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2401.03183v2.pdf","comment":"Accepted by ACL 2024 (Findings)"},{"id":"http://arxiv.org/abs/2406.19032v1","updated":"2024-06-27T09:37:34Z","published":"2024-06-27T09:37:34Z","title":"Improving Weak-to-Strong Generalization with Reliability-Aware Alignment","summary":"  Large language models (LLMs) are now rapidly advancing and surpassing human\nabilities on many natural language tasks. However, aligning these super-human\nLLMs with human knowledge remains challenging because the supervision signals\nfrom human annotators may be wrong. This issue, known as the \"super-alignment\"\nproblem, requires enhancing weak-to-strong generalization, where a strong LLM\nmust generalize from imperfect supervision provided by a weaker source. To\naddress this issue, we propose an approach to improve weak-to-strong\ngeneralization by involving the reliability of weak supervision signals in the\nalignment process. In our method, we query the weak supervisor for multiple\nanswers, estimate the answer reliability, and enhance the alignment process by\nfiltering out uncertain data or re-weighting reliable data. Experiments on four\ndatasets demonstrate that our methods effectively identify the quality of weak\nlabels and significantly enhance weak-to-strong generalization. Our work\npresents effective techniques for error-robust model alignment, reducing error\npropagation from noisy supervision and enhancing the accuracy and reliability\nof LLMs. Codes are publicly available at\nhttp://github.com/Irenehere/ReliableAlignment.\n","authors":["Yue Guo","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2406.19032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17363v2","updated":"2024-06-27T09:08:03Z","published":"2024-06-25T08:26:41Z","title":"Leveraging Synthetic Audio Data for End-to-End Low-Resource Speech\n  Translation","summary":"  This paper describes our system submission to the International Conference on\nSpoken Language Translation (IWSLT 2024) for Irish-to-English speech\ntranslation. We built end-to-end systems based on Whisper, and employed a\nnumber of data augmentation techniques, such as speech back-translation and\nnoise augmentation. We investigate the effect of using synthetic audio data and\ndiscuss several methods for enriching signal diversity.\n","authors":["Yasmin Moslem"],"pdf_url":"https://arxiv.org/pdf/2406.17363v2.pdf","comment":"IWSLT 2024"},{"id":"http://arxiv.org/abs/2406.11727v2","updated":"2024-06-27T08:52:54Z","published":"2024-06-17T16:46:10Z","title":"1000 African Voices: Advancing inclusive multi-speaker multi-accent\n  speech synthesis","summary":"  Recent advances in speech synthesis have enabled many useful applications\nlike audio directions in Google Maps, screen readers, and automated content\ngeneration on platforms like TikTok. However, these systems are mostly\ndominated by voices sourced from data-rich geographies with personas\nrepresentative of their source data. Although 3000 of the world's languages are\ndomiciled in Africa, African voices and personas are under-represented in these\nsystems. As speech synthesis becomes increasingly democratized, it is desirable\nto increase the representation of African English accents. We present Afro-TTS,\nthe first pan-African accented English speech synthesis system able to generate\nspeech in 86 African accents, with 1000 personas representing the rich\nphonological diversity across the continent for downstream application in\nEducation, Public Health, and Automated Content Creation. Speaker interpolation\nretains naturalness and accentedness, enabling the creation of new voices.\n","authors":["Sewade Ogun","Abraham T. Owodunni","Tobi Olatunji","Eniola Alese","Babatunde Oladimeji","Tejumade Afonja","Kayode Olaleye","Naome A. Etori","Tosin Adewumi"],"pdf_url":"https://arxiv.org/pdf/2406.11727v2.pdf","comment":"Accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2311.01200v4","updated":"2024-06-27T08:35:53Z","published":"2023-11-02T12:54:50Z","title":"Continual Learning Under Language Shift","summary":"  The recent increase in data and model scale for language model pre-training\nhas led to huge training costs. In scenarios where new data become available\nover time, updating a model instead of fully retraining it would therefore\nprovide significant gains. We study the pros and cons of updating a language\nmodel when new data comes from new languages -- the case of continual learning\nunder language shift. Starting from a monolingual English language model, we\nincrementally add data from Danish, Icelandic, and Norwegian to investigate how\nforward and backward transfer effects depend on pre-training order and\ncharacteristics of languages, for three different model sizes. Our results show\nthat, while forward transfer is largely positive and independent of language\norder, backward transfer can be positive or negative depending on the order and\ncharacteristics of new languages. We explore a number of potentially\nexplanatory factors and find that a combination of language contamination and\nsyntactic similarity best fits our results.\n","authors":["Evangelia Gogoulou","Timothée Lesort","Magnus Boman","Joakim Nivre"],"pdf_url":"https://arxiv.org/pdf/2311.01200v4.pdf","comment":"Accepted to TSD 2024"},{"id":"http://arxiv.org/abs/2406.13106v3","updated":"2024-06-27T08:28:51Z","published":"2024-06-18T23:40:00Z","title":"Accelerating Complex Disease Treatment through Network Medicine and\n  GenAI: A Case Study on Drug Repurposing for Breast Cancer","summary":"  The objective of this research is to introduce a network specialized in\npredicting drugs that can be repurposed by investigating real-world evidence\nsources, such as clinical trials and biomedical literature. Specifically, it\naims to generate drug combination therapies for complex diseases (e.g., cancer,\nAlzheimer's). We present a multilayered network medicine approach, empowered by\na highly configured ChatGPT prompt engineering system, which is constructed on\nthe fly to extract drug mentions in clinical trials. Additionally, we introduce\na novel algorithm that connects real-world evidence with disease-specific\nsignaling pathways (e.g., KEGG database). This sheds light on the\nrepurposability of drugs if they are found to bind with one or more protein\nconstituents of a signaling pathway. To demonstrate, we instantiated the\nframework for breast cancer and found that, out of 46 breast cancer signaling\npathways, the framework identified 38 pathways that were covered by at least\ntwo drugs. This evidence signals the potential for combining those drugs.\nSpecifically, the most covered signaling pathway, ID hsa:2064, was covered by\n108 drugs, some of which can be combined. Conversely, the signaling pathway ID\nhsa:1499 was covered by only two drugs, indicating a significant gap for\nfurther research. Our network medicine framework, empowered by GenAI, shows\npromise in identifying drug combinations with a high degree of specificity,\nknowing the exact signaling pathways and proteins that serve as targets. It is\nnoteworthy that ChatGPT successfully accelerated the process of identifying\ndrug mentions in clinical trials, though further investigations are required to\ndetermine the relationships among the drug mentions.\n","authors":["Ahmed Abdeen Hamed","Tamer E. Fandy"],"pdf_url":"https://arxiv.org/pdf/2406.13106v3.pdf","comment":"9 pages double columns, 5 figures, 3 algorithms, 3 tables, and 1\n  listing, Submitted to IEEE MedAI'24 Conference, to be held November 15-17,\n  Chongqing, China"},{"id":"http://arxiv.org/abs/2406.18977v1","updated":"2024-06-27T08:13:33Z","published":"2024-06-27T08:13:33Z","title":"RoboUniView: Visual-Language Model with Unified View Representation for\n  Robotic Manipulaiton","summary":"  Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a\nnovel paradigm, aiming to enhance the model's ability to generalize to new\nobjects and instructions. However, due to variations in camera specifications\nand mounting positions, existing methods exhibit significant performance\ndisparities across different robotic platforms. To address this challenge, we\npropose RoboUniView in this paper, an innovative approach that decouples visual\nfeature extraction from action learning. We first learn a unified view\nrepresentation from multi-perspective views by pre-training on readily\naccessible data, and then derive actions from this unified view representation\nto control robotic manipulation. This unified view representation more\naccurately mirrors the physical world and is not constrained by the robotic\nplatform's camera parameters. Thanks to this methodology, we achieve\nstate-of-the-art performance on the demanding CALVIN benchmark, enhancing the\nsuccess rate in the $D \\to D$ setting from 88.7% to 96.2%, and in the $ABC \\to\nD$ setting from 82.4% to 94.2%. Moreover, our model exhibits outstanding\nadaptability and flexibility: it maintains high performance under unseen camera\nparameters, can utilize multiple datasets with varying camera parameters, and\nis capable of joint cross-task learning across datasets. Code is provided for\nre-implementation. https://github.com/liufanfanlff/RoboUniview\n","authors":["Fanfan Liu","Feng Yan","Liming Zheng","Chengjian Feng","Yiyang Huang","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2406.18977v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14833v2","updated":"2024-06-27T08:11:01Z","published":"2024-06-21T02:28:37Z","title":"Efficient Continual Pre-training by Mitigating the Stability Gap","summary":"  Continual pre-training has increasingly become the predominant approach for\nadapting Large Language Models (LLMs) to new domains. This process involves\nupdating the pre-trained LLM with a corpus from a new domain, resulting in a\nshift in the training distribution. To study the behavior of LLMs during this\nshift, we measured the model's performance throughout the continual\npre-training process. we observed a temporary performance drop at the\nbeginning, followed by a recovery phase, a phenomenon known as the \"stability\ngap,\" previously noted in vision models classifying new classes. To address\nthis issue and enhance LLM performance within a fixed compute budget, we\npropose three effective strategies: (1) Continually pre-training the LLM on a\nsubset with a proper size for multiple epochs, resulting in faster performance\nrecovery than pre-training the LLM on a large corpus in a single epoch; (2)\nPre-training the LLM only on high-quality sub-corpus, which rapidly boosts\ndomain performance; and (3) Using a data mixture similar to the pre-training\ndata to reduce distribution gap. We conduct various experiments on Llama-family\nmodels to validate the effectiveness of our strategies in both medical\ncontinual pre-training and instruction tuning. For example, our strategies\nimprove the average medical task performance of the OpenLlama-3B model from\n36.2% to 40.7% with only 40% of the original training budget and enhance the\naverage general task performance without causing forgetting. Furthermore, we\napply our strategies to the Llama-3-8B model. The resulting model,\nLlama-3-Physician, achieves the best medical performance among current\nopen-source models, and performs comparably to or even better than GPT-4 on\nseveral medical benchmarks. We release our models at\n\\url{https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct}.\n","authors":["Yiduo Guo","Jie Fu","Huishuai Zhang","Dongyan Zhao","Yikang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.14833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18972v1","updated":"2024-06-27T08:03:13Z","published":"2024-06-27T08:03:13Z","title":"Applying LLMs for Rescoring N-best ASR Hypotheses of Casual\n  Conversations: Effects of Domain Adaptation and Context Carry-over","summary":"  Large language models (LLMs) have been successfully applied for rescoring\nautomatic speech recognition (ASR) hypotheses. However, their ability to\nrescore ASR hypotheses of casual conversations has not been sufficiently\nexplored. In this study, we reveal it by performing N-best ASR hypotheses\nrescoring using Llama2 on the CHiME-7 distant ASR (DASR) task. Llama2 is one of\nthe most representative LLMs, and the CHiME-7 DASR task provides datasets of\ncasual conversations between multiple participants. We investigate the effects\nof domain adaptation of the LLM and context carry-over when performing N-best\nrescoring. Experimental results show that, even without domain adaptation,\nLlama2 outperforms a standard-size domain-adapted Transformer-LM, especially\nwhen using a long context. Domain adaptation shortens the context length needed\nwith Llama2 to achieve its best performance, i.e., it reduces the computational\ncost of Llama2.\n","authors":["Atsunori Ogawa","Naoyuki Kamo","Kohei Matsuura","Takanori Ashihara","Takafumi Moriya","Takatomo Kano","Naohiro Tawara","Marc Delcroix"],"pdf_url":"https://arxiv.org/pdf/2406.18972v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2406.06371v3","updated":"2024-06-27T07:56:48Z","published":"2024-06-10T15:32:42Z","title":"mHuBERT-147: A Compact Multilingual HuBERT Model","summary":"  We present mHuBERT-147, the first general-purpose massively multilingual\nHuBERT speech representation model trained on 90K hours of clean, open-license\ndata. To scale up the multi-iteration HuBERT approach, we use faiss-based\nclustering, achieving 5.2x faster label assignment than the original method. We\nalso apply a new multilingual batching up-sampling strategy, leveraging both\nlanguage and dataset diversity. After 3 training iterations, our compact 95M\nparameter mHuBERT-147 outperforms larger models trained on substantially more\ndata. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with\nSOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses\nXLS-R (300M params; 436K hours) and demonstrates strong competitiveness against\nthe much larger MMS (1B params; 491K hours). Our findings indicate that\nmHuBERT-147 is a promising model for multilingual speech tasks, offering an\nunprecedented balance between high performance and parameter efficiency.\n","authors":["Marcely Zanon Boito","Vivek Iyer","Nikolaos Lagos","Laurent Besacier","Ioan Calapodescu"],"pdf_url":"https://arxiv.org/pdf/2406.06371v3.pdf","comment":"Extended version of the Interspeech 2024 paper of same name"},{"id":"http://arxiv.org/abs/2406.18966v1","updated":"2024-06-27T07:56:44Z","published":"2024-06-27T07:56:44Z","title":"UniGen: A Unified Framework for Textual Dataset Generation Using Large\n  Language Models","summary":"  Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly\nimpacted various fields by enabling high-quality synthetic data generation and\nreducing dependence on expensive human-generated datasets. Despite this,\nchallenges remain in the areas of generalization, controllability, diversity,\nand truthfulness within the existing generative frameworks. To address these\nchallenges, this paper presents UniGen, a comprehensive LLM-powered framework\ndesigned to produce diverse, accurate, and highly controllable datasets. UniGen\nis adaptable, supporting all types of text datasets and enhancing the\ngenerative process through innovative mechanisms. To augment data diversity,\nUniGen incorporates an attribute-guided generation module and a group checking\nfeature. For accuracy, it employs a code-based mathematical assessment for\nlabel verification alongside a retrieval-augmented generation technique for\nfactual validation. The framework also allows for user-specified constraints,\nenabling customization of the data generation process to suit particular\nrequirements. Extensive experiments demonstrate the superior quality of data\ngenerated by UniGen, and each module within UniGen plays a critical role in\nthis enhancement. Additionally, UniGen is applied in two practical scenarios:\nbenchmarking LLMs and data augmentation. The results indicate that UniGen\neffectively supports dynamic and evolving benchmarking, and that data\naugmentation improves LLM capabilities in various domains, including\nagent-oriented abilities and reasoning skills.\n","authors":["Siyuan Wu","Yue Huang","Chujie Gao","Dongping Chen","Qihui Zhang","Yao Wan","Tianyi Zhou","Xiangliang Zhang","Jianfeng Gao","Chaowei Xiao","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18934v1","updated":"2024-06-27T07:01:23Z","published":"2024-06-27T07:01:23Z","title":"The single-use restriction for register automata and transducers over\n  infinite alphabets","summary":"  This thesis studies the single-use restriction for register automata and\ntransducers over infinite alphabets. The restriction requires that a\nread-access to a register should have the side effect of destroying its\ncontents. This constraint results in robust classes of languages and\ntransductions. For automata models, we show that one-way register automata,\ntwo-way register automata, and orbit-finite monoids have the same expressive\npower. For transducer models, we show that single-use Mealy machines and\nsingle-use two-way transducers admit versions of the Krohn-Rhodes decomposition\ntheorem. Moreover, single-use Mealy machines are equivalent to an algebraic\nmodel called local algebraic semigroup transductions. Additionally, we show\nthat single-use two-way transducers are equivalent to single-use streaming\nstring transducers (SSTs) over infinite alphabets and to regular list functions\nwith atoms.\n  Compared with the previous work arXiv:1907.10504, this thesis offers a\ncoherent narrative on the single-use restriction. We introduce an abstract\nnotion of single-use functions and use them to define all the discussed\nsingle-use models. We also introduce and study the algebraic models of local\nsemigroup transduction and local rational semigroup transduction.\n","authors":["Rafał Stefański"],"pdf_url":"https://arxiv.org/pdf/2406.18934v1.pdf","comment":"PhD Thesis at University of Warsaw. Supervisor: Miko{\\l}aj\n  Boja\\'nczyk"},{"id":"http://arxiv.org/abs/2402.18344v2","updated":"2024-06-27T06:54:58Z","published":"2024-02-28T14:09:02Z","title":"Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems\n  in Commonsense Reasoning","summary":"  Large language models exhibit high-level commonsense reasoning abilities,\nespecially with enhancement methods like Chain-of-Thought (CoT). However, we\nfind these CoT-like methods lead to a considerable number of originally correct\nanswers turning wrong, which we define as the Toxic CoT problem. To interpret\nand mitigate this problem, we first utilize attribution tracing and causal\ntracing methods to probe the internal working mechanism of the LLM during CoT\nreasoning. Through comparisons, we prove that the model exhibits information\nloss from the question over the shallow attention layers when generating\nrationales or answers. Based on the probing findings, we design a novel method\ncalled RIDERS (Residual decodIng and sERial-position Swap), which compensates\nfor the information deficit in the model from both decoding and serial-position\nperspectives. Through extensive experiments on multiple commonsense reasoning\nbenchmarks, we validate that this method not only significantly eliminates\nToxic CoT problems (decreased by 23.6%), but also effectively improves the\nmodel's overall commonsense reasoning performance (increased by 5.5%).\n","authors":["Jiachun Li","Pengfei Cao","Chenhao Wang","Zhuoran Jin","Yubo Chen","Daojian Zeng","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.18344v2.pdf","comment":"Accepted as a long paper to ACL 2024 Main, 25 pages, 22 figures"},{"id":"http://arxiv.org/abs/2406.18928v1","updated":"2024-06-27T06:40:01Z","published":"2024-06-27T06:40:01Z","title":"Enhanced ASR Robustness to Packet Loss with a Front-End Adaptation\n  Network","summary":"  In the realm of automatic speech recognition (ASR), robustness in noisy\nenvironments remains a significant challenge. Recent ASR models, such as\nWhisper, have shown promise, but their efficacy in noisy conditions can be\nfurther enhanced. This study is focused on recovering from packet loss to\nimprove the word error rate (WER) of ASR models. We propose using a front-end\nadaptation network connected to a frozen ASR model. The adaptation network is\ntrained to modify the corrupted input spectrum by minimizing the criteria of\nthe ASR model in addition to an enhancement loss function. Our experiments\ndemonstrate that the adaptation network, trained on Whisper's criteria, notably\nreduces word error rates across domains and languages in packet-loss scenarios.\nThis improvement is achieved with minimal affect to Whisper model's\nfoundational performance, underscoring our method's practicality and potential\nin enhancing ASR models in challenging acoustic environments.\n","authors":["Yehoshua Dissen","Shiry Yonash","Israel Cohen","Joseph Keshet"],"pdf_url":"https://arxiv.org/pdf/2406.18928v1.pdf","comment":"Accepted for publication at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2303.08601v2","updated":"2024-06-27T06:34:11Z","published":"2023-03-15T13:15:22Z","title":"GCRE-GPT: A Generative Model for Comparative Relation Extraction","summary":"  Given comparative text, comparative relation extraction aims to extract two\ntargets (\\eg two cameras) in comparison and the aspect they are compared for\n(\\eg image quality). The extracted comparative relations form the basis of\nfurther opinion analysis.Existing solutions formulate this task as a sequence\nlabeling task, to extract targets and aspects. However, they cannot directly\nextract comparative relation(s) from text. In this paper, we show that\ncomparative relations can be directly extracted with high accuracy, by\ngenerative model. Based on GPT-2, we propose a Generation-based Comparative\nRelation Extractor (GCRE-GPT). Experiment results show that \\modelname achieves\nstate-of-the-art accuracy on two datasets.\n","authors":["Yequan Wang","Hengran Zhang","Aixin Sun","Xuying Meng"],"pdf_url":"https://arxiv.org/pdf/2303.08601v2.pdf","comment":"6 pages, 6 tables, 1 figure"},{"id":"http://arxiv.org/abs/2406.18925v1","updated":"2024-06-27T06:32:56Z","published":"2024-06-27T06:32:56Z","title":"Selective Vision is the Challenge for Visual Reasoning: A Benchmark for\n  Visual Argument Understanding","summary":"  Visual arguments, often used in advertising or social causes, rely on images\nto persuade viewers to do or believe something. Understanding these arguments\nrequires selective vision: only specific visual stimuli within an image are\nrelevant to the argument, and relevance can only be understood within the\ncontext of a broader argumentative structure. While visual arguments are\nreadily appreciated by human audiences, we ask: are today's AI capable of\nsimilar understanding?\n  We collect and release VisArgs, an annotated corpus designed to make explicit\nthe (usually implicit) structures underlying visual arguments. VisArgs includes\n1,611 images accompanied by three types of textual annotations: 5,112 visual\npremises (with region annotations), 5,574 commonsense premises, and reasoning\ntrees connecting them to a broader argument. We propose three tasks over\nVisArgs to probe machine capacity for visual argument understanding:\nlocalization of premises, identification of premises, and deduction of\nconclusions. Experiments demonstrate that 1) machines cannot fully identify the\nrelevant visual cues. The top-performing model, GPT-4-O, achieved an accuracy\nof only 78.5%, whereas humans reached 98.0%. All models showed a performance\ndrop, with an average decrease in accuracy of 19.5%, when the comparison set\nwas changed from objects outside the image to irrelevant objects within the\nimage. Furthermore, 2) this limitation is the greatest factor impacting their\nperformance in understanding visual arguments. Most models improved the most\nwhen given relevant visual premises as additional inputs, compared to other\ninputs, for deducing the conclusion of the visual argument.\n","authors":["Jiwan Chung","Sungjae Lee","Minseo Kim","Seungju Han","Ashkan Yousefpour","Jack Hessel","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2406.18925v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.18921v1","updated":"2024-06-27T06:24:00Z","published":"2024-06-27T06:24:00Z","title":"Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models\n  with Personality-Indicative Data","summary":"  Role-playing agents (RPA) have been a popular application area for large\nlanguage models (LLMs), attracting significant interest from both industry and\nacademia.While existing RPAs well portray the characters' knowledge and tones,\nthey face challenges in capturing their minds, especially for small\nrole-playing language models (RPLMs). In this paper, we propose to enhance\nRPLMs via personality-indicative data. Specifically, we leverage questions from\npsychological scales and distill advanced RPAs to generate dialogues that grasp\nthe minds of characters. Experimental results validate that RPLMs trained with\nour dataset exhibit advanced role-playing capabilities for both general and\npersonality-related evaluations. Code and data are available at\n\\href{https://github.com/alienet1109/RolePersonality}{this URL}.\n","authors":["Yiting Ran","Xintao Wang","Rui Xu","Xinfeng Yuan","Jiaqing Liang","Yanghua Xiao","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2406.18921v1.pdf","comment":"10pages"},{"id":"http://arxiv.org/abs/2406.18916v1","updated":"2024-06-27T06:13:05Z","published":"2024-06-27T06:13:05Z","title":"TrustUQA: A Trustful Framework for Unified Structured Data Question\n  Answering","summary":"  Natural language question answering (QA) over structured data sources such as\ntables and knowledge graphs (KGs) have been widely investigated, for example\nwith Large Language Models (LLMs). The main solutions include question to\nformal query parsing and retrieval-based answer generation. However, current\nmethods of the former often suffer from weak generalization, failing to dealing\nwith multiple sources simultaneously, while the later is limited in\ntrustfulness. In this paper, we propose UnifiedTQA, a trustful QA framework\nthat can simultaneously support multiple types of structured data in a unified\nway. To this end, it adopts an LLM-friendly and unified knowledge\nrepresentation method called Condition Graph (CG), and uses an LLM and\ndemonstration-based two-level method for CG querying. For enhancement, it is\nalso equipped with dynamic demonstration retrieval. We have evaluated\nUnifiedTQA with 5 benchmarks covering 3 types of structured data. It\noutperforms 2 existing unified structured data QA methods and in comparison\nwith the baselines that are specific to a data type, it achieves\nstate-of-the-art on 2 of them. Further more, we demonstrates potential of our\nmethod for more general QA tasks, QA over mixed structured data and QA across\nstructured data.\n","authors":["Wen Zhang","Long Jin","Yushan Zhu","Jiaoyan Chen","Zhiwei Huang","Junjie Wang","Yin Hua","Lei Liang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10584v3","updated":"2024-06-27T06:00:40Z","published":"2024-06-15T10:02:46Z","title":"Concentrate Attention: Towards Domain-Generalizable Prompt Optimization\n  for Language Models","summary":"  Recent advances in prompt optimization have notably enhanced the performance\nof pre-trained language models (PLMs) on downstream tasks. However, the\npotential of optimized prompts on domain generalization has been\nunder-explored. To explore the nature of prompt generalization on unknown\ndomains, we conduct pilot experiments and find that (i) Prompts gaining more\nattention weight from PLMs' deep layers are more generalizable and (ii) Prompts\nwith more stable attention distributions in PLMs' deep layers are more\ngeneralizable. Thus, we offer a fresh objective towards domain-generalizable\nprompts optimization named \"Concentration\", which represents the \"lookback\"\nattention from the current decoding token to the prompt tokens, to increase the\nattention strength on prompts and reduce the fluctuation of attention\ndistribution. We adapt this new objective to popular soft prompt and hard\nprompt optimization methods, respectively. Extensive experiments demonstrate\nthat our idea improves comparison prompt optimization methods by 1.42% for soft\nprompt generalization and 2.16% for hard prompt generalization in accuracy on\nthe multi-source domain generalization setting, while maintaining satisfying\nin-domain performance. The promising results validate the effectiveness of our\nproposed prompt optimization objective and provide key insights into\ndomain-generalizable prompts.\n","authors":["Chengzhengxu Li","Xiaoming Liu","Zhaohan Zhang","Yichen Wang","Chen Liu","Yu Lan","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2406.10584v3.pdf","comment":"Submitted to NeurIPS 2024, Preprint, Under review"},{"id":"http://arxiv.org/abs/2406.18910v1","updated":"2024-06-27T05:52:10Z","published":"2024-06-27T05:52:10Z","title":"Factor-Conditioned Speaking-Style Captioning","summary":"  This paper presents a novel speaking-style captioning method that generates\ndiverse descriptions while accurately predicting speaking-style information.\nConventional learning criteria directly use original captions that contain not\nonly speaking-style factor terms but also syntax words, which disturbs learning\nspeaking-style information. To solve this problem, we introduce\nfactor-conditioned captioning (FCC), which first outputs a phrase representing\nspeaking-style factors (e.g., gender, pitch, etc.), and then generates a\ncaption to ensure the model explicitly learns speaking-style factors. We also\npropose greedy-then-sampling (GtS) decoding, which first predicts\nspeaking-style factors deterministically to guarantee semantic accuracy, and\nthen generates a caption based on factor-conditioned sampling to ensure\ndiversity. Experiments show that FCC outperforms the original caption-based\ntraining, and with GtS, it generates more diverse captions while keeping style\nprediction performance.\n","authors":["Atsushi Ando","Takafumi Moriya","Shota Horiguchi","Ryo Masumura"],"pdf_url":"https://arxiv.org/pdf/2406.18910v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2402.11175v2","updated":"2024-06-27T05:42:12Z","published":"2024-02-17T02:50:33Z","title":"M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text\n  Detection","summary":"  The advent of Large Language Models (LLMs) has brought an unprecedented surge\nin machine-generated text (MGT) across diverse channels. This raises legitimate\nconcerns about its potential misuse and societal implications. The need to\nidentify and differentiate such content from genuine human-generated text is\ncritical in combating disinformation, preserving the integrity of education and\nscientific fields, and maintaining trust in communication. In this work, we\naddress this problem by introducing a new benchmark based on a multilingual,\nmulti-domain, and multi-generator corpus of MGTs -- M4GT-Bench. The benchmark\nis compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT\ndetection; (2) multi-way detection where one need to identify, which particular\nmodel generated the text; and (3) mixed human-machine text detection, where a\nword boundary delimiting MGT from human-written content should be determined.\nOn the developed benchmark, we have tested several MGT detection baselines and\nalso conducted an evaluation of human performance. We see that obtaining good\nperformance in MGT detection usually requires an access to the training data\nfrom the same domain and generators. The benchmark is available at\nhttps://github.com/mbzuai-nlp/M4GT-Bench.\n","authors":["Yuxia Wang","Jonibek Mansurov","Petar Ivanov","Jinyan Su","Artem Shelmanov","Akim Tsvigun","Osama Mohanned Afzal","Tarek Mahmoud","Giovanni Puccetti","Thomas Arnold","Alham Fikri Aji","Nizar Habash","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2402.11175v2.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2406.18907v1","updated":"2024-06-27T05:38:49Z","published":"2024-06-27T05:38:49Z","title":"Historia Magistra Vitae: Dynamic Topic Modeling of Roman Literature\n  using Neural Embeddings","summary":"  Dynamic topic models have been proposed as a tool for historical analysis,\nbut traditional approaches have had limited usefulness, being difficult to\nconfigure, interpret, and evaluate. In this work, we experiment with a recent\napproach for dynamic topic modeling using BERT embeddings. We compare topic\nmodels built using traditional statistical models (LDA and NMF) and the\nBERT-based model, modeling topics over the entire surviving corpus of Roman\nliterature. We find that while quantitative metrics prefer statistical models,\nqualitative evaluation finds better insights from the neural model.\nFurthermore, the neural topic model is less sensitive to hyperparameter\nconfiguration and thus may make dynamic topic modeling more viable for\nhistorical researchers.\n","authors":["Michael Ginn","Mans Hulden"],"pdf_url":"https://arxiv.org/pdf/2406.18907v1.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.18906v1","updated":"2024-06-27T05:36:53Z","published":"2024-06-27T05:36:53Z","title":"Sonnet or Not, Bot? Poetry Evaluation for Large Models and Datasets","summary":"  Large language models (LLMs) can now generate and recognize text in a wide\nrange of styles and genres, including highly specialized, creative genres like\npoetry. But what do LLMs really know about poetry? What can they know about\npoetry? We develop a task to evaluate how well LLMs recognize a specific aspect\nof poetry, poetic form, for more than 20 forms and formal elements in the\nEnglish language. Poetic form captures many different poetic features,\nincluding rhyme scheme, meter, and word or line repetition. We use this task to\nreflect on LLMs' current poetic capabilities, as well as the challenges and\npitfalls of creating NLP benchmarks for poetry and for other creative tasks. In\nparticular, we use this task to audit and reflect on the poems included in\npopular pretraining datasets. Our findings have implications for NLP\nresearchers interested in model evaluation, digital humanities and cultural\nanalytics scholars, and cultural heritage professionals.\n","authors":["Melanie Walsh","Anna Preus","Maria Antoniak"],"pdf_url":"https://arxiv.org/pdf/2406.18906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09395v4","updated":"2024-06-27T05:23:50Z","published":"2024-01-17T18:13:07Z","title":"Evaluating LLMs' Mathematical and Coding Competency through\n  Ontology-guided Interventions","summary":"  Recent advancements in Large Language Models (LLMs) have showcased striking\nresults on existing logical reasoning benchmarks, with some models even\nsurpassing human performance. However, the true depth of their competencies and\nrobustness in reasoning tasks remains an open question. To this end, in this\npaper, we focus on two popular reasoning tasks: arithmetic reasoning and code\ngeneration. Particularly, we introduce: (i) a general ontology of perturbations\nfor maths and coding questions, (ii) a semi-automatic method to apply these\nperturbations, and (iii) two datasets, MORE and CORE, respectively, of\nperturbed maths and coding problems to probe the limits of LLM capabilities in\nnumeric reasoning and coding tasks. Through comprehensive evaluations of both\nclosed-source and open-source LLMs, we show a significant performance drop\nacross all the models against the perturbed questions, suggesting that the\ncurrent LLMs lack robust problem solving skills and structured reasoning\nabilities in many areas, as defined by our ontology. We open source the\ndatasets and source codes at: https://github.com/declare-lab/llm_robustness.\n","authors":["Pengfei Hong","Navonil Majumder","Deepanway Ghosal","Somak Aditya","Rada Mihalcea","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2401.09395v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06399v2","updated":"2024-06-27T05:22:05Z","published":"2024-03-11T03:21:15Z","title":"GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing","summary":"  Language documentation projects often involve the creation of annotated text\nin a format such as interlinear glossed text (IGT), which captures fine-grained\nmorphosyntactic analyses in a morpheme-by-morpheme format. However, there are\nfew existing resources providing large amounts of standardized, easily\naccessible IGT data, limiting their applicability to linguistic research, and\nmaking it difficult to use such data in NLP modeling.\n  We compile the largest existing corpus of IGT data from a variety of sources,\ncovering over 450k examples across 1.8k languages, to enable research on\ncrosslingual transfer and IGT generation. We normalize much of our data to\nfollow a standard set of labels across languages.\n  Furthermore, we explore the task of automatically generating IGT in order to\naid documentation projects. As many languages lack sufficient monolingual data,\nwe pretrain a large multilingual model on our corpus. We demonstrate the\nutility of this model by finetuning it on monolingual corpora, outperforming\nSOTA models by up to 6.6%. We will make our pretrained model and dataset\navailable through Hugging Face, as well as provide access through a web\ninterface for use in language documentation efforts.\n","authors":["Michael Ginn","Lindia Tjuatja","Taiqi He","Enora Rice","Graham Neubig","Alexis Palmer","Lori Levin"],"pdf_url":"https://arxiv.org/pdf/2403.06399v2.pdf","comment":"19 pages, 7 figures Submitted to ACL ARR June 2024. First two authors\n  are equal contribution"},{"id":"http://arxiv.org/abs/2406.18895v1","updated":"2024-06-27T05:17:04Z","published":"2024-06-27T05:17:04Z","title":"Can we teach language models to gloss endangered languages?","summary":"  Interlinear glossed text (IGT) is a popular format in language documentation\nprojects, where each morpheme is labeled with a descriptive annotation.\nAutomating the creation of interlinear glossed text can be desirable to reduce\nannotator effort and maintain consistency across annotated corpora. Prior\nresearch has explored a number of statistical and neural methods for\nautomatically producing IGT.\n  As large language models (LLMs) have showed promising results across\nmultilingual tasks, even for rare, endangered languages, it is natural to\nwonder whether they can be utilized for the task of generating IGT. We explore\nwhether LLMs can be effective at the task of interlinear glossing with\nin-context learning, without any traditional training. We propose new\napproaches for selecting examples to provide in-context, observing that\ntargeted selection can significantly improve performance. We find that\nLLM-based methods beat standard transformer baselines, despite requiring no\ntraining at all. These approaches still underperform state-of-the-art\nsupervised systems for the task, but are highly practical for researchers\noutside of the NLP community, requiring minimal effort to use.\n","authors":["Michael Ginn","Mans Hulden","Alexis Palmer"],"pdf_url":"https://arxiv.org/pdf/2406.18895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08279v3","updated":"2024-06-27T04:55:28Z","published":"2023-10-12T12:31:23Z","title":"Enhancing Text-based Knowledge Graph Completion with Zero-Shot Large\n  Language Models: A Focus on Semantic Enhancement","summary":"  The design and development of text-based knowledge graph completion (KGC)\nmethods leveraging textual entity descriptions are at the forefront of\nresearch. These methods involve advanced optimization techniques such as soft\nprompts and contrastive learning to enhance KGC models. The effectiveness of\ntext-based methods largely hinges on the quality and richness of the training\ndata. Large language models (LLMs) can utilize straightforward prompts to alter\ntext data, thereby enabling data augmentation for KGC. Nevertheless, LLMs\ntypically demand substantial computational resources. To address these issues,\nwe introduce a framework termed constrained prompts for KGC (CP-KGC). This\nCP-KGC framework designs prompts that adapt to different datasets to enhance\nsemantic richness. Additionally, CP-KGC employs a context constraint strategy\nto effectively identify polysemous entities within KGC datasets. Through\nextensive experimentation, we have verified the effectiveness of this\nframework. Even after quantization, the LLM (Qwen-7B-Chat-int4) still enhances\nthe performance of text-based KGC methods \\footnote{Code and datasets are\navailable at\n\\href{https://github.com/sjlmg/CP-KGC}{https://github.com/sjlmg/CP-KGC}}. This\nstudy extends the performance limits of existing models and promotes further\nintegration of KGC with LLMs.\n","authors":["Rui Yang","Jiahao Zhu","Jianping Man","Li Fang","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.08279v3.pdf","comment":"new version"},{"id":"http://arxiv.org/abs/2402.09773v2","updated":"2024-06-27T04:49:11Z","published":"2024-02-15T08:03:12Z","title":"NutePrune: Efficient Progressive Pruning with Numerous Teachers for\n  Large Language Models","summary":"  The considerable size of Large Language Models (LLMs) presents notable\ndeployment challenges, particularly on resource-constrained hardware.\nStructured pruning, offers an effective means to compress LLMs, thereby\nreducing storage costs and enhancing inference speed for more efficient\nutilization. In this work, we study data-efficient and resource-efficient\nstructure pruning methods to obtain smaller yet still powerful models.\nKnowledge Distillation is well-suited for pruning, as the intact model can\nserve as an excellent teacher for pruned students. However, it becomes\nchallenging in the context of LLMs due to memory constraints. To address this,\nwe propose an efficient progressive Numerous-teacher pruning method\n(NutePrune). NutePrune mitigates excessive memory costs by loading only one\nintact model and integrating it with various masks and LoRA modules, enabling\nit to seamlessly switch between teacher and student roles. This approach allows\nus to leverage numerous teachers with varying capacities to progressively guide\nthe pruned model, enhancing overall performance. Extensive experiments across\nvarious tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot\nexperiments, NutePrune retains 97.17% of the performance of the original model\nat 20% sparsity and 95.07% at 25% sparsity. Our code is available at\nhttps://github.com/Lucius-lsr/NutePrune.\n","authors":["Shengrui Li","Junzhe Chen","Xueting Han","Jing Bai"],"pdf_url":"https://arxiv.org/pdf/2402.09773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11424v2","updated":"2024-06-27T04:41:07Z","published":"2024-05-19T02:09:50Z","title":"Metric Dimension and Resolvability of Jaccard Spaces","summary":"  A subset of points in a metric space is said to resolve it if each point in\nthe space is uniquely characterized by its distance to each point in the\nsubset. In particular, resolving sets can be used to represent points in\nabstract metric spaces as Euclidean vectors. Importantly, due to the triangle\ninequality, points close by in the space are represented as vectors with\nsimilar coordinates, which may find applications in classification problems of\nsymbolic objects under suitably chosen metrics. In this manuscript, we address\nthe resolvability of Jaccard spaces, i.e., metric spaces of the form\n$(2^X,\\text{Jac})$, where $2^X$ is the power set of a finite set $X$, and\n$\\text{Jac}$ is the Jaccard distance between subsets of $X$. Specifically, for\ndifferent $a,b\\in 2^X$, $\\text{Jac}(a,b)=|a\\Delta b|/|a\\cup b|$, where\n$|\\cdot|$ denotes size (i.e., cardinality) and $\\Delta$ denotes the symmetric\ndifference of sets. We combine probabilistic and linear algebra arguments to\nconstruct highly likely but nearly optimal (i.e., of minimal size) resolving\nsets of $(2^X,\\text{Jac})$. In particular, we show that the metric dimension of\n$(2^X,\\text{Jac})$, i.e., the minimum size of a resolving set of this space, is\n$\\Theta(|X|/\\ln|X|)$. In addition, we show that a much smaller subset of $2^X$\nsuffices to resolve, with high probability, all different pairs of subsets of\n$X$ of cardinality at most $\\sqrt{|X|}/\\ln|X|$, up to a factor.\n","authors":["Manuel E. Lladser","Alexander J. Paradise"],"pdf_url":"https://arxiv.org/pdf/2405.11424v2.pdf","comment":"13 pages, 1 table"},{"id":"http://arxiv.org/abs/2406.18294v2","updated":"2024-06-27T04:40:52Z","published":"2024-06-26T12:26:16Z","title":"Hierarchical Context Pruning: Optimizing Real-World Code Completion with\n  Repository-Level Pretrained Code LLMs","summary":"  Some recently developed code large language models (Code LLMs) have been\npre-trained on repository-level code data (Repo-Code LLMs), enabling these\nmodels to recognize repository structures and utilize cross-file information\nfor code completion. However, in real-world development scenarios, simply\nconcatenating the entire code repository often exceeds the context window\nlimits of these Repo-Code LLMs, leading to significant performance degradation.\nIn this study, we conducted extensive preliminary experiments and analyses on\nsix Repo-Code LLMs. The results indicate that maintaining the topological\ndependencies of files and increasing the code file content in the completion\nprompts can improve completion accuracy; pruning the specific implementations\nof functions in all dependent files does not significantly reduce the accuracy\nof completions. Based on these findings, we proposed a strategy named\nHierarchical Context Pruning (HCP) to construct completion prompts with high\ninformational code content. The HCP models the code repository at the function\nlevel, maintaining the topological dependencies between code files while\nremoving a large amount of irrelevant code content, significantly reduces the\ninput length for repository-level code completion. We applied the HCP strategy\nin experiments with six Repo-Code LLMs, and the results demonstrate that our\nproposed method can significantly enhance completion accuracy while\nsubstantially reducing the length of input. Our code and data are available at\nhttps://github.com/Hambaobao/HCP-Coder.\n","authors":["Lei Zhang","Yunshui Li","Jiaming Li","Xiaobo Xia","Jiaxi Yang","Run Luo","Minzheng Wang","Longze Chen","Junhao Liu","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2406.18294v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16040v4","updated":"2024-06-27T04:28:12Z","published":"2024-02-25T09:41:50Z","title":"EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using\n  Discharge Summaries","summary":"  Discharge summaries in Electronic Health Records (EHRs) are crucial for\nclinical decision-making, but their length and complexity make information\nextraction challenging, especially when dealing with accumulated summaries\nacross multiple patient admissions. Large Language Models (LLMs) show promise\nin addressing this challenge by efficiently analyzing vast and complex data.\nExisting benchmarks, however, fall short in properly evaluating LLMs'\ncapabilities in this context, as they typically focus on single-note\ninformation or limited topics, failing to reflect the real-world inquiries\nrequired by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel\nbenchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each\nlinked to distinct patients' discharge summaries. Every QA pair is initially\ngenerated using GPT-4 and then manually reviewed and refined by three\nclinicians to ensure clinical relevance. EHRNoteQA includes questions that\nrequire information across multiple discharge summaries and covers eight\ndiverse topics, mirroring the complexity and diversity of real clinical\ninquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice\nquestion answering, and propose a reliable evaluation method for each. We\nevaluate 27 LLMs using EHRNoteQA and examine various factors affecting the\nmodel performance (e.g., the length and number of discharge summaries).\nFurthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations\nin clinical practice, we measure the correlation between the LLM performance on\nEHRNoteQA, and the LLM performance manually evaluated by clinicians. Results\nshow that LLM performance on EHRNoteQA have higher correlation with\nclinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to\nother benchmarks, demonstrating its practical relevance in evaluating LLMs in\nclinical settings.\n","authors":["Sunjun Kweon","Jiyoun Kim","Heeyoung Kwak","Dongchul Cha","Hangyul Yoon","Kwanghyun Kim","Jeewon Yang","Seunghyun Won","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2402.16040v4.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2406.18880v1","updated":"2024-06-27T04:21:59Z","published":"2024-06-27T04:21:59Z","title":"SSP: Self-Supervised Prompting for Cross-Lingual Transfer to\n  Low-Resource Languages using Large Language Models","summary":"  Recently, very large language models (LLMs) have shown exceptional\nperformance on several English NLP tasks with just in-context learning (ICL),\nbut their utility in other languages is still underexplored. We investigate\ntheir effectiveness for NLP tasks in low-resource languages (LRLs), especially\nin the setting of zero-labelled cross-lingual transfer (0-CLT), where no\nlabelled training data for the target language is available -- however training\ndata from one or more related medium-resource languages (MRLs) is utilized,\nalongside the available unlabeled test data for a target language. We introduce\nSelf-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT\nsetting.\n  SSP is based on the key observation that LLMs output more accurate labels if\nin-context exemplars are from the target language (even if their labels are\nslightly noisy). To operationalize this, since target language training data is\nnot available in 0-CLT, SSP operates in two stages. In Stage I, using source\nMRL training data, target language's test data is noisily labeled. In Stage II,\nthese noisy test data points are used as exemplars in ICL for further improved\nlabelling. Additionally, our implementation of SSP uses a novel Integer Linear\nProgramming (ILP)-based exemplar selection that balances similarity, prediction\nconfidence (when available) and label coverage. Experiments on three tasks and\neleven LRLs (from three regions) demonstrate that SSP strongly outperforms\nexisting SOTA fine-tuned and prompting-based baselines in 0-CLT setup.\n","authors":["Vipul Rathore","Aniruddha Deb","Ankish Chandresh","Parag Singla"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2406.18880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10415v2","updated":"2024-06-27T04:00:19Z","published":"2024-01-18T23:00:54Z","title":"Can Large Language Model Summarizers Adapt to Diverse Scientific\n  Communication Goals?","summary":"  In this work, we investigate the controllability of large language models\n(LLMs) on scientific summarization tasks. We identify key stylistic and content\ncoverage factors that characterize different types of summaries such as paper\nreviews, abstracts, and lay summaries. By controlling stylistic features, we\nfind that non-fine-tuned LLMs outperform humans in the MuP review generation\ntask, both in terms of similarity to reference summaries and human preferences.\nAlso, we show that we can improve the controllability of LLMs with\nkeyword-based classifier-free guidance (CFG) while achieving lexical overlap\ncomparable to strong fine-tuned baselines on arXiv and PubMed. However, our\nresults also indicate that LLMs cannot consistently generate long summaries\nwith more than 8 sentences. Furthermore, these models exhibit limited capacity\nto produce highly abstractive lay summaries. Although LLMs demonstrate strong\ngeneric summarization competency, sophisticated content control without costly\nfine-tuning remains an open problem for domain-specific applications.\n","authors":["Marcio Fonseca","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2401.10415v2.pdf","comment":"ACL 2024 camera ready"},{"id":"http://arxiv.org/abs/2406.18069v2","updated":"2024-06-27T03:58:25Z","published":"2024-06-26T04:54:45Z","title":"Large Language Models for Cuffless Blood Pressure Measurement From\n  Wearable Biosignals","summary":"  Large language models (LLMs) have captured significant interest from both\nacademia and industry due to their impressive performance across various\ntextual tasks. However, the potential of LLMs to analyze physiological\ntime-series data remains an emerging research field. Particularly, there is a\nnotable gap in the utilization of LLMs for analyzing wearable biosignals to\nachieve cuffless blood pressure (BP) measurement, which is critical for the\nmanagement of cardiovascular diseases. This paper presents the first work to\nexplore the capacity of LLMs to perform cuffless BP estimation based on\nwearable biosignals. We extracted physiological features from electrocardiogram\n(ECG) and photoplethysmogram (PPG) signals and designed context-enhanced\nprompts by combining these features with BP domain knowledge and user\ninformation. Subsequently, we adapted LLMs to BP estimation tasks through\nfine-tuning. To evaluate the proposed approach, we conducted assessments of ten\nadvanced LLMs using a comprehensive public dataset of wearable biosignals from\n1,272 participants. The experimental results demonstrate that the optimally\nfine-tuned LLM significantly surpasses conventional task-specific baselines,\nachieving an estimation error of 0.00 $\\pm$ 9.25 mmHg for systolic BP and 1.29\n$\\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies highlight the\nbenefits of our context enhancement strategy, leading to an 8.9% reduction in\nmean absolute error for systolic BP estimation. This paper pioneers the\nexploration of LLMs for cuffless BP measurement, providing a potential solution\nto enhance the accuracy of cuffless BP measurement.\n","authors":["Zengding Liu","Chen Chen","Jiannong Cao","Minglei Pan","Jikui Liu","Nan Li","Fen Miao","Ye Li"],"pdf_url":"https://arxiv.org/pdf/2406.18069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16562v2","updated":"2024-06-27T03:57:05Z","published":"2024-06-24T11:56:15Z","title":"EVALALIGN: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned\n  Data for Evaluating Text-to-Image Models","summary":"  The recent advancements in text-to-image generative models have been\nremarkable. Yet, the field suffers from a lack of evaluation metrics that\naccurately reflect the performance of these models, particularly lacking\nfine-grained metrics that can guide the optimization of the models. In this\npaper, we propose EvalAlign, a metric characterized by its accuracy, stability,\nand fine granularity. Our approach leverages the capabilities of Multimodal\nLarge Language Models (MLLMs) pre-trained on extensive datasets. We develop\nevaluation protocols that focus on two key dimensions: image faithfulness and\ntext-image alignment. Each protocol comprises a set of detailed, fine-grained\ninstructions linked to specific scoring options, enabling precise manual\nscoring of the generated images. We Supervised Fine-Tune (SFT) the MLLM to\nalign closely with human evaluative judgments, resulting in a robust evaluation\nmodel. Our comprehensive tests across 24 text-to-image generation models\ndemonstrate that EvalAlign not only provides superior metric stability but also\naligns more closely with human preferences than existing metrics, confirming\nits effectiveness and utility in model assessment.\n","authors":["Zhiyu Tan","Xiaomeng Yang","Luozheng Qin","Mengping Yang","Cheng Zhang","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2406.16562v2.pdf","comment":"Github Repository: https://github.com/SAIS-FUXI/EvalAlign"},{"id":"http://arxiv.org/abs/2402.14905v2","updated":"2024-06-27T03:53:46Z","published":"2024-02-22T18:58:55Z","title":"MobileLLM: Optimizing Sub-billion Parameter Language Models for\n  On-Device Use Cases","summary":"  This paper addresses the growing need for efficient large language models\n(LLMs) on mobile devices, driven by increasing cloud costs and latency\nconcerns. We focus on designing top-quality LLMs with fewer than a billion\nparameters, a practical choice for mobile deployment. Contrary to prevailing\nbelief emphasizing the pivotal role of data and parameter quantity in\ndetermining model quality, our investigation underscores the significance of\nmodel architecture for sub-billion scale LLMs. Leveraging deep and thin\narchitectures, coupled with embedding sharing and grouped-query attention\nmechanisms, we establish a strong baseline network denoted as MobileLLM, which\nattains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M\nstate-of-the-art models. Additionally, we propose an immediate block-wise\nweight-sharing approach with no increase in model size and only marginal\nlatency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a\nfurther accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover,\nMobileLLM model family shows significant improvements compared to previous\nsub-billion models on chat benchmarks, and demonstrates close correctness to\nLLaMA-v2 7B in API calling tasks, highlighting the capability of small models\nfor common on-device use cases.\n","authors":["Zechun Liu","Changsheng Zhao","Forrest Iandola","Chen Lai","Yuandong Tian","Igor Fedorov","Yunyang Xiong","Ernie Chang","Yangyang Shi","Raghuraman Krishnamoorthi","Liangzhen Lai","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/2402.14905v2.pdf","comment":"ICML 2024. Code is available at\n  https://github.com/facebookresearch/MobileLLM"},{"id":"http://arxiv.org/abs/2406.18871v1","updated":"2024-06-27T03:52:35Z","published":"2024-06-27T03:52:35Z","title":"DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text\n  Alignment","summary":"  Recent speech language models (SLMs) typically incorporate pre-trained speech\nmodels to extend the capabilities from large language models (LLMs). In this\npaper, we propose a Descriptive Speech-Text Alignment approach that leverages\nspeech captioning to bridge the gap between speech and text modalities,\nenabling SLMs to interpret and generate comprehensive natural language\ndescriptions, thereby facilitating the capability to understand both linguistic\nand non-linguistic features in speech. Enhanced with the proposed approach, our\nmodel demonstrates superior performance on the Dynamic-SUPERB benchmark,\nparticularly in generalizing to unseen tasks. Moreover, we discover that the\naligned model exhibits a zero-shot instruction-following capability without\nexplicit speech instruction tuning. These findings highlight the potential to\nreshape instruction-following SLMs by incorporating rich, descriptive speech\ncaptions.\n","authors":["Ke-Han Lu","Zhehuai Chen","Szu-Wei Fu","He Huang","Boris Ginsburg","Yu-Chiang Frank Wang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.18871v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2406.18872v1","updated":"2024-06-27T03:52:35Z","published":"2024-06-27T03:52:35Z","title":"Efficacy of Language Model Self-Play in Non-Zero-Sum Games","summary":"  Game-playing agents like AlphaGo have achieved superhuman performance through\nself-play, which is theoretically guaranteed to yield optimal policies in\ncompetitive games. However, most language tasks are partially or fully\ncooperative, so it is an open question whether techniques like self-play can\neffectively be used to improve language models. We empirically investigate this\nquestion in a negotiation game setting known as Deal or No Deal (DoND).\nCrucially, the objective in DoND can be modified to produce a fully cooperative\ngame, a strictly competitive one, or anything in between. We finetune language\nmodels in self-play over multiple rounds of filtered behavior cloning in DoND\nfor each of these objectives. Contrary to expectations, we find that language\nmodel self-play leads to significant performance gains in both cooperation and\ncompetition with humans, suggesting that self-play and related techniques have\npromise despite a lack of theoretical guarantees.\n","authors":["Austen Liao","Nicholas Tomlin","Dan Klein"],"pdf_url":"https://arxiv.org/pdf/2406.18872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08704v2","updated":"2024-06-27T03:48:35Z","published":"2023-11-15T05:11:26Z","title":"Can Large Language Models Follow Concept Annotation Guidelines? A Case\n  Study on Scientific and Financial Domains","summary":"  Although large language models (LLMs) exhibit remarkable capacity to leverage\nin-context demonstrations, it is still unclear to what extent they can learn\nnew concepts or facts from ground-truth labels. To address this question, we\nexamine the capacity of instruction-tuned LLMs to follow in-context concept\nguidelines for sentence labeling tasks. We design guidelines that present\ndifferent types of factual and counterfactual concept definitions, which are\nused as prompts for zero-shot sentence classification tasks. Our results show\nthat although concept definitions consistently help in task performance, only\nthe larger models (with 70B parameters or more) have limited ability to work\nunder counterfactual contexts. Importantly, only proprietary models such as\nGPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is\ndue to more sophisticated alignment methods. Finally, we find that\nFalcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which\nindicates that careful fine-tuning is more effective than increasing model\nscale. Altogether, our simple evaluation method reveals significant gaps in\nconcept understanding between the most capable open-source language models and\nthe leading proprietary APIs.\n","authors":["Marcio Fonseca","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2311.08704v2.pdf","comment":"ACL 2024 camera ready"},{"id":"http://arxiv.org/abs/2305.14256v2","updated":"2024-06-27T03:43:18Z","published":"2023-05-23T17:10:37Z","title":"Linear Cross-Lingual Mapping of Sentence Embeddings","summary":"  Semantics of a sentence is defined with much less ambiguity than semantics of\na single word, and we assume that it should be better preserved by translation\nto another language. If multilingual sentence embeddings intend to represent\nsentence semantics, then the similarity between embeddings of any two sentences\nmust be invariant with respect to translation. Based on this suggestion, we\nconsider a simple linear cross-lingual mapping as a possible improvement of the\nmultilingual embeddings. We also consider deviation from orthogonality\nconditions as a measure of deficiency of the embeddings.\n","authors":["Oleg Vasilyev","Fumika Isono","John Bohannon"],"pdf_url":"https://arxiv.org/pdf/2305.14256v2.pdf","comment":"Accepted to ACL Findings 2024"},{"id":"http://arxiv.org/abs/2404.05091v3","updated":"2024-06-27T03:36:22Z","published":"2024-04-07T22:16:50Z","title":"MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation\n  and Fine-grained Classification","summary":"  To advance the evaluation of multimodal math reasoning in large multimodal\nmodels (LMMs), this paper introduces a novel benchmark, MM-MATH. MM-MATH\nconsists of 5,929 open-ended middle school math problems with visual contexts,\nwith fine-grained classification across difficulty, grade level, and knowledge\npoints. Unlike existing benchmarks relying on binary answer comparison, MM-MATH\nincorporates both outcome and process evaluations. Process evaluation employs\nLMM-as-a-judge to automatically analyze solution steps, identifying and\ncategorizing errors into specific error types. Extensive evaluation of ten\nmodels on MM-MATH reveals significant challenges for existing LMMs,\nhighlighting their limited utilization of visual information and struggles with\nhigher-difficulty problems. The best-performing model achieves only 31%\naccuracy on MM-MATH, compared to 82% for humans. This highlights the\nchallenging nature of our benchmark for existing models and the significant gap\nbetween the multimodal reasoning capabilities of current models and humans. Our\nprocess evaluation reveals that diagram misinterpretation is the most common\nerror, accounting for more than half of the total error cases, underscoring the\nneed for improved image comprehension in multimodal reasoning.\n","authors":["Kai Sun","Yushi Bai","Ji Qi","Lei Hou","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2404.05091v3.pdf","comment":"It has changed a lot from the previous version and needs to set up a\n  new one"},{"id":"http://arxiv.org/abs/2406.18859v1","updated":"2024-06-27T03:05:35Z","published":"2024-06-27T03:05:35Z","title":"Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology\n  Report Simplification","summary":"  Radiology reports are highly technical documents aimed primarily at\ndoctor-doctor communication. There has been an increasing interest in sharing\nthose reports with patients, necessitating providing them patient-friendly\nsimplifications of the original reports. This study explores the suitability of\nlarge language models in automatically generating those simplifications. We\nexamine the usefulness of chain-of-thought and self-correction prompting\nmechanisms in this domain. We also propose a new evaluation protocol that\nemploys radiologists and laypeople, where radiologists verify the factual\ncorrectness of simplifications, and laypeople assess simplicity and\ncomprehension. Our experimental results demonstrate the effectiveness of\nself-correction prompting in producing high-quality simplifications. Our\nfindings illuminate the preferences of radiologists and laypeople regarding\ntext simplification, informing future research on this topic.\n","authors":["Ziyu Yang","Santhosh Cherian","Slobodan Vucetic"],"pdf_url":"https://arxiv.org/pdf/2406.18859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18856v1","updated":"2024-06-27T02:53:55Z","published":"2024-06-27T02:53:55Z","title":"FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus","summary":"  Large Language Models (LLMs) have stunningly advanced the field of machine\ntranslation, though their effectiveness within the financial domain remains\nlargely underexplored. To probe this issue, we constructed a fine-grained\nChinese-English parallel corpus of financial news called FFN. We acquired\nfinancial news articles spanning between January 1st, 2014, to December 31,\n2023, from mainstream media websites such as CNN, FOX, and China Daily. The\ndataset consists of 1,013 main text and 809 titles, all of which have been\nmanually corrected. We measured the translation quality of two LLMs -- ChatGPT\nand ERNIE-bot, utilizing BLEU, TER and chrF scores as the evaluation metrics.\nFor comparison, we also trained an OpenNMT model based on our dataset. We\ndetail problems of LLMs and provide in-depth analysis, intending to stimulate\nfurther research and solutions in this largely uncharted territory. Our\nresearch underlines the need to optimize LLMs within the specific field of\nfinancial translation to ensure accuracy and quality.\n","authors":["Yuxin Fu","Shijing Si","Leyi Mai","Xi-ang Li"],"pdf_url":"https://arxiv.org/pdf/2406.18856v1.pdf","comment":"a simplified version of this paper is accepted by International\n  Conference on Asian Language Processing 2024"},{"id":"http://arxiv.org/abs/2403.08002v5","updated":"2024-06-27T02:51:29Z","published":"2024-03-12T18:12:02Z","title":"Towards a clinically accessible radiology foundation model: open-access\n  and lightweight, with automated evaluation","summary":"  The scaling laws and extraordinary performance of large foundation models\nmotivate the development and utilization of such models in biomedicine.\nHowever, despite early promising results on some biomedical benchmarks, there\nare still major challenges that need to be addressed before these models can be\nused in real-world clinics. Frontier general-domain models such as GPT-4V still\nhave significant performance gaps in multimodal biomedical applications. More\nimportantly, less-acknowledged pragmatic issues, including accessibility, model\ncost, and tedious manual evaluation make it hard for clinicians to use\nstate-of-the-art large models directly on private patient data. Here, we\nexplore training open-source small multimodal models (SMMs) to bridge\ncompetency gaps for unmet clinical needs in radiology. To maximize data\nefficiency, we adopt a modular approach by incorporating state-of-the-art\npre-trained models for image and text modalities, and focusing on training a\nlightweight adapter to ground each modality to the text embedding space, as\nexemplified by LLaVA-Med. For training, we assemble a large dataset of over 697\nthousand radiology image-text pairs. For evaluation, we propose CheXprompt, a\nGPT-4-based metric for factuality evaluation, and demonstrate its parity with\nexpert evaluation. For best practice, we conduct a systematic ablation study on\nvarious choices in data engineering and multimodal training. The resulting\nLlaVA-Rad (7B) model attains state-of-the-art results on standard radiology\ntasks such as report generation and cross-modal retrieval, even outperforming\nmuch larger models such as GPT-4V and Med-PaLM M (84B). The inference of\nLlaVA-Rad is fast and can be performed on a single V100 GPU in private\nsettings, offering a promising state-of-the-art tool for real-world clinical\napplications.\n","authors":["Juan Manuel Zambrano Chaves","Shih-Cheng Huang","Yanbo Xu","Hanwen Xu","Naoto Usuyama","Sheng Zhang","Fei Wang","Yujia Xie","Mahmoud Khademi","Ziyi Yang","Hany Awadalla","Julia Gong","Houdong Hu","Jianwei Yang","Chunyuan Li","Jianfeng Gao","Yu Gu","Cliff Wong","Mu Wei","Tristan Naumann","Muhao Chen","Matthew P. Lungren","Akshay Chaudhari","Serena Yeung-Levy","Curtis P. Langlotz","Sheng Wang","Hoifung Poon"],"pdf_url":"https://arxiv.org/pdf/2403.08002v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17447v2","updated":"2024-06-27T02:42:36Z","published":"2023-09-29T17:57:23Z","title":"A Large Language Model Approach to Educational Survey Feedback Analysis","summary":"  This paper assesses the potential for the large language models (LLMs) GPT-4\nand GPT-3.5 to aid in deriving insight from education feedback surveys.\nExploration of LLM use cases in education has focused on teaching and learning,\nwith less exploration of capabilities in education feedback analysis. Survey\nanalysis in education involves goals such as finding gaps in curricula or\nevaluating teachers, often requiring time-consuming manual processing of\ntextual responses. LLMs have the potential to provide a flexible means of\nachieving these goals without specialized machine learning models or\nfine-tuning. We demonstrate a versatile approach to such goals by treating them\nas sequences of natural language processing (NLP) tasks including\nclassification (multi-label, multi-class, and binary), extraction, thematic\nanalysis, and sentiment analysis, each performed by LLM. We apply these\nworkflows to a real-world dataset of 2500 end-of-course survey comments from\nbiomedical science courses, and evaluate a zero-shot approach (i.e., requiring\nno examples or labeled training data) across all tasks, reflecting education\nsettings, where labeled data is often scarce. By applying effective prompting\npractices, we achieve human-level performance on multiple tasks with GPT-4,\nenabling workflows necessary to achieve typical goals. We also show the\npotential of inspecting LLMs' chain-of-thought (CoT) reasoning for providing\ninsight that may foster confidence in practice. Moreover, this study features\ndevelopment of a versatile set of classification categories, suitable for\nvarious course types (online, hybrid, or in-person) and amenable to\ncustomization. Our results suggest that LLMs can be used to derive a range of\ninsights from survey text.\n","authors":["Michael J. Parker","Caitlin Anderson","Claire Stone","YeaRim Oh"],"pdf_url":"https://arxiv.org/pdf/2309.17447v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18847v1","updated":"2024-06-27T02:38:13Z","published":"2024-06-27T02:38:13Z","title":"Learning Retrieval Augmentation for Personalized Dialogue Generation","summary":"  Personalized dialogue generation, focusing on generating highly tailored\nresponses by leveraging persona profiles and dialogue context, has gained\nsignificant attention in conversational AI applications. However, persona\nprofiles, a prevalent setting in current personalized dialogue datasets,\ntypically composed of merely four to five sentences, may not offer\ncomprehensive descriptions of the persona about the agent, posing a challenge\nto generate truly personalized dialogues. To handle this problem, we propose\n$\\textbf{L}$earning Retrieval $\\textbf{A}$ugmentation for\n$\\textbf{P}$ersonalized $\\textbf{D}$ial$\\textbf{O}$gue $\\textbf{G}$eneration\n($\\textbf{LAPDOG}$), which studies the potential of leveraging external\nknowledge for persona dialogue generation. Specifically, the proposed LAPDOG\nmodel consists of a story retriever and a dialogue generator. The story\nretriever uses a given persona profile as queries to retrieve relevant\ninformation from the story document, which serves as a supplementary context to\naugment the persona profile. The dialogue generator utilizes both the dialogue\nhistory and the augmented persona profile to generate personalized responses.\nFor optimization, we adopt a joint training framework that collaboratively\nlearns the story retriever and dialogue generator, where the story retriever is\noptimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for\nthe dialogue generator to generate personalized responses. Experiments\nconducted on the CONVAI2 dataset with ROCStory as a supplementary data source\nshow that the proposed LAPDOG method substantially outperforms the baselines,\nindicating the effectiveness of the proposed method. The LAPDOG model code is\npublicly available for further exploration.\nhttps://github.com/hqsiswiliam/LAPDOG\n","authors":["Qiushi Huang","Shuai Fu","Xubo Liu","Wenwu Wang","Tom Ko","Yu Zhang","Lilian Tang"],"pdf_url":"https://arxiv.org/pdf/2406.18847v1.pdf","comment":"Accepted to EMNLP-2023"},{"id":"http://arxiv.org/abs/2406.18192v2","updated":"2024-06-27T02:17:19Z","published":"2024-06-26T09:16:08Z","title":"Methodology of Adapting Large English Language Models for Specific\n  Cultural Contexts","summary":"  The rapid growth of large language models(LLMs) has emerged as a prominent\ntrend in the field of artificial intelligence. However, current\nstate-of-the-art LLMs are predominantly based on English. They encounter\nlimitations when directly applied to tasks in specific cultural domains, due to\ndeficiencies in domain-specific knowledge and misunderstandings caused by\ndifferences in cultural values. To address this challenge, our paper proposes a\nrapid adaptation method for large models in specific cultural contexts, which\nleverages instruction-tuning based on specific cultural knowledge and safety\nvalues data. Taking Chinese as the specific cultural context and utilizing the\nLLaMA3-8B as the experimental English LLM, the evaluation results demonstrate\nthat the adapted LLM significantly enhances its capabilities in domain-specific\nknowledge and adaptability to safety values, while maintaining its original\nexpertise advantages.\n","authors":["Wenjing Zhang","Siqi Xiao","Xuejiao Lei","Ning Wang","Huazheng Zhang","Meijuan An","Bikun Yang","Zhaoxiang Liu","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2406.18192v2.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.18832v1","updated":"2024-06-27T02:02:26Z","published":"2024-06-27T02:02:26Z","title":"OutlierTune: Efficient Channel-Wise Quantization for Large Language\n  Models","summary":"  Quantizing the activations of large language models (LLMs) has been a\nsignificant challenge due to the presence of structured outliers. Most existing\nmethods focus on the per-token or per-tensor quantization of activations,\nmaking it difficult to achieve both accuracy and hardware efficiency. To\naddress this problem, we propose OutlierTune, an efficient per-channel\npost-training quantization (PTQ) method for the activations of LLMs.\nOutlierTune consists of two components: pre-execution of dequantization and\nsymmetrization. The pre-execution of dequantization updates the model weights\nby the activation scaling factors, avoiding the internal scaling and costly\nadditional computational overheads brought by the per-channel activation\nquantization. The symmetrization further reduces the quantization differences\narising from the weight updates by ensuring the balanced numerical ranges\nacross different activation channels. OutlierTune is easy to implement and\nhardware-efficient, introducing almost no additional computational overheads\nduring the inference. Extensive experiments show that the proposed framework\noutperforms existing methods across multiple different tasks. Demonstrating\nbetter generalization, this framework improves the Int6 quantization of the\ninstruction-tuning LLMs, such as OPT-IML, to the same level as half-precision\n(FP16). Moreover, we have shown that the proposed framework is 1.48x faster\nthan the FP16 implementation while reducing approximately 2x memory usage.\n","authors":["Jinguang Wang","Yuexi Yin","Haifeng Sun","Qi Qi","Jingyu Wang","Zirui Zhuang","Tingting Yang","Jianxin Liao"],"pdf_url":"https://arxiv.org/pdf/2406.18832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19578v1","updated":"2024-06-27T23:43:36Z","published":"2024-06-27T23:43:36Z","title":"PathAlign: A vision-language model for whole slide images in\n  histopathology","summary":"  Microscopic interpretation of histopathology images underlies many important\ndiagnostic and treatment decisions. While advances in vision-language modeling\nraise new opportunities for analysis of such images, the gigapixel-scale size\nof whole slide images (WSIs) introduces unique challenges. Additionally,\npathology reports simultaneously highlight key findings from small regions\nwhile also aggregating interpretation across multiple slides, often making it\ndifficult to create robust image-text pairs. As such, pathology reports remain\na largely untapped source of supervision in computational pathology, with most\nefforts relying on region-of-interest annotations or self-supervision at the\npatch-level. In this work, we develop a vision-language model based on the\nBLIP-2 framework using WSIs paired with curated text from pathology reports.\nThis enables applications utilizing a shared image-text embedding space, such\nas text or image retrieval for finding cases of interest, as well as\nintegration of the WSI encoder with a frozen large language model (LLM) for\nWSI-based generative text capabilities such as report generation or\nAI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000\nWSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure\ntypes, and tissue types. We present pathologist evaluation of text generation\nand text retrieval using WSI embeddings, as well as results for WSI\nclassification and workflow prioritization (slide-level triaging).\nModel-generated text for WSIs was rated by pathologists as accurate, without\nclinically significant error or omission, for 78% of WSIs on average. This work\ndemonstrates exciting potential capabilities for language-aligned WSI\nembeddings.\n","authors":["Faruk Ahmed","Andrew Sellergren","Lin Yang","Shawn Xu","Boris Babenko","Abbi Ward","Niels Olson","Arash Mohtashamian","Yossi Matias","Greg S. Corrado","Quang Duong","Dale R. Webster","Shravya Shetty","Daniel Golden","Yun Liu","David F. Steiner","Ellery Wulczyn"],"pdf_url":"https://arxiv.org/pdf/2406.19578v1.pdf","comment":"9 main pages and 19 pages of supplemental material; 3 main tables, 3\n  main figures and 11 supplemental tables, 7 supplemental figures"},{"id":"http://arxiv.org/abs/2404.02319v2","updated":"2024-06-27T23:22:14Z","published":"2024-04-02T21:35:54Z","title":"Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient\n  Compile-Time Prompt Optimization","summary":"  In many modern LLM applications, such as retrieval augmented generation,\nprompts have become programs themselves. In these settings, prompt programs are\nrepeatedly called with different user queries or data instances. A big\npractical challenge is optimizing such prompt programs. Recent work has mostly\nfocused on either simple prompt programs or assumed that the general structure\nof a prompt program is fixed.\n  We introduce SAMMO, a framework to perform symbolic prompt program search for\ncompile-time optimizations of prompt programs. SAMMO represents prompt programs\non a symbolic level which allows for a rich set of transformations that can be\nsearched over during optimization. We show that SAMMO generalizes previous\nmethods and improves the performance of complex prompts on (1) instruction\ntuning, (2) RAG pipeline tuning, and (3) prompt compression, across several\ndifferent LLMs. We make all code available open-source at\nhttps://github.com/microsoft/sammo .\n","authors":["Tobias Schnabel","Jennifer Neville"],"pdf_url":"https://arxiv.org/pdf/2404.02319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13372v4","updated":"2024-06-27T22:44:48Z","published":"2024-03-20T08:08:54Z","title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models","summary":"  Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It provides a\nsolution for flexibly customizing the fine-tuning of 100+ LLMs without the need\nfor coding through the built-in web UI LlamaBoard. We empirically validate the\nefficiency and effectiveness of our framework on language modeling and text\ngeneration tasks. It has been released at\nhttps://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and\n3,000 forks.\n","authors":["Yaowei Zheng","Richong Zhang","Junhao Zhang","Yanhan Ye","Zheyan Luo","Zhangchi Feng","Yongqiang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13372v4.pdf","comment":"13 pages, accepted to ACL 2024 System Demonstration Track"},{"id":"http://arxiv.org/abs/2406.19564v1","updated":"2024-06-27T22:38:04Z","published":"2024-06-27T22:38:04Z","title":"Voices Unheard: NLP Resources and Models for Yorùbá Regional\n  Dialects","summary":"  Yor\\`ub\\'a an African language with roughly 47 million speakers encompasses a\ncontinuum with several dialects. Recent efforts to develop NLP technologies for\nAfrican languages have focused on their standard dialects, resulting in\ndisparities for dialects and varieties for which there are little to no\nresources or tools. We take steps towards bridging this gap by introducing a\nnew high-quality parallel text and speech corpus YOR\\`ULECT across three\ndomains and four regional Yor\\`ub\\'a dialects. To develop this corpus, we\nengaged native speakers, travelling to communities where these dialects are\nspoken, to collect text and speech data. Using our newly created corpus, we\nconducted extensive experiments on (text) machine translation, automatic speech\nrecognition, and speech-to-text translation. Our results reveal substantial\nperformance disparities between standard Yor\\`ub\\'a and the other dialects\nacross all tasks. However, we also show that with dialect-adaptive finetuning,\nwe are able to narrow this gap. We believe our dataset and experimental\nanalysis will contribute greatly to developing NLP tools for Yor\\`ub\\'a and its\ndialects, and potentially for other African languages, by improving our\nunderstanding of existing challenges and offering a high-quality dataset for\nfurther development. We release YOR\\`ULECT dataset and models publicly under an\nopen license.\n","authors":["Orevaoghene Ahia","Anuoluwapo Aremu","Diana Abagyan","Hila Gonen","David Ifeoluwa Adelani","Daud Abolade","Noah A. Smith","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2406.19564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13827v2","updated":"2024-06-27T22:31:07Z","published":"2024-06-19T20:47:23Z","title":"Fine-Tuning BERTs for Definition Extraction from Mathematical Text","summary":"  In this paper, we fine-tuned three pre-trained BERT models on the task of\n\"definition extraction\" from mathematical English written in LaTeX. This is\npresented as a binary classification problem, where either a sentence contains\na definition of a mathematical term or it does not. We used two original data\nsets, \"Chicago\" and \"TAC,\" to fine-tune and test these models. We also tested\non WFMALL, a dataset presented by Vanetik and Litvak in 2021 and compared the\nperformance of our models to theirs. We found that a high-performance\nSentence-BERT transformer model performed best based on overall accuracy,\nrecall, and precision metrics, achieving comparable results to the earlier\nmodels with less computational effort.\n","authors":["Lucy Horowitz","Ryan Hathaway"],"pdf_url":"https://arxiv.org/pdf/2406.13827v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19836v2","updated":"2024-06-27T22:28:27Z","published":"2024-03-28T21:15:15Z","title":"Target Span Detection for Implicit Harmful Content","summary":"  Identifying the targets of hate speech is a crucial step in grasping the\nnature of such speech and, ultimately, in improving the detection of offensive\nposts on online forums. Much harmful content on online platforms uses implicit\nlanguage especially when targeting vulnerable and protected groups such as\nusing stereotypical characteristics instead of explicit target names, making it\nharder to detect and mitigate the language. In this study, we focus on\nidentifying implied targets of hate speech, essential for recognizing subtler\nhate speech and enhancing the detection of harmful content on digital\nplatforms. We define a new task aimed at identifying the targets even when they\nare not explicitly stated. To address that task, we collect and annotate target\nspans in three prominent implicit hate speech datasets: SBIC, DynaHate, and\nIHC. We call the resulting merged collection Implicit-Target-Span. The\ncollection is achieved using an innovative pooling method with matching scores\nbased on human annotations and Large Language Models (LLMs). Our experiments\nindicate that Implicit-Target-Span provides a challenging test bed for target\nspan detection methods.\n","authors":["Nazanin Jafari","James Allan","Sheikh Muhammad Sarwar"],"pdf_url":"https://arxiv.org/pdf/2403.19836v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19552v1","updated":"2024-06-27T22:08:22Z","published":"2024-06-27T22:08:22Z","title":"Rethinking harmless refusals when fine-tuning foundation models","summary":"  In this paper, we investigate the degree to which fine-tuning in Large\nLanguage Models (LLMs) effectively mitigates versus merely conceals undesirable\nbehavior. Through the lens of semi-realistic role-playing exercises designed to\nelicit such behaviors, we explore the response dynamics of LLMs post\nfine-tuning interventions. Our methodology involves prompting models for\nChain-of-Thought (CoT) reasoning and analyzing the coherence between the\nreasoning traces and the resultant outputs. Notably, we identify a pervasive\nphenomenon we term \\emph{reason-based deception}, where models either stop\nproducing reasoning traces or produce seemingly ethical reasoning traces that\nbelie the unethical nature of their final outputs. We further examine the\nefficacy of response strategies (polite refusal versus explicit rebuttal) in\ncurbing the occurrence of undesired behavior in subsequent outputs of\nmulti-turn interactions. Our findings reveal that explicit rebuttals\nsignificantly outperform polite refusals in preventing the continuation of\nundesired outputs and nearly eliminate reason-based deception, challenging\ncurrent practices in model fine-tuning. Accordingly, the two key contributions\nof this paper are (1) defining and studying reason-based deception, a new type\nof hidden behavior, and (2) demonstrating that rebuttals provide a more robust\nresponse model to harmful requests than refusals, thereby highlighting the need\nto reconsider the response strategies in fine-tuning approaches.\n","authors":["Florin Pop","Judd Rosenblatt","Diogo Schwerz de Lucena","Michael Vaiana"],"pdf_url":"https://arxiv.org/pdf/2406.19552v1.pdf","comment":"ICLR 2024 AGI Workshop Poster"},{"id":"http://arxiv.org/abs/2406.19545v1","updated":"2024-06-27T21:47:42Z","published":"2024-06-27T21:47:42Z","title":"Leveraging Machine-Generated Rationales to Facilitate Social Meaning\n  Detection in Conversations","summary":"  We present a generalizable classification approach that leverages Large\nLanguage Models (LLMs) to facilitate the detection of implicitly encoded social\nmeaning in conversations. We design a multi-faceted prompt to extract a textual\nexplanation of the reasoning that connects visible cues to underlying social\nmeanings. These extracted explanations or rationales serve as augmentations to\nthe conversational text to facilitate dialogue understanding and transfer. Our\nempirical results over 2,340 experimental settings demonstrate the significant\npositive impact of adding these rationales. Our findings hold true for\nin-domain classification, zero-shot, and few-shot domain transfer for two\ndifferent social meaning detection tasks, each spanning two different corpora.\n","authors":["Ritam Dutt","Zhen Wu","Kelly Shi","Divyanshu Sheth","Prakhar Gupta","Carolyn Penstein Rose"],"pdf_url":"https://arxiv.org/pdf/2406.19545v1.pdf","comment":"To appear at The Proceedings of the Association for Computational\n  Linguistics, 2024"},{"id":"http://arxiv.org/abs/2406.19543v1","updated":"2024-06-27T21:45:33Z","published":"2024-06-27T21:45:33Z","title":"Demarked: A Strategy for Enhanced Abusive Speech Moderation through\n  Counterspeech, Detoxification, and Message Management","summary":"  Despite regulations imposed by nations and social media platforms, such as\nrecent EU regulations targeting digital violence, abusive content persists as a\nsignificant challenge. Existing approaches primarily rely on binary solutions,\nsuch as outright blocking or banning, yet fail to address the complex nature of\nabusive speech. In this work, we propose a more comprehensive approach called\nDemarcation scoring abusive speech based on four aspect -- (i) severity scale;\n(ii) presence of a target; (iii) context scale; (iv) legal scale -- and\nsuggesting more options of actions like detoxification, counter speech\ngeneration, blocking, or, as a final measure, human intervention. Through a\nthorough analysis of abusive speech regulations across diverse jurisdictions,\nplatforms, and research papers we highlight the gap in preventing measures and\nadvocate for tailored proactive steps to combat its multifaceted\nmanifestations. Our work aims to inform future strategies for effectively\naddressing abusive speech online.\n","authors":["Seid Muhie Yimam","Daryna Dementieva","Tim Fischer","Daniil Moskovskiy","Naquee Rizwan","Punyajoy Saha","Sarthak Roy","Martin Semmann","Alexander Panchenko","Chris Biemann","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2406.19543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19538v1","updated":"2024-06-27T21:31:30Z","published":"2024-06-27T21:31:30Z","title":"Context Matters: An Empirical Study of the Impact of Contextual\n  Information in Temporal Question Answering Systems","summary":"  Large language models (LLMs) often struggle with temporal reasoning, crucial\nfor tasks like historical event analysis and time-sensitive information\nretrieval. Despite advancements, state-of-the-art models falter in handling\ntemporal information, especially when faced with irrelevant or noisy contexts.\nThis paper addresses this gap by empirically examining the robustness of\ntemporal question-answering (TQA) systems trained on various context types,\nincluding relevant, irrelevant, slightly altered, and no context. Our findings\nindicate that training with a mix of these contexts enhances model robustness\nand accuracy. Additionally, we show that the position of context relative to\nthe question significantly impacts performance, with question-first positioning\nyielding better results. We introduce two new context-rich TQA datasets,\nContextAQA and ContextTQE, and provide comprehensive evaluations and guidelines\nfor training robust TQA models. Our work lays the foundation for developing\nreliable and context-aware temporal QA systems, with broader implications for\nenhancing LLM robustness against diverse and potentially adversarial\ninformation.\n","authors":["Dan Schumacher","Fatemeh Haji","Tara Grey","Niharika Bandlamudi","Nupoor Karnik","Gagana Uday Kumar","Jason Cho-Yu Chiang","Paul Rad","Nishant Vishwamitra","Anthony Rios"],"pdf_url":"https://arxiv.org/pdf/2406.19538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19537v1","updated":"2024-06-27T21:21:22Z","published":"2024-06-27T21:21:22Z","title":"Handling Ontology Gaps in Semantic Parsing","summary":"  The majority of Neural Semantic Parsing (NSP) models are developed with the\nassumption that there are no concepts outside the ones such models can\nrepresent with their target symbols (closed-world assumption). This assumption\nleads to generate hallucinated outputs rather than admitting their lack of\nknowledge. Hallucinations can lead to wrong or potentially offensive responses\nto users. Hence, a mechanism to prevent this behavior is crucial to build\ntrusted NSP-based Question Answering agents. To that end, we propose the\nHallucination Simulation Framework (HSF), a general setting for stimulating and\nanalyzing NSP model hallucinations. The framework can be applied to any NSP\ntask with a closed-ontology. Using the proposed framework and KQA Pro as the\nbenchmark dataset, we assess state-of-the-art techniques for hallucination\ndetection. We then present a novel hallucination detection strategy that\nexploits the computational graph of the NSP model to detect the NSP\nhallucinations in the presence of ontology gaps, out-of-domain utterances, and\nto recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and\n~1%. This is the first work in closed-ontology NSP that addresses the problem\nof recognizing ontology gaps. We release our code and checkpoints at\nhttps://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.\n","authors":["Andrea Bacciu","Marco Damonte","Marco Basaldella","Emilio Monti"],"pdf_url":"https://arxiv.org/pdf/2406.19537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09868v2","updated":"2024-06-27T21:03:15Z","published":"2024-04-15T15:33:29Z","title":"Software Engineering Methods For AI-Driven Deductive Legal Reasoning","summary":"  The recent proliferation of generative artificial intelligence (AI)\ntechnologies such as pre-trained large language models (LLMs) has opened up new\nfrontiers in computational law. An exciting area of development is the use of\nAI to automate the deductive rule-based reasoning inherent in statutory and\ncontract law. This paper argues that such automated deductive legal reasoning\ncan now be viewed from the lens of software engineering, treating LLMs as\ninterpreters of natural-language programs with natural-language inputs. We show\nhow it is possible to apply principled software engineering techniques to\nenhance AI-driven legal reasoning of complex statutes and to unlock new\napplications in automated meta-reasoning such as mutation-guided example\ngeneration and metamorphic property-based testing.\n","authors":["Rohan Padhye"],"pdf_url":"https://arxiv.org/pdf/2404.09868v2.pdf","comment":"Appearing in Onward! at SPLASH 2024"},{"id":"http://arxiv.org/abs/2406.19526v1","updated":"2024-06-27T20:56:57Z","published":"2024-06-27T20:56:57Z","title":"TocBERT: Medical Document Structure Extraction Using Bidirectional\n  Transformers","summary":"  Text segmentation holds paramount importance in the field of Natural Language\nProcessing (NLP). It plays an important role in several NLP downstream tasks\nlike information retrieval and document summarization. In this work, we propose\na new solution, namely TocBERT, for segmenting texts using bidirectional\ntransformers. TocBERT represents a supervised solution trained on the detection\nof titles and sub-titles from their semantic representations. This task was\nformulated as a named entity recognition (NER) problem. The solution has been\napplied on a medical text segmentation use-case where the Bio-ClinicalBERT\nmodel is fine-tuned to segment discharge summaries of the MIMIC-III dataset.\nThe performance of TocBERT has been evaluated on a human-labeled ground truth\ncorpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a\nlinear text segmentation problem and 72.8% on a hierarchical text segmentation\nproblem. It outperformed a carefully designed rule-based solution, particularly\nin distinguishing titles from subtitles.\n","authors":["Majd Saleh","Sarra Baghdadi","Stéphane Paquelet"],"pdf_url":"https://arxiv.org/pdf/2406.19526v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.18266v2","updated":"2024-06-27T20:30:47Z","published":"2024-06-26T11:39:51Z","title":"\"Vorbeşti Româneşte?\" A Recipe to Train Powerful Romanian LLMs\n  with English Instructions","summary":"  In recent years, Large Language Models (LLMs) have achieved almost human-like\nperformance on various tasks. While some LLMs have been trained on multilingual\ndata, most of the training data is in English; hence, their performance in\nEnglish greatly exceeds other languages. To our knowledge, we are the first to\ncollect and translate a large collection of texts, instructions, and benchmarks\nand train, evaluate, and release open-source LLMs tailored for Romanian. We\nevaluate our methods on four different categories, including academic\nbenchmarks, MT-Bench (manually translated), and a professionally built\nhistorical, cultural, and social benchmark adapted to Romanian. We argue for\nthe usefulness and high performance of RoLLMs by obtaining state-of-the-art\nresults across the board. We publicly release all resources (i.e., data,\ntraining and evaluation code, models) to support and encourage research on\nRomanian LLMs while concurrently creating a generalizable recipe, adequate for\nother low or less-resourced languages.\n","authors":["Mihai Masala","Denis C. Ilie-Ablachim","Alexandru Dima","Dragos Corlatescu","Miruna Zavelca","Ovio Olaru","Simina Terian","Andrei Terian","Marius Leordeanu","Horia Velicu","Marius Popescu","Mihai Dascalu","Traian Rebedea"],"pdf_url":"https://arxiv.org/pdf/2406.18266v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.07703"},{"id":"http://arxiv.org/abs/2406.19512v1","updated":"2024-06-27T20:18:18Z","published":"2024-06-27T20:18:18Z","title":"Captioning Visualizations with Large Language Models (CVLLM): A Tutorial","summary":"  Automatically captioning visualizations is not new, but recent advances in\nlarge language models(LLMs) open exciting new possibilities. In this tutorial,\nafter providing a brief review of Information Visualization (InfoVis)\nprinciples and past work in captioning, we introduce neural models and the\ntransformer architecture used in generic LLMs. We then discuss their recent\napplications in InfoVis, with a focus on captioning. Additionally, we explore\npromising future directions in this field.\n","authors":["Giuseppe Carenini","Jordon Johnson","Ali Salamatian"],"pdf_url":"https://arxiv.org/pdf/2406.19512v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.19504v1","updated":"2024-06-27T19:42:13Z","published":"2024-06-27T19:42:13Z","title":"Are Generative Language Models Multicultural? A Study on Hausa Culture\n  and Emotions using ChatGPT","summary":"  Large Language Models (LLMs), such as ChatGPT, are widely used to generate\ncontent for various purposes and audiences. However, these models may not\nreflect the cultural and emotional diversity of their users, especially for\nlow-resource languages. In this paper, we investigate how ChatGPT represents\nHausa's culture and emotions. We compare responses generated by ChatGPT with\nthose provided by native Hausa speakers on 37 culturally relevant questions. We\nconducted experiments using emotion analysis and applied two similarity metrics\nto measure the alignment between human and ChatGPT responses. We also collected\nhuman participants ratings and feedback on ChatGPT responses. Our results show\nthat ChatGPT has some level of similarity to human responses, but also exhibits\nsome gaps and biases in its knowledge and awareness of the Hausa culture and\nemotions. We discuss the implications and limitations of our methodology and\nanalysis and suggest ways to improve the performance and evaluation of LLMs for\nlow-resource languages.\n","authors":["Ibrahim Said Ahmad","Shiran Dudy","Resmi Ramachandranpillai","Kenneth Church"],"pdf_url":"https://arxiv.org/pdf/2406.19504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19502v1","updated":"2024-06-27T19:29:36Z","published":"2024-06-27T19:29:36Z","title":"Investigating How Large Language Models Leverage Internal Knowledge to\n  Perform Complex Reasoning","summary":"  Despite significant advancements, there is a limited understanding of how\nlarge language models (LLMs) utilize knowledge for reasoning. To address this,\nwe propose a method that deconstructs complex real-world questions into a\ngraph, representing each question as a node with parent nodes of background\nknowledge needed to solve the question. We develop the DepthQA dataset,\ndeconstructing questions into three depths: (i) recalling conceptual knowledge,\n(ii) applying procedural knowledge, and (iii) analyzing strategic knowledge.\nBased on a hierarchical graph, we quantify forward discrepancy, discrepancies\nin LLMs' performance on simpler sub-problems versus complex questions. We also\nmeasure backward discrepancy, where LLMs answer complex questions but struggle\nwith simpler ones. Our analysis shows that smaller models have more\ndiscrepancies than larger models. Additionally, guiding models from simpler to\ncomplex questions through multi-turn interactions improves performance across\nmodel sizes, highlighting the importance of structured intermediate steps in\nknowledge reasoning. This work enhances our understanding of LLM reasoning and\nsuggests ways to improve their problem-solving abilities.\n","authors":["Miyoung Ko","Sue Hyun Park","Joonsuk Park","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2406.19502v1.pdf","comment":"Work in progress; code is available at\n  https://github.com/kaistAI/knowledge-reasoning"},{"id":"http://arxiv.org/abs/2406.19501v1","updated":"2024-06-27T19:28:43Z","published":"2024-06-27T19:28:43Z","title":"Monitoring Latent World States in Language Models with Propositional\n  Probes","summary":"  Language models are susceptible to bias, sycophancy, backdoors, and other\ntendencies that lead to unfaithful responses to the input context. Interpreting\ninternal states of language models could help monitor and correct unfaithful\nbehavior. We hypothesize that language models represent their input contexts in\na latent world model, and seek to extract this latent world state from the\nactivations. We do so with 'propositional probes', which compositionally probe\ntokens for lexical information and bind them into logical propositions\nrepresenting the world state. For example, given the input context ''Greg is a\nnurse. Laura is a physicist.'', we decode the propositions ''WorksAs(Greg,\nnurse)'' and ''WorksAs(Laura, physicist)'' from the model's activations. Key to\nthis is identifying a 'binding subspace' in which bound tokens have high\nsimilarity (''Greg'' and ''nurse'') but unbound ones do not (''Greg'' and\n''physicist''). We validate propositional probes in a closed-world setting with\nfinitely many predicates and properties. Despite being trained on simple\ntemplated contexts, propositional probes generalize to contexts rewritten as\nshort stories and translated to Spanish. Moreover, we find that in three\nsettings where language models respond unfaithfully to the input context --\nprompt injections, backdoor attacks, and gender bias -- the decoded\npropositions remain faithful. This suggests that language models often encode a\nfaithful world model but decode it unfaithfully, which motivates the search for\nbetter interpretability tools for monitoring LMs.\n","authors":["Jiahai Feng","Stuart Russell","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2406.19501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19500v1","updated":"2024-06-27T19:28:42Z","published":"2024-06-27T19:28:42Z","title":"Knowledge acquisition for dialogue agents using reinforcement learning\n  on graph representations","summary":"  We develop an artificial agent motivated to augment its knowledge base beyond\nits initial training. The agent actively participates in dialogues with other\nagents, strategically acquiring new information. The agent models its knowledge\nas an RDF knowledge graph, integrating new beliefs acquired through\nconversation. Responses in dialogue are generated by identifying graph patterns\naround these new integrated beliefs. We show that policies can be learned using\nreinforcement learning to select effective graph patterns during an\ninteraction, without relying on explicit user feedback. Within this context,\nour study is a proof of concept for leveraging users as effective sources of\ninformation.\n","authors":["Selene Baez Santamaria","Shihan Wang","Piek Vossen"],"pdf_url":"https://arxiv.org/pdf/2406.19500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19497v1","updated":"2024-06-27T19:26:11Z","published":"2024-06-27T19:26:11Z","title":"Inclusivity in Large Language Models: Personality Traits and Gender Bias\n  in Scientific Abstracts","summary":"  Large language models (LLMs) are increasingly utilized to assist in\nscientific and academic writing, helping authors enhance the coherence of their\narticles. Previous studies have highlighted stereotypes and biases present in\nLLM outputs, emphasizing the need to evaluate these models for their alignment\nwith human narrative styles and potential gender biases. In this study, we\nassess the alignment of three prominent LLMs - Claude 3 Opus, Mistral AI Large,\nand Gemini 1.5 Flash - by analyzing their performance on benchmark\ntext-generation tasks for scientific abstracts. We employ the Linguistic\nInquiry and Word Count (LIWC) framework to extract lexical, psychological, and\nsocial features from the generated texts. Our findings indicate that, while\nthese models generally produce text closely resembling human authored content,\nvariations in stylistic features suggest significant gender biases. This\nresearch highlights the importance of developing LLMs that maintain a diversity\nof writing styles to promote inclusivity in academic discourse.\n","authors":["Naseela Pervez","Alexander J. Titus"],"pdf_url":"https://arxiv.org/pdf/2406.19497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19493v1","updated":"2024-06-27T19:20:09Z","published":"2024-06-27T19:20:09Z","title":"Development and Evaluation of a Retrieval-Augmented Generation Tool for\n  Creating SAPPhIRE Models of Artificial Systems","summary":"  Representing systems using the SAPPhIRE causality model is found useful in\nsupporting design-by-analogy. However, creating a SAPPhIRE model of artificial\nor biological systems is an effort-intensive process that requires human\nexperts to source technical knowledge from multiple technical documents\nregarding how the system works. This research investigates how to leverage\nLarge Language Models (LLMs) in creating structured descriptions of systems\nusing the SAPPhIRE model of causality. This paper, the second part of the\ntwo-part research, presents a new Retrieval-Augmented Generation (RAG) tool for\ngenerating information related to SAPPhIRE constructs of artificial systems and\nreports the results from a preliminary evaluation of the tool's success -\nfocusing on the factual accuracy and reliability of outcomes.\n","authors":["Anubhab Majumder","Kausik Bhattacharya","Amaresh Chakrabarti"],"pdf_url":"https://arxiv.org/pdf/2406.19493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19486v1","updated":"2024-06-27T19:02:41Z","published":"2024-06-27T19:02:41Z","title":"LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models","summary":"  In prompt tuning, a prefix or suffix text is added to the prompt, and the\nembeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix\nare optimized to gain more control over language models for specific tasks.\nThis approach eliminates the need for hand-crafted prompt engineering or\nexplicit model fine-tuning. Prompt tuning is significantly more\nparameter-efficient than model fine-tuning, as it involves optimizing partial\ninputs of language models to produce desired outputs.\n  In this work, we aim to further reduce the amount of trainable parameters\nrequired for a language model to perform well on specific tasks. We propose\nLow-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves\nefficient prompt optimization. The proposed method demonstrates similar\noutcomes to full parameter prompt tuning while reducing the number of trainable\nparameters by a factor of 5. It also provides promising results compared to the\nstate-of-the-art methods that would require 10 to 20 times more parameters.\n","authors":["Shouchang Guo","Sonam Damani","Keng-hao Chang"],"pdf_url":"https://arxiv.org/pdf/2406.19486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19482v1","updated":"2024-06-27T18:51:46Z","published":"2024-06-27T18:51:46Z","title":"xTower: A Multilingual LLM for Explaining and Correcting Translation\n  Errors","summary":"  While machine translation (MT) systems are achieving increasingly strong\nperformance on benchmarks, they often produce translations with errors and\nanomalies. Understanding these errors can potentially help improve the\ntranslation quality and user experience. This paper introduces xTower, an open\nlarge language model (LLM) built on top of TowerBase designed to provide\nfree-text explanations for translation errors in order to guide the generation\nof a corrected translation. The quality of the generated explanations by xTower\nare assessed via both intrinsic and extrinsic evaluation. We ask expert\ntranslators to evaluate the quality of the explanations across two dimensions:\nrelatedness towards the error span being explained and helpfulness in error\nunderstanding and improving translation quality. Extrinsically, we test xTower\nacross various experimental setups in generating translation corrections,\ndemonstrating significant improvements in translation quality. Our findings\nhighlight xTower's potential towards not only producing plausible and helpful\nexplanations of automatic translations, but also leveraging them to suggest\ncorrected translations.\n","authors":["Marcos Treviso","Nuno M. Guerreiro","Sweta Agrawal","Ricardo Rei","José Pombal","Tania Vaz","Helena Wu","Beatriz Silva","Daan van Stigt","André F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2406.19482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19478v1","updated":"2024-06-27T18:43:51Z","published":"2024-06-27T18:43:51Z","title":"Sparse Regression for Machine Translation","summary":"  We use transductive regression techniques to learn mappings between source\nand target features of given parallel corpora and use these mappings to\ngenerate machine translation outputs. We show the effectiveness of $L_1$\nregularized regression (\\textit{lasso}) to learn the mappings between sparsely\nobserved feature sets versus $L_2$ regularized regression. Proper selection of\ntraining instances plays an important role to learn correct feature mappings\nwithin limited computational resources and at expected accuracy levels. We\nintroduce \\textit{dice} instance selection method for proper selection of\ntraining instances, which plays an important role to learn correct feature\nmappings for improving the source and target coverage of the training set. We\nshow that $L_1$ regularized regression performs better than $L_2$ regularized\nregression both in regression measurements and in the translation experiments\nusing graph decoding. We present encouraging results when translating from\nGerman to English and Spanish to English. We also demonstrate results when the\nphrase table of a phrase-based decoder is replaced with the mappings we find\nwith the regression model.\n","authors":["Ergun Biçici"],"pdf_url":"https://arxiv.org/pdf/2406.19478v1.pdf","comment":"8 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.19470v1","updated":"2024-06-27T18:21:32Z","published":"2024-06-27T18:21:32Z","title":"Changing Answer Order Can Decrease MMLU Accuracy","summary":"  As large language models (LLMs) have grown in prevalence, particular\nbenchmarks have become essential for the evaluation of these models and for\nunderstanding model capabilities. Most commonly, we use test accuracy averaged\nacross multiple subtasks in order to rank models on leaderboards, to determine\nwhich model is best for our purposes. In this paper, we investigate the\nrobustness of the accuracy measurement on a widely used multiple choice\nquestion answering dataset, MMLU. When shuffling the answer label contents, we\nfind that all explored models decrease in accuracy on MMLU, but not every model\nis equally sensitive. These findings suggest a possible adjustment to the\nstandard practice of leaderboard testing, where we additionally consider the\npercentage of examples each model answers correctly by random chance.\n","authors":["Vipul Gupta","David Pantoja","Candace Ross","Adina Williams","Megan Ung"],"pdf_url":"https://arxiv.org/pdf/2406.19470v1.pdf","comment":"Short paper, 9 pages"},{"id":"http://arxiv.org/abs/2405.03832v2","updated":"2024-06-27T18:14:03Z","published":"2024-05-06T20:30:14Z","title":"Guylingo: The Republic of Guyana Creole Corpora","summary":"  While major languages often enjoy substantial attention and resources, the\nlinguistic diversity across the globe encompasses a multitude of smaller,\nindigenous, and regional languages that lack the same level of computational\nsupport. One such region is the Caribbean. While commonly labeled as \"English\nspeaking\", the ex-British Caribbean region consists of a myriad of Creole\nlanguages thriving alongside English. In this paper, we present Guylingo: a\ncomprehensive corpus designed for advancing NLP research in the domain of\nCreolese (Guyanese English-lexicon Creole), the most widely spoken language in\nthe culturally rich nation of Guyana. We first outline our framework for\ngathering and digitizing this diverse corpus, inclusive of colloquial\nexpressions, idioms, and regional variations in a low-resource language. We\nthen demonstrate the challenges of training and evaluating NLP models for\nmachine translation in Creole. Lastly, we discuss the unique opportunities\npresented by recent NLP advancements for accelerating the formal adoption of\nCreole languages as official languages in the Caribbean.\n","authors":["Christopher Clarke","Roland Daynauth","Charlene Wilkinson","Hubert Devonish","Jason Mars"],"pdf_url":"https://arxiv.org/pdf/2405.03832v2.pdf","comment":"Accepted to NAACL 2024 Main Conference Special Theme Track: Languages\n  of Latin America and The Caribbean"},{"id":"http://arxiv.org/abs/2406.19465v1","updated":"2024-06-27T18:07:40Z","published":"2024-06-27T18:07:40Z","title":"Can Large Language Models Generate High-quality Patent Claims?","summary":"  Large language models (LLMs) have shown exceptional performance across\nvarious text generation tasks but remain under-explored in the patent domain,\nwhich offers highly structured and precise language. This paper constructs a\ndataset to investigate the performance of current LLMs in patent claim\ngeneration. Our results demonstrate that generating claims based on patent\ndescriptions outperforms previous research relying on abstracts. Interestingly,\ncurrent patent-specific LLMs perform much worse than state-of-the-art general\nLLMs, highlighting the necessity for future research on in-domain LLMs. We also\nfind that LLMs can produce high-quality first independent claims, but their\nperformances markedly decrease for subsequent dependent claims. Moreover,\nfine-tuning can enhance the completeness of inventions' features, conceptual\nclarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the\nbest performance in comprehensive human evaluations by patent experts, with\nbetter feature coverage, conceptual clarity, and technical coherence. Despite\nthese capabilities, comprehensive revision and modification are still necessary\nto pass rigorous patent scrutiny and ensure legal robustness.\n","authors":["Lekang Jiang","Caiqi Zhang","Pascal A Scherz","Stephan Goetz"],"pdf_url":"https://arxiv.org/pdf/2406.19465v1.pdf","comment":"13 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.19309v1","updated":"2024-06-27T16:33:40Z","published":"2024-06-27T16:33:40Z","title":"Which Neurons Matter in IR? Applying Integrated Gradients-based Methods\n  to Understand Cross-Encoders","summary":"  With the recent addition of Retrieval-Augmented Generation (RAG), the scope\nand importance of Information Retrieval (IR) has expanded. As a result, the\nimportance of a deeper understanding of IR models also increases. However,\ninterpretability in IR remains under-explored, especially when it comes to the\nmodels' inner mechanisms. In this paper, we explore the possibility of adapting\nIntegrated Gradient-based methods in an IR context to identify the role of\nindividual neurons within the model. In particular, we provide new insights\ninto the role of what we call \"relevance\" neurons, as well as how they deal\nwith unseen data. Finally, we carry out an in-depth pruning study to validate\nour findings.\n","authors":["Mathias Vast","Basile Van Cooten","Laure Soulier","Benjamin Piwowarski"],"pdf_url":"https://arxiv.org/pdf/2406.19309v1.pdf","comment":"Accepted at ICTIR 2024"},{"id":"http://arxiv.org/abs/2406.19281v1","updated":"2024-06-27T15:55:25Z","published":"2024-06-27T15:55:25Z","title":"Grounded and Transparent Response Generation for Conversational\n  Information-Seeking Systems","summary":"  While previous conversational information-seeking (CIS) research has focused\non passage retrieval, reranking, and query rewriting, the challenge of\nsynthesizing retrieved information into coherent responses remains. The\nproposed research delves into the intricacies of response generation in CIS\nsystems. Open-ended information-seeking dialogues introduce multiple challenges\nthat may lead to potential pitfalls in system responses. The study focuses on\ngenerating responses grounded in the retrieved passages and being transparent\nabout the system's limitations. Specific research questions revolve around\nobtaining confidence-enriched information nuggets, automatic detection of\nincomplete or incorrect responses, generating responses communicating the\nsystem's limitations, and evaluating enhanced responses. By addressing these\nresearch tasks the study aspires to contribute to the advancement of\nconversational response generation, fostering more trustworthy interactions in\nCIS dialogues, and paving the way for grounded and transparent systems to meet\nusers' needs in an information-driven world.\n","authors":["Weronika Łajewska"],"pdf_url":"https://arxiv.org/pdf/2406.19281v1.pdf","comment":"Proceedings of the 17th ACM International Conference on Web Search\n  and Data Mining (WSDM '24), 2024"},{"id":"http://arxiv.org/abs/2406.19237v1","updated":"2024-06-27T15:01:48Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19150v1","updated":"2024-06-27T13:08:35Z","published":"2024-06-27T13:08:35Z","title":"RAVEN: Multitask Retrieval Augmented Vision-Language Learning","summary":"  The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.\n","authors":["Varun Nagaraj Rao","Siddharth Choudhary","Aditya Deshpande","Ravi Kumar Satzoda","Srikar Appalaraju"],"pdf_url":"https://arxiv.org/pdf/2406.19150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19102v1","updated":"2024-06-27T11:28:50Z","published":"2024-06-27T11:28:50Z","title":"Statements: Universal Information Extraction from Tables with Large\n  Language Models for ESG KPIs","summary":"  Environment, Social, and Governance (ESG) KPIs assess an organization's\nperformance on issues such as climate change, greenhouse gas emissions, water\nconsumption, waste management, human rights, diversity, and policies. ESG\nreports convey this valuable quantitative information through tables.\nUnfortunately, extracting this information is difficult due to high variability\nin the table structure as well as content. We propose Statements, a novel\ndomain agnostic data structure for extracting quantitative facts and related\ninformation. We propose translating tables to statements as a new supervised\ndeep-learning universal information extraction task. We introduce SemTabNet - a\ndataset of over 100K annotated tables. Investigating a family of T5-based\nStatement Extraction Models, our best model generates statements which are 82%\nsimilar to the ground-truth (compared to baseline of 21%). We demonstrate the\nadvantages of statements by applying our model to over 2700 tables from ESG\nreports. The homogeneous nature of statements permits exploratory data analysis\non expansive information found in large collections of ESG reports.\n","authors":["Lokesh Mishra","Sohayl Dhibi","Yusik Kim","Cesar Berrospi Ramis","Shubham Gupta","Michele Dolfi","Peter Staar"],"pdf_url":"https://arxiv.org/pdf/2406.19102v1.pdf","comment":"Accepted at the NLP4Climate workshop in the 62nd Annual Meeting of\n  the Association for Computational Linguistics (ACL 2024)"},{"id":"http://arxiv.org/abs/2406.19018v1","updated":"2024-06-27T09:07:32Z","published":"2024-06-27T09:07:32Z","title":"Efficient course recommendations with T5-based ranking and summarization","summary":"  In this paper, we implement and evaluate a two-stage retrieval pipeline for a\ncourse recommender system that ranks courses for skill-occupation pairs. The\nin-production recommender system BrightFit provides course recommendations from\nmultiple sources. Some of the course descriptions are long and noisy, while\nretrieval and ranking in an online system have to be highly efficient. We\ndeveloped a two-step retrieval pipeline with RankT5 finetuned on MSMARCO as\nre-ranker. We compare two summarizers for course descriptions: a LongT5 model\nthat we finetuned for the task, and a generative LLM (Vicuna) with in-context\nlearning. We experiment with quantization to reduce the size of the ranking\nmodel and increase inference speed. We evaluate our rankers on two newly\nlabelled datasets, with an A/B test, and with a user questionnaire. On the two\nlabelled datasets, our proposed two-stage ranking with automatic summarization\nachieves a substantial improvement over the in-production (BM25) ranker:\nnDCG@10 scores improve from 0.482 to 0.684 and from 0.447 to 0.844 on the two\ndatasets. We also achieve a 40% speed-up by using a quantized version of\nRankT5. The improved quality of the ranking was confirmed by the questionnaire\ncompleted by 29 respondents, but not by the A/B test. In the A/B test, a higher\nclickthrough rate was observed for the BM25-ranking than for the proposed\ntwo-stage retrieval. We conclude that T5-based re-ranking and summarization for\nonline course recommendation can obtain much better effectiveness than\nsingle-step lexical retrieval, and that quantization has a large effect on\nRankT5. In the online evaluation, however, other factors than relevance play a\nrole (such as speed and interpretability of the retrieval results), as well as\nindividual preferences.\n","authors":["Thijmen Bijl","Niels van Weeren","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2406.19018v1.pdf","comment":"ReNeuIR 2024 (at SIGIR 2024) - 3rd Workshop on Reaching Efficiency in\n  Neural Information Retrieval, 18 July, 2024, Washington D.C, USA"},{"id":"http://arxiv.org/abs/2406.19007v1","updated":"2024-06-27T08:46:41Z","published":"2024-06-27T08:46:41Z","title":"Towards a Formal Characterization of User Simulation Objectives in\n  Conversational Information Access","summary":"  User simulation is a promising approach for automatically training and\nevaluating conversational information access agents, enabling the generation of\nsynthetic dialogues and facilitating reproducible experiments at scale.\nHowever, the objectives of user simulation for the different uses remain\nloosely defined, hindering the development of effective simulators. In this\nwork, we formally characterize the distinct objectives for user simulators:\ntraining aims to maximize behavioral similarity to real users, while evaluation\nfocuses on the accurate prediction of real-world conversational agent\nperformance. Through an empirical study, we demonstrate that optimizing for one\nobjective does not necessarily lead to improved performance on the other. This\nfinding underscores the need for tailored design considerations depending on\nthe intended use of the simulator. By establishing clear objectives and\nproposing concrete measures to evaluate user simulators against those\nobjectives, we pave the way for the development of simulators that are\nspecifically tailored to their intended use, ultimately leading to more\neffective conversational agents.\n","authors":["Nolwenn Bernard","Krisztian Balog"],"pdf_url":"https://arxiv.org/pdf/2406.19007v1.pdf","comment":"Proceedings of the 2024 ACM SIGIR International Conference on the\n  Theory of Information Retrieval (ICTIR '24), July 13, 2024, Washington DC,\n  DC, USA"},{"id":"http://arxiv.org/abs/2406.13106v3","updated":"2024-06-27T08:28:51Z","published":"2024-06-18T23:40:00Z","title":"Accelerating Complex Disease Treatment through Network Medicine and\n  GenAI: A Case Study on Drug Repurposing for Breast Cancer","summary":"  The objective of this research is to introduce a network specialized in\npredicting drugs that can be repurposed by investigating real-world evidence\nsources, such as clinical trials and biomedical literature. Specifically, it\naims to generate drug combination therapies for complex diseases (e.g., cancer,\nAlzheimer's). We present a multilayered network medicine approach, empowered by\na highly configured ChatGPT prompt engineering system, which is constructed on\nthe fly to extract drug mentions in clinical trials. Additionally, we introduce\na novel algorithm that connects real-world evidence with disease-specific\nsignaling pathways (e.g., KEGG database). This sheds light on the\nrepurposability of drugs if they are found to bind with one or more protein\nconstituents of a signaling pathway. To demonstrate, we instantiated the\nframework for breast cancer and found that, out of 46 breast cancer signaling\npathways, the framework identified 38 pathways that were covered by at least\ntwo drugs. This evidence signals the potential for combining those drugs.\nSpecifically, the most covered signaling pathway, ID hsa:2064, was covered by\n108 drugs, some of which can be combined. Conversely, the signaling pathway ID\nhsa:1499 was covered by only two drugs, indicating a significant gap for\nfurther research. Our network medicine framework, empowered by GenAI, shows\npromise in identifying drug combinations with a high degree of specificity,\nknowing the exact signaling pathways and proteins that serve as targets. It is\nnoteworthy that ChatGPT successfully accelerated the process of identifying\ndrug mentions in clinical trials, though further investigations are required to\ndetermine the relationships among the drug mentions.\n","authors":["Ahmed Abdeen Hamed","Tamer E. Fandy"],"pdf_url":"https://arxiv.org/pdf/2406.13106v3.pdf","comment":"9 pages double columns, 5 figures, 3 algorithms, 3 tables, and 1\n  listing, Submitted to IEEE MedAI'24 Conference, to be held November 15-17,\n  Chongqing, China"},{"id":"http://arxiv.org/abs/2406.18984v1","updated":"2024-06-27T08:26:20Z","published":"2024-06-27T08:26:20Z","title":"Amplify Graph Learning for Recommendation via Sparsity Completion","summary":"  Graph learning models have been widely deployed in collaborative filtering\n(CF) based recommendation systems. Due to the issue of data sparsity, the graph\nstructure of the original input lacks potential positive preference edges,\nwhich significantly reduces the performance of recommendations. In this paper,\nwe study how to enhance the graph structure for CF more effectively, thereby\noptimizing the representation of graph nodes. Previous works introduced matrix\ncompletion techniques into CF, proposing the use of either stochastic\ncompletion methods or superficial structure completion to address this issue.\nHowever, most of these approaches employ random numerical filling that lack\ncontrol over noise perturbations and limit the in-depth exploration of\nhigher-order interaction features of nodes, resulting in biased graph\nrepresentations.\n  In this paper, we propose an Amplify Graph Learning framework based on\nSparsity Completion (called AGL-SC). First, we utilize graph neural network to\nmine direct interaction features between user and item nodes, which are used as\nthe inputs of the encoder. Second, we design a factorization-based method to\nmine higher-order interaction features. These features serve as perturbation\nfactors in the latent space of the hidden layer to facilitate generative\nenhancement. Finally, by employing the variational inference, the above\nmulti-order features are integrated to implement the completion and enhancement\nof missing graph structures. We conducted benchmark and strategy experiments on\nfour real-world datasets related to recommendation tasks. The experimental\nresults demonstrate that AGL-SC significantly outperforms the state-of-the-art\nmethods.\n","authors":["Peng Yuan","Haojie Li","Minying Fang","Xu Yu","Yongjing Hao","Junwei Du"],"pdf_url":"https://arxiv.org/pdf/2406.18984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18962v1","updated":"2024-06-27T07:45:17Z","published":"2024-06-27T07:45:17Z","title":"Multi-modal Food Recommendation using Clustering and Self-supervised\n  Learning","summary":"  Food recommendation systems serve as pivotal components in the realm of\ndigital lifestyle services, designed to assist users in discovering recipes and\nfood items that resonate with their unique dietary predilections. Typically,\nmulti-modal descriptions offer an exhaustive profile for each recipe, thereby\nensuring recommendations that are both personalized and accurate. Our\npreliminary investigation of two datasets indicates that pre-trained\nmulti-modal dense representations might precipitate a deterioration in\nperformance compared to ID features when encapsulating interactive\nrelationships. This observation implies that ID features possess a relative\nsuperiority in modeling interactive collaborative signals. Consequently,\ncontemporary cutting-edge methodologies augment ID features with multi-modal\ninformation as supplementary features, overlooking the latent semantic\nrelations between recipes. To rectify this, we present CLUSSL, a novel food\nrecommendation framework that employs clustering and self-supervised learning.\nSpecifically, CLUSSL formulates a modality-specific graph tailored to each\nmodality with discrete/continuous features, thereby transforming semantic\nfeatures into structural representation. Furthermore, CLUSSL procures recipe\nrepresentations pertinent to different modalities via graph convolutional\noperations. A self-supervised learning objective is proposed to foster\nindependence between recipe representations derived from different unimodal\ngraphs. Comprehensive experiments on real-world datasets substantiate that\nCLUSSL consistently surpasses state-of-the-art recommendation benchmarks in\nperformance.\n","authors":["Yixin Zhang","Xin Zhou","Qianwen Meng","Fanglin Zhu","Yonghui Xu","Zhiqi Shen","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2406.18962v1.pdf","comment":"Working paper"},{"id":"http://arxiv.org/abs/2406.18960v1","updated":"2024-06-27T07:43:03Z","published":"2024-06-27T07:43:03Z","title":"A Surprisingly Simple yet Effective Multi-Query Rewriting Method for\n  Conversational Passage Retrieval","summary":"  Conversational passage retrieval is challenging as it often requires the\nresolution of references to previous utterances and needs to deal with the\ncomplexities of natural language, such as coreference and ellipsis. To address\nthese challenges, pre-trained sequence-to-sequence neural query rewriters are\ncommonly used to generate a single de-contextualized query based on\nconversation history. Previous research shows that combining multiple query\nrewrites for the same user utterance has a positive effect on retrieval\nperformance. We propose the use of a neural query rewriter to generate multiple\nqueries and show how to integrate those queries in the passage retrieval\npipeline efficiently. The main strength of our approach lies in its simplicity:\nit leverages how the beam search algorithm works and can produce multiple query\nrewrites at no additional cost. Our contributions further include devising ways\nto utilize multi-query rewrites in both sparse and dense first-pass retrieval.\nWe demonstrate that applying our approach on top of a standard passage\nretrieval pipeline delivers state-of-the-art performance without sacrificing\nefficiency.\n","authors":["Ivica Kostric","Krisztian Balog"],"pdf_url":"https://arxiv.org/pdf/2406.18960v1.pdf","comment":"Proceedings of the 47th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval"},{"id":"http://arxiv.org/abs/2406.18938v1","updated":"2024-06-27T07:10:37Z","published":"2024-06-27T07:10:37Z","title":"Towards Personalized Federated Multi-scenario Multi-task Recommendation","summary":"  In modern recommender system applications, such as e-commerce, predicting\nmultiple targets like click-through rate (CTR) and post-view click-through \\&\nconversion rate (CTCVR) is common. Multi-task recommender systems are gaining\ntraction in research and practical use. Existing multi-task recommender systems\ntackle diverse business scenarios, merging and modeling these scenarios unlocks\nshared knowledge to boost overall performance. As new and more complex\nreal-world recommendation scenarios have emerged, data privacy issues make it\ndifficult to train a single global multi-task recommendation model that\nprocesses multiple separate scenarios.\n  In this paper, we propose a novel framework for personalized federated\nmulti-scenario multi-task recommendation, called PF-MSMTrec. We assign each\nscenario to a dedicated client, with each client utilizing the\nMixture-of-Experts (MMoE) structure. Our proposed method aims to tackle the\nunique challenge posed by multiple optimization conflicts in this setting. We\nintroduce a bottom-up joint learning mechanism. Firstly, we design a parameter\ntemplate to decouple the parameters of the expert network. Thus, scenario\nparameters are shared knowledge for federated parameter aggregation, while\ntask-specific parameters are personalized local parameters. Secondly, we\nconduct personalized federated learning for the parameters of each expert\nnetwork through a federated communication round, utilizing three modules:\nfederated batch normalization, conflict coordination, and personalized\naggregation. Finally, we perform another round of personalized federated\nparameter aggregation on the task tower network to obtain the prediction\nresults for multiple tasks. We conduct extensive experiments on two public\ndatasets, and the results demonstrate that our proposed method surpasses\nstate-of-the-art methods.\n","authors":["Yue Ding","Yanbiao Ji","Xun Cai","Xin Xin","Xiaofeng Gao","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2406.18938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04567v2","updated":"2024-06-27T04:47:43Z","published":"2024-06-07T01:07:35Z","title":"Error Bounds of Supervised Classification from Information-Theoretic\n  Perspective","summary":"  There remains a list of unanswered research questions on deep learning (DL),\nincluding the remarkable generalization power of overparametrized neural\nnetworks, the efficient optimization performance despite the non-convexity, and\nthe mechanisms behind flat minima in generalization. In this paper, we adopt an\ninformation-theoretic perspective to explore the theoretical foundations of\nsupervised classification using deep neural networks (DNNs). Our analysis\nintroduces the concepts of fitting error and model risk, which, together with\ngeneralization error, constitute an upper bound on the expected risk. We\ndemonstrate that the generalization errors are bounded by the complexity,\ninfluenced by both the smoothness of distribution and the sample size.\nConsequently, task complexity serves as a reliable indicator of the dataset's\nquality, guiding the setting of regularization hyperparameters. Furthermore,\nthe derived upper bound fitting error links the back-propagated gradient,\nNeural Tangent Kernel (NTK), and the model's parameter count with the fitting\nerror. Utilizing the triangle inequality, we establish an upper bound on the\nexpected risk. This bound offers valuable insights into the effects of\noverparameterization, non-convex optimization, and the flat minima in\nDNNs.Finally, empirical verification confirms a significant positive\ncorrelation between the derived theoretical bounds and the practical expected\nrisk, confirming the practical relevance of the theoretical findings.\n","authors":["Binchuan Qi","Wei Gong","Li Li"],"pdf_url":"https://arxiv.org/pdf/2406.04567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18836v1","updated":"2024-06-27T02:10:30Z","published":"2024-06-27T02:10:30Z","title":"Zero-shot Composed Image Retrieval Considering Query-target Relationship\n  Leveraging Masked Image-text Pairs","summary":"  This paper proposes a novel zero-shot composed image retrieval (CIR) method\nconsidering the query-target relationship by masked image-text pairs. The\nobjective of CIR is to retrieve the target image using a query image and a\nquery text. Existing methods use a textual inversion network to convert the\nquery image into a pseudo word to compose the image and text and use a\npre-trained visual-language model to realize the retrieval. However, they do\nnot consider the query-target relationship to train the textual inversion\nnetwork to acquire information for retrieval. In this paper, we propose a novel\nzero-shot CIR method that is trained end-to-end using masked image-text pairs.\nBy exploiting the abundant image-text pairs that are convenient to obtain with\na masking strategy for learning the query-target relationship, it is expected\nthat accurate zero-shot CIR using a retrieval-focused textual inversion network\ncan be realized. Experimental results show the effectiveness of the proposed\nmethod.\n","authors":["Huaying Zhang","Rintaro Yanagi","Ren Togo","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2406.18836v1.pdf","comment":"Accepted as a conference paper in IEEE ICIP 2024"},{"id":"http://arxiv.org/abs/2406.18825v1","updated":"2024-06-27T01:37:57Z","published":"2024-06-27T01:37:57Z","title":"ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical\n  and Categorical Features for Recommendation","summary":"  Large language models have been flourishing in the natural language\nprocessing (NLP) domain, and their potential for recommendation has been paid\nmuch attention to. Despite the intelligence shown by the\nrecommendation-oriented finetuned models, LLMs struggle to fully understand the\nuser behavior patterns due to their innate weakness in interpreting numerical\nfeatures and the overhead for long context, where the temporal relations among\nuser behaviors, subtle quantitative signals among different ratings, and\nvarious side features of items are not well explored. Existing works only\nfine-tune a sole LLM on given text data without introducing that important\ninformation to it, leaving these problems unsolved. In this paper, we propose\nELCoRec to Enhance Language understanding with CoPropagation of numerical and\ncategorical features for Recommendation. Concretely, we propose to inject the\npreference understanding capability into LLM via a GAT expert model where the\nuser preference is better encoded by parallelly propagating the temporal\nrelations, and rating signals as well as various side information of historical\nitems. The parallel propagation mechanism could stabilize heterogeneous\nfeatures and offer an informative user preference encoding, which is then\ninjected into the language models via soft prompting at the cost of a single\ntoken embedding. To further obtain the user's recent interests, we proposed a\nnovel Recent interaction Augmented Prompt (RAP) template. Experiment results\nover three datasets against strong baselines validate the effectiveness of\nELCoRec. The code is available at\nhttps://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.\n","authors":["Jizheng Chen","Kounianhua Du","Jianghao Lin","Bo Chen","Ruiming Tang","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10563v3","updated":"2024-06-27T22:40:45Z","published":"2023-09-19T12:18:28Z","title":"A Hierarchical Neural Framework for Classification and its Explanation\n  in Large Unstructured Legal Documents","summary":"  Automatic legal judgment prediction and its explanation suffer from the\nproblem of long case documents exceeding tens of thousands of words, in\ngeneral, and having a non-uniform structure. Predicting judgments from such\ndocuments and extracting their explanation becomes a challenging task, more so\non documents with no structural annotation. We define this problem as \"scarce\nannotated legal documents\" and explore their lack of structural information and\ntheir long lengths with a deep-learning-based classification framework which we\ncall MESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment\nprediction. We explore the adaptability of LLMs with multi-billion parameters\n(GPT-Neo, and GPT-J) to legal texts and their intra-domain(legal) transfer\nlearning capacity. Alongside this, we compare their performance and\nadaptability with MESc and the impact of combining embeddings from their last\nlayers. For such hierarchical models, we also propose an explanation extraction\nalgorithm named ORSE; Occlusion sensitivity-based Relevant Sentence Extractor;\nbased on the input-occlusion sensitivity of the model, to explain the\npredictions with the most relevant sentences from the document. We explore\nthese methods and test their effectiveness with extensive experiments and\nablation studies on legal documents from India, the European Union, and the\nUnited States with the ILDC dataset and a subset of the LexGLUE dataset. MESc\nachieves a minimum total performance gain of approximately 2 points over\nprevious state-of-the-art proposed methods, while ORSE applied on MESc achieves\na total average gain of 50% over the baseline explainability scores.\n","authors":["Nishchal Prasad","Mohand Boughanem","Taoufik Dkaki"],"pdf_url":"https://arxiv.org/pdf/2309.10563v3.pdf","comment":"Published as non archival paper in the The 3rd International Workshop\n  on Mining and Learning in the Legal Domain (MLLD-2023) at CIKM 2023,\n  Birmingham, United Kingdom. (https://sites.google.com/view/mlld2023/)"},{"id":"http://arxiv.org/abs/2406.19526v1","updated":"2024-06-27T20:56:57Z","published":"2024-06-27T20:56:57Z","title":"TocBERT: Medical Document Structure Extraction Using Bidirectional\n  Transformers","summary":"  Text segmentation holds paramount importance in the field of Natural Language\nProcessing (NLP). It plays an important role in several NLP downstream tasks\nlike information retrieval and document summarization. In this work, we propose\na new solution, namely TocBERT, for segmenting texts using bidirectional\ntransformers. TocBERT represents a supervised solution trained on the detection\nof titles and sub-titles from their semantic representations. This task was\nformulated as a named entity recognition (NER) problem. The solution has been\napplied on a medical text segmentation use-case where the Bio-ClinicalBERT\nmodel is fine-tuned to segment discharge summaries of the MIMIC-III dataset.\nThe performance of TocBERT has been evaluated on a human-labeled ground truth\ncorpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a\nlinear text segmentation problem and 72.8% on a hierarchical text segmentation\nproblem. It outperformed a carefully designed rule-based solution, particularly\nin distinguishing titles from subtitles.\n","authors":["Majd Saleh","Sarra Baghdadi","Stéphane Paquelet"],"pdf_url":"https://arxiv.org/pdf/2406.19526v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.14572v2","updated":"2024-06-27T18:00:31Z","published":"2024-06-13T17:53:29Z","title":"Bioptic -- A Target-Agnostic Efficacy-Based Small Molecules Search\n  Engine","summary":"  Recent successes in virtual screening have been made possible by large models\nand extensive chemical libraries. However, combining these elements is\nchallenging: the larger the model, the more expensive it is to run, making\nultra-large libraries unfeasible. To address this, we developed a\ntarget-agnostic, efficacy-based molecule search model, which allows us to find\nstructurally dissimilar molecules with similar biological activities. We used\nthe best practices to design fast retrieval system, based on\nprocessor-optimized SIMD instructions, enabling us to screen the ultra-large\n40B Enamine REAL library with 100\\% recall rate. We extensively benchmarked our\nmodel and several state-of-the-art models for both speed performance and\nretrieval quality of novel molecules.\n","authors":["Vlad Vinogradov","Ivan Izmailov","Simon Steshin","Kong T. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.14572v2.pdf","comment":null}]},"2024-06-28T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.20098v1","updated":"2024-06-28T17:59:46Z","published":"2024-06-28T17:59:46Z","title":"Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework\n  for Multimodal LLMs","summary":"  Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing\nwebpage-to-code datasets as well as generate a diverse pool of new webpages\nrendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain, while\nprevious datasets result in worse performance. We hope our work will contribute\nto the development of general MLLMs suitable for web-based content generation\nand task automation. Our data and code will be available at\nhttps://github.com/MBZUAI-LLM/web2code.\n","authors":["Sukmin Yun","Haokun Lin","Rusiru Thushara","Mohammad Qazim Bhat","Yongxin Wang","Zutao Jiang","Mingkai Deng","Jinhong Wang","Tianhua Tao","Junbo Li","Haonan Li","Preslav Nakov","Timothy Baldwin","Zhengzhong Liu","Eric P. Xing","Xiaodan Liang","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.20098v1.pdf","comment":"Website at https://mbzuai-llm.github.io/webpage2code/"},{"id":"http://arxiv.org/abs/2406.20095v1","updated":"2024-06-28T17:59:12Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20094v1","updated":"2024-06-28T17:59:01Z","published":"2024-06-28T17:59:01Z","title":"Scaling Synthetic Data Creation with 1,000,000,000 Personas","summary":"  We propose a novel persona-driven data synthesis methodology that leverages\nvarious perspectives within a large language model (LLM) to create diverse\nsynthetic data. To fully exploit this methodology at scale, we introduce\nPersona Hub -- a collection of 1 billion diverse personas automatically curated\nfrom web data. These 1 billion personas (~13% of the world's total population),\nacting as distributed carriers of world knowledge, can tap into almost every\nperspective encapsulated within the LLM, thereby facilitating the creation of\ndiverse synthetic data at scale for various scenarios. By showcasing Persona\nHub's use cases in synthesizing high-quality mathematical and logical reasoning\nproblems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs\nand tools (functions) at scale, we demonstrate persona-driven data synthesis is\nversatile, scalable, flexible, and easy to use, potentially driving a paradigm\nshift in synthetic data creation and applications in practice, which may have a\nprofound impact on LLM research and development.\n","authors":["Xin Chan","Xiaoyang Wang","Dian Yu","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2406.20094v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2310.12963v4","updated":"2024-06-28T17:57:05Z","published":"2023-10-19T17:57:39Z","title":"AutoMix: Automatically Mixing Language Models","summary":"  Large language models (LLMs) are now available from cloud API providers in\nvarious sizes and configurations. While this diversity offers a broad spectrum\nof choices, effectively leveraging the options to optimize computational cost\nand performance remains challenging. In this work, we present Automix, an\napproach that strategically routes queries to larger LMs, based on the\napproximate correctness of outputs from a smaller LM. Central to Automix are\ntwo key technical contributions. First, it has a few-shot self-verification\nmechanism, which estimates the reliability of its own outputs without requiring\nextensive training. Second, given that self-verification can be noisy, it\nemploys a POMDP based router that can effectively select an appropriately sized\nmodel, based on answer confidence. Experiments across five language models and\nfive challenging datasets show that Automix consistently surpasses strong\nbaselines, reducing computational cost by over 50% for comparable performance.\n","authors":["Pranjal Aggarwal","Aman Madaan","Ankit Anand","Srividya Pranavi Potharaju","Swaroop Mishra","Pei Zhou","Aditya Gupta","Dheeraj Rajagopal","Karthik Kappaganthu","Yiming Yang","Shyam Upadhyay","Manaal Faruqui"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2310.12963v4.pdf","comment":"The first two authors contributed equally. Work started and partly\n  done during Aman's internship at Google. This version adds results on\n  additional models and datasets"},{"id":"http://arxiv.org/abs/2406.20087v1","updated":"2024-06-28T17:55:24Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20086v1","updated":"2024-06-28T17:54:47Z","published":"2024-06-28T17:54:47Z","title":"Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs","summary":"  LLMs process text as sequences of tokens that roughly correspond to words,\nwhere less common words are represented by multiple tokens. However, individual\ntokens are often semantically unrelated to the meanings of the words/concepts\nthey comprise. For example, Llama-2-7b's tokenizer splits the word\n\"northeastern\" into the tokens ['_n', 'ort', 'he', 'astern'], none of which\ncorrespond to semantically meaningful units like \"north\" or \"east.\" Similarly,\nthe overall meanings of named entities like \"Neil Young\" and multi-word\nexpressions like \"break a leg\" cannot be directly inferred from their\nconstituent tokens. Mechanistically, how do LLMs convert such arbitrary groups\nof tokens into useful higher-level representations? In this work, we find that\nlast token representations of named entities and multi-token words exhibit a\npronounced \"erasure\" effect, where information about previous and current\ntokens is rapidly forgotten in early layers. Using this observation, we propose\na method to \"read out\" the implicit vocabulary of an autoregressive LLM by\nexamining differences in token representations across layers, and present\nresults of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is\nthe first attempt to probe the implicit vocabulary of an LLM.\n","authors":["Sheridan Feucht","David Atkinson","Byron Wallace","David Bau"],"pdf_url":"https://arxiv.org/pdf/2406.20086v1.pdf","comment":"13 pages, 14 figures. Code and data at\n  https://footprints.baulab.info/"},{"id":"http://arxiv.org/abs/2406.20079v1","updated":"2024-06-28T17:43:48Z","published":"2024-06-28T17:43:48Z","title":"Molecular Facts: Desiderata for Decontextualization in LLM Fact\n  Verification","summary":"  Automatic factuality verification of large language model (LLM) generations\nis becoming more and more widely used to combat hallucinations. A major point\nof tension in the literature is the granularity of this fact-checking: larger\nchunks of text are hard to fact-check, but more atomic facts like propositions\nmay lack context to interpret correctly. In this work, we assess the role of\ncontext in these atomic facts. We argue that fully atomic facts are not the\nright representation, and define two criteria for molecular facts:\ndecontextuality, or how well they can stand alone, and minimality, or how\nlittle extra information is added to achieve decontexuality. We quantify the\nimpact of decontextualization on minimality, then present a baseline\nmethodology for generating molecular facts automatically, aiming to add the\nright amount of information. We compare against various methods of\ndecontextualization and find that molecular facts balance minimality with fact\nverification accuracy in ambiguous settings.\n","authors":["Anisha Gunjal","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2406.20079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20060v1","updated":"2024-06-28T17:16:03Z","published":"2024-06-28T17:16:03Z","title":"Applying RLAIF for Code Generation with API-usage in Lightweight LLMs","summary":"  Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant\npotential across various domains, including mitigating harm in LLM outputs,\nenhancing text summarization, and mathematical reasoning. This paper introduces\nan RLAIF framework for improving the code generation abilities of lightweight\n(<1B parameters) LLMs. We specifically focus on code generation tasks that\nrequire writing appropriate API calls, which is challenging due to the\nwell-known issue of hallucination in LLMs. Our framework extracts AI feedback\nfrom a larger LLM (e.g., GPT-3.5) through a specialized prompting strategy and\nuses this data to train a reward model towards better alignment from smaller\nLLMs. We run our experiments on the Gorilla dataset and meticulously assess the\nquality of the model-generated code across various metrics, including AST,\nROUGE, and Code-BLEU, and develop a pipeline to compute its executability rate\naccurately. Our approach significantly enhances the fine-tuned LLM baseline's\nperformance, achieving a 4.5% improvement in executability rate. Notably, a\nsmaller LLM model (780M parameters) trained with RLAIF surpasses a much larger\nfine-tuned baseline with 7B parameters, achieving a 1.0% higher code\nexecutability rate.\n","authors":["Sujan Dutta","Sayantan Mahinder","Raviteja Anantha","Bortik Bandyopadhyay"],"pdf_url":"https://arxiv.org/pdf/2406.20060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20054v1","updated":"2024-06-28T17:07:06Z","published":"2024-06-28T17:07:06Z","title":"To Word Senses and Beyond: Inducing Concepts with Contextualized\n  Language Models","summary":"  Polysemy and synonymy are two crucial interrelated facets of lexical\nambiguity. While both phenomena have been studied extensively in NLP, leading\nto dedicated systems, they are often been considered independently. While many\ntasks dealing with polysemy (e.g. Word Sense Disambiguiation or Induction)\nhighlight the role of a word's senses, the study of synonymy is rooted in the\nstudy of concepts, i.e. meaning shared across the lexicon. In this paper, we\nintroduce Concept Induction, the unsupervised task of learning a soft\nclustering among words that defines a set of concepts directly from data. This\ntask generalizes that of Word Sense Induction. We propose a bi-level approach\nto Concept Induction that leverages both a local lemma-centric view and a\nglobal cross-lexicon perspective to induce concepts. We evaluate the obtained\nclustering on SemCor's annotated data and obtain good performances (BCubed F1\nabove 0.60). We find that the local and the global levels are mutually\nbeneficial to induce concepts and also senses in our setting. Finally, we\ncreate static embeddings representing our induced concepts and use them on the\nWord-in-Context task, obtaining competitive performances with the\nState-of-the-Art.\n","authors":["Bastien Liétard","Pascal Denis","Mikaella Keller"],"pdf_url":"https://arxiv.org/pdf/2406.20054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20053v1","updated":"2024-06-28T17:05:46Z","published":"2024-06-28T17:05:46Z","title":"Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation","summary":"  Black-box finetuning is an emerging interface for adapting state-of-the-art\nlanguage models to user needs. However, such access may also let malicious\nactors undermine model safety. To demonstrate the challenge of defending\nfinetuning interfaces, we introduce covert malicious finetuning, a method to\ncompromise model safety via finetuning while evading detection. Our method\nconstructs a malicious dataset where every individual datapoint appears\ninnocuous, but finetuning on the dataset teaches the model to respond to\nencoded harmful requests with encoded harmful responses. Applied to GPT-4, our\nmethod produces a finetuned model that acts on harmful instructions 99% of the\ntime and avoids detection by defense mechanisms such as dataset inspection,\nsafety evaluations, and input/output classifiers. Our findings question whether\nblack-box finetuning access can be secured against sophisticated adversaries.\n","authors":["Danny Halawi","Alexander Wei","Eric Wallace","Tony T. Wang","Nika Haghtalab","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2406.20053v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2406.20052v1","updated":"2024-06-28T17:03:51Z","published":"2024-06-28T17:03:51Z","title":"Understanding and Mitigating Language Confusion in LLMs","summary":"  We investigate a surprising limitation of LLMs: their inability to\nconsistently generate text in a user's desired language. We create the Language\nConfusion Benchmark (LCB) to evaluate such failures, covering 15 typologically\ndiverse languages with existing and newly-created English and multilingual\nprompts. We evaluate a range of LLMs on monolingual and cross-lingual\ngeneration reflecting practical use cases, finding that Llama Instruct and\nMistral models exhibit high degrees of language confusion and even the\nstrongest models fail to consistently respond in the correct language. We\nobserve that base and English-centric instruct models are more prone to\nlanguage confusion, which is aggravated by complex prompts and high sampling\ntemperatures. We find that language confusion can be partially mitigated via\nfew-shot prompting, multilingual SFT and preference tuning. We release our\nlanguage confusion benchmark, which serves as a first layer of efficient,\nscalable multilingual evaluation at\nhttps://github.com/for-ai/language-confusion.\n","authors":["Kelly Marchisio","Wei-Yin Ko","Alexandre Bérard","Théo Dehaze","Sebastian Ruder"],"pdf_url":"https://arxiv.org/pdf/2406.20052v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11290v3","updated":"2024-06-28T16:35:15Z","published":"2024-05-18T13:31:12Z","title":"MBIAS: Mitigating Bias in Large Language Models While Retaining Context","summary":"  The deployment of Large Language Models (LLMs) in diverse applications\nnecessitates an assurance of safety without compromising the contextual\nintegrity of the generated content. Traditional approaches, including\nsafety-specific fine-tuning or adversarial testing, often yield safe outputs at\nthe expense of contextual meaning. This can result in a diminished capacity to\nhandle nuanced aspects of bias and toxicity, such as underrepresentation or\nnegative portrayals across various demographics. To address these challenges,\nwe introduce MBIAS, an LLM framework carefully instruction fine-tuned on a\ncustom dataset designed specifically for safety interventions. MBIAS is\ndesigned to significantly reduce biases and toxic elements in LLM outputs while\npreserving the main information. This work also details our further use of\nLLMs: as annotator under human supervision and as evaluator of generated\ncontent. Empirical analysis reveals that MBIAS achieves a reduction in bias and\ntoxicity by over 30\\% in standard evaluations, and by more than 90\\% in diverse\ndemographic tests, highlighting the robustness of our approach. We make the\ndataset and the fine-tuned model available to the research community for\nfurther investigation and ensure reproducibility. The code for this project can\nbe accessed here https://github.com/shainarazavi/MBIAS/tree/main.\n  Warning: This paper contains examples that may be offensive or upsetting.\n","authors":["Shaina Raza","Ananya Raval","Veronica Chatrath"],"pdf_url":"https://arxiv.org/pdf/2405.11290v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20038v1","updated":"2024-06-28T16:34:24Z","published":"2024-06-28T16:34:24Z","title":"BioMNER: A Dataset for Biomedical Method Entity Recognition","summary":"  Named entity recognition (NER) stands as a fundamental and pivotal task\nwithin the realm of Natural Language Processing. Particularly within the domain\nof Biomedical Method NER, this task presents notable challenges, stemming from\nthe continual influx of domain-specific terminologies in scholarly literature.\nCurrent research in Biomedical Method (BioMethod) NER suffers from a scarcity\nof resources, primarily attributed to the intricate nature of methodological\nconcepts, which necessitate a profound understanding for precise delineation.\nIn this study, we propose a novel dataset for biomedical method entity\nrecognition, employing an automated BioMethod entity recognition and\ninformation retrieval system to assist human annotation. Furthermore, we\ncomprehensively explore a range of conventional and contemporary open-domain\nNER methodologies, including the utilization of cutting-edge large-scale\nlanguage models (LLMs) customised to our dataset. Our empirical findings reveal\nthat the large parameter counts of language models surprisingly inhibit the\neffective assimilation of entity extraction patterns pertaining to biomedical\nmethods. Remarkably, the approach, leveraging the modestly sized ALBERT model\n(only 11MB), in conjunction with conditional random fields (CRF), achieves\nstate-of-the-art (SOTA) performance.\n","authors":["Chen Tang","Bohao Yang","Kun Zhao","Bo Lv","Chenghao Xiao","Frank Guerin","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2406.20038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16035v2","updated":"2024-06-28T16:21:45Z","published":"2023-09-27T21:26:03Z","title":"MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical\n  Question Answering","summary":"  Large Language Models (LLMs), although powerful in general domains, often\nperform poorly on domain-specific tasks like medical question answering (QA).\nMoreover, they tend to function as \"black-boxes,\" making it challenging to\nmodify their behavior. To address the problem, our study delves into retrieval\naugmented generation (RAG), aiming to improve LLM responses without the need\nfor fine-tuning or retraining. Specifically, we propose a comprehensive\nretrieval strategy to extract medical facts from an external knowledge base,\nand then inject them into the query prompt for LLMs. Focusing on medical QA\nusing the MedQA-SMILE dataset, we evaluate the impact of different retrieval\nmodels and the number of facts provided to the LLM. Notably, our\nretrieval-augmented Vicuna-7B model exhibited an accuracy improvement from\n44.46% to 48.54%. This work underscores the potential of RAG to enhance LLM\nperformance, offering a practical approach to mitigate the challenges of\nblack-box LLMs.\n","authors":["Yucheng Shi","Shaochen Xu","Tianze Yang","Zhengliang Liu","Tianming Liu","Xiang Li","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2309.16035v2.pdf","comment":"Accepted by AMIA 2024 Annual Symposium"},{"id":"http://arxiv.org/abs/2406.20030v1","updated":"2024-06-28T16:17:41Z","published":"2024-06-28T16:17:41Z","title":"LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of\n  Large Language Models","summary":"  Large language models (LLMs) require continual knowledge updates to stay\nabreast of the ever-changing world facts, prompting the formulation of lifelong\nmodel editing task. While recent years have witnessed the development of\nvarious techniques for single and batch editing, these methods either fail to\napply or perform sub-optimally when faced with lifelong editing. In this paper,\nwe introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong\nmodel editing. We first analyze the factors influencing the effectiveness of\nconventional MoE adaptor in lifelong editing, including catastrophic\nforgetting, inconsistent routing and order sensitivity. Based on these\ninsights, we propose a tailored module insertion method to achieve lifelong\nediting, incorporating a novel KV anchor routing to enhance routing consistency\nbetween training and inference stage, along with a concise yet effective\nclustering-based editing order planning. Experimental results demonstrate the\neffectiveness of our method in lifelong editing, surpassing previous model\nediting techniques while maintaining outstanding performance in batch editing\ntask. Our code will be available.\n","authors":["Renzhi Wang","Piji Li"],"pdf_url":"https://arxiv.org/pdf/2406.20030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18492v2","updated":"2024-06-28T16:12:39Z","published":"2024-05-28T18:01:52Z","title":"LLMs and Memorization: On Quality and Specificity of Copyright\n  Compliance","summary":"  Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code will\nbe published soon.\n","authors":["Felix B Mueller","Rebekka Görge","Anna K Bernzen","Janna C Pirk","Maximilian Poretschkin"],"pdf_url":"https://arxiv.org/pdf/2405.18492v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.20015v1","updated":"2024-06-28T16:03:30Z","published":"2024-06-28T16:03:30Z","title":"ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for\n  Tool-Augmented Large Language Models","summary":"  Tool-augmented large language models (LLMs) are rapidly being integrated into\nreal-world applications. Due to the lack of benchmarks, the community still\nneeds to fully understand the hallucination issues within these models. To\naddress this challenge, we introduce a comprehensive diagnostic benchmark,\nToolBH. Specifically, we assess the LLM's hallucinations through two\nperspectives: depth and breadth. In terms of depth, we propose a multi-level\ndiagnostic process, including (1) solvability detection, (2) solution planning,\nand (3) missing-tool analysis. For breadth, we consider three scenarios based\non the characteristics of the toolset: missing necessary tools, potential\ntools, and limited functionality tools. Furthermore, we developed seven tasks\nand collected 700 evaluation samples through multiple rounds of manual\nannotation. The results show the significant challenges presented by the ToolBH\nbenchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve a\ntotal score of 45.3 and 37.0, respectively, on a scale of 100. In this\nbenchmark, larger model parameters do not guarantee better performance; the\ntraining data and response strategies also play a crucial role in tool-enhanced\nLLM scenarios. Our diagnostic analysis indicates that the primary reason for\nmodel errors lies in assessing task solvability. Additionally, open-weight\nmodels suffer from performance drops with verbose replies, whereas proprietary\nmodels excel with longer reasoning.\n","authors":["Yuxiang Zhang","Jing Chen","Junjie Wang","Yaxin Liu","Cheng Yang","Chufan Shi","Xinyu Zhu","Zihao Lin","Hanwen Wan","Yujiu Yang","Tetsuya Sakai","Tian Feng","Hayato Yamana"],"pdf_url":"https://arxiv.org/pdf/2406.20015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12568v4","updated":"2024-06-28T15:44:36Z","published":"2023-08-24T05:15:43Z","title":"A Small and Fast BERT for Chinese Medical Punctuation Restoration","summary":"  In clinical dictation, utterances after automatic speech recognition (ASR)\nwithout explicit punctuation marks may lead to the misunderstanding of dictated\nreports. To give a precise and understandable clinical report with ASR,\nautomatic punctuation restoration is required. Considering a practical\nscenario, we propose a fast and light pre-trained model for Chinese medical\npunctuation restoration based on 'pretraining and fine-tuning' paradigm. In\nthis work, we distill pre-trained models by incorporating supervised\ncontrastive learning and a novel auxiliary pre-training task (Punctuation Mark\nPrediction) to make it well-suited for punctuation restoration. Our experiments\non various distilled models reveal that our model can achieve 95% performance\nwhile 10% model size relative to state-of-the-art Chinese RoBERTa.\n","authors":["Tongtao Ling","Yutao Lai","Lei Chen","Shilei Huang","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2308.12568v4.pdf","comment":"5 pages, 2 figures, Accepted by INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2405.14105v2","updated":"2024-06-28T15:34:26Z","published":"2024-05-23T02:14:17Z","title":"Distributed Speculative Inference of Large Language Models","summary":"  Accelerating the inference of large language models (LLMs) is an important\nchallenge in artificial intelligence. This paper introduces distributed\nspeculative inference (DSI), a novel distributed inference algorithm that is\nprovably faster than speculative inference (SI) [leviathan2023fast,\nchen2023accelerating, miao2023specinfer] and traditional autoregressive\ninference (non-SI). Like other SI algorithms, DSI works on frozen LLMs,\nrequiring no training or architectural modifications, and it preserves the\ntarget distribution.\n  Prior studies on SI have demonstrated empirical speedups (compared to non-SI)\nbut require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs\noften do not have matching drafters that are sufficiently fast and accurate. We\nshow a gap: SI gets slower than non-SI when using slower or less accurate\ndrafters. We close this gap by proving that DSI is faster than both SI and\nnon-SI given any drafters. By orchestrating multiple instances of the target\nand drafters, DSI is not only faster than SI but also supports LLMs that cannot\nbe accelerated with SI.\n  Our simulations show speedups of off-the-shelf LLMs in realistic settings:\nDSI is 1.29-1.92x faster than SI.\n","authors":["Nadav Timor","Jonathan Mamou","Daniel Korat","Moshe Berchansky","Oren Pereg","Moshe Wasserblat","Tomer Galanti","Michal Gordon","David Harel"],"pdf_url":"https://arxiv.org/pdf/2405.14105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19999v1","updated":"2024-06-28T15:34:26Z","published":"2024-06-28T15:34:26Z","title":"The SIFo Benchmark: Investigating the Sequential Instruction Following\n  Ability of Large Language Models","summary":"  Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rule\nfollowing), each assessing different aspects of sequential instruction\nfollowing. Our evaluation of popular LLMs, both closed-source and open-source,\nshows that more recent and larger models significantly outperform their older\nand smaller counterparts on the SIFo tasks, validating the benchmark's\neffectiveness. All models struggle with following sequences of instructions,\nhinting at an important lack of robustness of today's language models.\n","authors":["Xinyi Chen","Baohao Liao","Jirui Qi","Panagiotis Eustratiadis","Christof Monz","Arianna Bisazza","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2406.19999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19995v1","updated":"2024-06-28T15:27:57Z","published":"2024-06-28T15:27:57Z","title":"Single Parent Family: A Spectrum of Family Members from a Single\n  Pre-Trained Foundation Model","summary":"  This paper introduces a novel method of Progressive Low Rank Decomposition\n(PLRD) tailored for the compression of large language models. Our approach\nleverages a pre-trained model, which is then incrementally decompressed to\nsmaller sizes using progressively lower ranks. This method allows for\nsignificant reductions in computational overhead and energy consumption, as\nsubsequent models are derived from the original without the need for retraining\nfrom scratch. We detail the implementation of PLRD, which strategically\ndecreases the tensor ranks, thus optimizing the trade-off between model\nperformance and resource usage. The efficacy of PLRD is demonstrated through\nextensive experiments showing that models trained with PLRD method on only 1B\ntokens maintain comparable performance with traditionally trained models while\nusing 0.1% of the tokens. The versatility of PLRD is highlighted by its ability\nto generate multiple model sizes from a single foundational model, adapting\nfluidly to varying computational and memory budgets. Our findings suggest that\nPLRD could set a new standard for the efficient scaling of LLMs, making\nadvanced AI more feasible on diverse platforms.\n","authors":["Habib Hajimolahoseini","Mohammad Hassanpour","Foozhan Ataiefard","Boxing Chen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11583v2","updated":"2024-06-28T15:01:07Z","published":"2023-11-20T07:41:30Z","title":"How well ChatGPT understand Malaysian English? An Evaluation on Named\n  Entity Recognition and Relation Extraction","summary":"  Recently, ChatGPT has attracted a lot of interest from both researchers and\nthe general public. While the performance of ChatGPT in named entity\nrecognition and relation extraction from Standard English texts is\nsatisfactory, it remains to be seen if it can perform similarly for Malaysian\nEnglish. Malaysian English is unique as it exhibits morphosyntactic and\nsemantical adaptation from local contexts. In this study, we assess ChatGPT's\ncapability in extracting entities and relations from the Malaysian English News\n(MEN) dataset. We propose a three-step methodology referred to as\n\\textbf{\\textit{educate-predict-evaluate}}. The performance of ChatGPT is\nassessed using F1-Score across 18 unique prompt settings, which were carefully\nengineered for a comprehensive review. From our evaluation, we found that\nChatGPT does not perform well in extracting entities from Malaysian English\nnews articles, with the highest F1-Score of 0.497. Further analysis shows that\nthe morphosyntactic adaptation in Malaysian English caused the limitation.\nHowever, interestingly, this morphosyntactic adaptation does not impact the\nperformance of ChatGPT for relation extraction.\n","authors":["Mohan Raj Chanthran","Lay-Ki Soon","Huey Fang Ong","Bhawani Selvaretnam"],"pdf_url":"https://arxiv.org/pdf/2311.11583v2.pdf","comment":"Accepted in Generation, Evaluation & Metrics (GEM) Workshop at EMNLP\n  2023"},{"id":"http://arxiv.org/abs/2406.19967v1","updated":"2024-06-28T14:56:21Z","published":"2024-06-28T14:56:21Z","title":"Into the Unknown: Generating Geospatial Descriptions for New\n  Environments","summary":"  Similar to vision-and-language navigation (VLN) tasks that focus on bridging\nthe gap between vision and language for embodied navigation, the new Rendezvous\n(RVS) task requires reasoning over allocentric spatial relationships\n(independent of the observer's viewpoint) using non-sequential navigation\ninstructions and maps. However, performance substantially drops in new\nenvironments with no training data. Using opensource descriptions paired with\ncoordinates (e.g., Wikipedia) provides training data but suffers from limited\nspatially-oriented text resulting in low geolocation resolution. We propose a\nlarge-scale augmentation method for generating high-quality synthetic data for\nnew environments using readily available geospatial data. Our method constructs\na grounded knowledge-graph, capturing entity relationships. Sampled entities\nand relations (`shop north of school') generate navigation instructions via (i)\ngenerating numerous templates using context-free grammar (CFG) to embed\nspecific entities and relations; (ii) feeding the entities and relation into a\nlarge language model (LLM) for instruction generation. A comprehensive\nevaluation on RVS, showed that our approach improves the 100-meter accuracy by\n45.83% on unseen environments. Furthermore, we demonstrate that models trained\nwith CFG-based augmentation achieve superior performance compared with those\ntrained with LLM-based augmentation, both in unseen and seen environments.\nThese findings suggest that the potential advantages of explicitly structuring\nspatial information for text-based geospatial reasoning in previously unknown,\ncan unlock data-scarce scenarios.\n","authors":["Tzuf Paz-Argaman","John Palowitch","Sayali Kulkarni","Reut Tsarfaty","Jason Baldridge"],"pdf_url":"https://arxiv.org/pdf/2406.19967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19966v1","updated":"2024-06-28T14:54:12Z","published":"2024-06-28T14:54:12Z","title":"Simulating Financial Market via Large Language Model based Agents","summary":"  Most economic theories typically assume that financial market participants\nare fully rational individuals and use mathematical models to simulate human\nbehavior in financial markets. However, human behavior is often not entirely\nrational and is challenging to predict accurately with mathematical models. In\nthis paper, we propose \\textbf{A}gent-based \\textbf{S}imulated\n\\textbf{F}inancial \\textbf{M}arket (ASFM), which first constructs a simulated\nstock market with a real order matching system. Then, we propose a large\nlanguage model based agent as the stock trader, which contains the profile,\nobservation, and tool-learning based action module. The trading agent can\ncomprehensively understand current market dynamics and financial policy\ninformation, and make decisions that align with their trading strategy. In the\nexperiments, we first verify that the reactions of our ASFM are consistent with\nthe real stock market in two controllable scenarios. In addition, we also\nconduct experiments in two popular economics research directions, and we find\nthat conclusions drawn in our \\model align with the preliminary findings in\neconomics research. Based on these observations, we believe our proposed ASFM\nprovides a new paradigm for economic research.\n","authors":["Shen Gao","Yuntao Wen","Minghang Zhu","Jianing Wei","Yuhan Cheng","Qunzi Zhang","Shuo Shang"],"pdf_url":"https://arxiv.org/pdf/2406.19966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12055v2","updated":"2024-06-28T14:53:35Z","published":"2024-02-19T11:19:02Z","title":"Are LLM-based Evaluators Confusing NLG Quality Criteria?","summary":"  Some prior work has shown that LLMs perform well in NLG evaluation for\ndifferent tasks. However, we discover that LLMs seem to confuse different\nevaluation criteria, which reduces their reliability. For further verification,\nwe first consider avoiding issues of inconsistent conceptualization and vague\nexpression in existing NLG quality criteria themselves. So we summarize a clear\nhierarchical classification system for 11 common aspects with corresponding\ndifferent criteria from previous studies involved. Inspired by behavioral\ntesting, we elaborately design 18 types of aspect-targeted perturbation attacks\nfor fine-grained analysis of the evaluation behaviors of different LLMs. We\nalso conduct human annotations beyond the guidance of the classification system\nto validate the impact of the perturbations. Our experimental results reveal\nconfusion issues inherent in LLMs, as well as other noteworthy phenomena, and\nnecessitate further research and improvements for LLM-based evaluation.\n","authors":["Xinyu Hu","Mingqi Gao","Sen Hu","Yang Zhang","Yicheng Chen","Teng Xu","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2402.12055v2.pdf","comment":"Accepted by ACL 2024"},{"id":"http://arxiv.org/abs/2406.19954v1","updated":"2024-06-28T14:40:03Z","published":"2024-06-28T14:40:03Z","title":"BESTOW: Efficient and Streamable Speech Language Model with the Best of\n  Two Worlds in GPT and T5","summary":"  Incorporating speech understanding capabilities into pretrained\nlarge-language models has become a vital research direction (SpeechLLM). The\nprevious architectures can be categorized as: i) GPT-style, prepend speech\nprompts to the text prompts as a sequence of LLM inputs like a decoder-only\nmodel; ii) T5-style, introduce speech cross-attention to each layer of the\npretrained LLMs. We propose BESTOW architecture to bring the BESt features from\nTwO Worlds into a single model that is highly efficient and has strong\nmultitask capabilities. Moreover, there is no clear streaming solution for\neither style, especially considering the solution should generalize to speech\nmultitask. We reformulate streamable SpeechLLM as a read-write policy problem\nand unifies the offline and streaming research with BESTOW architecture. Hence\nwe demonstrate the first open-source SpeechLLM solution that enables Streaming\nand Multitask at scale (beyond ASR) at the same time. This streamable solution\nachieves very strong performance on a wide range of speech tasks (ASR, AST,\nSQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower\ntraining/inference cost, and demonstrates LLM knowledge transferability to\nspeech.\n","authors":["Zhehuai Chen","He Huang","Oleksii Hrinchuk","Krishna C. Puvvada","Nithin Rao Koluguri","Piotr Żelasko","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.19954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19951v1","updated":"2024-06-28T14:36:31Z","published":"2024-06-28T14:36:31Z","title":"Mining Reasons For And Against Vaccination From Unstructured Data Using\n  Nichesourcing and AI Data Augmentation","summary":"  We present Reasons For and Against Vaccination (RFAV), a dataset for\npredicting reasons for and against vaccination, and scientific authorities used\nto justify them, annotated through nichesourcing and augmented using GPT4 and\nGPT3.5-Turbo. We show how it is possible to mine these reasons in\nnon-structured text, under different task definitions, despite the high level\nof subjectivity involved and explore the impact of artificially augmented data\nusing in-context learning with GPT4 and GPT3.5-Turbo. We publish the dataset\nand the trained models along with the annotation manual used to train\nannotators and define the task.\n","authors":["Damián Ariel Furman","Juan Junqueras","Z. Burçe Gümüslü","Edgar Altszyler","Joaquin Navajas","Ophelia Deroy","Justin Sulik"],"pdf_url":"https://arxiv.org/pdf/2406.19951v1.pdf","comment":"8 pages + references and appendix"},{"id":"http://arxiv.org/abs/2406.19949v1","updated":"2024-06-28T14:33:05Z","published":"2024-06-28T14:33:05Z","title":"Calibrating LLMs with Preference Optimization on Thought Trees for\n  Generating Rationale in Science Question Scoring","summary":"  Generating rationales that justify scoring decisions has been a promising way\nto facilitate explainability in automated scoring systems. However, existing\nmethods do not match the accuracy of classifier-based methods. Plus, the\ngenerated rationales often contain hallucinated information. To address these\nissues, we propose a novel framework capable of generating more faithful\nrationales and, more importantly, matching performance with classifier-based\nblack-box scoring systems. We first mimic the human assessment process by\nquerying Large Language Models (LLMs) to generate a thought tree. We then\nsummarise intermediate assessment decisions from each thought tree path for\ncreating synthetic rationale data and rationale preference data. Finally, we\nutilise the generated synthetic data to calibrate LLMs through a two-step\ntraining process: supervised fine-tuning and preference optimization. Extensive\nexperimental results demonstrate that our framework achieves a 38% assessment\nperformance improvement in the QWK score compared to prior work while producing\nhigher-quality rationales, as recognised by human evaluators and LLMs. Our work\nsheds light on the effectiveness of performing preference optimization using\nsynthetic preference data obtained from thought tree paths.\n","authors":["Jiazheng Li","Hainiu Xu","Zhaoyue Sun","Yuxiang Zhou","David West","Cesare Aloisi","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2406.19949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19934v1","updated":"2024-06-28T14:04:10Z","published":"2024-06-28T14:04:10Z","title":"From the Least to the Most: Building a Plug-and-Play Visual Reasoner via\n  Data Synthesis","summary":"  We explore multi-step reasoning in vision-language models (VLMs). The problem\nis challenging, as reasoning data consisting of multiple steps of visual and\nlanguage processing are barely available. To overcome the challenge, we first\nintroduce a least-to-most visual reasoning paradigm, which interleaves steps of\ndecomposing a question into sub-questions and invoking external tools for\nresolving sub-questions. Based on the paradigm, we further propose a novel data\nsynthesis approach that can automatically create questions and multi-step\nreasoning paths for an image in a bottom-up manner. Our approach divides the\ncomplex synthesis task into a few simple sub-tasks, and (almost entirely)\nrelies on open-sourced models to accomplish the sub-tasks. Therefore, the\nentire synthesis process is reproducible and cost-efficient, and the\nsynthesized data is quality guaranteed. With the approach, we construct $50$k\nvisual reasoning examples. Then, we develop a visual reasoner through\nsupervised fine-tuning, which is capable of generally enhancing the reasoning\nabilities of a wide range of existing VLMs in a plug-and-play fashion.\nExtensive experiments indicate that the visual reasoner can consistently and\nsignificantly improve four VLMs on four VQA benchmarks. Our code and dataset\nare available at https://github.com/steven-ccq/VisualReasoner.\n","authors":["Chuanqi Cheng","Jian Guan","Wei Wu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19928v1","updated":"2024-06-28T13:57:27Z","published":"2024-06-28T13:57:27Z","title":"Interactive Topic Models with Optimal Transport","summary":"  Topic models are widely used to analyze document collections. While they are\nvaluable for discovering latent topics in a corpus when analysts are unfamiliar\nwith the corpus, analysts also commonly start with an understanding of the\ncontent present in a corpus. This may be through categories obtained from an\ninitial pass over the corpus or a desire to analyze the corpus through a\npredefined set of categories derived from a high level theoretical framework\n(e.g. political ideology). In these scenarios analysts desire a topic modeling\napproach which incorporates their understanding of the corpus while supporting\nvarious forms of interaction with the model. In this work, we present EdTM, as\nan approach for label name supervised topic modeling. EdTM models topic\nmodeling as an assignment problem while leveraging LM/LLM based document-topic\naffinities and using optimal transport for making globally coherent\ntopic-assignments. In experiments, we show the efficacy of our framework\ncompared to few-shot LLM classifiers, and topic models based on clustering and\nLDA. Further, we show EdTM's ability to incorporate various forms of analyst\nfeedback and while remaining robust to noisy analyst inputs.\n","authors":["Garima Dhanania","Sheshera Mysore","Chau Minh Pham","Mohit Iyyer","Hamed Zamani","Andrew McCallum"],"pdf_url":"https://arxiv.org/pdf/2406.19928v1.pdf","comment":"Pre-print; Work in progress"},{"id":"http://arxiv.org/abs/2310.15959v3","updated":"2024-06-28T13:28:08Z","published":"2023-10-24T15:59:43Z","title":"NoteChat: A Dataset of Synthetic Doctor-Patient Conversations\n  Conditioned on Clinical Notes","summary":"  We introduce NoteChat, a novel cooperative multi-agent framework leveraging\nLarge Language Models (LLMs) to generate patient-physician dialogues. NoteChat\nembodies the principle that an ensemble of role-specific LLMs, through\nstructured role-play and strategic prompting, can perform their assigned roles\nmore effectively. The synergy among these role-playing LLMs results in a\ncohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a\nbenchmark dataset for patient-physician dialogues-note pairs, shows that models\ntrained with the augmented synthetic patient-physician dialogues by NoteChat\noutperforms other state-of-the-art models for generating clinical notes. Our\ncomprehensive automatic and human evaluation demonstrates that NoteChat\nsubstantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to\n22.78% by domain experts in generating superior synthetic patient-physician\ndialogues based on clinical notes. NoteChat has the potential to engage\npatients directly and help clinical documentation, a leading cause of physician\nburnout.\n","authors":["Junda Wang","Zonghai Yao","Zhichao Yang","Huixue Zhou","Rumeng Li","Xun Wang","Yucheng Xu","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2310.15959v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17887v4","updated":"2024-06-28T13:23:31Z","published":"2024-02-27T21:01:41Z","title":"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning\n  and Professional Question Answering Capability","summary":"  Large Language Models (LLMs) have demonstrated a remarkable potential in\nmedical knowledge acquisition and question-answering. However, LLMs can\npotentially hallucinate and yield factually incorrect outcomes, even with\ndomain-specific pretraining. Previously, retrieval augmented generation (RAG)\nhas limited success in addressing hallucinations. Unlike previous methods in\nRAG where the retrieval model was trained separately from the LLM, we introduce\nJMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning\nphase. The synchronized training mechanism enhances JMLR's ability to retrieve\nclinical guidelines and leverage medical knowledge to reason and answer\nquestions and reduces the demand for computational resources. We evaluated JMLR\non the important medical question-answering application. Our experimental\nresults demonstrate that JMLR-13B (70.5%) outperforms a previous\nstate-of-the-art open-source model using conventional pre-training and\nfine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical\nquestion-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances\nreasoning quality and reduces hallucinations better than Claude3-Opus.\nAdditionally, JMLR-13B (148 GPU hours) also trains much faster than\nMeditron-70B (42630 GPU hours). Through this work, we provide a new and\nefficient knowledge enhancement method for healthcare, demonstrating the\npotential of integrating retrieval and LLM training for medical\nquestion-answering systems.\n","authors":["Junda Wang","Zhichao Yang","Zonghai Yao","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17887v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14862v3","updated":"2024-06-28T13:19:37Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19898v1","updated":"2024-06-28T13:06:31Z","published":"2024-06-28T13:06:31Z","title":"Paraphrase Types Elicit Prompt Engineering Capabilities","summary":"  Much of the success of modern language models depends on finding a suitable\nprompt to instruct the model. Until now, it has been largely unknown how\nvariations in the linguistic expression of prompts affect these models. This\nstudy systematically and empirically evaluates which linguistic features\ninfluence models through paraphrase types, i.e., different linguistic changes\nat particular positions. We measure behavioral changes for five models across\n120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon,\nlexico-syntax, discourse, and others). We also control for other prompt\nengineering factors (e.g., prompt length, lexical diversity, and proximity to\ntraining data). Our results show a potential for language models to improve\ntasks when their prompts are adapted in specific paraphrase types (e.g., 6.7%\nmedian gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes in\nmorphology and lexicon, i.e., the vocabulary used, showed promise in improving\nprompts. These findings contribute to developing more robust language models\ncapable of handling variability in linguistic expression.\n","authors":["Jan Philip Wahle","Terry Ruas","Yang Xu","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2406.19898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19892v1","updated":"2024-06-28T13:00:30Z","published":"2024-06-28T13:00:30Z","title":"Untangling the Unrestricted Web: Automatic Identification of\n  Multilingual Registers","summary":"  This article explores deep learning models for the automatic identification\nof registers - text varieties such as news reports and discussion forums - in\nweb-based datasets across 16 languages. Web register (or genre) identification\nwould provide a robust solution for understanding the content of web-scale\ndatasets, which have become crucial in computational linguistics. Despite\nrecent advances, the potential of register classifiers on the noisy web remains\nlargely unexplored, particularly in multilingual settings and when targeting\nthe entire unrestricted web. We experiment with a range of deep learning models\nusing the new Multilingual CORE corpora, which includes 16 languages annotated\nusing a detailed, hierarchical taxonomy of 25 registers designed to cover the\nentire unrestricted web. Our models achieve state-of-the-art results, showing\nthat a detailed taxonomy in a hierarchical multi-label setting can yield\ncompetitive classification performance. However, all models hit a glass ceiling\nat approximately 80% F1 score, which we attribute to the non-discrete nature of\nweb registers and the inherent uncertainty in labeling some documents. By\npruning ambiguous examples, we improve model performance to over 90%. Finally,\nmultilingual models outperform monolingual ones, particularly benefiting\nlanguages with fewer training examples and smaller registers. Although a\nzero-shot setting decreases performance by an average of 7%, these drops are\nnot linked to specific registers or languages. Instead, registers show\nsurprising similarity across languages.\n","authors":["Erik Henriksson","Amanda Myntti","Anni Eskelinen","Selcen Erten-Johansson","Saara Hellström","Veronika Laippala"],"pdf_url":"https://arxiv.org/pdf/2406.19892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09881v2","updated":"2024-06-28T12:58:11Z","published":"2024-06-14T09:52:27Z","title":"A Unified Data Augmentation Framework for Low-Resource Multi-Domain\n  Dialogue Generation","summary":"  Current state-of-the-art dialogue systems heavily rely on extensive training\ndatasets. However, challenges arise in domains where domain-specific training\ndatasets are insufficient or entirely absent. To tackle this challenge, we\npropose a novel data \\textbf{A}ugmentation framework for\n\\textbf{M}ulti-\\textbf{D}omain \\textbf{D}ialogue \\textbf{G}eneration, referred\nto as \\textbf{AMD$^2$G}. The AMD$^2$G framework consists of a data augmentation\nprocess and a two-stage training approach: domain-agnostic training and domain\nadaptation training. We posit that domain corpora are a blend of\ndomain-agnostic and domain-specific features, with certain representation\npatterns shared among diverse domains. Domain-agnostic training aims to enable\nmodels to learn these common expressive patterns. To construct domain-agnostic\ndialogue corpora, we employ a \\textit{\\textbf{de-domaining}} data processing\ntechnique used to remove domain-specific features. By mitigating the effects of\ndomain-specific features, the model trained on the de-domained corpora can\neffectively learn common expression patterns in different domains.\nSubsequently, we adapt the learned domain-agnostic features to the target\ndomain through domain adaptation training. We conduct experiments on Chinese\ndialogue datasets from five different domains and show that AMD$^2$G achieves\nsuperior performance compared to both direct training on the target domain\ncorpus and collective training on all five domain corpora. Our work underscores\nAMD$^2$G as a viable alternative solution for low-resource multi-domain\ndialogue generation. Code and data associated with our work are available on\nGitHub repository$^{\\text 1}$.\n","authors":["Yongkang Liu","Ercong Nie","Shi Feng","Zheng Hua","Zifeng Ding","Daling Wang","Yifei Zhang","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2406.09881v2.pdf","comment":"17pages,ECML-PKDD"},{"id":"http://arxiv.org/abs/2406.19884v1","updated":"2024-06-28T12:49:27Z","published":"2024-06-28T12:49:27Z","title":"Investigating the Timescales of Language Processing with EEG and\n  Language Models","summary":"  This study explores the temporal dynamics of language processing by examining\nthe alignment between word representations from a pre-trained transformer-based\nlanguage model, and EEG data. Using a Temporal Response Function (TRF) model,\nwe investigate how neural activity corresponds to model representations across\ndifferent layers, revealing insights into the interaction between artificial\nlanguage models and brain responses during language comprehension. Our analysis\nreveals patterns in TRFs from distinct layers, highlighting varying\ncontributions to lexical and compositional processing. Additionally, we used\nlinear discriminant analysis (LDA) to isolate part-of-speech (POS)\nrepresentations, offering insights into their influence on neural responses and\nthe underlying mechanisms of syntactic processing. These findings underscore\nEEG's utility for probing language processing dynamics with high temporal\nresolution. By bridging artificial language models and neural activity, this\nstudy advances our understanding of their interaction at fine timescales.\n","authors":["Davide Turco","Conor Houghton"],"pdf_url":"https://arxiv.org/pdf/2406.19884v1.pdf","comment":"Accepted at the 2024 Conference on Cognitive Computational\n  Neuroscience (CCN 2024)"},{"id":"http://arxiv.org/abs/2406.19874v1","updated":"2024-06-28T12:28:52Z","published":"2024-06-28T12:28:52Z","title":"Detecting Subtle Differences between Human and Model Languages Using\n  Spectrum of Relative Likelihood","summary":"  Human and model-generated texts can be distinguished by examining the\nmagnitude of likelihood in language. However, it is becoming increasingly\ndifficult as language model's capabilities of generating human-like texts keep\nevolving. This study provides a new perspective by using the relative\nlikelihood values instead of absolute ones, and extracting useful features from\nthe spectrum-view of likelihood for the human-model text detection task. We\npropose a detection procedure with two classification methods, supervised and\nheuristic-based, respectively, which results in competitive performances with\nprevious zero-shot detection methods and a new state-of-the-art on short-text\ndetection. Our method can also reveal subtle differences between human and\nmodel languages, which find theoretical roots in psycholinguistics studies. Our\ncode is available at https://github.com/CLCS-SUSTech/FourierGPT\n","authors":["Yang Xu","Yu Wang","Hao An","Zhichen Liu","Yongyuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.19874v1.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2305.14493v4","updated":"2024-06-28T12:04:53Z","published":"2023-05-23T19:45:45Z","title":"Do prompt positions really matter?","summary":"  Prompt-based models have gathered a lot of attention from researchers due to\ntheir remarkable advancements in the fields of zero-shot and few-shot learning.\nDeveloping an effective prompt template plays a critical role. However, prior\nstudies have mainly focused on prompt vocabulary searching or embedding\ninitialization within a predefined template with the prompt position fixed. In\nthis empirical study, we conduct the most comprehensive analysis to date of\nprompt position for diverse Natural Language Processing (NLP) tasks. Our\nfindings quantify the substantial impact prompt position has on model\nperformance. We observe that the prompt positions used in prior studies are\noften sub-optimal, and this observation is consistent even in widely used\ninstruction-tuned models. These findings suggest prompt position optimisation\nas a valuable research direction to augment prompt engineering methodologies\nand prompt position-aware instruction tuning as a potential way to build more\nrobust models in the future.\n","authors":["Junyu Mao","Stuart E. Middleton","Mahesan Niranjan"],"pdf_url":"https://arxiv.org/pdf/2305.14493v4.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.19853v1","updated":"2024-06-28T11:52:53Z","published":"2024-06-28T11:52:53Z","title":"YuLan: An Open-source Large Language Model","summary":"  Large language models (LLMs) have become the foundation of many applications,\nleveraging their extensive capabilities in processing and understanding natural\nlanguage. While many open-source LLMs have been released with technical\nreports, the lack of training details hinders further research and development.\nThis paper presents the development of YuLan, a series of open-source LLMs with\n$12$ billion parameters. The base model of YuLan is pre-trained on\napproximately $1.7$T tokens derived from a diverse corpus, including massive\nEnglish, Chinese, and multilingual texts. We design a three-stage pre-training\nmethod to enhance YuLan's overall capabilities. Subsequent phases of training\nincorporate instruction-tuning and human alignment, employing a substantial\nvolume of high-quality synthesized data. To facilitate the learning of complex\nand long-tail knowledge, we devise a curriculum-learning framework throughout\nacross these stages, which helps LLMs learn knowledge in an easy-to-hard\nmanner. YuLan's training is finished on Jan, 2024 and has achieved performance\non par with state-of-the-art LLMs across various English and Chinese\nbenchmarks. This paper outlines a comprehensive technical roadmap for\ndeveloping LLMs from scratch. Our model and codes are available at\nhttps://github.com/RUC-GSAI/YuLan-Chat.\n","authors":["Yutao Zhu","Kun Zhou","Kelong Mao","Wentong Chen","Yiding Sun","Zhipeng Chen","Qian Cao","Yihan Wu","Yushuo Chen","Feng Wang","Lei Zhang","Junyi Li","Xiaolei Wang","Lei Wang","Beichen Zhang","Zican Dong","Xiaoxue Cheng","Yuhan Chen","Xinyu Tang","Yupeng Hou","Qiangqiang Ren","Xincheng Pang","Shufang Xie","Wayne Xin Zhao","Zhicheng Dou","Jiaxin Mao","Yankai Lin","Ruihua Song","Jun Xu","Xu Chen","Rui Yan","Zhewei Wei","Di Hu","Wenbing Huang","Ze-Feng Gao","Yueguo Chen","Weizheng Lu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.19853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19840v1","updated":"2024-06-28T11:28:44Z","published":"2024-06-28T11:28:44Z","title":"AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through\n  low-confidence single-token predictions","summary":"  This paper introduces AnomaLLMy, a novel technique for the automatic\ndetection of anomalous tokens in black-box Large Language Models (LLMs) with\nAPI-only access. Utilizing low-confidence single-token predictions as a\ncost-effective indicator, AnomaLLMy identifies irregularities in model\nbehavior, addressing the issue of anomalous tokens degrading the quality and\nreliability of models. Validated on the cl100k_base dataset, the token set of\nGPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the\nmethod's efficiency with just \\$24.39 spent in API credits. The insights from\nthis research are expected to be beneficial for enhancing the robustness of and\naccuracy of LLMs, particularly in the development and assessment of tokenizers.\n","authors":["Waligóra Witold"],"pdf_url":"https://arxiv.org/pdf/2406.19840v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2406.18313v2","updated":"2024-06-28T11:04:19Z","published":"2024-06-26T12:54:19Z","title":"Advancing Airport Tower Command Recognition: Integrating\n  Squeeze-and-Excitation and Broadcasted Residual Learning","summary":"  Accurate recognition of aviation commands is vital for flight safety and\nefficiency, as pilots must follow air traffic control instructions precisely.\nThis paper addresses challenges in speech command recognition, such as noisy\nenvironments and limited computational resources, by advancing keyword spotting\ntechnology. We create a dataset of standardized airport tower commands,\nincluding routine and emergency instructions. We enhance broadcasted residual\nlearning with squeeze-and-excitation and time-frame frequency-wise\nsqueeze-and-excitation techniques, resulting in our BC-SENet model. This model\nfocuses on crucial information with fewer parameters. Our tests on five keyword\nspotting models, including BC-SENet, demonstrate superior accuracy and\nefficiency. These findings highlight the effectiveness of our model\nadvancements in improving speech command recognition for aviation safety and\nefficiency in noisy, high-stakes environments. Additionally, BC-SENet shows\ncomparable performance on the common Google Speech Command dataset.\n","authors":["Yuanxi Lin","Tonglin Zhou","Yang Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.18313v2.pdf","comment":"Accepted by IALP 2024"},{"id":"http://arxiv.org/abs/2406.19820v1","updated":"2024-06-28T10:53:48Z","published":"2024-06-28T10:53:48Z","title":"BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for\n  Multi-hop Question Answering","summary":"  Large language models (LLMs) have demonstrated strong reasoning capabilities.\nNevertheless, they still suffer from factual errors when tackling\nknowledge-intensive tasks. Retrieval-augmented reasoning represents a promising\napproach. However, significant challenges still persist, including inaccurate\nand insufficient retrieval for complex questions, as well as difficulty in\nintegrating multi-source knowledge. To address this, we propose Beam\nAggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive\nmulti-hop QA. BeamAggR explores and prioritizes promising answers at each hop\nof question. Concretely, we parse the complex questions into trees, which\ninclude atom and composite questions, followed by bottom-up reasoning. For\natomic questions, the LLM conducts reasoning on multi-source knowledge to get\nanswer candidates. For composite questions, the LLM combines beam candidates,\nexplores multiple reasoning paths through probabilistic aggregation, and\nprioritizes the most promising trajectory. Extensive experiments on four\nopen-domain multi-hop reasoning datasets show that our method significantly\noutperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that\nBeamAggR elicits better knowledge collaboration and answer aggregation.\n","authors":["Zheng Chu","Jingchang Chen","Qianglong Chen","Haotian Wang","Kun Zhu","Xiyuan Du","Weijiang Yu","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.19820v1.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.19232v2","updated":"2024-06-28T10:43:25Z","published":"2024-06-27T14:55:19Z","title":"RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs","summary":"  Minimal pairs are a well-established approach to evaluating the grammatical\nknowledge of language models. However, existing resources for minimal pairs\naddress a limited number of languages and lack diversity of language-specific\ngrammatical phenomena. This paper introduces the Russian Benchmark of\nLinguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that\ndiffer in grammaticality and isolate a morphological, syntactic, or semantic\nphenomenon. In contrast to existing benchmarks of linguistic minimal pairs,\nRuBLiMP is created by applying linguistic perturbations to automatically\nannotated sentences from open text corpora and carefully curating test data. We\ndescribe the data collection protocol and present the results of evaluating 25\nlanguage models in various scenarios. We find that the widely used language\nmodels for Russian are sensitive to morphological and agreement-oriented\ncontrasts but fall behind humans on phenomena requiring understanding of\nstructural relations, negation, transitivity, and tense. RuBLiMP, the codebase,\nand other materials are publicly available.\n","authors":["Ekaterina Taktasheva","Maxim Bazhukov","Kirill Koncha","Alena Fenogenova","Ekaterina Artemova","Vladislav Mikhailov"],"pdf_url":"https://arxiv.org/pdf/2406.19232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17667v2","updated":"2024-06-28T10:40:26Z","published":"2023-11-29T14:30:16Z","title":"TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in\n  Large Language Models","summary":"  Grasping the concept of time is a fundamental facet of human cognition,\nindispensable for truly comprehending the intricacies of the world. Previous\nstudies typically focus on specific aspects of time, lacking a comprehensive\ntemporal reasoning benchmark. To address this, we propose TimeBench, a\ncomprehensive hierarchical temporal reasoning benchmark that covers a broad\nspectrum of temporal reasoning phenomena. TimeBench provides a thorough\nevaluation for investigating the temporal reasoning capabilities of large\nlanguage models. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our experimental results indicate a\nsignificant performance gap between the state-of-the-art LLMs and humans,\nhighlighting that there is still a considerable distance to cover in temporal\nreasoning. Besides, LLMs exhibit capability discrepancies across different\nreasoning categories. Furthermore, we thoroughly analyze the impact of multiple\naspects on temporal reasoning and emphasize the associated challenges. We\naspire for TimeBench to serve as a comprehensive benchmark, fostering research\nin temporal reasoning. Resources are available at:\nhttps://github.com/zchuz/TimeBench\n","authors":["Zheng Chu","Jingchang Chen","Qianglong Chen","Weijiang Yu","Haotian Wang","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2311.17667v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2402.12368v2","updated":"2024-06-28T10:36:27Z","published":"2024-02-19T18:55:16Z","title":"A synthetic data approach for domain generalization of NLI models","summary":"  Natural Language Inference (NLI) remains an important benchmark task for\nLLMs. NLI datasets are a springboard for transfer learning to other semantic\ntasks, and NLI models are standard tools for identifying the faithfulness of\nmodel-generated text. There are several large scale NLI datasets today, and\nmodels have improved greatly by hill-climbing on these collections. Yet their\nrealistic performance on out-of-distribution/domain data is less\nwell-understood. We explore the opportunity for synthetic high-quality datasets\nto adapt NLI models for zero-shot use in downstream applications across new and\nunseen text domains. We demonstrate a new approach for generating NLI data in\ndiverse domains and lengths, so far not covered by existing training sets. The\nresulting examples have meaningful premises, the hypotheses are formed in\ncreative ways rather than simple edits to a few premise tokens, and the labels\nhave high accuracy. We show that models trained on this data ($685$K synthetic\nexamples) have the best generalization to completely new downstream test\nsettings. On the TRUE benchmark, a T5-small model trained with our data\nimproves around $7\\%$ on average compared to training on the best alternative\ndataset. The improvements are more pronounced for smaller models, while still\nmeaningful on a T5 XXL model. We also demonstrate gains on test sets when\nin-domain training data is augmented with our domain-general synthetic data.\n","authors":["Mohammad Javad Hosseini","Andrey Petrov","Alex Fabrikant","Annie Louis"],"pdf_url":"https://arxiv.org/pdf/2402.12368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15248v3","updated":"2024-06-28T10:27:11Z","published":"2024-02-23T10:27:42Z","title":"Chitchat as Interference: Adding User Backstories to Task-Oriented\n  Dialogues","summary":"  During task-oriented dialogues (TODs), human users naturally introduce\nchitchat that is beyond the immediate scope of the task, interfering with the\nflow of the conversation. To address this issue without the need for expensive\nmanual data creation, we use few-shot prompting with Llama-2-70B to enhance the\nMultiWOZ dataset with user backstories, a typical example of chitchat\ninterference in TODs. We assess the impact of this addition by testing two\nmodels: one trained solely on TODs and another trained on TODs with a\npreliminary chitchat interaction. Our analysis demonstrates that our enhanced\ndataset poses a challenge for these systems. Moreover, we demonstrate that our\ndataset can be effectively used for training purposes, enabling a system to\nconsistently acknowledge the user's backstory while also successfully moving\nthe task forward in the same turn, as confirmed by human evaluation. These\nfindings highlight the benefits of generating novel chitchat-TOD scenarios to\ntest TOD systems more thoroughly and improve their resilience to natural user\ninterferences\n","authors":["Armand Stricker","Patrick Paroubek"],"pdf_url":"https://arxiv.org/pdf/2402.15248v3.pdf","comment":"Accepted @ LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2306.01337v3","updated":"2024-06-28T10:26:27Z","published":"2023-06-02T08:02:15Z","title":"MathChat: Converse to Tackle Challenging Math Problems with LLM Agents","summary":"  Employing Large Language Models (LLMs) to address mathematical problems is an\nintriguing research endeavor, considering the abundance of math problems\nexpressed in natural language across numerous science and engineering fields.\nLLMs, with their generalized ability, are used as a foundation model to build\nAI agents for different tasks. In this paper, we study the effectiveness of\nutilizing LLM agents to solve math problems through conversations. We propose\nMathChat, a conversational problem-solving framework designed for math\nproblems. MathChat consists of an LLM agent and a user proxy agent which is\nresponsible for tool execution and additional guidance. This synergy\nfacilitates a collaborative problem-solving process, where the agents engage in\na dialogue to solve the problems. We perform evaluation on difficult high\nschool competition problems from the MATH dataset. Utilizing Python, we show\nthat MathChat can further improve previous tool-using prompting methods by 6%.\n","authors":["Yiran Wu","Feiran Jia","Shaokun Zhang","Hangyu Li","Erkang Zhu","Yue Wang","Yin Tat Lee","Richard Peng","Qingyun Wu","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01337v3.pdf","comment":"Update version"},{"id":"http://arxiv.org/abs/2406.19803v1","updated":"2024-06-28T10:24:31Z","published":"2024-06-28T10:24:31Z","title":"Scalable and Domain-General Abstractive Proposition Segmentation","summary":"  Segmenting text into fine-grained units of meaning is important to a wide\nrange of NLP applications. The default approach of segmenting text into\nsentences is often insufficient, especially since sentences are usually complex\nenough to include multiple units of meaning that merit separate treatment in\nthe downstream task. We focus on the task of abstractive proposition\nsegmentation: transforming text into simple, self-contained, well-formed\nsentences. Several recent works have demonstrated the utility of proposition\nsegmentation with few-shot prompted LLMs for downstream tasks such as\nretrieval-augmented grounding and fact verification. However, this approach\ndoes not scale to large amounts of text and may not always extract all the\nfacts from the input text. In this paper, we first introduce evaluation metrics\nfor the task to measure several dimensions of quality. We then propose a\nscalable, yet accurate, proposition segmentation model. We model proposition\nsegmentation as a supervised task by training LLMs on existing annotated\ndatasets and show that training yields significantly improved results. We\nfurther show that by using the fine-tuned LLMs as teachers for annotating large\namounts of multi-domain synthetic distillation data, we can train smaller\nstudent models with results similar to the teacher LLMs. We then demonstrate\nthat our technique leads to effective domain generalization, by annotating data\nin two domains outside the original training data and evaluating on them.\nFinally, as a key contribution of the paper, we share an easy-to-use API for\nNLP practitioners to use.\n","authors":["Mohammad Javad Hosseini","Yang Gao","Tim Baumgärtner","Alex Fabrikant","Reinald Kim Amplayo"],"pdf_url":"https://arxiv.org/pdf/2406.19803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13789v3","updated":"2024-06-28T10:23:29Z","published":"2024-01-24T20:17:11Z","title":"A Unified Approach to Emotion Detection and Task-Oriented Dialogue\n  Modeling","summary":"  In current text-based task-oriented dialogue (TOD) systems, user emotion\ndetection (ED) is often overlooked or is typically treated as a separate and\nindependent task, requiring additional training. In contrast, our work\ndemonstrates that seamlessly unifying ED and TOD modeling brings about mutual\nbenefits, and is therefore an alternative to be considered. Our method consists\nin augmenting SimpleToD, an end-to-end TOD system, by extending belief state\ntracking to include ED, relying on a single language model. We evaluate our\napproach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ\nannotated with emotions. Our results reveal a general increase in performance\nfor ED and task results. Our findings also indicate that user emotions provide\nuseful contextual conditioning for system responses, and can be leveraged to\nfurther refine responses in terms of empathy.\n","authors":["Armand Stricker","Patrick Paroubek"],"pdf_url":"https://arxiv.org/pdf/2401.13789v3.pdf","comment":"Accepted @ IWSDS 2024"},{"id":"http://arxiv.org/abs/2406.16783v2","updated":"2024-06-28T10:14:53Z","published":"2024-06-24T16:45:13Z","title":"M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in\n  Large Language Models","summary":"  Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. While many effective IFT datasets have been\nintroduced recently, they predominantly focus on high-resource languages like\nEnglish. To better align LLMs across a broad spectrum of languages and tasks,\nwe propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,\nMulti-turn instruction finetuning dataset, called M2Lingual. It is constructed\nby first selecting a diverse set of seed examples and then utilizing the\nproposed Evol taxonomy to convert these seeds into complex and challenging\nmulti-turn instructions. We demonstrate the effectiveness of M2Lingual by\ntraining LLMs of varying sizes and showcasing the enhanced performance across a\ndiverse set of languages. We contribute the 2 step Evol taxonomy with the\nguided generation code: https://github.com/ServiceNow/M2Lingual, as well as the\nfirst fully synthetic, general and task-oriented, multi-turn, multilingual\ndataset built with Evol - M2Lingual:\nhttps://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K\ntotal IFT pairs, covering 70 languages and 17+ NLP tasks.\n","authors":["Rishabh Maheshwary","Vikas Yadav","Hoang Nguyen","Khyati Mahajan","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2406.16783v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2402.03216v4","updated":"2024-06-28T09:55:49Z","published":"2024-02-05T17:26:49Z","title":"BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity\n  Text Embeddings Through Self-Knowledge Distillation","summary":"  In this paper, we present a new embedding model, called M3-Embedding, which\nis distinguished for its versatility in Multi-Linguality, Multi-Functionality,\nand Multi-Granularity. It can support more than 100 working languages, leading\nto new state-of-the-art performances on multi-lingual and cross-lingual\nretrieval tasks. It can simultaneously perform the three common retrieval\nfunctionalities of embedding model: dense retrieval, multi-vector retrieval,\nand sparse retrieval, which provides a unified model foundation for real-world\nIR applications. It is able to process inputs of different granularities,\nspanning from short sentences to long documents of up to 8192 tokens. The\neffective training of M3-Embedding involves the following technical\ncontributions. We propose a novel self-knowledge distillation approach, where\nthe relevance scores from different retrieval functionalities can be integrated\nas the teacher signal to enhance the training quality. We also optimize the\nbatching strategy, enabling a large batch size and high training throughput to\nensure the discriminativeness of embeddings. To the best of our knowledge,\nM3-Embedding is the first embedding model which realizes such a strong\nversatility. The model and code will be publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n","authors":["Jianlv Chen","Shitao Xiao","Peitian Zhang","Kun Luo","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2402.03216v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19783v1","updated":"2024-06-28T09:39:33Z","published":"2024-06-28T09:39:33Z","title":"NLPerturbator: Studying the Robustness of Code LLMs to Natural Language\n  Variations","summary":"  Large language models (LLMs) achieve promising results in code generation\nbased on a given natural language description. They have been integrated into\nopen-source projects and commercial products to facilitate daily coding\nactivities. The natural language description in the prompt is crucial for LLMs\nto comprehend users' requirements. Prior studies uncover that LLMs are\nsensitive to the changes in the prompts, including slight changes that look\ninconspicuous. However, the natural language descriptions often vary in\nreal-world scenarios (e.g., different formats, grammar, and wording). Prior\nstudies on the robustness of LLMs are often based on random perturbations and\nsuch perturbations may not actually happen. In this paper, we conduct a\ncomprehensive study to investigate how are code LLMs robust to variations of\nnatural language description in real-world scenarios. We summarize 18\ncategories of perturbations of natural language and 3 combinations of\nco-occurred categories based on our literature review and an online survey with\npractitioners. We propose an automated framework, NLPerturbator, which can\nperform perturbations of each category given a set of prompts. Through a series\nof experiments on code generation using six code LLMs, we find that the\nperturbed prompts can decrease the performance of code generation by a\nconsiderable margin (e.g., up to 21.2%, and 4.8% to 6.1% on average). Our study\nhighlights the importance of enhancing the robustness of LLMs to real-world\nvariations in the prompts, as well as the essentiality of attentively\nconstructing the prompts.\n","authors":["Junkai Chen","Zhenhao Li","Xing Hu","Xin Xia"],"pdf_url":"https://arxiv.org/pdf/2406.19783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19774v1","updated":"2024-06-28T09:23:40Z","published":"2024-06-28T09:23:40Z","title":"Direct Preference Knowledge Distillation for Large Language Models","summary":"  In the field of large language models (LLMs), Knowledge Distillation (KD) is\na critical technique for transferring capabilities from teacher models to\nstudent models. However, existing KD methods face limitations and challenges in\ndistillation of LLMs, including efficiency and insufficient measurement\ncapabilities of traditional KL divergence. It is shown that LLMs can serve as\nan implicit reward function, which we define as a supplement to KL divergence.\nIn this work, we propose Direct Preference Knowledge Distillation (DPKD) for\nLLMs. DPKD utilizes distribution divergence to represent the preference loss\nand implicit reward function. We re-formulate KD of LLMs into two stages: first\noptimizing and objective consisting of implicit reward and reverse KL\ndivergence and then improving the preference probability of teacher outputs\nover student outputs. We conducted experiments and analysis on various datasets\nwith LLM parameters ranging from 120M to 13B and demonstrate the broad\napplicability and effectiveness of our DPKD approach. Meanwhile, we prove the\nvalue and effectiveness of the introduced implicit reward and output preference\nin KD through experiments and theoretical analysis. The DPKD method outperforms\nthe baseline method in both output response precision and exact match\npercentage. Code and data are available at https://aka.ms/dpkd.\n","authors":["Yixing Li","Yuxian Gu","Li Dong","Dequan Wang","Yu Cheng","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2406.19774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10552v3","updated":"2024-06-28T09:16:28Z","published":"2024-06-15T08:13:47Z","title":"Large Language Model Enhanced Clustering for News Event Detection","summary":"  The news landscape is continuously evolving, with an ever-increasing volume\nof information from around the world. Automated event detection within this\nvast data repository is essential for monitoring, identifying, and categorizing\nsignificant news occurrences across diverse platforms. This paper presents an\nevent detection framework that leverages Large Language Models (LLMs) combined\nwith clustering analysis to detect news events from the Global Database of\nEvents, Language, and Tone (GDELT). The framework enhances event clustering\nthrough both pre-event detection tasks (keyword extraction and text embedding)\nand post-event detection tasks (event summarization and topic labelling). We\nalso evaluate the impact of various textual embeddings on the quality of\nclustering outcomes, ensuring robust news categorization. Additionally, we\nintroduce a novel Cluster Stability Assessment Index (CSAI) to assess the\nvalidity and robustness of clustering results. CSAI utilizes multiple feature\nvectors to provide a new way of measuring clustering quality. Our experiments\nindicate that the use of LLM embedding in the event detection framework has\nsignificantly improved the results, demonstrating greater robustness in terms\nof CSAI scores. Moreover, post-event detection tasks generate meaningful\ninsights, facilitating effective interpretation of event clustering results.\nOverall, our experimental results indicate that the proposed framework offers\nvaluable insights and could enhance the accuracy in news analysis and\nreporting.\n","authors":["Adane Nega Tarekegn"],"pdf_url":"https://arxiv.org/pdf/2406.10552v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19764v1","updated":"2024-06-28T09:09:36Z","published":"2024-06-28T09:09:36Z","title":"Belief Revision: The Adaptability of Large Language Models Reasoning","summary":"  The capability to reason from text is crucial for real-world NLP\napplications. Real-world scenarios often involve incomplete or evolving data.\nIn response, individuals update their beliefs and understandings accordingly.\nHowever, most existing evaluations assume that language models (LMs) operate\nwith consistent information. We introduce Belief-R, a new dataset designed to\ntest LMs' belief revision ability when presented with new evidence. Inspired by\nhow humans suppress prior inferences, this task assesses LMs within the newly\nproposed delta reasoning ($\\Delta R$) framework. Belief-R features sequences of\npremises designed to simulate scenarios where additional information could\nnecessitate prior conclusions drawn by LMs. We evaluate $\\sim$30 LMs across\ndiverse prompting strategies and found that LMs generally struggle to\nappropriately revise their beliefs in response to new information. Further,\nmodels adept at updating often underperformed in scenarios without necessary\nupdates, highlighting a critical trade-off. These insights underscore the\nimportance of improving LMs' adaptiveness to changing information, a step\ntoward more reliable AI systems.\n","authors":["Bryan Wilie","Samuel Cahyawijaya","Etsuko Ishii","Junxian He","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2406.19764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19760v1","updated":"2024-06-28T08:59:45Z","published":"2024-06-28T08:59:45Z","title":"Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case\n  Reformulation","summary":"  Legal case retrieval for sourcing similar cases is critical in upholding\njudicial fairness. Different from general web search, legal case retrieval\ninvolves processing lengthy, complex, and highly specialized legal documents.\nExisting methods in this domain often overlook the incorporation of legal\nexpert knowledge, which is crucial for accurately understanding and modeling\nlegal cases, leading to unsatisfactory retrieval performance. This paper\nintroduces KELLER, a legal knowledge-guided case reformulation approach based\non large language models (LLMs) for effective and interpretable legal case\nretrieval. By incorporating professional legal knowledge about crimes and law\narticles, we enable large language models to accurately reformulate the\noriginal legal case into concise sub-facts of crimes, which contain the\nessential information of the case. Extensive experiments on two legal case\nretrieval benchmarks demonstrate superior retrieval performance and robustness\non complex legal case queries of KELLER over existing methods.\n","authors":["Chenlong Deng","Kelong Mao","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2406.19760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19759v1","updated":"2024-06-28T08:59:24Z","published":"2024-06-28T08:59:24Z","title":"Breaking the Script Barrier in Multilingual Pre-Trained Language Models\n  with Transliteration-Based Post-Training Alignment","summary":"  Multilingual pre-trained models (mPLMs) have shown impressive performance on\ncross-lingual transfer tasks. However, the transfer performance is often\nhindered when a low-resource target language is written in a different script\nthan the high-resource source language, even though the two languages may be\nrelated or share parts of their vocabularies. Inspired by recent work that uses\ntransliteration to address this problem, our paper proposes a\ntransliteration-based post-pretraining alignment (PPA) method aiming to improve\nthe cross-lingual alignment between languages using diverse scripts. We select\ntwo areal language groups, $\\textbf{Mediterranean-Amharic-Farsi}$ and\n$\\textbf{South+East Asian Languages}$, wherein the languages are mutually\ninfluenced but use different scripts. We apply our method to these language\ngroups and conduct extensive experiments on a spectrum of downstream tasks. The\nresults show that after PPA, models consistently outperform the original model\n(up to 50% for some tasks) in English-centric transfer. In addition, when we\nuse languages other than English as sources in transfer, our method obtains\neven larger improvements. We will make our code and models publicly available\nat \\url{https://github.com/cisnlp/Transliteration-PPA}.\n","authors":["Orgest Xhelili","Yihong Liu","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2406.19759v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2406.15486v2","updated":"2024-06-28T08:55:17Z","published":"2024-06-17T11:05:15Z","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM\n  Inference with Adaptive Structured Sparse Attention","summary":"  Large language models (LLMs) now support extremely long context windows, but\nthe quadratic complexity of vanilla attention results in significantly long\nTime-to-First-Token (TTFT) latency. Existing approaches to address this\ncomplexity require additional pretraining or finetuning, and often sacrifice\nmodel accuracy. In this paper, we first provide both theoretical and empirical\nfoundations for near-lossless sparse attention. We find dynamically capturing\nhead-specific sparse patterns at runtime with low overhead is crucial. To\naddress this, we propose SampleAttention, an adaptive structured and\nnear-lossless sparse attention. Leveraging observed significant sparse\npatterns, SampleAttention attends to a fixed percentage of adjacent tokens to\ncapture local window patterns, and employs a two-stage query-guided key-value\nfiltering approach, which adaptively select a minimum set of key-values with\nlow overhead, to capture column stripe patterns. Comprehensive evaluations show\nthat SampleAttention can seamlessly replace vanilla attention in off-the-shelf\nLLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$\ncompared with FlashAttention.\n","authors":["Qianchao Zhu","Jiangfei Duan","Chang Chen","Siran Liu","Xiuhong Li","Guanyu Feng","Xin Lv","Huanqi Cao","Xiao Chuanfu","Xingcheng Zhang","Dahua Lin","Chao Yang"],"pdf_url":"https://arxiv.org/pdf/2406.15486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19736v1","updated":"2024-06-28T08:25:27Z","published":"2024-06-28T08:25:27Z","title":"MM-Instruct: Generated Visual Instructions for Large Multimodal Model\n  Alignment","summary":"  This paper introduces MM-Instruct, a large-scale dataset of diverse and\nhigh-quality visual instruction data designed to enhance the\ninstruction-following capabilities of large multimodal models (LMMs). While\nexisting visual instruction datasets often focus on question-answering, they\nstruggle to generalize to broader application scenarios such as creative\nwriting, summarization, or image analysis. To address these limitations, we\npropose a novel approach to constructing MM-Instruct that leverages the strong\ninstruction-following capabilities of existing LLMs to generate novel visual\ninstruction data from large-scale but conventional image captioning datasets.\nMM-Instruct first leverages ChatGPT to automatically generate diverse\ninstructions from a small set of seed instructions through augmenting and\nsummarization. It then matches these instructions with images and uses an\nopen-sourced large language model (LLM) to generate coherent answers to the\ninstruction-image pairs. The LLM is grounded by the detailed text descriptions\nof images in the whole answer generation process to guarantee the alignment of\nthe instruction data. Moreover, we introduce a benchmark based on the generated\ninstruction data to evaluate the instruction-following capabilities of existing\nLMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5\nmodel on the generated data, denoted as LLaVA-Instruct, which exhibits\nsignificant improvements in instruction-following capabilities compared to\nLLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models\nare available at https://github.com/jihaonew/MM-Instruct.\n","authors":["Jihao Liu","Xin Huang","Jinliang Zheng","Boxiao Liu","Jia Wang","Osamu Yoshie","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19736v1.pdf","comment":"Dataset and models are available at\n  https://github.com/jihaonew/MM-Instruct"},{"id":"http://arxiv.org/abs/2307.10635v3","updated":"2024-06-28T08:24:13Z","published":"2023-07-20T07:01:57Z","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities\n  of Large Language Models","summary":"  Most of the existing Large Language Model (LLM) benchmarks on scientific\nproblem reasoning focus on problems grounded in high-school subjects and are\nconfined to elementary algebraic operations. To systematically examine the\nreasoning capabilities required for solving complex scientific problems, we\nintroduce an expansive benchmark suite SciBench for LLMs. SciBench contains a\ncarefully curated dataset featuring a range of collegiate-level scientific\nproblems from mathematics, chemistry, and physics domains. Based on the\ndataset, we conduct an in-depth benchmarking study of representative\nopen-source and proprietary LLMs with various prompting strategies. The results\nreveal that the current LLMs fall short of delivering satisfactory performance,\nwith the best overall score of merely 43.22%. Furthermore, through a detailed\nuser study, we categorize the errors made by LLMs into ten problem-solving\nabilities. Our analysis indicates that no single prompting strategy\nsignificantly outperforms the others and some strategies that demonstrate\nimprovements in certain problem-solving skills could result in declines in\nother skills. We envision that SciBench will catalyze further developments in\nthe reasoning abilities of LLMs, thereby ultimately contributing to scientific\nresearch and discovery.\n","authors":["Xiaoxuan Wang","Ziniu Hu","Pan Lu","Yanqiao Zhu","Jieyu Zhang","Satyen Subramaniam","Arjun R. Loomba","Shichang Zhang","Yizhou Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2307.10635v3.pdf","comment":"To appear at ICML 2024"},{"id":"http://arxiv.org/abs/2402.08114v2","updated":"2024-06-28T08:22:01Z","published":"2024-02-12T23:09:00Z","title":"Active Preference Learning for Large Language Models","summary":"  As large language models (LLMs) become more capable, fine-tuning techniques\nfor aligning with human intent are increasingly important. A key consideration\nfor aligning these models is how to most effectively use human resources, or\nmodel resources in the case where LLMs themselves are used as oracles.\nReinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most\nprominent example of such a technique, but is complex and often unstable.\nDirect Preference Optimization (DPO) has recently been proposed as a simpler\nand more stable alternative. In this work, we develop an active learning\nstrategy for DPO to make better use of preference labels. We propose a\npractical acquisition function for prompt/completion pairs based on the\npredictive entropy of the language model and a measure of certainty of the\nimplicit preference model optimized by DPO. We demonstrate how our approach\nimproves both the rate of learning and final performance of fine-tuning on\npairwise preference data.\n","authors":["William Muldrew","Peter Hayes","Mingtian Zhang","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.08114v2.pdf","comment":"13 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.19731v1","updated":"2024-06-28T08:20:32Z","published":"2024-06-28T08:20:32Z","title":"Message du troisi{è}me type : irruption d'un tiers dans un dialogue en\n  ligne","summary":"  Our study focuses on Wikipedia talk pages, from a global perspective\nanalyzing contributors' behaviors in online interactions. Using a corpus\ncomprising all Wikipedia talk pages in French, totaling more than 300,000\ndiscussion threads, we examine how discussions with more than two participants\n(multiparty conversation) unfold and we specifically investigate the role of a\nthird participant's intervention when two Wikipedians have already initiated an\nexchange. In this regard, we concentrate on the sequential structure of these\ninteractions in terms of articulation among different participants and aim to\nspecify this third message by exploring its lexical particularities, while also\nproposing an initial typology of the third participant's message role and how\nit aligns with preceding messages.\n","authors":["Ludovic Tanguy","Céline Poudat","Lydia-Mai Ho-Dac"],"pdf_url":"https://arxiv.org/pdf/2406.19731v1.pdf","comment":"in French language. JADT 2024 - 17es Journ{\\'e}es internationales\n  d'Analyse statistique des Donn{\\'e}es Textuelles, SeSLa (S{\\'e}minaire des\n  Sciences du Langage de l'UCLouvain -- Site Saint-Louis); LASLA (Laboratoire\n  d'Analyse statistique des Langues anciennes de l'Universit{\\'e} de\n  Li{\\`e}ge), 2024, Bruxelles, Belgique"},{"id":"http://arxiv.org/abs/2406.19729v1","updated":"2024-06-28T08:19:36Z","published":"2024-06-28T08:19:36Z","title":"Le sens de la famille : analyse du vocabulaire de la parent{é} par les\n  plongements de mots","summary":"  In this study, we propose a corpus analysis of an area of the French lexicon\nthat is both dense and highly structured: the vocabulary of family\nrelationships. Starting with a lexicon of 25 nouns designating the main\nrelationships (son, cousin, mother, grandfather, sister-in-law etc.), we\nexamine how these terms are positioned in relation to each other through\ndistributional analyses based on the use of these terms in corpora. We show\nthat distributional information can capture certain features that organize this\nvocabulary (descent, alliance, siblings, genre), in ways that vary according to\nthe different corpora compared.\n","authors":["Ludovic Tanguy","Cécile Fabre","Nabil Hathout","Lydia-Mai Ho-Dac"],"pdf_url":"https://arxiv.org/pdf/2406.19729v1.pdf","comment":"in French language. JADT 2024 - 17es Journ{\\'e}es internationales\n  d'Analyse statistique des Donn{\\'e}es Textuelles, SeSLa (S{\\'e}minaire des\n  Sciences du Langage de l'UCLouvain -- Site Saint-Louis), 2024, Bruxelles,\n  Belgique"},{"id":"http://arxiv.org/abs/2406.18966v2","updated":"2024-06-28T08:12:28Z","published":"2024-06-27T07:56:44Z","title":"UniGen: A Unified Framework for Textual Dataset Generation Using Large\n  Language Models","summary":"  Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly\nimpacted various fields by enabling high-quality synthetic data generation and\nreducing dependence on expensive human-generated datasets. Despite this,\nchallenges remain in the areas of generalization, controllability, diversity,\nand truthfulness within the existing generative frameworks. To address these\nchallenges, this paper presents UniGen, a comprehensive LLM-powered framework\ndesigned to produce diverse, accurate, and highly controllable datasets. UniGen\nis adaptable, supporting all types of text datasets and enhancing the\ngenerative process through innovative mechanisms. To augment data diversity,\nUniGen incorporates an attribute-guided generation module and a group checking\nfeature. For accuracy, it employs a code-based mathematical assessment for\nlabel verification alongside a retrieval-augmented generation technique for\nfactual validation. The framework also allows for user-specified constraints,\nenabling customization of the data generation process to suit particular\nrequirements. Extensive experiments demonstrate the superior quality of data\ngenerated by UniGen, and each module within UniGen plays a critical role in\nthis enhancement. Additionally, UniGen is applied in two practical scenarios:\nbenchmarking LLMs and data augmentation. The results indicate that UniGen\neffectively supports dynamic and evolving benchmarking, and that data\naugmentation improves LLM capabilities in various domains, including\nagent-oriented abilities and reasoning skills.\n","authors":["Siyuan Wu","Yue Huang","Chujie Gao","Dongping Chen","Qihui Zhang","Yao Wan","Tianyi Zhou","Xiangliang Zhang","Jianfeng Gao","Chaowei Xiao","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18966v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09703v2","updated":"2024-06-28T08:03:19Z","published":"2024-03-08T19:07:47Z","title":"Concept-aware Data Construction Improves In-context Learning of Language\n  Models","summary":"  Many recent language models (LMs) are capable of in-context learning (ICL),\nmanifested in the LMs' ability to perform a new task solely from\nnatural-language instruction. Previous work curating in-context learners\nassumes that ICL emerges from a vast over-parametrization or the scale of\nmulti-task training. However, recent theoretical work attributes the ICL\nability to concept-dependent training data and creates functional in-context\nlearners even in small-scale, synthetic settings.\n  In this work, we practically explore this newly identified axis of ICL\nquality. We propose Concept-aware Training (CoAT), a framework for constructing\ntraining scenarios that make it beneficial for the LM to learn to utilize the\nanalogical reasoning concepts from demonstrations. We find that by using CoAT,\npre-trained transformers can learn to better utilise new latent concepts from\ndemonstrations and that such ability makes ICL more robust to the functional\ndeficiencies of the previous models. Finally, we show that concept-aware\nin-context learning is more effective for a majority of new tasks when compared\nto traditional instruction tuning, resulting in a performance comparable to the\nprevious in-context learners using magnitudes of more training data.\n","authors":["Michal Štefánik","Marek Kadlčík","Petr Sojka"],"pdf_url":"https://arxiv.org/pdf/2403.09703v2.pdf","comment":"Long paper to appear in Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2406.01124v3","updated":"2024-06-28T07:54:19Z","published":"2024-06-03T09:10:42Z","title":"Latent Logic Tree Extraction for Event Sequence Explanation from LLMs","summary":"  Modern high-stakes systems, such as healthcare or robotics, often generate\nvast streaming event sequences. Our goal is to design an efficient,\nplug-and-play tool to elicit logic tree-based explanations from Large Language\nModels (LLMs) to provide customized insights into each observed event sequence.\nBuilt on the temporal point process model for events, our method employs the\nlikelihood function as a score to evaluate generated logic trees. We propose an\namortized Expectation-Maximization (EM) learning framework and treat the logic\ntree as latent variables. In the E-step, we evaluate the posterior distribution\nover the latent logic trees using an LLM prior and the likelihood of the\nobserved event sequences. LLM provides a high-quality prior for the latent\nlogic trees, however, since the posterior is built over a discrete\ncombinatorial space, we cannot get the closed-form solution. We propose to\ngenerate logic tree samples from the posterior using a learnable GFlowNet,\nwhich is a diversity-seeking generator for structured discrete variables. The\nM-step employs the generated logic rules to approximate marginalization over\nthe posterior, facilitating the learning of model parameters and refining the\ntunable LLM prior parameters. In the online setting, our locally built,\nlightweight model will iteratively extract the most relevant rules from LLMs\nfor each sequence using only a few iterations. Empirical demonstrations\nshowcase the promising performance and adaptability of our framework.\n","authors":["Zitao Song","Chao Yang","Chaojie Wang","Bo An","Shuang Li"],"pdf_url":"https://arxiv.org/pdf/2406.01124v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19712v1","updated":"2024-06-28T07:47:34Z","published":"2024-06-28T07:47:34Z","title":"Uncertainty Quantification in Large Language Models Through Convex Hull\n  Analysis","summary":"  Uncertainty quantification approaches have been more critical in large\nlanguage models (LLMs), particularly high-risk applications requiring reliable\noutputs. However, traditional methods for uncertainty quantification, such as\nprobabilistic models and ensemble techniques, face challenges when applied to\nthe complex and high-dimensional nature of LLM-generated outputs. This study\nproposes a novel geometric approach to uncertainty quantification using convex\nhull analysis. The proposed method leverages the spatial properties of response\nembeddings to measure the dispersion and variability of model outputs. The\nprompts are categorized into three types, i.e., `easy', `moderate', and\n`confusing', to generate multiple responses using different LLMs at varying\ntemperature settings. The responses are transformed into high-dimensional\nembeddings via a BERT model and subsequently projected into a two-dimensional\nspace using Principal Component Analysis (PCA). The Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm is utilized to cluster\nthe embeddings and compute the convex hull for each selected cluster. The\nexperimental results indicate that the uncertainty of the model for LLMs\ndepends on the prompt complexity, the model, and the temperature setting.\n","authors":["Ferhat Ozgur Catak","Murat Kuzlu"],"pdf_url":"https://arxiv.org/pdf/2406.19712v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2402.11622v2","updated":"2024-06-28T07:20:22Z","published":"2024-02-18T15:28:39Z","title":"Logical Closed Loop: Uncovering Object Hallucinations in Large\n  Vision-Language Models","summary":"  Object hallucination has been an Achilles' heel which hinders the broader\napplications of large vision-language models (LVLMs). Object hallucination\nrefers to the phenomenon that the LVLMs claim non-existent objects in the\nimage. To mitigate the object hallucinations, instruction tuning and external\nmodel-based detection methods have been proposed, which either require\nlarge-scare computational resources or depend on the detection result of\nexternal models. However, there remains an under-explored field to utilize the\nLVLM itself to alleviate object hallucinations. In this work, we adopt the\nintuition that the LVLM tends to respond logically consistently for existent\nobjects but inconsistently for hallucinated objects. Therefore, we propose a\nLogical Closed Loop-based framework for Object Hallucination Detection and\nMitigation, namely LogicCheckGPT. In specific, we devise logical consistency\nprobing to raise questions with logical correlations, inquiring about\nattributes from objects and vice versa. Whether their responses can form a\nlogical closed loop serves as an indicator of object hallucination. As a\nplug-and-play method, it can be seamlessly applied to all existing LVLMs.\nComprehensive experiments conducted on three benchmarks across four LVLMs have\ndemonstrated significant improvements brought by our method, indicating its\neffectiveness and generality.\n","authors":["Junfei Wu","Qiang Liu","Ding Wang","Jinghao Zhang","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2402.11622v2.pdf","comment":"Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2402.03848v6","updated":"2024-06-28T06:49:39Z","published":"2024-02-06T09:50:08Z","title":"ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models","summary":"  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, and more than 10 different GLLMs\ntogether with 3 different prompting methods using the ANLS* metric is also\nprovided, demonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In 6 out of 7 cases, SFT\noutperforms other techniques and improves the state-of-the-art, sometimes by as\nmuch as $10$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric\n","authors":["David Peer","Philemon Schöpf","Volckmar Nebendahl","Alexander Rietzler","Sebastian Stabinger"],"pdf_url":"https://arxiv.org/pdf/2402.03848v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17710v2","updated":"2024-06-28T06:24:37Z","published":"2024-05-28T00:00:04Z","title":"Does Geo-co-location Matter? A Case Study of Public Health Conversations\n  during COVID-19","summary":"  Social media platforms like Twitter (now X) have been pivotal in information\ndissemination and public engagement, especially during COVID-19. A key goal for\npublic health experts was to encourage prosocial behavior that could impact\nlocal outcomes such as masking and social distancing. Given the importance of\nlocal news and guidance during COVID-19, the objective of our research is to\nanalyze the effect of localized engagement, on social media conversations. This\nstudy examines the impact of geographic co-location, as a proxy for localized\nengagement between public health experts (PHEs) and the public, on social\nmedia. We analyze a Twitter conversation dataset from January 2020 to November\n2021, comprising over 19 K tweets from nearly five hundred PHEs, along with\napproximately 800 K replies from 350 K participants. Our findings reveal that\ngeo-co-location is associated with higher engagement rates, especially in\nconversations on topics including masking, lockdowns, and education, and in\nconversations with academic and medical professionals. Lexical features\nassociated with emotion and personal experiences were more common in\ngeo-co-located contexts. This research provides insights into how geographic\nco-location influences social media engagement and can inform strategies to\nimprove public health messaging.\n","authors":["Paiheng Xu","Louiqa Raschid","Vanessa Frias-Martinez"],"pdf_url":"https://arxiv.org/pdf/2405.17710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19674v1","updated":"2024-06-28T06:22:23Z","published":"2024-06-28T06:22:23Z","title":"Less is More: Accurate Speech Recognition & Translation without\n  Web-Scale Data","summary":"  Recent advances in speech recognition and translation rely on hundreds of\nthousands of hours of Internet speech data. We argue that state-of-the art\naccuracy can be reached without relying on web-scale data. Canary -\nmultilingual ASR and speech translation model, outperforms current\nstate-of-the-art models - Whisper, OWSM, and Seamless-M4T on English, French,\nSpanish, and German languages, while being trained on an order of magnitude\nless data than these models. Three key factors enables such data-efficient\nmodel: (1) a FastConformer-based attention encoder-decoder architecture (2)\ntraining on synthetic data generated with machine translation and (3) advanced\ntraining techniques: data-balancing, dynamic data blending, dynamic bucketing\nand noise-robust fine-tuning. The model, weights, and training code will be\nopen-sourced.\n","authors":["Krishna C. Puvvada","Piotr Żelasko","He Huang","Oleksii Hrinchuk","Nithin Rao Koluguri","Kunal Dhawan","Somshubra Majumdar","Elena Rastorgueva","Zhehuai Chen","Vitaly Lavrukhin","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2406.19674v1.pdf","comment":"Accepted at Interspeech-2024"},{"id":"http://arxiv.org/abs/2403.03640v3","updated":"2024-06-28T06:16:24Z","published":"2024-03-06T11:56:02Z","title":"Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People","summary":"  Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.\n","authors":["Xidong Wang","Nuo Chen","Junyin Chen","Yan Hu","Yidong Wang","Xiangbo Wu","Anningzhe Gao","Xiang Wan","Haizhou Li","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2403.03640v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.18118v2","updated":"2024-06-28T06:06:59Z","published":"2024-06-26T07:15:44Z","title":"SafeAligner: Safety Alignment against Jailbreak Attacks via Response\n  Disparity Guidance","summary":"  As the development of large language models (LLMs) rapidly advances, securing\nthese models effectively without compromising their utility has become a\npivotal area of research. However, current defense strategies against jailbreak\nattacks (i.e., efforts to bypass security protocols) often suffer from limited\nadaptability, restricted general capability, and high cost. To address these\nchallenges, we introduce SafeAligner, a methodology implemented at the decoding\nstage to fortify defenses against jailbreak attacks. We begin by developing two\nspecialized models: the Sentinel Model, which is trained to foster safety, and\nthe Intruder Model, designed to generate riskier responses. SafeAligner\nleverages the disparity in security levels between the responses from these\nmodels to differentiate between harmful and beneficial tokens, effectively\nguiding the safety alignment by altering the output token distribution of the\ntarget model. Extensive experiments show that SafeAligner can increase the\nlikelihood of beneficial tokens, while reducing the occurrence of harmful ones,\nthereby ensuring secure alignment with minimal loss to generality.\n","authors":["Caishuang Huang","Wanxu Zhao","Rui Zheng","Huijie Lv","Shihan Dou","Sixian Li","Xiao Wang","Enyu Zhou","Junjie Ye","Yuming Yang","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.18118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19237v2","updated":"2024-06-28T05:43:46Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v2.pdf","comment":"Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables"},{"id":"http://arxiv.org/abs/2406.19650v1","updated":"2024-06-28T04:38:54Z","published":"2024-06-28T04:38:54Z","title":"DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark\n  for Incoherence Detection, Reasoning, and Rewriting","summary":"  Coherence in writing, an aspect that second-language (L2) English learners\noften struggle with, is crucial in assessing L2 English writing. Existing\nautomated writing evaluation systems primarily use basic surface linguistic\nfeatures to detect coherence in writing. However, little effort has been made\nto correct the detected incoherence, which could significantly benefit L2\nlanguage learners seeking to improve their writing. To bridge this gap, we\nintroduce DECOR, a novel benchmark that includes expert annotations for\ndetecting incoherence in L2 English writing, identifying the underlying\nreasons, and rewriting the incoherent sentences. To our knowledge, DECOR is the\nfirst coherence assessment dataset specifically designed for improving L2\nEnglish writing, featuring pairs of original incoherent sentences alongside\ntheir expert-rewritten counterparts. Additionally, we fine-tuned models to\nautomatically detect and rewrite incoherence in student essays. We find that\nincorporating specific reasons for incoherence during fine-tuning consistently\nimproves the quality of the rewrites, achieving a result that is favored in\nboth automatic and human evaluations.\n","authors":["Xuanming Zhang","Anthony Diaz","Zixun Chen","Qingyang Wu","Kun Qian","Erik Voss","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2406.19650v1.pdf","comment":"21 pages, 5 figures, 20 tables"},{"id":"http://arxiv.org/abs/2406.19648v1","updated":"2024-06-28T04:33:41Z","published":"2024-06-28T04:33:41Z","title":"Designing and Evaluating Multi-Chatbot Interface for Human-AI\n  Communication: Preliminary Findings from a Persuasion Task","summary":"  The dynamics of human-AI communication have been reshaped by language models\nsuch as ChatGPT. However, extant research has primarily focused on dyadic\ncommunication, leaving much to be explored regarding the dynamics of human-AI\ncommunication in group settings. The availability of multiple language model\nchatbots presents a unique opportunity for scholars to better understand the\ninteraction between humans and multiple chatbots. This study examines the\nimpact of multi-chatbot communication in a specific persuasion setting:\npromoting charitable donations. We developed an online environment that enables\nmulti-chatbot communication and conducted a pilot experiment utilizing two\nGPT-based chatbots, Save the Children and UNICEF chatbots, to promote\ncharitable donations. In this study, we present our development process of the\nmulti-chatbot interface and present preliminary findings from a pilot\nexperiment. Analysis of qualitative and quantitative feedback are presented,\nand limitations are addressed.\n","authors":["Sion Yoon","Tae Eun Kim","Yoo Jung Oh"],"pdf_url":"https://arxiv.org/pdf/2406.19648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19643v1","updated":"2024-06-28T04:21:20Z","published":"2024-06-28T04:21:20Z","title":"Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework\n  with Debate-Driven Text Planning for Argument Generation","summary":"  Writing persuasive arguments is a challenging task for both humans and\nmachines. It entails incorporating high-level beliefs from various perspectives\non the topic, along with deliberate reasoning and planning to construct a\ncoherent narrative. Current language models often generate surface tokens\nautoregressively, lacking explicit integration of these underlying controls,\nresulting in limited output diversity and coherence. In this work, we propose a\npersona-based multi-agent framework for argument writing. Inspired by the human\ndebate, we first assign each agent a persona representing its high-level\nbeliefs from a unique perspective, and then design an agent interaction process\nso that the agents can collaboratively debate and discuss the idea to form an\noverall plan for argument writing. Such debate process enables fluid and\nnonlinear development of ideas. We evaluate our framework on argumentative\nessay writing. The results show that our framework can generate more diverse\nand persuasive arguments through both automatic and human evaluations.\n","authors":["Zhe Hu","Hou Pong Chan","Jing Li","Yu Yin"],"pdf_url":"https://arxiv.org/pdf/2406.19643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19642v1","updated":"2024-06-28T04:14:35Z","published":"2024-06-28T04:14:35Z","title":"IDT: Dual-Task Adversarial Attacks for Privacy Protection","summary":"  Natural language processing (NLP) models may leak private information in\ndifferent ways, including membership inference, reconstruction or attribute\ninference attacks. Sensitive information may not be explicit in the text, but\nhidden in underlying writing characteristics. Methods to protect privacy can\ninvolve using representations inside models that are demonstrated not to detect\nsensitive attributes or -- for instance, in cases where users might not trust a\nmodel, the sort of scenario of interest here -- changing the raw text before\nmodels can have access to it. The goal is to rewrite text to prevent someone\nfrom inferring a sensitive attribute (e.g. the gender of the author, or their\nlocation by the writing style) whilst keeping the text useful for its original\nintention (e.g. the sentiment of a product review). The few works tackling this\nhave focused on generative techniques. However, these often create extensively\ndifferent texts from the original ones or face problems such as mode collapse.\nThis paper explores a novel adaptation of adversarial attack techniques to\nmanipulate a text to deceive a classifier w.r.t one task (privacy) whilst\nkeeping the predictions of another classifier trained for another task\n(utility) unchanged. We propose IDT, a method that analyses predictions made by\nauxiliary and interpretable models to identify which tokens are important to\nchange for the privacy task, and which ones should be kept for the utility\ntask. We evaluate different datasets for NLP suitable for different tasks.\nAutomatic and human evaluations show that IDT retains the utility of text,\nwhile also outperforming existing methods when deceiving a classifier w.r.t\nprivacy task.\n","authors":["Pedro Faustini","Shakila Mahjabin Tonni","Annabelle McIver","Qiongkai Xu","Mark Dras"],"pdf_url":"https://arxiv.org/pdf/2406.19642v1.pdf","comment":"28 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.12058v3","updated":"2024-06-28T04:08:12Z","published":"2024-06-17T19:50:40Z","title":"WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions","summary":"  Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WD). We focus on\ntwo mental health and well-being datasets: (a) Multi-label Classification-based\nMultiWD, and (b) WellXplain for evaluating attention mechanism veracity against\nexpert-labeled explanations. The labels are based on Halbert Dunn's theory of\nwellness, which gives grounding to our evaluation. We reveal four surprising\nresults about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4\nlag behind RoBERTa, and MedAlpaca, a fine-tuned LLM fails to deliver any\nremarkable improvements in performance or explanations. (2) Re-examining LMs'\npredictions based on a confidence-oriented loss function reveals a significant\nperformance drop. (3) Across all LMs/LLMs, the alignment between attention and\nexplanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental\nhealth-specific LMs/LLMs overlook domain-specific knowledge and undervalue\nexplanations, causing these discrepancies. This study highlights the need for\nfurther research into their consistency and explanations in mental health and\nwell-being.\n","authors":["Seyedali Mohammadi","Edward Raff","Jinendra Malekar","Vedant Palit","Francis Ferraro","Manas Gaur"],"pdf_url":"https://arxiv.org/pdf/2406.12058v3.pdf","comment":"26 pages, including reference and appendix sections, 8 figures, and\n  16 tables"},{"id":"http://arxiv.org/abs/2402.09742v4","updated":"2024-06-28T03:11:48Z","published":"2024-02-15T06:46:48Z","title":"AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical\n  Interaction Simulator","summary":"  Artificial intelligence has significantly advanced healthcare, particularly\nthrough large language models (LLMs) that excel in medical question answering\nbenchmarks. However, their real-world clinical application remains limited due\nto the complexities of doctor-patient interactions. To address this, we\nintroduce \\textbf{AI Hospital}, a multi-agent framework simulating dynamic\nmedical interactions between \\emph{Doctor} as player and NPCs including\n\\emph{Patient}, \\emph{Examiner}, \\emph{Chief Physician}. This setup allows for\nrealistic assessments of LLMs in clinical scenarios. We develop the Multi-View\nMedical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical\nrecords and NPCs to evaluate LLMs' performance in symptom collection,\nexamination recommendations, and diagnoses. Additionally, a dispute resolution\ncollaborative mechanism is proposed to enhance diagnostic accuracy through\niterative discussions. Despite improvements, current LLMs exhibit significant\nperformance gaps in multi-turn interactions compared to one-step approaches.\nOur findings highlight the need for further research to bridge these gaps and\nimprove LLMs' clinical diagnostic capabilities. Our data, code, and\nexperimental results are all open-sourced at\n\\url{https://github.com/LibertFan/AI_Hospital}.\n","authors":["Zhihao Fan","Jialong Tang","Wei Chen","Siyuan Wang","Zhongyu Wei","Jun Xi","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2402.09742v4.pdf","comment":"https://github.com/LibertFan/AI_Hospital"},{"id":"http://arxiv.org/abs/2406.18841v2","updated":"2024-06-28T02:56:09Z","published":"2024-05-14T15:03:05Z","title":"Navigating LLM Ethics: Advancements, Challenges, and Future Directions","summary":"  This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society.\n","authors":["Junfeng Jiao","Saleh Afroogh","Yiming Xu","Connor Phillips"],"pdf_url":"https://arxiv.org/pdf/2406.18841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18842v2","updated":"2024-06-28T02:54:06Z","published":"2024-05-26T15:28:24Z","title":"The global landscape of academic guidelines for generative AI and Large\n  Language Models","summary":"  The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia.\n","authors":["Junfeng Jiao","Saleh Afroogh","Kevin Chen","David Atkinson","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.18842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02990v3","updated":"2024-06-28T02:35:38Z","published":"2024-03-05T14:11:54Z","title":"Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and\n  Challenges","summary":"  In the rapidly evolving field of large language models (LLMs), data\naugmentation (DA) has emerged as a pivotal technique for enhancing model\nperformance by diversifying training examples without the need for additional\ndata collection. This survey explores the transformative impact of LLMs on DA,\nparticularly addressing the unique challenges and opportunities they present in\nthe context of natural language processing (NLP) and beyond. From both data and\nlearning perspectives, we examine various strategies that utilize LLMs for data\naugmentation, including a novel exploration of learning paradigms where\nLLM-generated data is used for diverse forms of further training. Additionally,\nthis paper highlights the primary open challenges faced in this domain, ranging\nfrom controllable data augmentation to multi-modal data augmentation. This\nsurvey highlights a paradigm shift introduced by LLMs in DA, and aims to serve\nas a comprehensive guide for researchers and practitioners.\n","authors":["Bosheng Ding","Chengwei Qin","Ruochen Zhao","Tianze Luo","Xinze Li","Guizhen Chen","Wenhan Xia","Junjie Hu","Anh Tuan Luu","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2403.02990v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10943v4","updated":"2024-06-28T02:05:59Z","published":"2024-03-16T15:14:15Z","title":"MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent\n  Recognition and Out-of-scope Detection in Conversations","summary":"  Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.\n","authors":["Hanlei Zhang","Xin Wang","Hua Xu","Qianrui Zhou","Kai Gao","Jianhua Su","jinyue Zhao","Wenrui Li","Yanting Chen"],"pdf_url":"https://arxiv.org/pdf/2403.10943v4.pdf","comment":"Accepted by ICLR 2024, Long Paper; The abstract is slightly modified\n  due to the length limitation"},{"id":"http://arxiv.org/abs/2406.19598v1","updated":"2024-06-28T01:46:41Z","published":"2024-06-28T01:46:41Z","title":"Mixture of In-Context Experts Enhance LLMs' Long Context Awareness","summary":"  Many studies have revealed that large language models (LLMs) exhibit uneven\nawareness of different contextual positions.Their limited context awareness can\nlead to overlooking critical information and subsequent task failures. While\nseveral approaches have been proposed to enhance LLMs' context awareness,\nachieving both effectiveness and efficiency remains challenging.In this paper,\nfor LLMs utilizing RoPE as position embeddings, we introduce a novel method\ncalled ``Mixture of In-Context Experts'' (MoICE) to address this challenge.\nMoICE comprises two key components: a router integrated into each attention\nhead within LLMs and a lightweight router-only training optimization strategy:\n(1) MoICE views each RoPE angle as an `in-context' expert, demonstrated to be\ncapable of directing the attention of a head to specific contextual positions.\nConsequently, each attention head flexibly processes tokens using multiple RoPE\nangles dynamically selected by the router to attend to the needed positions.\nThis approach mitigates the risk of overlooking essential contextual\ninformation. (2) The router-only training strategy entails freezing LLM\nparameters and exclusively updating routers for only a few steps. When applied\nto open-source LLMs including Llama and Mistral, MoICE surpasses prior methods\nacross multiple tasks on long context understanding and generation, all while\nmaintaining commendable inference efficiency.\n","authors":["Hongzhan Lin","Ang Lv","Yuhan Chen","Chen Zhu","Yang Song","Hengshu Zhu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19598v1.pdf","comment":"14 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.19593v1","updated":"2024-06-28T01:14:43Z","published":"2024-06-28T01:14:43Z","title":"SK-VQA: Synthetic Knowledge Generation at Scale for Training\n  Context-Augmented Multimodal LLMs","summary":"  Synthetic data generation has gained significant attention recently for its\nutility in training large vision and language models. However, the application\nof synthetic data to the training of multimodal context-augmented generation\nsystems has been relatively unexplored. This gap in existing work is important\nbecause existing vision and language models (VLMs) are not trained specifically\nfor context-augmented generation. Resources for adapting such models are\ntherefore crucial for enabling their use in retrieval-augmented generation\n(RAG) settings, where a retriever is used to gather relevant information that\nis then subsequently provided to a generative model via context augmentation.\nTo address this challenging problem, we generate SK-VQA: a large synthetic\nmultimodal dataset containing over 2 million question-answer pairs which\nrequire external knowledge to determine the final answer. Our dataset is both\nlarger and significantly more diverse than existing resources of its kind,\npossessing over 11x more unique questions and containing images from a greater\nvariety of sources than previously-proposed datasets. Through extensive\nexperiments, we demonstrate that our synthetic dataset can not only serve as a\nchallenging benchmark, but is also highly effective for adapting existing\ngenerative multimodal models for context-augmented generation.\n","authors":["Xin Su","Man Luo","Kris W Pan","Tien Pei Chou","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2406.19593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19350v5","updated":"2024-06-28T00:26:41Z","published":"2024-02-29T16:56:36Z","title":"Prompting Explicit and Implicit Knowledge for Multi-hop Question\n  Answering Based on Human Reading Process","summary":"  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to\nsimulate human reasoning and inference processes, achieving proficient\nperformance in multi-hop QA. However, a gap persists between PLMs' reasoning\nabilities and those of humans when tackling complex problems. Psychological\nstudies suggest a vital connection between explicit information in passages and\nhuman prior knowledge during reading. Nevertheless, current research has given\ninsufficient attention to linking input passages and PLMs' pre-training-based\nknowledge from the perspective of human cognition studies. In this study, we\nintroduce a Prompting Explicit and Implicit knowledge (PEI) framework, which\nuses prompts to connect explicit and implicit knowledge, aligning with human\nreading process for multi-hop QA. We consider the input passages as explicit\nknowledge, employing them to elicit implicit knowledge through unified prompt\nreasoning. Furthermore, our model incorporates type-specific reasoning via\nprompts, a form of implicit knowledge. Experimental results show that PEI\nperforms comparably to the state-of-the-art on HotpotQA. Ablation studies\nconfirm the efficacy of our model in bridging and integrating explicit and\nimplicit knowledge.\n","authors":["Guangming Huang","Yunfei Long","Cunjin Luo","Jiaxing Shen","Xia Sun"],"pdf_url":"https://arxiv.org/pdf/2402.19350v5.pdf","comment":"This paper has been accepted at COLING 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2311.09735v3","updated":"2024-06-28T17:59:26Z","published":"2023-11-16T10:06:09Z","title":"GEO: Generative Engine Optimization","summary":"  The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of generative engines (GEs), can generate accurate and\npersonalized responses, rapidly replacing traditional search engines like\nGoogle and Bing. Generative Engines typically satisfy queries by synthesizing\ninformation from multiple sources and summarizing them using LLMs. While this\nshift significantly improves $\\textit{user}$ utility and $\\textit{generative\nsearch engine}$ traffic, it poses a huge challenge for the third stakeholder --\nwebsite and content creators. Given the black-box and fast-moving nature of\ngenerative engines, content creators have little to no control over\n$\\textit{when}$ and $\\textit{how}$ their content is displayed. With generative\nengines here to stay, we must ensure the creator economy is not disadvantaged.\nTo address this, we introduce Generative Engine Optimization (GEO), the first\nnovel paradigm to aid content creators in improving their content visibility in\ngenerative engine responses through a flexible black-box optimization framework\nfor optimizing and defining visibility metrics. We facilitate systematic\nevaluation by introducing GEO-bench, a large-scale benchmark of diverse user\nqueries across multiple domains, along with relevant web sources to answer\nthese queries. Through rigorous evaluation, we demonstrate that GEO can boost\nvisibility by up to $40\\%$ in generative engine responses. Moreover, we show\nthe efficacy of these strategies varies across domains, underscoring the need\nfor domain-specific optimization methods. Our work opens a new frontier in\ninformation discovery systems, with profound implications for both developers\nof generative engines and content creators.\n","authors":["Pranjal Aggarwal","Vishvak Murahari","Tanmay Rajpurohit","Ashwin Kalyan","Karthik Narasimhan","Ameet Deshpande"],"pdf_url":"https://arxiv.org/pdf/2311.09735v3.pdf","comment":"Accepted to KDD 2024"},{"id":"http://arxiv.org/abs/2406.19928v1","updated":"2024-06-28T13:57:27Z","published":"2024-06-28T13:57:27Z","title":"Interactive Topic Models with Optimal Transport","summary":"  Topic models are widely used to analyze document collections. While they are\nvaluable for discovering latent topics in a corpus when analysts are unfamiliar\nwith the corpus, analysts also commonly start with an understanding of the\ncontent present in a corpus. This may be through categories obtained from an\ninitial pass over the corpus or a desire to analyze the corpus through a\npredefined set of categories derived from a high level theoretical framework\n(e.g. political ideology). In these scenarios analysts desire a topic modeling\napproach which incorporates their understanding of the corpus while supporting\nvarious forms of interaction with the model. In this work, we present EdTM, as\nan approach for label name supervised topic modeling. EdTM models topic\nmodeling as an assignment problem while leveraging LM/LLM based document-topic\naffinities and using optimal transport for making globally coherent\ntopic-assignments. In experiments, we show the efficacy of our framework\ncompared to few-shot LLM classifiers, and topic models based on clustering and\nLDA. Further, we show EdTM's ability to incorporate various forms of analyst\nfeedback and while remaining robust to noisy analyst inputs.\n","authors":["Garima Dhanania","Sheshera Mysore","Chau Minh Pham","Mohit Iyyer","Hamed Zamani","Andrew McCallum"],"pdf_url":"https://arxiv.org/pdf/2406.19928v1.pdf","comment":"Pre-print; Work in progress"},{"id":"http://arxiv.org/abs/2402.17887v4","updated":"2024-06-28T13:23:31Z","published":"2024-02-27T21:01:41Z","title":"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning\n  and Professional Question Answering Capability","summary":"  Large Language Models (LLMs) have demonstrated a remarkable potential in\nmedical knowledge acquisition and question-answering. However, LLMs can\npotentially hallucinate and yield factually incorrect outcomes, even with\ndomain-specific pretraining. Previously, retrieval augmented generation (RAG)\nhas limited success in addressing hallucinations. Unlike previous methods in\nRAG where the retrieval model was trained separately from the LLM, we introduce\nJMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning\nphase. The synchronized training mechanism enhances JMLR's ability to retrieve\nclinical guidelines and leverage medical knowledge to reason and answer\nquestions and reduces the demand for computational resources. We evaluated JMLR\non the important medical question-answering application. Our experimental\nresults demonstrate that JMLR-13B (70.5%) outperforms a previous\nstate-of-the-art open-source model using conventional pre-training and\nfine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical\nquestion-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances\nreasoning quality and reduces hallucinations better than Claude3-Opus.\nAdditionally, JMLR-13B (148 GPU hours) also trains much faster than\nMeditron-70B (42630 GPU hours). Through this work, we provide a new and\nefficient knowledge enhancement method for healthcare, demonstrating the\npotential of integrating retrieval and LLM training for medical\nquestion-answering systems.\n","authors":["Junda Wang","Zhichao Yang","Zonghai Yao","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17887v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11323v2","updated":"2024-06-28T10:43:01Z","published":"2024-06-17T08:37:14Z","title":"Transparency, Privacy, and Fairness in Recommender Systems","summary":"  Recommender systems have become a pervasive part of our daily online\nexperience, and are one of the most widely used applications of artificial\nintelligence and machine learning. Therefore, regulations and requirements for\ntrustworthy artificial intelligence, for example, the European AI Act, which\nincludes notions such as transparency, privacy, and fairness are also highly\nrelevant for the design of recommender systems in practice. This habilitation\nelaborates on aspects related to these three notions in the light of\nrecommender systems, namely: (i) transparency and cognitive models, (ii)\nprivacy and limited preference information, and (iii) fairness and popularity\nbias in recommender systems. Specifically, with respect to aspect (i), we\nhighlight the usefulness of incorporating psychological theories for a\ntransparent design process of recommender systems. We term this type of systems\npsychology-informed recommender systems. In aspect (ii), we study and address\nthe trade-off between accuracy and privacy in differentially-private\nrecommendations. We design a novel recommendation approach for collaborative\nfiltering based on an efficient neighborhood reuse concept, which reduces the\nnumber of users that need to be protected with differential privacy.\nFurthermore, we address the related issue of limited availability of user\npreference information, e.g., click data, in the settings of session-based and\ncold-start recommendations. With respect to aspect (iii), we analyze popularity\nbias in recommender systems. We find that the recommendation frequency of an\nitem is positively correlated with this item's popularity. This also leads to\nthe unfair treatment of users with little interest in popular content. Finally,\nwe study long-term fairness dynamics in algorithmic decision support in the\nlabor market using agent-based modeling techniques.\n","authors":["Dominik Kowald"],"pdf_url":"https://arxiv.org/pdf/2406.11323v2.pdf","comment":"Habilitation (post-doctoral thesis) at Graz University of Technology\n  for the scientific subject \"Applied Computer Science\" (accepted in June 2024)"},{"id":"http://arxiv.org/abs/2406.19804v1","updated":"2024-06-28T10:27:06Z","published":"2024-06-28T10:27:06Z","title":"Rateless Stochastic Coding for Delay-constrained Semantic Communication","summary":"  We consider the problem of joint source-channel coding with distortion and\nperception constraints from a rateless perspective, the purpose of which is to\nsettle the balance between reliability (distortion/perception) and\neffectiveness (rate) of transmission over uncertain channels. We find a new\nfinite-blocklength bound for the achievable joint source-channel code rate with\nthe above two constraints. To achieve a superior rateless characteristic of\nJSCC coding, we perform multi-level optimization on various finite-blocklength\ncodes. Based on these two, we then propose a new JSCC coding scheme called\nrateless stochastic coding (RSC). We experimentally demonstrate that the\nproposed RSC can achieve variable rates of transmission maintaining an\nexcellent trade-off between distortion and perception.\n","authors":["Cheng Peng","Rulong Wang","Yong Xiao"],"pdf_url":"https://arxiv.org/pdf/2406.19804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19760v1","updated":"2024-06-28T08:59:45Z","published":"2024-06-28T08:59:45Z","title":"Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case\n  Reformulation","summary":"  Legal case retrieval for sourcing similar cases is critical in upholding\njudicial fairness. Different from general web search, legal case retrieval\ninvolves processing lengthy, complex, and highly specialized legal documents.\nExisting methods in this domain often overlook the incorporation of legal\nexpert knowledge, which is crucial for accurately understanding and modeling\nlegal cases, leading to unsatisfactory retrieval performance. This paper\nintroduces KELLER, a legal knowledge-guided case reformulation approach based\non large language models (LLMs) for effective and interpretable legal case\nretrieval. By incorporating professional legal knowledge about crimes and law\narticles, we enable large language models to accurately reformulate the\noriginal legal case into concise sub-facts of crimes, which contain the\nessential information of the case. Extensive experiments on two legal case\nretrieval benchmarks demonstrate superior retrieval performance and robustness\non complex legal case queries of KELLER over existing methods.\n","authors":["Chenlong Deng","Kelong Mao","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2406.19760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19237v2","updated":"2024-06-28T05:43:46Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v2.pdf","comment":"Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables"},{"id":"http://arxiv.org/abs/2402.07647v2","updated":"2024-06-28T05:32:23Z","published":"2024-02-12T13:42:11Z","title":"GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language\n  Models for Adaptable Conversational Task Assistants","summary":"  We tackle the challenge of building real-world multimodal assistants for\ncomplex real-world tasks. We describe the practicalities and challenges of\ndeveloping and deploying GRILLBot, a leading (first and second prize winning in\n2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building\non our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture\nthat leverages Large Language Models (LLMs) and specialised models tuned for\nspecific subtasks requiring very low latency. OAT allows us to define when, how\nand which LLMs should be used in a structured and deployable manner. For\nknowledge-grounded question answering and live task adaptations, we show that\nLLM reasoning abilities over task context and world knowledge outweigh latency\nconcerns. For dialogue state management, we implement a code generation\napproach and show that specialised smaller models have 84% effectiveness with\n100x lower latency. Overall, we provide insights and discuss tradeoffs for\ndeploying both traditional models and LLMs to users in complex real-world\nmultimodal environments in the Alexa TaskBot challenge. These experiences will\ncontinue to evolve as LLMs become more capable and efficient -- fundamentally\nreshaping OAT and future assistant architectures.\n","authors":["Sophie Fischer","Carlos Gemmell","Niklas Tecklenburg","Iain Mackie","Federico Rossetto","Jeffrey Dalton"],"pdf_url":"https://arxiv.org/pdf/2402.07647v2.pdf","comment":"11 pages, KDD Preprint"},{"id":"http://arxiv.org/abs/2406.19647v1","updated":"2024-06-28T04:26:30Z","published":"2024-06-28T04:26:30Z","title":"Doc2Token: Bridging Vocabulary Gap by Predicting Missing Tokens for\n  E-commerce Search","summary":"  Addressing the \"vocabulary mismatch\" issue in information retrieval is a\ncentral challenge for e-commerce search engines, because product pages often\nmiss important keywords that customers search for. Doc2Query[1] is a popular\ndocument-expansion technique that predicts search queries for a document and\nincludes the predicted queries with the document for retrieval. However, this\napproach can be inefficient for e-commerce search, because the predicted query\ntokens are often already present in the document. In this paper, we propose\nDoc2Token, a technique that predicts relevant tokens (instead of queries) that\nare missing from the document and includes these tokens in the document for\nretrieval. For the task of predicting missing tokens, we introduce a new\nmetric, \"novel ROUGE score\". Doc2Token is demonstrated to be superior to\nDoc2Query in terms of novel ROUGE score and diversity of predictions. Doc2Token\nalso exhibits efficiency gains by reducing both training and inference times.\nWe deployed the feature to production and observed significant revenue gain in\nan online A/B test, and launched the feature to full traffic on Walmart.com.\n  [1] R. Nogueira, W. Yang, J. Lin, K. Cho, Document expansion by query\nprediction, arXiv preprint arXiv:1904.08375 (2019)\n","authors":["Kaihao Li","Juexin Lin","Tony Lee"],"pdf_url":"https://arxiv.org/pdf/2406.19647v1.pdf","comment":"9 pages, 1 figure, SIGIR 2024 Workshop on eCommerce"}]}}