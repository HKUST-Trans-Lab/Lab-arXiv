<!DOCTYPE html>
<html lang="en">

<head>
    <title>Lab-arXiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                HKUST-Trans-Lab/Lab-arXiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-28T00:00:00Z">2024-06-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">88</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web2Code: A Large-scale Webpage-to-Code <span class="highlight-title">Dataset</span> and Evaluation Framework
  for Multimodal <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, Haonan Li, Preslav Nakov, Timothy Baldwin, Zhengzhong Liu, Eric P. Xing, Xiaodan Liang, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown impressive success across
modalities such as image, video, and audio in a variety of understanding and
generation tasks. However, current MLLMs are surprisingly poor at understanding
webpage screenshots and generating their corresponding HTML code. To address
this problem, we propose Web2Code, a benchmark consisting of a new large-scale
webpage-to-code dataset for instruction tuning and an evaluation framework for
the webpage understanding and HTML code translation abilities of MLLMs. For
dataset construction, we leverage pretrained LLMs to enhance existing
webpage-to-code datasets as well as generate a diverse pool of new webpages
rendered into images. Specifically, the inputs are webpage images and
instructions, while the responses are the webpage's HTML code. We further
include diverse natural language QA pairs about the webpage content in the
responses to enable a more comprehensive understanding of the web content. To
evaluate model performance in these tasks, we develop an evaluation framework
for testing MLLMs' abilities in webpage understanding and web-to-code
generation. Extensive experiments show that our proposed dataset is beneficial
not only to our proposed tasks but also in the general visual domain, while
previous datasets result in worse performance. We hope our work will contribute
to the development of general MLLMs suitable for web-based content generation
and task automation. Our data and code will be available at
https://github.com/MBZUAI-LLM/web2code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at https://mbzuai-llm.github.io/webpage2code/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) equipped with extensive world knowledge and
strong reasoning skills can tackle diverse tasks across domains, often by
posing them as conversation-style instruction-response pairs. In this paper, we
propose LLaRA: Large Language and Robotics Assistant, a framework which
formulates robot action policy as conversations, and provides improved
responses when trained with auxiliary data that complements policy learning.
LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity
to process state information as visual-textual prompts and generate optimal
policy decisions in text. To train such action policy VLMs, we first introduce
an automated pipeline to generate diverse high-quality robotics instruction
data from existing behavior cloning data. A VLM finetuned with the resulting
collection of datasets based on a conversation-style formulation tailored for
robotics tasks, can generate meaningful robot action policy decisions. Our
experiments across multiple simulated and real-world environments demonstrate
the state-of-the-art performance of the proposed LLaRA framework. The code,
datasets, and pretrained models are available at
https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Synthetic Data Creation with 1,000,000,000 Personas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel persona-driven data synthesis methodology that leverages
various perspectives within a large language model (LLM) to create diverse
synthetic data. To fully exploit this methodology at scale, we introduce
Persona Hub -- a collection of 1 billion diverse personas automatically curated
from web data. These 1 billion personas (~13% of the world's total population),
acting as distributed carriers of world knowledge, can tap into almost every
perspective encapsulated within the LLM, thereby facilitating the creation of
diverse synthetic data at scale for various scenarios. By showcasing Persona
Hub's use cases in synthesizing high-quality mathematical and logical reasoning
problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs
and tools (functions) at scale, we demonstrate persona-driven data synthesis is
versatile, scalable, flexible, and easy to use, potentially driving a paradigm
shift in synthetic data creation and applications in practice, which may have a
profound impact on LLM research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProgressGym: Alignment with a Millennium of Moral Progress 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Qiu, Yang Zhang, Xuchuan Huang, Jasmine Xinze Li, Jiaming Ji, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frontier AI systems, including large language models (LLMs), hold increasing
influence over the epistemology of human users. Such influence can reinforce
prevailing societal values, potentially contributing to the lock-in of
misguided moral beliefs and, consequently, the perpetuation of problematic
moral practices on a broad scale. We introduce progress alignment as a
technical solution to mitigate this imminent risk. Progress alignment
algorithms learn to emulate the mechanics of human moral progress, thereby
addressing the susceptibility of existing alignment methods to contemporary
moral blindspots. To empower research in progress alignment, we introduce
ProgressGym, an experimental framework allowing the learning of moral progress
mechanics from history, in order to facilitate future progress in real-world
moral decisions. Leveraging 9 centuries of historical text and 18 historical
LLMs, ProgressGym enables codification of real-world progress alignment
challenges into concrete benchmarks. Specifically, we introduce three core
challenges: tracking evolving values (PG-Follow), preemptively anticipating
moral progress (PG-Predict), and regulating the feedback loop between human and
AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension
are inapplicable to these tasks. In response, we present lifelong and
extrapolative algorithms as baseline methods of progress alignment, and build
an open leaderboard soliciting novel algorithms and challenges. The framework
and the leaderboard are available at
https://github.com/PKU-Alignment/ProgressGym and
https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Erasure as a Footprint of Implicit Vocabulary Items in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheridan Feucht, David Atkinson, Byron Wallace, David Bau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs process text as sequences of tokens that roughly correspond to words,
where less common words are represented by multiple tokens. However, individual
tokens are often semantically unrelated to the meanings of the words/concepts
they comprise. For example, Llama-2-7b's tokenizer splits the word
"northeastern" into the tokens ['_n', 'ort', 'he', 'astern'], none of which
correspond to semantically meaningful units like "north" or "east." Similarly,
the overall meanings of named entities like "Neil Young" and multi-word
expressions like "break a leg" cannot be directly inferred from their
constituent tokens. Mechanistically, how do LLMs convert such arbitrary groups
of tokens into useful higher-level representations? In this work, we find that
last token representations of named entities and multi-token words exhibit a
pronounced "erasure" effect, where information about previous and current
tokens is rapidly forgotten in early layers. Using this observation, we propose
a method to "read out" the implicit vocabulary of an autoregressive LLM by
examining differences in token representations across layers, and present
results of this method for Llama-2-7b and Llama-3-8B. To our knowledge, this is
the first attempt to probe the implicit vocabulary of an LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures. Code and data at
  https://footprints.baulab.info/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular Facts: Desiderata for Decontextualization in <span class="highlight-title">LLM</span> Fact
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anisha Gunjal, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic factuality verification of large language model (LLM) generations
is becoming more and more widely used to combat hallucinations. A major point
of tension in the literature is the granularity of this fact-checking: larger
chunks of text are hard to fact-check, but more atomic facts like propositions
may lack context to interpret correctly. In this work, we assess the role of
context in these atomic facts. We argue that fully atomic facts are not the
right representation, and define two criteria for molecular facts:
decontextuality, or how well they can stand alone, and minimality, or how
little extra information is added to achieve decontexuality. We quantify the
impact of decontextualization on minimality, then present a baseline
methodology for generating molecular facts automatically, aiming to add the
right amount of information. We compare against various methods of
decontextualization and find that molecular facts balance minimality with fact
verification accuracy in ambiguous settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying RLAIF for Code Generation with API-usage in Lightweight <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujan Dutta, Sayantan Mahinder, Raviteja Anantha, Bortik Bandyopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from AI Feedback (RLAIF) has demonstrated significant
potential across various domains, including mitigating harm in LLM outputs,
enhancing text summarization, and mathematical reasoning. This paper introduces
an RLAIF framework for improving the code generation abilities of lightweight
(<1B parameters) LLMs. We specifically focus on code generation tasks that
require writing appropriate API calls, which is challenging due to the
well-known issue of hallucination in LLMs. Our framework extracts AI feedback
from a larger LLM (e.g., GPT-3.5) through a specialized prompting strategy and
uses this data to train a reward model towards better alignment from smaller
LLMs. We run our experiments on the Gorilla dataset and meticulously assess the
quality of the model-generated code across various metrics, including AST,
ROUGE, and Code-BLEU, and develop a pipeline to compute its executability rate
accurately. Our approach significantly enhances the fine-tuned LLM baseline's
performance, achieving a 4.5% improvement in executability rate. Notably, a
smaller LLM model (780M parameters) trained with RLAIF surpasses a much larger
fine-tuned baseline with 7B parameters, achieving a 1.0% higher code
executability rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Word Senses and Beyond: Inducing Concepts with Contextualized
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastien Liétard, Pascal Denis, Mikaella Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polysemy and synonymy are two crucial interrelated facets of lexical
ambiguity. While both phenomena have been studied extensively in NLP, leading
to dedicated systems, they are often been considered independently. While many
tasks dealing with polysemy (e.g. Word Sense Disambiguiation or Induction)
highlight the role of a word's senses, the study of synonymy is rooted in the
study of concepts, i.e. meaning shared across the lexicon. In this paper, we
introduce Concept Induction, the unsupervised task of learning a soft
clustering among words that defines a set of concepts directly from data. This
task generalizes that of Word Sense Induction. We propose a bi-level approach
to Concept Induction that leverages both a local lemma-centric view and a
global cross-lexicon perspective to induce concepts. We evaluate the obtained
clustering on SemCor's annotated data and obtain good performances (BCubed F1
above 0.60). We find that the local and the global levels are mutually
beneficial to induce concepts and also senses in our setting. Finally, we
create static embeddings representing our induced concepts and use them on the
Word-in-Context task, obtaining competitive performances with the
State-of-the-Art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Covert Malicious Finetuning: Challenges in Safeguarding <span class="highlight-title">LLM</span> Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Halawi, Alexander Wei, Eric Wallace, Tony T. Wang, Nika Haghtalab, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box finetuning is an emerging interface for adapting state-of-the-art
language models to user needs. However, such access may also let malicious
actors undermine model safety. To demonstrate the challenge of defending
finetuning interfaces, we introduce covert malicious finetuning, a method to
compromise model safety via finetuning while evading detection. Our method
constructs a malicious dataset where every individual datapoint appears
innocuous, but finetuning on the dataset teaches the model to respond to
encoded harmful requests with encoded harmful responses. Applied to GPT-4, our
method produces a finetuned model that acts on harmful instructions 99% of the
time and avoids detection by defense mechanisms such as dataset inspection,
safety evaluations, and input/output classifiers. Our findings question whether
black-box finetuning access can be secured against sophisticated adversaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Mitigating Language Confusion in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly Marchisio, Wei-Yin Ko, Alexandre Bérard, Théo Dehaze, Sebastian Ruder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a surprising limitation of LLMs: their inability to
consistently generate text in a user's desired language. We create the Language
Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically
diverse languages with existing and newly-created English and multilingual
prompts. We evaluate a range of LLMs on monolingual and cross-lingual
generation reflecting practical use cases, finding that Llama Instruct and
Mistral models exhibit high degrees of language confusion and even the
strongest models fail to consistently respond in the correct language. We
observe that base and English-centric instruct models are more prone to
language confusion, which is aggravated by complex prompts and high sampling
temperatures. We find that language confusion can be partially mitigated via
few-shot prompting, multilingual SFT and preference tuning. We release our
language confusion benchmark, which serves as a first layer of efficient,
scalable multilingual evaluation at
https://github.com/for-ai/language-confusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioMNER: A <span class="highlight-title">Dataset</span> for Biomedical Method Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Bohao Yang, Kun Zhao, Bo Lv, Chenghao Xiao, Frank Guerin, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named entity recognition (NER) stands as a fundamental and pivotal task
within the realm of Natural Language Processing. Particularly within the domain
of Biomedical Method NER, this task presents notable challenges, stemming from
the continual influx of domain-specific terminologies in scholarly literature.
Current research in Biomedical Method (BioMethod) NER suffers from a scarcity
of resources, primarily attributed to the intricate nature of methodological
concepts, which necessitate a profound understanding for precise delineation.
In this study, we propose a novel dataset for biomedical method entity
recognition, employing an automated BioMethod entity recognition and
information retrieval system to assist human annotation. Furthermore, we
comprehensively explore a range of conventional and contemporary open-domain
NER methodologies, including the utilization of cutting-edge large-scale
language models (LLMs) customised to our dataset. Our empirical findings reveal
that the large parameter counts of language models surprisingly inhibit the
effective assimilation of entity extraction patterns pertaining to biomedical
methods. Remarkably, the approach, leveraging the modestly sized ALBERT model
(only 11MB), in conjunction with conditional random fields (CRF), achieves
state-of-the-art (SOTA) performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEMoE: Advanced Mixture of Experts Adaptor for Lifelong Model Editing of
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renzhi Wang, Piji Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) require continual knowledge updates to stay
abreast of the ever-changing world facts, prompting the formulation of lifelong
model editing task. While recent years have witnessed the development of
various techniques for single and batch editing, these methods either fail to
apply or perform sub-optimally when faced with lifelong editing. In this paper,
we introduce LEMoE, an advanced Mixture of Experts (MoE) adaptor for lifelong
model editing. We first analyze the factors influencing the effectiveness of
conventional MoE adaptor in lifelong editing, including catastrophic
forgetting, inconsistent routing and order sensitivity. Based on these
insights, we propose a tailored module insertion method to achieve lifelong
editing, incorporating a novel KV anchor routing to enhance routing consistency
between training and inference stage, along with a concise yet effective
clustering-based editing order planning. Experimental results demonstrate the
effectiveness of our method in lifelong editing, surpassing previous model
editing techniques while maintaining outstanding performance in batch editing
task. Our code will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for
  Tool-Augmented <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, Tetsuya Sakai, Tian Feng, Hayato Yamana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool-augmented large language models (LLMs) are rapidly being integrated into
real-world applications. Due to the lack of benchmarks, the community still
needs to fully understand the hallucination issues within these models. To
address this challenge, we introduce a comprehensive diagnostic benchmark,
ToolBH. Specifically, we assess the LLM's hallucinations through two
perspectives: depth and breadth. In terms of depth, we propose a multi-level
diagnostic process, including (1) solvability detection, (2) solution planning,
and (3) missing-tool analysis. For breadth, we consider three scenarios based
on the characteristics of the toolset: missing necessary tools, potential
tools, and limited functionality tools. Furthermore, we developed seven tasks
and collected 700 evaluation samples through multiple rounds of manual
annotation. The results show the significant challenges presented by the ToolBH
benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve a
total score of 45.3 and 37.0, respectively, on a scale of 100. In this
benchmark, larger model parameters do not guarantee better performance; the
training data and response strategies also play a crucial role in tool-enhanced
LLM scenarios. Our diagnostic analysis indicates that the primary reason for
model errors lies in assessing task solvability. Additionally, open-weight
models suffer from performance drops with verbose replies, whereas proprietary
models excel with longer reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The SIFo Benchmark: Investigating the Sequential Instruction Following
  Ability of <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Chen, Baohao Liao, Jirui Qi, Panagiotis Eustratiadis, Christof Monz, Arianna Bisazza, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following multiple instructions is a crucial ability for large language
models (LLMs). Evaluating this ability comes with significant challenges: (i)
limited coherence between multiple instructions, (ii) positional bias where the
order of instructions affects model performance, and (iii) a lack of
objectively verifiable tasks. To address these issues, we introduce a benchmark
designed to evaluate models' abilities to follow multiple instructions through
sequential instruction following (SIFo) tasks. In SIFo, the successful
completion of multiple instructions is verifiable by examining only the final
instruction. Our benchmark evaluates instruction following using four tasks
(text modification, question answering, mathematics, and security rule
following), each assessing different aspects of sequential instruction
following. Our evaluation of popular LLMs, both closed-source and open-source,
shows that more recent and larger models significantly outperform their older
and smaller counterparts on the SIFo tasks, validating the benchmark's
effectiveness. All models struggle with following sequences of instructions,
hinting at an important lack of robustness of today's language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Parent Family: A Spectrum of Family Members from a Single
  Pre-Trained Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Habib Hajimolahoseini, Mohammad Hassanpour, Foozhan Ataiefard, Boxing Chen, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method of Progressive Low Rank Decomposition
(PLRD) tailored for the compression of large language models. Our approach
leverages a pre-trained model, which is then incrementally decompressed to
smaller sizes using progressively lower ranks. This method allows for
significant reductions in computational overhead and energy consumption, as
subsequent models are derived from the original without the need for retraining
from scratch. We detail the implementation of PLRD, which strategically
decreases the tensor ranks, thus optimizing the trade-off between model
performance and resource usage. The efficacy of PLRD is demonstrated through
extensive experiments showing that models trained with PLRD method on only 1B
tokens maintain comparable performance with traditionally trained models while
using 0.1% of the tokens. The versatility of PLRD is highlighted by its ability
to generate multiple model sizes from a single foundational model, adapting
fluidly to varying computational and memory budgets. Our findings suggest that
PLRD could set a new standard for the efficient scaling of LLMs, making
advanced AI more feasible on diverse platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Into the Unknown: Generating Geospatial Descriptions for New
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzuf Paz-Argaman, John Palowitch, Sayali Kulkarni, Reut Tsarfaty, Jason Baldridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Similar to vision-and-language navigation (VLN) tasks that focus on bridging
the gap between vision and language for embodied navigation, the new Rendezvous
(RVS) task requires reasoning over allocentric spatial relationships
(independent of the observer's viewpoint) using non-sequential navigation
instructions and maps. However, performance substantially drops in new
environments with no training data. Using opensource descriptions paired with
coordinates (e.g., Wikipedia) provides training data but suffers from limited
spatially-oriented text resulting in low geolocation resolution. We propose a
large-scale augmentation method for generating high-quality synthetic data for
new environments using readily available geospatial data. Our method constructs
a grounded knowledge-graph, capturing entity relationships. Sampled entities
and relations (`shop north of school') generate navigation instructions via (i)
generating numerous templates using context-free grammar (CFG) to embed
specific entities and relations; (ii) feeding the entities and relation into a
large language model (LLM) for instruction generation. A comprehensive
evaluation on RVS, showed that our approach improves the 100-meter accuracy by
45.83% on unseen environments. Furthermore, we demonstrate that models trained
with CFG-based augmentation achieve superior performance compared with those
trained with LLM-based augmentation, both in unseen and seen environments.
These findings suggest that the potential advantages of explicitly structuring
spatial information for text-based geospatial reasoning in previously unknown,
can unlock data-scarce scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating Financial Market via <span class="highlight-title">Large Language Model</span> based Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Gao, Yuntao Wen, Minghang Zhu, Jianing Wei, Yuhan Cheng, Qunzi Zhang, Shuo Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most economic theories typically assume that financial market participants
are fully rational individuals and use mathematical models to simulate human
behavior in financial markets. However, human behavior is often not entirely
rational and is challenging to predict accurately with mathematical models. In
this paper, we propose \textbf{A}gent-based \textbf{S}imulated
\textbf{F}inancial \textbf{M}arket (ASFM), which first constructs a simulated
stock market with a real order matching system. Then, we propose a large
language model based agent as the stock trader, which contains the profile,
observation, and tool-learning based action module. The trading agent can
comprehensively understand current market dynamics and financial policy
information, and make decisions that align with their trading strategy. In the
experiments, we first verify that the reactions of our ASFM are consistent with
the real stock market in two controllable scenarios. In addition, we also
conduct experiments in two popular economics research directions, and we find
that conclusions drawn in our \model align with the preliminary findings in
economics research. Based on these observations, we believe our proposed ASFM
provides a new paradigm for economic research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BESTOW: Efficient and Streamable Speech Language Model with the Best of
  Two Worlds in GPT and T5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehuai Chen, He Huang, Oleksii Hrinchuk, Krishna C. Puvvada, Nithin Rao Koluguri, Piotr Żelasko, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating speech understanding capabilities into pretrained
large-language models has become a vital research direction (SpeechLLM). The
previous architectures can be categorized as: i) GPT-style, prepend speech
prompts to the text prompts as a sequence of LLM inputs like a decoder-only
model; ii) T5-style, introduce speech cross-attention to each layer of the
pretrained LLMs. We propose BESTOW architecture to bring the BESt features from
TwO Worlds into a single model that is highly efficient and has strong
multitask capabilities. Moreover, there is no clear streaming solution for
either style, especially considering the solution should generalize to speech
multitask. We reformulate streamable SpeechLLM as a read-write policy problem
and unifies the offline and streaming research with BESTOW architecture. Hence
we demonstrate the first open-source SpeechLLM solution that enables Streaming
and Multitask at scale (beyond ASR) at the same time. This streamable solution
achieves very strong performance on a wide range of speech tasks (ASR, AST,
SQA, unseen DynamicSuperb). It is end-to-end optimizable, with lower
training/inference cost, and demonstrates LLM knowledge transferability to
speech.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining Reasons For And Against Vaccination From Unstructured Data Using
  Nichesourcing and AI Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damián Ariel Furman, Juan Junqueras, Z. Burçe Gümüslü, Edgar Altszyler, Joaquin Navajas, Ophelia Deroy, Justin Sulik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Reasons For and Against Vaccination (RFAV), a dataset for
predicting reasons for and against vaccination, and scientific authorities used
to justify them, annotated through nichesourcing and augmented using GPT4 and
GPT3.5-Turbo. We show how it is possible to mine these reasons in
non-structured text, under different task definitions, despite the high level
of subjectivity involved and explore the impact of artificially augmented data
using in-context learning with GPT4 and GPT3.5-Turbo. We publish the dataset
and the trained models along with the annotation manual used to train
annotators and define the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages + references and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating <span class="highlight-title">LLM</span>s with Preference Optimization on Thought Trees for
  Generating Rationale in Science Question Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazheng Li, Hainiu Xu, Zhaoyue Sun, Yuxiang Zhou, David West, Cesare Aloisi, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating rationales that justify scoring decisions has been a promising way
to facilitate explainability in automated scoring systems. However, existing
methods do not match the accuracy of classifier-based methods. Plus, the
generated rationales often contain hallucinated information. To address these
issues, we propose a novel framework capable of generating more faithful
rationales and, more importantly, matching performance with classifier-based
black-box scoring systems. We first mimic the human assessment process by
querying Large Language Models (LLMs) to generate a thought tree. We then
summarise intermediate assessment decisions from each thought tree path for
creating synthetic rationale data and rationale preference data. Finally, we
utilise the generated synthetic data to calibrate LLMs through a two-step
training process: supervised fine-tuning and preference optimization. Extensive
experimental results demonstrate that our framework achieves a 38% assessment
performance improvement in the QWK score compared to prior work while producing
higher-quality rationales, as recognised by human evaluators and LLMs. Our work
sheds light on the effectiveness of performing preference optimization using
synthetic preference data obtained from thought tree paths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From the Least to the Most: Building a Plug-and-Play Visual Reasoner via
  Data Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore multi-step reasoning in vision-language models (VLMs). The problem
is challenging, as reasoning data consisting of multiple steps of visual and
language processing are barely available. To overcome the challenge, we first
introduce a least-to-most visual reasoning paradigm, which interleaves steps of
decomposing a question into sub-questions and invoking external tools for
resolving sub-questions. Based on the paradigm, we further propose a novel data
synthesis approach that can automatically create questions and multi-step
reasoning paths for an image in a bottom-up manner. Our approach divides the
complex synthesis task into a few simple sub-tasks, and (almost entirely)
relies on open-sourced models to accomplish the sub-tasks. Therefore, the
entire synthesis process is reproducible and cost-efficient, and the
synthesized data is quality guaranteed. With the approach, we construct $50$k
visual reasoning examples. Then, we develop a visual reasoner through
supervised fine-tuning, which is capable of generally enhancing the reasoning
abilities of a wide range of existing VLMs in a plug-and-play fashion.
Extensive experiments indicate that the visual reasoner can consistently and
significantly improve four VLMs on four VQA benchmarks. Our code and dataset
are available at https://github.com/steven-ccq/VisualReasoner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Topic Models with Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Garima Dhanania, Sheshera Mysore, Chau Minh Pham, Mohit Iyyer, Hamed Zamani, Andrew McCallum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic models are widely used to analyze document collections. While they are
valuable for discovering latent topics in a corpus when analysts are unfamiliar
with the corpus, analysts also commonly start with an understanding of the
content present in a corpus. This may be through categories obtained from an
initial pass over the corpus or a desire to analyze the corpus through a
predefined set of categories derived from a high level theoretical framework
(e.g. political ideology). In these scenarios analysts desire a topic modeling
approach which incorporates their understanding of the corpus while supporting
various forms of interaction with the model. In this work, we present EdTM, as
an approach for label name supervised topic modeling. EdTM models topic
modeling as an assignment problem while leveraging LM/LLM based document-topic
affinities and using optimal transport for making globally coherent
topic-assignments. In experiments, we show the efficacy of our framework
compared to few-shot LLM classifiers, and topic models based on clustering and
LDA. Further, we show EdTM's ability to incorporate various forms of analyst
feedback and while remaining robust to noisy analyst inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print; Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paraphrase Types Elicit Prompt Engineering Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Philip Wahle, Terry Ruas, Yang Xu, Bela Gipp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much of the success of modern language models depends on finding a suitable
prompt to instruct the model. Until now, it has been largely unknown how
variations in the linguistic expression of prompts affect these models. This
study systematically and empirically evaluates which linguistic features
influence models through paraphrase types, i.e., different linguistic changes
at particular positions. We measure behavioral changes for five models across
120 tasks and six families of paraphrases (i.e., morphology, syntax, lexicon,
lexico-syntax, discourse, and others). We also control for other prompt
engineering factors (e.g., prompt length, lexical diversity, and proximity to
training data). Our results show a potential for language models to improve
tasks when their prompts are adapted in specific paraphrase types (e.g., 6.7%
median gain in Mixtral 8x7B; 5.5% in LLaMA 3 8B). In particular, changes in
morphology and lexicon, i.e., the vocabulary used, showed promise in improving
prompts. These findings contribute to developing more robust language models
capable of handling variability in linguistic expression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Untangling the Unrestricted Web: Automatic Identification of
  Multilingual Registers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Henriksson, Amanda Myntti, Anni Eskelinen, Selcen Erten-Johansson, Saara Hellström, Veronika Laippala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article explores deep learning models for the automatic identification
of registers - text varieties such as news reports and discussion forums - in
web-based datasets across 16 languages. Web register (or genre) identification
would provide a robust solution for understanding the content of web-scale
datasets, which have become crucial in computational linguistics. Despite
recent advances, the potential of register classifiers on the noisy web remains
largely unexplored, particularly in multilingual settings and when targeting
the entire unrestricted web. We experiment with a range of deep learning models
using the new Multilingual CORE corpora, which includes 16 languages annotated
using a detailed, hierarchical taxonomy of 25 registers designed to cover the
entire unrestricted web. Our models achieve state-of-the-art results, showing
that a detailed taxonomy in a hierarchical multi-label setting can yield
competitive classification performance. However, all models hit a glass ceiling
at approximately 80% F1 score, which we attribute to the non-discrete nature of
web registers and the inherent uncertainty in labeling some documents. By
pruning ambiguous examples, we improve model performance to over 90%. Finally,
multilingual models outperform monolingual ones, particularly benefiting
languages with fewer training examples and smaller registers. Although a
zero-shot setting decreases performance by an average of 7%, these drops are
not linked to specific registers or languages. Instead, registers show
surprising similarity across languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Timescales of Language Processing with EEG and
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Turco, Conor Houghton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the temporal dynamics of language processing by examining
the alignment between word representations from a pre-trained transformer-based
language model, and EEG data. Using a Temporal Response Function (TRF) model,
we investigate how neural activity corresponds to model representations across
different layers, revealing insights into the interaction between artificial
language models and brain responses during language comprehension. Our analysis
reveals patterns in TRFs from distinct layers, highlighting varying
contributions to lexical and compositional processing. Additionally, we used
linear discriminant analysis (LDA) to isolate part-of-speech (POS)
representations, offering insights into their influence on neural responses and
the underlying mechanisms of syntactic processing. These findings underscore
EEG's utility for probing language processing dynamics with high temporal
resolution. By bridging artificial language models and neural activity, this
study advances our understanding of their interaction at fine timescales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2024 Conference on Cognitive Computational
  Neuroscience (CCN 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Subtle Differences between Human and Model Languages Using
  Spectrum of Relative Likelihood 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xu, Yu Wang, Hao An, Zhichen Liu, Yongyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human and model-generated texts can be distinguished by examining the
magnitude of likelihood in language. However, it is becoming increasingly
difficult as language model's capabilities of generating human-like texts keep
evolving. This study provides a new perspective by using the relative
likelihood values instead of absolute ones, and extracting useful features from
the spectrum-view of likelihood for the human-model text detection task. We
propose a detection procedure with two classification methods, supervised and
heuristic-based, respectively, which results in competitive performances with
previous zero-shot detection methods and a new state-of-the-art on short-text
detection. Our method can also reveal subtle differences between human and
model languages, which find theoretical roots in psycholinguistics studies. Our
code is available at https://github.com/CLCS-SUSTech/FourierGPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YuLan: An Open-source <span class="highlight-title">Large Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Zhu, Kun Zhou, Kelong Mao, Wentong Chen, Yiding Sun, Zhipeng Chen, Qian Cao, Yihan Wu, Yushuo Chen, Feng Wang, Lei Zhang, Junyi Li, Xiaolei Wang, Lei Wang, Beichen Zhang, Zican Dong, Xiaoxue Cheng, Yuhan Chen, Xinyu Tang, Yupeng Hou, Qiangqiang Ren, Xincheng Pang, Shufang Xie, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu, Wenbing Huang, Ze-Feng Gao, Yueguo Chen, Weizheng Lu, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become the foundation of many applications,
leveraging their extensive capabilities in processing and understanding natural
language. While many open-source LLMs have been released with technical
reports, the lack of training details hinders further research and development.
This paper presents the development of YuLan, a series of open-source LLMs with
$12$ billion parameters. The base model of YuLan is pre-trained on
approximately $1.7$T tokens derived from a diverse corpus, including massive
English, Chinese, and multilingual texts. We design a three-stage pre-training
method to enhance YuLan's overall capabilities. Subsequent phases of training
incorporate instruction-tuning and human alignment, employing a substantial
volume of high-quality synthesized data. To facilitate the learning of complex
and long-tail knowledge, we devise a curriculum-learning framework throughout
across these stages, which helps LLMs learn knowledge in an easy-to-hard
manner. YuLan's training is finished on Jan, 2024 and has achieved performance
on par with state-of-the-art LLMs across various English and Chinese
benchmarks. This paper outlines a comprehensive technical roadmap for
developing LLMs from scratch. Our model and codes are available at
https://github.com/RUC-GSAI/YuLan-Chat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anoma<span class="highlight-title">LLM</span>y -- Detecting anomalous tokens in black-box <span class="highlight-title">LLM</span>s through
  low-confidence single-token predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waligóra Witold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces AnomaLLMy, a novel technique for the automatic
detection of anomalous tokens in black-box Large Language Models (LLMs) with
API-only access. Utilizing low-confidence single-token predictions as a
cost-effective indicator, AnomaLLMy identifies irregularities in model
behavior, addressing the issue of anomalous tokens degrading the quality and
reliability of models. Validated on the cl100k_base dataset, the token set of
GPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the
method's efficiency with just \$24.39 spent in API credits. The insights from
this research are expected to be beneficial for enhancing the robustness of and
accuracy of LLMs, particularly in the development and assessment of tokenizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for
  Multi-hop Question Answering <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated strong reasoning capabilities.
Nevertheless, they still suffer from factual errors when tackling
knowledge-intensive tasks. Retrieval-augmented reasoning represents a promising
approach. However, significant challenges still persist, including inaccurate
and insufficient retrieval for complex questions, as well as difficulty in
integrating multi-source knowledge. To address this, we propose Beam
Aggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive
multi-hop QA. BeamAggR explores and prioritizes promising answers at each hop
of question. Concretely, we parse the complex questions into trees, which
include atom and composite questions, followed by bottom-up reasoning. For
atomic questions, the LLM conducts reasoning on multi-source knowledge to get
answer candidates. For composite questions, the LLM combines beam candidates,
explores multiple reasoning paths through probabilistic aggregation, and
prioritizes the most promising trajectory. Extensive experiments on four
open-domain multi-hop reasoning datasets show that our method significantly
outperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that
BeamAggR elicits better knowledge collaboration and answer aggregation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and Domain-General Abstractive Proposition Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Javad Hosseini, Yang Gao, Tim Baumgärtner, Alex Fabrikant, Reinald Kim Amplayo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting text into fine-grained units of meaning is important to a wide
range of NLP applications. The default approach of segmenting text into
sentences is often insufficient, especially since sentences are usually complex
enough to include multiple units of meaning that merit separate treatment in
the downstream task. We focus on the task of abstractive proposition
segmentation: transforming text into simple, self-contained, well-formed
sentences. Several recent works have demonstrated the utility of proposition
segmentation with few-shot prompted LLMs for downstream tasks such as
retrieval-augmented grounding and fact verification. However, this approach
does not scale to large amounts of text and may not always extract all the
facts from the input text. In this paper, we first introduce evaluation metrics
for the task to measure several dimensions of quality. We then propose a
scalable, yet accurate, proposition segmentation model. We model proposition
segmentation as a supervised task by training LLMs on existing annotated
datasets and show that training yields significantly improved results. We
further show that by using the fine-tuned LLMs as teachers for annotating large
amounts of multi-domain synthetic distillation data, we can train smaller
student models with results similar to the teacher LLMs. We then demonstrate
that our technique leads to effective domain generalization, by annotating data
in two domains outside the original training data and evaluating on them.
Finally, as a key contribution of the paper, we share an easy-to-use API for
NLP practitioners to use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLPerturbator: Studying the Robustness of Code <span class="highlight-title">LLM</span>s to Natural Language
  Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Chen, Zhenhao Li, Xing Hu, Xin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) achieve promising results in code generation
based on a given natural language description. They have been integrated into
open-source projects and commercial products to facilitate daily coding
activities. The natural language description in the prompt is crucial for LLMs
to comprehend users' requirements. Prior studies uncover that LLMs are
sensitive to the changes in the prompts, including slight changes that look
inconspicuous. However, the natural language descriptions often vary in
real-world scenarios (e.g., different formats, grammar, and wording). Prior
studies on the robustness of LLMs are often based on random perturbations and
such perturbations may not actually happen. In this paper, we conduct a
comprehensive study to investigate how are code LLMs robust to variations of
natural language description in real-world scenarios. We summarize 18
categories of perturbations of natural language and 3 combinations of
co-occurred categories based on our literature review and an online survey with
practitioners. We propose an automated framework, NLPerturbator, which can
perform perturbations of each category given a set of prompts. Through a series
of experiments on code generation using six code LLMs, we find that the
perturbed prompts can decrease the performance of code generation by a
considerable margin (e.g., up to 21.2%, and 4.8% to 6.1% on average). Our study
highlights the importance of enhancing the robustness of LLMs to real-world
variations in the prompts, as well as the essentiality of attentively
constructing the prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Preference Knowledge Distillation for <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixing Li, Yuxian Gu, Li Dong, Dequan Wang, Yu Cheng, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of large language models (LLMs), Knowledge Distillation (KD) is
a critical technique for transferring capabilities from teacher models to
student models. However, existing KD methods face limitations and challenges in
distillation of LLMs, including efficiency and insufficient measurement
capabilities of traditional KL divergence. It is shown that LLMs can serve as
an implicit reward function, which we define as a supplement to KL divergence.
In this work, we propose Direct Preference Knowledge Distillation (DPKD) for
LLMs. DPKD utilizes distribution divergence to represent the preference loss
and implicit reward function. We re-formulate KD of LLMs into two stages: first
optimizing and objective consisting of implicit reward and reverse KL
divergence and then improving the preference probability of teacher outputs
over student outputs. We conducted experiments and analysis on various datasets
with LLM parameters ranging from 120M to 13B and demonstrate the broad
applicability and effectiveness of our DPKD approach. Meanwhile, we prove the
value and effectiveness of the introduced implicit reward and output preference
in KD through experiments and theoretical analysis. The DPKD method outperforms
the baseline method in both output response precision and exact match
percentage. Code and data are available at https://aka.ms/dpkd.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Belief Revision: The Adaptability of <span class="highlight-title">Large Language Models</span> Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Wilie, Samuel Cahyawijaya, Etsuko Ishii, Junxian He, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability to reason from text is crucial for real-world NLP
applications. Real-world scenarios often involve incomplete or evolving data.
In response, individuals update their beliefs and understandings accordingly.
However, most existing evaluations assume that language models (LMs) operate
with consistent information. We introduce Belief-R, a new dataset designed to
test LMs' belief revision ability when presented with new evidence. Inspired by
how humans suppress prior inferences, this task assesses LMs within the newly
proposed delta reasoning ($\Delta R$) framework. Belief-R features sequences of
premises designed to simulate scenarios where additional information could
necessitate prior conclusions drawn by LMs. We evaluate $\sim$30 LMs across
diverse prompting strategies and found that LMs generally struggle to
appropriately revise their beliefs in response to new information. Further,
models adept at updating often underperformed in scenarios without necessary
updates, highlighting a critical trade-off. These insights underscore the
importance of improving LMs' adaptiveness to changing information, a step
toward more reliable AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case
  Reformulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlong Deng, Kelong Mao, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal case retrieval for sourcing similar cases is critical in upholding
judicial fairness. Different from general web search, legal case retrieval
involves processing lengthy, complex, and highly specialized legal documents.
Existing methods in this domain often overlook the incorporation of legal
expert knowledge, which is crucial for accurately understanding and modeling
legal cases, leading to unsatisfactory retrieval performance. This paper
introduces KELLER, a legal knowledge-guided case reformulation approach based
on large language models (LLMs) for effective and interpretable legal case
retrieval. By incorporating professional legal knowledge about crimes and law
articles, we enable large language models to accurately reformulate the
original legal case into concise sub-facts of crimes, which contain the
essential information of the case. Extensive experiments on two legal case
retrieval benchmarks demonstrate superior retrieval performance and robustness
on complex legal case queries of KELLER over existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking the Script Barrier in Multilingual Pre-Trained Language Models
  with Transliteration-Based Post-Training Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orgest Xhelili, Yihong Liu, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual pre-trained models (mPLMs) have shown impressive performance on
cross-lingual transfer tasks. However, the transfer performance is often
hindered when a low-resource target language is written in a different script
than the high-resource source language, even though the two languages may be
related or share parts of their vocabularies. Inspired by recent work that uses
transliteration to address this problem, our paper proposes a
transliteration-based post-pretraining alignment (PPA) method aiming to improve
the cross-lingual alignment between languages using diverse scripts. We select
two areal language groups, $\textbf{Mediterranean-Amharic-Farsi}$ and
$\textbf{South+East Asian Languages}$, wherein the languages are mutually
influenced but use different scripts. We apply our method to these language
groups and conduct extensive experiments on a spectrum of downstream tasks. The
results show that after PPA, models consistently outperform the original model
(up to 50% for some tasks) in English-centric transfer. In addition, when we
use languages other than English as sources in transfer, our method obtains
even larger improvements. We will make our code and models publicly available
at \url{https://github.com/cisnlp/Transliteration-PPA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Instruct: Generated Visual Instructions for Large Multimodal Model
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Liu, Xin Huang, Jinliang Zheng, Boxiao Liu, Jia Wang, Osamu Yoshie, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MM-Instruct, a large-scale dataset of diverse and
high-quality visual instruction data designed to enhance the
instruction-following capabilities of large multimodal models (LMMs). While
existing visual instruction datasets often focus on question-answering, they
struggle to generalize to broader application scenarios such as creative
writing, summarization, or image analysis. To address these limitations, we
propose a novel approach to constructing MM-Instruct that leverages the strong
instruction-following capabilities of existing LLMs to generate novel visual
instruction data from large-scale but conventional image captioning datasets.
MM-Instruct first leverages ChatGPT to automatically generate diverse
instructions from a small set of seed instructions through augmenting and
summarization. It then matches these instructions with images and uses an
open-sourced large language model (LLM) to generate coherent answers to the
instruction-image pairs. The LLM is grounded by the detailed text descriptions
of images in the whole answer generation process to guarantee the alignment of
the instruction data. Moreover, we introduce a benchmark based on the generated
instruction data to evaluate the instruction-following capabilities of existing
LMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5
model on the generated data, denoted as LLaVA-Instruct, which exhibits
significant improvements in instruction-following capabilities compared to
LLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models
are available at https://github.com/jihaonew/MM-Instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset and models are available at
  https://github.com/jihaonew/MM-Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Message du troisi{è}me type : irruption d'un tiers dans un dialogue en
  ligne 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovic Tanguy, Céline Poudat, Lydia-Mai Ho-Dac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our study focuses on Wikipedia talk pages, from a global perspective
analyzing contributors' behaviors in online interactions. Using a corpus
comprising all Wikipedia talk pages in French, totaling more than 300,000
discussion threads, we examine how discussions with more than two participants
(multiparty conversation) unfold and we specifically investigate the role of a
third participant's intervention when two Wikipedians have already initiated an
exchange. In this regard, we concentrate on the sequential structure of these
interactions in terms of articulation among different participants and aim to
specify this third message by exploring its lexical particularities, while also
proposing an initial typology of the third participant's message role and how
it aligns with preceding messages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language. JADT 2024 - 17es Journ{\'e}es internationales
  d'Analyse statistique des Donn{\'e}es Textuelles, SeSLa (S{\'e}minaire des
  Sciences du Langage de l'UCLouvain -- Site Saint-Louis); LASLA (Laboratoire
  d'Analyse statistique des Langues anciennes de l'Universit{\'e} de
  Li{\`e}ge), 2024, Bruxelles, Belgique</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Le sens de la famille : analyse du vocabulaire de la parent{é} par les
  plongements de mots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ludovic Tanguy, Cécile Fabre, Nabil Hathout, Lydia-Mai Ho-Dac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a corpus analysis of an area of the French lexicon
that is both dense and highly structured: the vocabulary of family
relationships. Starting with a lexicon of 25 nouns designating the main
relationships (son, cousin, mother, grandfather, sister-in-law etc.), we
examine how these terms are positioned in relation to each other through
distributional analyses based on the use of these terms in corpora. We show
that distributional information can capture certain features that organize this
vocabulary (descent, alliance, siblings, genre), in ways that vary according to
the different corpora compared.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language. JADT 2024 - 17es Journ{\'e}es internationales
  d'Analyse statistique des Donn{\'e}es Textuelles, SeSLa (S{\'e}minaire des
  Sciences du Langage de l'UCLouvain -- Site Saint-Louis), 2024, Bruxelles,
  Belgique</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification in <span class="highlight-title">Large Language Models</span> Through Convex Hull
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferhat Ozgur Catak, Murat Kuzlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification approaches have been more critical in large
language models (LLMs), particularly high-risk applications requiring reliable
outputs. However, traditional methods for uncertainty quantification, such as
probabilistic models and ensemble techniques, face challenges when applied to
the complex and high-dimensional nature of LLM-generated outputs. This study
proposes a novel geometric approach to uncertainty quantification using convex
hull analysis. The proposed method leverages the spatial properties of response
embeddings to measure the dispersion and variability of model outputs. The
prompts are categorized into three types, i.e., `easy', `moderate', and
`confusing', to generate multiple responses using different LLMs at varying
temperature settings. The responses are transformed into high-dimensional
embeddings via a BERT model and subsequently projected into a two-dimensional
space using Principal Component Analysis (PCA). The Density-Based Spatial
Clustering of Applications with Noise (DBSCAN) algorithm is utilized to cluster
the embeddings and compute the convex hull for each selected cluster. The
experimental results indicate that the uncertainty of the model for LLMs
depends on the prompt complexity, the model, and the temperature setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: Accurate Speech Recognition & Translation without
  Web-Scale Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna C. Puvvada, Piotr Żelasko, He Huang, Oleksii Hrinchuk, Nithin Rao Koluguri, Kunal Dhawan, Somshubra Majumdar, Elena Rastorgueva, Zhehuai Chen, Vitaly Lavrukhin, Jagadeesh Balam, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in speech recognition and translation rely on hundreds of
thousands of hours of Internet speech data. We argue that state-of-the art
accuracy can be reached without relying on web-scale data. Canary -
multilingual ASR and speech translation model, outperforms current
state-of-the-art models - Whisper, OWSM, and Seamless-M4T on English, French,
Spanish, and German languages, while being trained on an order of magnitude
less data than these models. Three key factors enables such data-efficient
model: (1) a FastConformer-based attention encoder-decoder architecture (2)
training on synthetic data generated with machine translation and (3) advanced
training techniques: data-balancing, dynamic data blending, dynamic bucketing
and noise-robust fine-tuning. The model, weights, and training code will be
open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DECOR: Improving Coherence in L2 English Writing with a Novel Benchmark
  for Incoherence Detection, Reasoning, and Rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanming Zhang, Anthony Diaz, Zixun Chen, Qingyang Wu, Kun Qian, Erik Voss, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coherence in writing, an aspect that second-language (L2) English learners
often struggle with, is crucial in assessing L2 English writing. Existing
automated writing evaluation systems primarily use basic surface linguistic
features to detect coherence in writing. However, little effort has been made
to correct the detected incoherence, which could significantly benefit L2
language learners seeking to improve their writing. To bridge this gap, we
introduce DECOR, a novel benchmark that includes expert annotations for
detecting incoherence in L2 English writing, identifying the underlying
reasons, and rewriting the incoherent sentences. To our knowledge, DECOR is the
first coherence assessment dataset specifically designed for improving L2
English writing, featuring pairs of original incoherent sentences alongside
their expert-rewritten counterparts. Additionally, we fine-tuned models to
automatically detect and rewrite incoherence in student essays. We find that
incorporating specific reasons for incoherence during fine-tuning consistently
improves the quality of the rewrites, achieving a result that is favored in
both automatic and human evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, 20 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing and Evaluating Multi-Chatbot Interface for Human-AI
  Communication: Preliminary Findings from a Persuasion Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sion Yoon, Tae Eun Kim, Yoo Jung Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamics of human-AI communication have been reshaped by language models
such as ChatGPT. However, extant research has primarily focused on dyadic
communication, leaving much to be explored regarding the dynamics of human-AI
communication in group settings. The availability of multiple language model
chatbots presents a unique opportunity for scholars to better understand the
interaction between humans and multiple chatbots. This study examines the
impact of multi-chatbot communication in a specific persuasion setting:
promoting charitable donations. We developed an online environment that enables
multi-chatbot communication and conducted a pilot experiment utilizing two
GPT-based chatbots, Save the Children and UNICEF chatbots, to promote
charitable donations. In this study, we present our development process of the
multi-chatbot interface and present preliminary findings from a pilot
experiment. Analysis of qualitative and quantitative feedback are presented,
and limitations are addressed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework
  with Debate-Driven Text Planning for Argument Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Hu, Hou Pong Chan, Jing Li, Yu Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing persuasive arguments is a challenging task for both humans and
machines. It entails incorporating high-level beliefs from various perspectives
on the topic, along with deliberate reasoning and planning to construct a
coherent narrative. Current language models often generate surface tokens
autoregressively, lacking explicit integration of these underlying controls,
resulting in limited output diversity and coherence. In this work, we propose a
persona-based multi-agent framework for argument writing. Inspired by the human
debate, we first assign each agent a persona representing its high-level
beliefs from a unique perspective, and then design an agent interaction process
so that the agents can collaboratively debate and discuss the idea to form an
overall plan for argument writing. Such debate process enables fluid and
nonlinear development of ideas. We evaluate our framework on argumentative
essay writing. The results show that our framework can generate more diverse
and persuasive arguments through both automatic and human evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDT: Dual-Task Adversarial Attacks for Privacy Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Faustini, Shakila Mahjabin Tonni, Annabelle McIver, Qiongkai Xu, Mark Dras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing (NLP) models may leak private information in
different ways, including membership inference, reconstruction or attribute
inference attacks. Sensitive information may not be explicit in the text, but
hidden in underlying writing characteristics. Methods to protect privacy can
involve using representations inside models that are demonstrated not to detect
sensitive attributes or -- for instance, in cases where users might not trust a
model, the sort of scenario of interest here -- changing the raw text before
models can have access to it. The goal is to rewrite text to prevent someone
from inferring a sensitive attribute (e.g. the gender of the author, or their
location by the writing style) whilst keeping the text useful for its original
intention (e.g. the sentiment of a product review). The few works tackling this
have focused on generative techniques. However, these often create extensively
different texts from the original ones or face problems such as mode collapse.
This paper explores a novel adaptation of adversarial attack techniques to
manipulate a text to deceive a classifier w.r.t one task (privacy) whilst
keeping the predictions of another classifier trained for another task
(utility) unchanged. We propose IDT, a method that analyses predictions made by
auxiliary and interpretable models to identify which tokens are important to
change for the privacy task, and which ones should be kept for the utility
task. We evaluate different datasets for NLP suitable for different tasks.
Automatic and human evaluations show that IDT retains the utility of text,
while also outperforming existing methods when deceiving a classifier w.r.t
privacy task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture of In-Context Experts Enhance <span class="highlight-title">LLM</span>s' Long Context Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Lin, Ang Lv, Yuhan Chen, Chen Zhu, Yang Song, Hengshu Zhu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many studies have revealed that large language models (LLMs) exhibit uneven
awareness of different contextual positions.Their limited context awareness can
lead to overlooking critical information and subsequent task failures. While
several approaches have been proposed to enhance LLMs' context awareness,
achieving both effectiveness and efficiency remains challenging.In this paper,
for LLMs utilizing RoPE as position embeddings, we introduce a novel method
called ``Mixture of In-Context Experts'' (MoICE) to address this challenge.
MoICE comprises two key components: a router integrated into each attention
head within LLMs and a lightweight router-only training optimization strategy:
(1) MoICE views each RoPE angle as an `in-context' expert, demonstrated to be
capable of directing the attention of a head to specific contextual positions.
Consequently, each attention head flexibly processes tokens using multiple RoPE
angles dynamically selected by the router to attend to the needed positions.
This approach mitigates the risk of overlooking essential contextual
information. (2) The router-only training strategy entails freezing LLM
parameters and exclusively updating routers for only a few steps. When applied
to open-source LLMs including Llama and Mistral, MoICE surpasses prior methods
across multiple tasks on long context understanding and generation, all while
maintaining commendable inference efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SK-VQA: Synthetic Knowledge Generation at Scale for Training
  Context-Augmented Multimodal <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation has gained significant attention recently for its
utility in training large vision and language models. However, the application
of synthetic data to the training of multimodal context-augmented generation
systems has been relatively unexplored. This gap in existing work is important
because existing vision and language models (VLMs) are not trained specifically
for context-augmented generation. Resources for adapting such models are
therefore crucial for enabling their use in retrieval-augmented generation
(RAG) settings, where a retriever is used to gather relevant information that
is then subsequently provided to a generative model via context augmentation.
To address this challenging problem, we generate SK-VQA: a large synthetic
multimodal dataset containing over 2 million question-answer pairs which
require external knowledge to determine the final answer. Our dataset is both
larger and significantly more diverse than existing resources of its kind,
possessing over 11x more unique questions and containing images from a greater
variety of sources than previously-proposed datasets. Through extensive
experiments, we demonstrate that our synthetic dataset can not only serve as a
challenging benchmark, but is also highly effective for adapting existing
generative multimodal models for context-augmented generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoMix: Automatically Mixing Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12963v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12963v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjal Aggarwal, Aman Madaan, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Manaal Faruqui,  Mausam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are now available from cloud API providers in
various sizes and configurations. While this diversity offers a broad spectrum
of choices, effectively leveraging the options to optimize computational cost
and performance remains challenging. In this work, we present Automix, an
approach that strategically routes queries to larger LMs, based on the
approximate correctness of outputs from a smaller LM. Central to Automix are
two key technical contributions. First, it has a few-shot self-verification
mechanism, which estimates the reliability of its own outputs without requiring
extensive training. Second, given that self-verification can be noisy, it
employs a POMDP based router that can effectively select an appropriately sized
model, based on answer confidence. Experiments across five language models and
five challenging datasets show that Automix consistently surpasses strong
baselines, reducing computational cost by over 50% for comparable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Work started and partly
  done during Aman's internship at Google. This version adds results on
  additional models and datasets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MBIAS: Mitigating Bias in <span class="highlight-title">Large Language Models</span> While Retaining Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11290v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11290v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaina Raza, Ananya Raval, Veronica Chatrath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of Large Language Models (LLMs) in diverse applications
necessitates an assurance of safety without compromising the contextual
integrity of the generated content. Traditional approaches, including
safety-specific fine-tuning or adversarial testing, often yield safe outputs at
the expense of contextual meaning. This can result in a diminished capacity to
handle nuanced aspects of bias and toxicity, such as underrepresentation or
negative portrayals across various demographics. To address these challenges,
we introduce MBIAS, an LLM framework carefully instruction fine-tuned on a
custom dataset designed specifically for safety interventions. MBIAS is
designed to significantly reduce biases and toxic elements in LLM outputs while
preserving the main information. This work also details our further use of
LLMs: as annotator under human supervision and as evaluator of generated
content. Empirical analysis reveals that MBIAS achieves a reduction in bias and
toxicity by over 30\% in standard evaluations, and by more than 90\% in diverse
demographic tests, highlighting the robustness of our approach. We make the
dataset and the fine-tuned model available to the research community for
further investigation and ensure reproducibility. The code for this project can
be accessed here https://github.com/shainarazavi/MBIAS/tree/main.
  Warning: This paper contains examples that may be offensive or upsetting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MK<span class="highlight-title">RAG</span>: Medical Knowledge <span class="highlight-title">Retrieval Augmented Generation</span> for Medical
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Shi, Shaochen Xu, Tianze Yang, Zhengliang Liu, Tianming Liu, Xiang Li, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks like medical question answering (QA).
Moreover, they tend to function as "black-boxes," making it challenging to
modify their behavior. To address the problem, our study delves into retrieval
augmented generation (RAG), aiming to improve LLM responses without the need
for fine-tuning or retraining. Specifically, we propose a comprehensive
retrieval strategy to extract medical facts from an external knowledge base,
and then inject them into the query prompt for LLMs. Focusing on medical QA
using the MedQA-SMILE dataset, we evaluate the impact of different retrieval
models and the number of facts provided to the LLM. Notably, our
retrieval-augmented Vicuna-7B model exhibited an accuracy improvement from
44.46% to 48.54%. This work underscores the potential of RAG to enhance LLM
performance, offering a practical approach to mitigate the challenges of
black-box LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AMIA 2024 Annual Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s and Memorization: On Quality and Specificity of Copyright
  Compliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix B Mueller, Rebekka Görge, Anna K Bernzen, Janna C Pirk, Maximilian Poretschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memorization in large language models (LLMs) is a growing concern. LLMs have
been shown to easily reproduce parts of their training data, including
copyrighted work. This is an important problem to solve, as it may violate
existing copyright laws as well as the European AI Act. In this work, we
propose a systematic analysis to quantify the extent of potential copyright
infringements in LLMs using European law as an example. Unlike previous work,
we evaluate instruction-finetuned models in a realistic end-user scenario. Our
analysis builds on a proposed threshold of 160 characters, which we borrow from
the German Copyright Service Provider Act and a fuzzy text matching algorithm
to identify potentially copyright-infringing textual reproductions. The
specificity of countermeasures against copyright infringement is analyzed by
comparing model behavior on copyrighted and public domain data. We investigate
what behaviors models show instead of producing protected text (such as refusal
or hallucination) and provide a first legal assessment of these behaviors. We
find that there are huge differences in copyright compliance, specificity, and
appropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous
perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing
a particularly low absolute number of potential copyright violations. Code will
be published soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Small and Fast BERT for Chinese Medical Punctuation Restoration <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12568v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12568v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongtao Ling, Yutao Lai, Lei Chen, Shilei Huang, Yi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical dictation, utterances after automatic speech recognition (ASR)
without explicit punctuation marks may lead to the misunderstanding of dictated
reports. To give a precise and understandable clinical report with ASR,
automatic punctuation restoration is required. Considering a practical
scenario, we propose a fast and light pre-trained model for Chinese medical
punctuation restoration based on 'pretraining and fine-tuning' paradigm. In
this work, we distill pre-trained models by incorporating supervised
contrastive learning and a novel auxiliary pre-training task (Punctuation Mark
Prediction) to make it well-suited for punctuation restoration. Our experiments
on various distilled models reveal that our model can achieve 95% performance
while 10% model size relative to state-of-the-art Chinese RoBERTa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, Accepted by INTERSPEECH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Speculative Inference of <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Moshe Wasserblat, Tomer Galanti, Michal Gordon, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating the inference of large language models (LLMs) is an important
challenge in artificial intelligence. This paper introduces distributed
speculative inference (DSI), a novel distributed inference algorithm that is
provably faster than speculative inference (SI) [leviathan2023fast,
chen2023accelerating, miao2023specinfer] and traditional autoregressive
inference (non-SI). Like other SI algorithms, DSI works on frozen LLMs,
requiring no training or architectural modifications, and it preserves the
target distribution.
  Prior studies on SI have demonstrated empirical speedups (compared to non-SI)
but require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs
often do not have matching drafters that are sufficiently fast and accurate. We
show a gap: SI gets slower than non-SI when using slower or less accurate
drafters. We close this gap by proving that DSI is faster than both SI and
non-SI given any drafters. By orchestrating multiple instances of the target
and drafters, DSI is not only faster than SI but also supports LLMs that cannot
be accelerated with SI.
  Our simulations show speedups of off-the-shelf LLMs in realistic settings:
DSI is 1.29-1.92x faster than SI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How well ChatGPT understand Malaysian English? An Evaluation on Named
  Entity Recognition and Relation Extraction <span class="chip">EMNLP
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohan Raj Chanthran, Lay-Ki Soon, Huey Fang Ong, Bhawani Selvaretnam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, ChatGPT has attracted a lot of interest from both researchers and
the general public. While the performance of ChatGPT in named entity
recognition and relation extraction from Standard English texts is
satisfactory, it remains to be seen if it can perform similarly for Malaysian
English. Malaysian English is unique as it exhibits morphosyntactic and
semantical adaptation from local contexts. In this study, we assess ChatGPT's
capability in extracting entities and relations from the Malaysian English News
(MEN) dataset. We propose a three-step methodology referred to as
\textbf{\textit{educate-predict-evaluate}}. The performance of ChatGPT is
assessed using F1-Score across 18 unique prompt settings, which were carefully
engineered for a comprehensive review. From our evaluation, we found that
ChatGPT does not perform well in extracting entities from Malaysian English
news articles, with the highest F1-Score of 0.497. Further analysis shows that
the morphosyntactic adaptation in Malaysian English caused the limitation.
However, interestingly, this morphosyntactic adaptation does not impact the
performance of ChatGPT for relation extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Generation, Evaluation & Metrics (GEM) Workshop at EMNLP
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are <span class="highlight-title">LLM</span>-based Evaluators Confusing NLG Quality Criteria? <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some prior work has shown that LLMs perform well in NLG evaluation for
different tasks. However, we discover that LLMs seem to confuse different
evaluation criteria, which reduces their reliability. For further verification,
we first consider avoiding issues of inconsistent conceptualization and vague
expression in existing NLG quality criteria themselves. So we summarize a clear
hierarchical classification system for 11 common aspects with corresponding
different criteria from previous studies involved. Inspired by behavioral
testing, we elaborately design 18 types of aspect-targeted perturbation attacks
for fine-grained analysis of the evaluation behaviors of different LLMs. We
also conduct human annotations beyond the guidance of the classification system
to validate the impact of the perturbations. Our experimental results reveal
confusion issues inherent in LLMs, as well as other noteworthy phenomena, and
necessitate further research and improvements for LLM-based evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoteChat: A <span class="highlight-title">Dataset</span> of Synthetic Doctor-Patient Conversations
  Conditioned on Clinical Notes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15959v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15959v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wang, Zonghai Yao, Zhichao Yang, Huixue Zhou, Rumeng Li, Xun Wang, Yucheng Xu, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce NoteChat, a novel cooperative multi-agent framework leveraging
Large Language Models (LLMs) to generate patient-physician dialogues. NoteChat
embodies the principle that an ensemble of role-specific LLMs, through
structured role-play and strategic prompting, can perform their assigned roles
more effectively. The synergy among these role-playing LLMs results in a
cohesive and efficient dialogue generation. Evaluation on MTS-dialogue, a
benchmark dataset for patient-physician dialogues-note pairs, shows that models
trained with the augmented synthetic patient-physician dialogues by NoteChat
outperforms other state-of-the-art models for generating clinical notes. Our
comprehensive automatic and human evaluation demonstrates that NoteChat
substantially surpasses state-of-the-art models like ChatGPT and GPT-4 up to
22.78% by domain experts in generating superior synthetic patient-physician
dialogues based on clinical notes. NoteChat has the potential to engage
patients directly and help clinical documentation, a leading cause of physician
burnout.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JMLR: Joint Medical <span class="highlight-title">LLM</span> and Retrieval Training for Enhancing Reasoning
  and Professional Question Answering Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17887v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17887v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated a remarkable potential in
medical knowledge acquisition and question-answering. However, LLMs can
potentially hallucinate and yield factually incorrect outcomes, even with
domain-specific pretraining. Previously, retrieval augmented generation (RAG)
has limited success in addressing hallucinations. Unlike previous methods in
RAG where the retrieval model was trained separately from the LLM, we introduce
JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning
phase. The synchronized training mechanism enhances JMLR's ability to retrieve
clinical guidelines and leverage medical knowledge to reason and answer
questions and reduces the demand for computational resources. We evaluated JMLR
on the important medical question-answering application. Our experimental
results demonstrate that JMLR-13B (70.5%) outperforms a previous
state-of-the-art open-source model using conventional pre-training and
fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical
question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances
reasoning quality and reduces hallucinations better than Claude3-Opus.
Additionally, JMLR-13B (148 GPU hours) also trains much faster than
Meditron-70B (42630 GPU hours). Through this work, we provide a new and
efficient knowledge enhancement method for healthcare, demonstrating the
potential of integrating retrieval and LLM training for medical
question-answering systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentExplainer: Explaining Latent Representations in Deep Generative
  Models with Multi-modal Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14862v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14862v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
LatentExplainer, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
LatentExplainer tackles three main challenges: inferring the meaning of latent
variables, aligning explanations with inductive biases, and handling varying
degrees of explainability. By perturbing latent variables and interpreting
changes in generated data, the framework provides a systematic approach to
understanding and controlling the data generation process, enhancing the
transparency and interpretability of deep generative models. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations of
latent variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Data Augmentation Framework for Low-Resource Multi-Domain
  Dialogue Generation <span class="chip">ECML-PKDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09881v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09881v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkang Liu, Ercong Nie, Shi Feng, Zheng Hua, Zifeng Ding, Daling Wang, Yifei Zhang, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current state-of-the-art dialogue systems heavily rely on extensive training
datasets. However, challenges arise in domains where domain-specific training
datasets are insufficient or entirely absent. To tackle this challenge, we
propose a novel data \textbf{A}ugmentation framework for
\textbf{M}ulti-\textbf{D}omain \textbf{D}ialogue \textbf{G}eneration, referred
to as \textbf{AMD$^2$G}. The AMD$^2$G framework consists of a data augmentation
process and a two-stage training approach: domain-agnostic training and domain
adaptation training. We posit that domain corpora are a blend of
domain-agnostic and domain-specific features, with certain representation
patterns shared among diverse domains. Domain-agnostic training aims to enable
models to learn these common expressive patterns. To construct domain-agnostic
dialogue corpora, we employ a \textit{\textbf{de-domaining}} data processing
technique used to remove domain-specific features. By mitigating the effects of
domain-specific features, the model trained on the de-domained corpora can
effectively learn common expression patterns in different domains.
Subsequently, we adapt the learned domain-agnostic features to the target
domain through domain adaptation training. We conduct experiments on Chinese
dialogue datasets from five different domains and show that AMD$^2$G achieves
superior performance compared to both direct training on the target domain
corpus and collective training on all five domain corpora. Our work underscores
AMD$^2$G as a viable alternative solution for low-resource multi-domain
dialogue generation. Code and data associated with our work are available on
GitHub repository$^{\text 1}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17pages,ECML-PKDD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do prompt positions really matter? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14493v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14493v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Mao, Stuart E. Middleton, Mahesan Niranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based models have gathered a lot of attention from researchers due to
their remarkable advancements in the fields of zero-shot and few-shot learning.
Developing an effective prompt template plays a critical role. However, prior
studies have mainly focused on prompt vocabulary searching or embedding
initialization within a predefined template with the prompt position fixed. In
this empirical study, we conduct the most comprehensive analysis to date of
prompt position for diverse Natural Language Processing (NLP) tasks. Our
findings quantify the substantial impact prompt position has on model
performance. We observe that the prompt positions used in prior studies are
often sub-optimal, and this observation is consistent even in widely used
instruction-tuned models. These findings suggest prompt position optimisation
as a valuable research direction to augment prompt engineering methodologies
and prompt position-aware instruction tuning as a potential way to build more
robust models in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Airport Tower Command Recognition: Integrating
  Squeeze-and-Excitation and Broadcasted Residual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxi Lin, Tonglin Zhou, Yang Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate recognition of aviation commands is vital for flight safety and
efficiency, as pilots must follow air traffic control instructions precisely.
This paper addresses challenges in speech command recognition, such as noisy
environments and limited computational resources, by advancing keyword spotting
technology. We create a dataset of standardized airport tower commands,
including routine and emergency instructions. We enhance broadcasted residual
learning with squeeze-and-excitation and time-frame frequency-wise
squeeze-and-excitation techniques, resulting in our BC-SENet model. This model
focuses on crucial information with fewer parameters. Our tests on five keyword
spotting models, including BC-SENet, demonstrate superior accuracy and
efficiency. These findings highlight the effectiveness of our model
advancements in improving speech command recognition for aviation safety and
efficiency in noisy, high-stakes environments. Additionally, BC-SENet shows
comparable performance on the common Google Speech Command dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IALP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Taktasheva, Maxim Bazhukov, Kirill Koncha, Alena Fenogenova, Ekaterina Artemova, Vladislav Mikhailov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimal pairs are a well-established approach to evaluating the grammatical
knowledge of language models. However, existing resources for minimal pairs
address a limited number of languages and lack diversity of language-specific
grammatical phenomena. This paper introduces the Russian Benchmark of
Linguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that
differ in grammaticality and isolate a morphological, syntactic, or semantic
phenomenon. In contrast to existing benchmarks of linguistic minimal pairs,
RuBLiMP is created by applying linguistic perturbations to automatically
annotated sentences from open text corpora and carefully curating test data. We
describe the data collection protocol and present the results of evaluating 25
language models in various scenarios. We find that the widely used language
models for Russian are sensitive to morphological and agreement-oriented
contrasts but fall behind humans on phenomena requiring understanding of
structural relations, negation, transitivity, and tense. RuBLiMP, the codebase,
and other materials are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in
  <span class="highlight-title">Large Language Models</span> <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping the concept of time is a fundamental facet of human cognition,
indispensable for truly comprehending the intricacies of the world. Previous
studies typically focus on specific aspects of time, lacking a comprehensive
temporal reasoning benchmark. To address this, we propose TimeBench, a
comprehensive hierarchical temporal reasoning benchmark that covers a broad
spectrum of temporal reasoning phenomena. TimeBench provides a thorough
evaluation for investigating the temporal reasoning capabilities of large
language models. We conduct extensive experiments on GPT-4, LLaMA2, and other
popular LLMs under various settings. Our experimental results indicate a
significant performance gap between the state-of-the-art LLMs and humans,
highlighting that there is still a considerable distance to cover in temporal
reasoning. Besides, LLMs exhibit capability discrepancies across different
reasoning categories. Furthermore, we thoroughly analyze the impact of multiple
aspects on temporal reasoning and emphasize the associated challenges. We
aspire for TimeBench to serve as a comprehensive benchmark, fostering research
in temporal reasoning. Resources are available at:
https://github.com/zchuz/TimeBench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A synthetic data approach for domain generalization of NLI models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Javad Hosseini, Andrey Petrov, Alex Fabrikant, Annie Louis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Inference (NLI) remains an important benchmark task for
LLMs. NLI datasets are a springboard for transfer learning to other semantic
tasks, and NLI models are standard tools for identifying the faithfulness of
model-generated text. There are several large scale NLI datasets today, and
models have improved greatly by hill-climbing on these collections. Yet their
realistic performance on out-of-distribution/domain data is less
well-understood. We explore the opportunity for synthetic high-quality datasets
to adapt NLI models for zero-shot use in downstream applications across new and
unseen text domains. We demonstrate a new approach for generating NLI data in
diverse domains and lengths, so far not covered by existing training sets. The
resulting examples have meaningful premises, the hypotheses are formed in
creative ways rather than simple edits to a few premise tokens, and the labels
have high accuracy. We show that models trained on this data ($685$K synthetic
examples) have the best generalization to completely new downstream test
settings. On the TRUE benchmark, a T5-small model trained with our data
improves around $7\%$ on average compared to training on the best alternative
dataset. The improvements are more pronounced for smaller models, while still
meaningful on a T5 XXL model. We also demonstrate gains on test sets when
in-domain training data is augmented with our domain-general synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chitchat as Interference: Adding User Backstories to Task-Oriented
  Dialogues <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15248v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15248v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Stricker, Patrick Paroubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During task-oriented dialogues (TODs), human users naturally introduce
chitchat that is beyond the immediate scope of the task, interfering with the
flow of the conversation. To address this issue without the need for expensive
manual data creation, we use few-shot prompting with Llama-2-70B to enhance the
MultiWOZ dataset with user backstories, a typical example of chitchat
interference in TODs. We assess the impact of this addition by testing two
models: one trained solely on TODs and another trained on TODs with a
preliminary chitchat interaction. Our analysis demonstrates that our enhanced
dataset poses a challenge for these systems. Moreover, we demonstrate that our
dataset can be effectively used for training purposes, enabling a system to
consistently acknowledge the user's backstory while also successfully moving
the task forward in the same turn, as confirmed by human evaluation. These
findings highlight the benefits of generating novel chitchat-TOD scenarios to
test TOD systems more thoroughly and improve their resilience to natural user
interferences
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @ LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MathChat: Converse to Tackle Challenging Math Problems with <span class="highlight-title">LLM</span> Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01337v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01337v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing Large Language Models (LLMs) to address mathematical problems is an
intriguing research endeavor, considering the abundance of math problems
expressed in natural language across numerous science and engineering fields.
LLMs, with their generalized ability, are used as a foundation model to build
AI agents for different tasks. In this paper, we study the effectiveness of
utilizing LLM agents to solve math problems through conversations. We propose
MathChat, a conversational problem-solving framework designed for math
problems. MathChat consists of an LLM agent and a user proxy agent which is
responsible for tool execution and additional guidance. This synergy
facilitates a collaborative problem-solving process, where the agents engage in
a dialogue to solve the problems. We perform evaluation on difficult high
school competition problems from the MATH dataset. Utilizing Python, we show
that MathChat can further improve previous tool-using prompting methods by 6%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Approach to Emotion Detection and Task-Oriented Dialogue
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Stricker, Patrick Paroubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In current text-based task-oriented dialogue (TOD) systems, user emotion
detection (ED) is often overlooked or is typically treated as a separate and
independent task, requiring additional training. In contrast, our work
demonstrates that seamlessly unifying ED and TOD modeling brings about mutual
benefits, and is therefore an alternative to be considered. Our method consists
in augmenting SimpleToD, an end-to-end TOD system, by extending belief state
tracking to include ED, relying on a single language model. We evaluate our
approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ
annotated with emotions. Our results reveal a general increase in performance
for ED and task results. Our findings also indicate that user emotions provide
useful contextual conditioning for system responses, and can be leveraged to
further refine responses in terms of empathy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @ IWSDS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction finetuning (IFT) is critical for aligning Large Language Models
(LLMs) to follow instructions. While many effective IFT datasets have been
introduced recently, they predominantly focus on high-resource languages like
English. To better align LLMs across a broad spectrum of languages and tasks,
we propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,
Multi-turn instruction finetuning dataset, called M2Lingual. It is constructed
by first selecting a diverse set of seed examples and then utilizing the
proposed Evol taxonomy to convert these seeds into complex and challenging
multi-turn instructions. We demonstrate the effectiveness of M2Lingual by
training LLMs of varying sizes and showcasing the enhanced performance across a
diverse set of languages. We contribute the 2 step Evol taxonomy with the
guided generation code: https://github.com/ServiceNow/M2Lingual, as well as the
first fully synthetic, general and task-oriented, multi-turn, multilingual
dataset built with Evol - M2Lingual:
https://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K
total IFT pairs, covering 70 languages and 17+ NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
  Text Embeddings Through Self-Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03216v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03216v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new embedding model, called M3-Embedding, which
is distinguished for its versatility in Multi-Linguality, Multi-Functionality,
and Multi-Granularity. It can support more than 100 working languages, leading
to new state-of-the-art performances on multi-lingual and cross-lingual
retrieval tasks. It can simultaneously perform the three common retrieval
functionalities of embedding model: dense retrieval, multi-vector retrieval,
and sparse retrieval, which provides a unified model foundation for real-world
IR applications. It is able to process inputs of different granularities,
spanning from short sentences to long documents of up to 8192 tokens. The
effective training of M3-Embedding involves the following technical
contributions. We propose a novel self-knowledge distillation approach, where
the relevance scores from different retrieval functionalities can be integrated
as the teacher signal to enhance the training quality. We also optimize the
batching strategy, enabling a large batch size and high training throughput to
ensure the discriminativeness of embeddings. To the best of our knowledge,
M3-Embedding is the first embedding model which realizes such a strong
versatility. The model and code will be publicly available at
https://github.com/FlagOpen/FlagEmbedding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Model</span> Enhanced Clustering for News Event Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adane Nega Tarekegn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The news landscape is continuously evolving, with an ever-increasing volume
of information from around the world. Automated event detection within this
vast data repository is essential for monitoring, identifying, and categorizing
significant news occurrences across diverse platforms. This paper presents an
event detection framework that leverages Large Language Models (LLMs) combined
with clustering analysis to detect news events from the Global Database of
Events, Language, and Tone (GDELT). The framework enhances event clustering
through both pre-event detection tasks (keyword extraction and text embedding)
and post-event detection tasks (event summarization and topic labelling). We
also evaluate the impact of various textual embeddings on the quality of
clustering outcomes, ensuring robust news categorization. Additionally, we
introduce a novel Cluster Stability Assessment Index (CSAI) to assess the
validity and robustness of clustering results. CSAI utilizes multiple feature
vectors to provide a new way of measuring clustering quality. Our experiments
indicate that the use of LLM embedding in the event detection framework has
significantly improved the results, demonstrating greater robustness in terms
of CSAI scores. Moreover, post-event detection tasks generate meaningful
insights, facilitating effective interpretation of event clustering results.
Overall, our experimental results indicate that the proposed framework offers
valuable insights and could enhance the accuracy in news analysis and
reporting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SampleAttention: Near-Lossless Acceleration of Long Context <span class="highlight-title">LLM</span>
  Inference with Adaptive Structured Sparse Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Xiao Chuanfu, Xingcheng Zhang, Dahua Lin, Chao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) now support extremely long context windows, but
the quadratic complexity of vanilla attention results in significantly long
Time-to-First-Token (TTFT) latency. Existing approaches to address this
complexity require additional pretraining or finetuning, and often sacrifice
model accuracy. In this paper, we first provide both theoretical and empirical
foundations for near-lossless sparse attention. We find dynamically capturing
head-specific sparse patterns at runtime with low overhead is crucial. To
address this, we propose SampleAttention, an adaptive structured and
near-lossless sparse attention. Leveraging observed significant sparse
patterns, SampleAttention attends to a fixed percentage of adjacent tokens to
capture local window patterns, and employs a two-stage query-guided key-value
filtering approach, which adaptively select a minimum set of key-values with
low overhead, to capture column stripe patterns. Comprehensive evaluations show
that SampleAttention can seamlessly replace vanilla attention in off-the-shelf
LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$
compared with FlashAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciBench: Evaluating College-Level Scientific Problem-Solving Abilities
  of <span class="highlight-title">Large Language Models</span> <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the existing Large Language Model (LLM) benchmarks on scientific
problem reasoning focus on problems grounded in high-school subjects and are
confined to elementary algebraic operations. To systematically examine the
reasoning capabilities required for solving complex scientific problems, we
introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a
carefully curated dataset featuring a range of collegiate-level scientific
problems from mathematics, chemistry, and physics domains. Based on the
dataset, we conduct an in-depth benchmarking study of representative
open-source and proprietary LLMs with various prompting strategies. The results
reveal that the current LLMs fall short of delivering satisfactory performance,
with the best overall score of merely 43.22%. Furthermore, through a detailed
user study, we categorize the errors made by LLMs into ten problem-solving
abilities. Our analysis indicates that no single prompting strategy
significantly outperforms the others and some strategies that demonstrate
improvements in certain problem-solving skills could result in declines in
other skills. We envision that SciBench will catalyze further developments in
the reasoning abilities of LLMs, thereby ultimately contributing to scientific
research and discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Preference Learning for <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Muldrew, Peter Hayes, Mingtian Zhang, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become more capable, fine-tuning techniques
for aligning with human intent are increasingly important. A key consideration
for aligning these models is how to most effectively use human resources, or
model resources in the case where LLMs themselves are used as oracles.
Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most
prominent example of such a technique, but is complex and often unstable.
Direct Preference Optimization (DPO) has recently been proposed as a simpler
and more stable alternative. In this work, we develop an active learning
strategy for DPO to make better use of preference labels. We propose a
practical acquisition function for prompt/completion pairs based on the
predictive entropy of the language model and a measure of certainty of the
implicit preference model optimized by DPO. We demonstrate how our approach
improves both the rate of learning and final performance of fine-tuning on
pairwise preference data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniGen: A Unified Framework for Textual <span class="highlight-title">Dataset</span> Generation Using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Wu, Yue Huang, Chujie Gao, Dongping Chen, Qihui Zhang, Yao Wan, Tianyi Zhou, Xiangliang Zhang, Jianfeng Gao, Chaowei Xiao, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly
impacted various fields by enabling high-quality synthetic data generation and
reducing dependence on expensive human-generated datasets. Despite this,
challenges remain in the areas of generalization, controllability, diversity,
and truthfulness within the existing generative frameworks. To address these
challenges, this paper presents UniGen, a comprehensive LLM-powered framework
designed to produce diverse, accurate, and highly controllable datasets. UniGen
is adaptable, supporting all types of text datasets and enhancing the
generative process through innovative mechanisms. To augment data diversity,
UniGen incorporates an attribute-guided generation module and a group checking
feature. For accuracy, it employs a code-based mathematical assessment for
label verification alongside a retrieval-augmented generation technique for
factual validation. The framework also allows for user-specified constraints,
enabling customization of the data generation process to suit particular
requirements. Extensive experiments demonstrate the superior quality of data
generated by UniGen, and each module within UniGen plays a critical role in
this enhancement. Additionally, UniGen is applied in two practical scenarios:
benchmarking LLMs and data augmentation. The results indicate that UniGen
effectively supports dynamic and evolving benchmarking, and that data
augmentation improves LLM capabilities in various domains, including
agent-oriented abilities and reasoning skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-aware Data Construction Improves In-context Learning of Language
  Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Štefánik, Marek Kadlčík, Petr Sojka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language models (LMs) are capable of in-context learning (ICL),
manifested in the LMs' ability to perform a new task solely from
natural-language instruction. Previous work curating in-context learners
assumes that ICL emerges from a vast over-parametrization or the scale of
multi-task training. However, recent theoretical work attributes the ICL
ability to concept-dependent training data and creates functional in-context
learners even in small-scale, synthetic settings.
  In this work, we practically explore this newly identified axis of ICL
quality. We propose Concept-aware Training (CoAT), a framework for constructing
training scenarios that make it beneficial for the LM to learn to utilize the
analogical reasoning concepts from demonstrations. We find that by using CoAT,
pre-trained transformers can learn to better utilise new latent concepts from
demonstrations and that such ability makes ICL more robust to the functional
deficiencies of the previous models. Finally, we show that concept-aware
in-context learning is more effective for a majority of new tasks when compared
to traditional instruction tuning, resulting in a performance comparable to the
previous in-context learners using magnitudes of more training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long paper to appear in Findings of ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Logic Tree Extraction for Event Sequence Explanation from <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01124v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01124v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitao Song, Chao Yang, Chaojie Wang, Bo An, Shuang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern high-stakes systems, such as healthcare or robotics, often generate
vast streaming event sequences. Our goal is to design an efficient,
plug-and-play tool to elicit logic tree-based explanations from Large Language
Models (LLMs) to provide customized insights into each observed event sequence.
Built on the temporal point process model for events, our method employs the
likelihood function as a score to evaluate generated logic trees. We propose an
amortized Expectation-Maximization (EM) learning framework and treat the logic
tree as latent variables. In the E-step, we evaluate the posterior distribution
over the latent logic trees using an LLM prior and the likelihood of the
observed event sequences. LLM provides a high-quality prior for the latent
logic trees, however, since the posterior is built over a discrete
combinatorial space, we cannot get the closed-form solution. We propose to
generate logic tree samples from the posterior using a learnable GFlowNet,
which is a diversity-seeking generator for structured discrete variables. The
M-step employs the generated logic rules to approximate marginalization over
the posterior, facilitating the learning of model parameters and refining the
tunable LLM prior parameters. In the online setting, our locally built,
lightweight model will iteratively extract the most relevant rules from LLMs
for each sequence using only a few iterations. Empirical demonstrations
showcase the promising performance and adaptability of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logical Closed Loop: Uncovering Object Hallucinations in Large
  Vision-Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object hallucination has been an Achilles' heel which hinders the broader
applications of large vision-language models (LVLMs). Object hallucination
refers to the phenomenon that the LVLMs claim non-existent objects in the
image. To mitigate the object hallucinations, instruction tuning and external
model-based detection methods have been proposed, which either require
large-scare computational resources or depend on the detection result of
external models. However, there remains an under-explored field to utilize the
LVLM itself to alleviate object hallucinations. In this work, we adopt the
intuition that the LVLM tends to respond logically consistently for existent
objects but inconsistently for hallucinated objects. Therefore, we propose a
Logical Closed Loop-based framework for Object Hallucination Detection and
Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency
probing to raise questions with logical correlations, inquiring about
attributes from objects and vice versa. Whether their responses can form a
logical closed loop serves as an indicator of object hallucination. As a
plug-and-play method, it can be seamlessly applied to all existing LVLMs.
Comprehensive experiments conducted on three benchmarks across four LVLMs have
demonstrated significant improvements brought by our method, indicating its
effectiveness and generality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANLS* -- A Universal Document Processing Metric for Generative Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03848v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03848v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Peer, Philemon Schöpf, Volckmar Nebendahl, Alexander Rietzler, Sebastian Stabinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, discriminative models have been the predominant choice for
tasks like document classification and information extraction. These models
make predictions that fall into a limited number of predefined classes,
facilitating a binary true or false evaluation and enabling the direct
calculation of metrics such as the F1 score. However, recent advancements in
generative large language models (GLLMs) have prompted a shift in the field due
to their enhanced zero-shot capabilities, which eliminate the need for a
downstream dataset and computationally expensive fine-tuning. However,
evaluating GLLMs presents a challenge as the binary true or false evaluation
used for discriminative models is not applicable to the predictions made by
GLLMs.
  This paper introduces a new metric for generative models called ANLS* for
evaluating a wide variety of tasks, including information extraction and
classification tasks. The ANLS* metric extends existing ANLS metrics as a
drop-in-replacement and is still compatible with previously reported ANLS
scores. An evaluation of 7 different datasets, and more than 10 different GLLMs
together with 3 different prompting methods using the ANLS* metric is also
provided, demonstrating the importance of the proposed metric.
  We also benchmark a novel approach to generate prompts for documents, called
SFT, against other prompting techniques such as LATIN. In 6 out of 7 cases, SFT
outperforms other techniques and improves the state-of-the-art, sometimes by as
much as $10$ percentage points.
  Sources are available at https://github.com/deepopinion/anls_star_metric
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does Geo-co-location Matter? A Case Study of Public Health Conversations
  during COVID-19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paiheng Xu, Louiqa Raschid, Vanessa Frias-Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms like Twitter (now X) have been pivotal in information
dissemination and public engagement, especially during COVID-19. A key goal for
public health experts was to encourage prosocial behavior that could impact
local outcomes such as masking and social distancing. Given the importance of
local news and guidance during COVID-19, the objective of our research is to
analyze the effect of localized engagement, on social media conversations. This
study examines the impact of geographic co-location, as a proxy for localized
engagement between public health experts (PHEs) and the public, on social
media. We analyze a Twitter conversation dataset from January 2020 to November
2021, comprising over 19 K tweets from nearly five hundred PHEs, along with
approximately 800 K replies from 350 K participants. Our findings reveal that
geo-co-location is associated with higher engagement rates, especially in
conversations on topics including masking, lockdowns, and education, and in
conversations with academic and medical professionals. Lexical features
associated with emotion and personal experiences were more common in
geo-co-located contexts. This research provides insights into how geographic
co-location influences social media engagement and can inform strategies to
improve public health messaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Apollo: A Lightweight Multilingual Medical <span class="highlight-title">LLM</span> towards Democratizing
  Medical AI to 6B People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the vast repository of global medical knowledge predominantly being
in English, local languages are crucial for delivering tailored healthcare
services, particularly in areas with limited medical resources. To extend the
reach of medical AI advancements to a broader population, we aim to develop
medical LLMs across the six most widely spoken languages, encompassing a global
population of 6.1 billion. This effort culminates in the creation of the
ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the
multilingual medical benchmark, the released Apollo models, at various
relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best
performance among models of equivalent size. Especially, Apollo-7B is the
state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite
models could be used to improve the multi-lingual medical capabilities of
larger models without fine-tuning in a proxy-tuning fashion. We will
open-source training corpora, code, model weights and evaluation benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeAligner: Safety Alignment against Jailbreak Attacks via Response
  Disparity Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the development of large language models (LLMs) rapidly advances, securing
these models effectively without compromising their utility has become a
pivotal area of research. However, current defense strategies against jailbreak
attacks (i.e., efforts to bypass security protocols) often suffer from limited
adaptability, restricted general capability, and high cost. To address these
challenges, we introduce SafeAligner, a methodology implemented at the decoding
stage to fortify defenses against jailbreak attacks. We begin by developing two
specialized models: the Sentinel Model, which is trained to foster safety, and
the Intruder Model, designed to generate riskier responses. SafeAligner
leverages the disparity in security levels between the responses from these
models to differentiate between harmful and beneficial tokens, effectively
guiding the safety alignment by altering the output token distribution of the
target model. Extensive experiments show that SafeAligner can increase the
likelihood of beneficial tokens, while reducing the occurrence of harmful ones,
thereby ensuring secure alignment with minimal loss to generality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowVQA: Mapping Multimodal Logic in Visual Question Answering with
  Flowcharts <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for visual question answering lack in visual grounding
and complexity, particularly in evaluating spatial reasoning skills. We
introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of
visual question-answering multimodal language models in reasoning with
flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and
human-verified flowchart images from three distinct content sources, along with
22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,
including information localization, decision-making, and logical progression.
We conduct a thorough baseline evaluation on a suite of both open-source and
proprietary multimodal language models using various strategies, followed by an
analysis of directional bias. The results underscore the benchmark's potential
as a vital tool for advancing the field of multimodal modeling, providing a
focused and challenging environment for enhancing model performance in visual
and logical reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WellDunn: On the Robustness and Explainability of Language Models and
  <span class="highlight-title">Large Language Models</span> in Identifying Wellness Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12058v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12058v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedali Mohammadi, Edward Raff, Jinendra Malekar, Vedant Palit, Francis Ferraro, Manas Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) are being proposed for mental health applications where
the heightened risk of adverse outcomes means predictive performance may not be
a sufficient litmus test of a model's utility in clinical practice. A model
that can be trusted for practice should have a correspondence between
explanation and clinical determination, yet no prior research has examined the
attention fidelity of these models and their effect on ground truth
explanations. We introduce an evaluation design that focuses on the robustness
and explainability of LMs in identifying Wellness Dimensions (WD). We focus on
two mental health and well-being datasets: (a) Multi-label Classification-based
MultiWD, and (b) WellXplain for evaluating attention mechanism veracity against
expert-labeled explanations. The labels are based on Halbert Dunn's theory of
wellness, which gives grounding to our evaluation. We reveal four surprising
results about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4
lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM fails to deliver any
remarkable improvements in performance or explanations. (2) Re-examining LMs'
predictions based on a confidence-oriented loss function reveals a significant
performance drop. (3) Across all LMs/LLMs, the alignment between attention and
explanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental
health-specific LMs/LLMs overlook domain-specific knowledge and undervalue
explanations, causing these discrepancies. This study highlights the need for
further research into their consistency and explanations in mental health and
well-being.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, including reference and appendix sections, 8 figures, and
  16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Hospital: Benchmarking <span class="highlight-title">Large Language Models</span> in a Multi-agent Medical
  Interaction Simulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09742v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09742v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has significantly advanced healthcare, particularly
through large language models (LLMs) that excel in medical question answering
benchmarks. However, their real-world clinical application remains limited due
to the complexities of doctor-patient interactions. To address this, we
introduce \textbf{AI Hospital}, a multi-agent framework simulating dynamic
medical interactions between \emph{Doctor} as player and NPCs including
\emph{Patient}, \emph{Examiner}, \emph{Chief Physician}. This setup allows for
realistic assessments of LLMs in clinical scenarios. We develop the Multi-View
Medical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical
records and NPCs to evaluate LLMs' performance in symptom collection,
examination recommendations, and diagnoses. Additionally, a dispute resolution
collaborative mechanism is proposed to enhance diagnostic accuracy through
iterative discussions. Despite improvements, current LLMs exhibit significant
performance gaps in multi-turn interactions compared to one-step approaches.
Our findings highlight the need for further research to bridge these gaps and
improve LLMs' clinical diagnostic capabilities. Our data, code, and
experimental results are all open-sourced at
\url{https://github.com/LibertFan/AI_Hospital}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/LibertFan/AI_Hospital</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating <span class="highlight-title">LLM</span> Ethics: Advancements, Challenges, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Jiao, Saleh Afroogh, Yiming Xu, Connor Phillips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses ethical issues surrounding Large Language Models (LLMs)
within the field of artificial intelligence. It explores the common ethical
challenges posed by both LLMs and other AI systems, such as privacy and
fairness, as well as ethical challenges uniquely arising from LLMs. It
highlights challenges such as hallucination, verifiable accountability, and
decoding censorship complexity, which are unique to LLMs and distinct from
those encountered in traditional AI systems. The study underscores the need to
tackle these complexities to ensure accountability, reduce biases, and enhance
transparency in the influential role that LLMs play in shaping information
dissemination. It proposes mitigation strategies and future directions for LLM
ethics, advocating for interdisciplinary collaboration. It recommends ethical
frameworks tailored to specific domains and dynamic auditing systems adapted to
diverse contexts. This roadmap aims to guide responsible development and
integration of LLMs, envisioning a future where ethical considerations govern
AI advancements in society.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The global landscape of academic guidelines for generative AI and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit Dhurandhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Generative Artificial Intelligence (GAI) and Large
Language Models (LLMs) in academia has spurred a global discourse on their
potential pedagogical benefits and ethical considerations. Positive reactions
highlight some potential, such as collaborative creativity, increased access to
education, and empowerment of trainers and trainees. However, negative
reactions raise concerns about ethical complexities, balancing innovation and
academic integrity, unequal access, and misinformation risks. Through a
systematic survey and text-mining-based analysis of global and national
directives, insights from independent research, and eighty university-level
guidelines, this study provides a nuanced understanding of the opportunities
and challenges posed by GAI and LLMs in education. It emphasizes the importance
of balanced approaches that harness the benefits of these technologies while
addressing ethical considerations and ensuring equitable access and educational
outcomes. The paper concludes with recommendations for fostering responsible
innovation and ethical practices to guide the integration of GAI and LLMs in
academia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Augmentation using <span class="highlight-title">LLM</span>s: Data Perspectives, Learning Paradigms and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02990v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02990v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of large language models (LLMs), data
augmentation (DA) has emerged as a pivotal technique for enhancing model
performance by diversifying training examples without the need for additional
data collection. This survey explores the transformative impact of LLMs on DA,
particularly addressing the unique challenges and opportunities they present in
the context of natural language processing (NLP) and beyond. From both data and
learning perspectives, we examine various strategies that utilize LLMs for data
augmentation, including a novel exploration of learning paradigms where
LLM-generated data is used for diverse forms of further training. Additionally,
this paper highlights the primary open challenges faced in this domain, ranging
from controllable data augmentation to multi-modal data augmentation. This
survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve
as a comprehensive guide for researchers and practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIntRec2.0: A Large-scale Benchmark <span class="highlight-title">Dataset</span> for Multimodal Intent
  Recognition and Out-of-scope Detection in Conversations <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10943v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10943v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su, jinyue Zhao, Wenrui Li, Yanting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal intent recognition poses significant challenges, requiring the
incorporation of non-verbal modalities from real-world contexts to enhance the
comprehension of human intentions. Existing benchmark datasets are limited in
scale and suffer from difficulties in handling out-of-scope samples that arise
in multi-turn conversational interactions. We introduce MIntRec2.0, a
large-scale benchmark dataset for multimodal intent recognition in multi-party
conversations. It contains 1,245 dialogues with 15,040 samples, each annotated
within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope
samples, it also includes 5,736 out-of-scope samples appearing in multi-turn
contexts, which naturally occur in real-world scenarios. Furthermore, we
provide comprehensive information on the speakers in each utterance, enriching
its utility for multi-party conversational research. We establish a general
framework supporting the organization of single-turn and multi-turn dialogue
data, modality feature extraction, multimodal fusion, as well as in-scope
classification and out-of-scope detection. Evaluation benchmarks are built
using classic multimodal fusion methods, ChatGPT, and human evaluators. While
existing methods incorporating nonverbal information yield improvements,
effectively leveraging context information and detecting out-of-scope samples
remains a substantial challenge. Notably, large language models exhibit a
significant performance gap compared to humans, highlighting the limitations of
machine learning methods in the cognitive intent understanding task. We believe
that MIntRec2.0 will serve as a valuable resource, providing a pioneering
foundation for research in human-machine conversational interactions, and
significantly facilitating related applications. The full dataset and codes are
available at https://github.com/thuiar/MIntRec2.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024, Long Paper; The abstract is slightly modified
  due to the length limitation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting Explicit and Implicit Knowledge for Multi-hop Question
  Answering Based on Human Reading Process <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19350v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19350v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to
simulate human reasoning and inference processes, achieving proficient
performance in multi-hop QA. However, a gap persists between PLMs' reasoning
abilities and those of humans when tackling complex problems. Psychological
studies suggest a vital connection between explicit information in passages and
human prior knowledge during reading. Nevertheless, current research has given
insufficient attention to linking input passages and PLMs' pre-training-based
knowledge from the perspective of human cognition studies. In this study, we
introduce a Prompting Explicit and Implicit knowledge (PEI) framework, which
uses prompts to connect explicit and implicit knowledge, aligning with human
reading process for multi-hop QA. We consider the input passages as explicit
knowledge, employing them to elicit implicit knowledge through unified prompt
reasoning. Furthermore, our model incorporates type-specific reasoning via
prompts, a form of implicit knowledge. Experimental results show that PEI
performs comparably to the state-of-the-art on HotpotQA. Ablation studies
confirm the efficacy of our model in bridging and integrating explicit and
implicit knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Topic Models with Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Garima Dhanania, Sheshera Mysore, Chau Minh Pham, Mohit Iyyer, Hamed Zamani, Andrew McCallum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic models are widely used to analyze document collections. While they are
valuable for discovering latent topics in a corpus when analysts are unfamiliar
with the corpus, analysts also commonly start with an understanding of the
content present in a corpus. This may be through categories obtained from an
initial pass over the corpus or a desire to analyze the corpus through a
predefined set of categories derived from a high level theoretical framework
(e.g. political ideology). In these scenarios analysts desire a topic modeling
approach which incorporates their understanding of the corpus while supporting
various forms of interaction with the model. In this work, we present EdTM, as
an approach for label name supervised topic modeling. EdTM models topic
modeling as an assignment problem while leveraging LM/LLM based document-topic
affinities and using optimal transport for making globally coherent
topic-assignments. In experiments, we show the efficacy of our framework
compared to few-shot LLM classifiers, and topic models based on clustering and
LDA. Further, we show EdTM's ability to incorporate various forms of analyst
feedback and while remaining robust to noisy analyst inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print; Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rateless Stochastic Coding for Delay-constrained Semantic Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Peng, Rulong Wang, Yong Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of joint source-channel coding with distortion and
perception constraints from a rateless perspective, the purpose of which is to
settle the balance between reliability (distortion/perception) and
effectiveness (rate) of transmission over uncertain channels. We find a new
finite-blocklength bound for the achievable joint source-channel code rate with
the above two constraints. To achieve a superior rateless characteristic of
JSCC coding, we perform multi-level optimization on various finite-blocklength
codes. Based on these two, we then propose a new JSCC coding scheme called
rateless stochastic coding (RSC). We experimentally demonstrate that the
proposed RSC can achieve variable rates of transmission maintaining an
excellent trade-off between distortion and perception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case
  Reformulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlong Deng, Kelong Mao, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal case retrieval for sourcing similar cases is critical in upholding
judicial fairness. Different from general web search, legal case retrieval
involves processing lengthy, complex, and highly specialized legal documents.
Existing methods in this domain often overlook the incorporation of legal
expert knowledge, which is crucial for accurately understanding and modeling
legal cases, leading to unsatisfactory retrieval performance. This paper
introduces KELLER, a legal knowledge-guided case reformulation approach based
on large language models (LLMs) for effective and interpretable legal case
retrieval. By incorporating professional legal knowledge about crimes and law
articles, we enable large language models to accurately reformulate the
original legal case into concise sub-facts of crimes, which contain the
essential information of the case. Extensive experiments on two legal case
retrieval benchmarks demonstrate superior retrieval performance and robustness
on complex legal case queries of KELLER over existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doc2Token: Bridging Vocabulary Gap by Predicting Missing Tokens for
  E-commerce Search <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaihao Li, Juexin Lin, Tony Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the "vocabulary mismatch" issue in information retrieval is a
central challenge for e-commerce search engines, because product pages often
miss important keywords that customers search for. Doc2Query[1] is a popular
document-expansion technique that predicts search queries for a document and
includes the predicted queries with the document for retrieval. However, this
approach can be inefficient for e-commerce search, because the predicted query
tokens are often already present in the document. In this paper, we propose
Doc2Token, a technique that predicts relevant tokens (instead of queries) that
are missing from the document and includes these tokens in the document for
retrieval. For the task of predicting missing tokens, we introduce a new
metric, "novel ROUGE score". Doc2Token is demonstrated to be superior to
Doc2Query in terms of novel ROUGE score and diversity of predictions. Doc2Token
also exhibits efficiency gains by reducing both training and inference times.
We deployed the feature to production and observed significant revenue gain in
an online A/B test, and launched the feature to full traffic on Walmart.com.
  [1] R. Nogueira, W. Yang, J. Lin, K. Cho, Document expansion by query
prediction, arXiv preprint arXiv:1904.08375 (2019)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure, SIGIR 2024 Workshop on eCommerce</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEO: Generative Engine Optimization <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has ushered in a new paradigm of
search engines that use generative models to gather and summarize information
to answer user queries. This emerging technology, which we formalize under the
unified framework of generative engines (GEs), can generate accurate and
personalized responses, rapidly replacing traditional search engines like
Google and Bing. Generative Engines typically satisfy queries by synthesizing
information from multiple sources and summarizing them using LLMs. While this
shift significantly improves $\textit{user}$ utility and $\textit{generative
search engine}$ traffic, it poses a huge challenge for the third stakeholder --
website and content creators. Given the black-box and fast-moving nature of
generative engines, content creators have little to no control over
$\textit{when}$ and $\textit{how}$ their content is displayed. With generative
engines here to stay, we must ensure the creator economy is not disadvantaged.
To address this, we introduce Generative Engine Optimization (GEO), the first
novel paradigm to aid content creators in improving their content visibility in
generative engine responses through a flexible black-box optimization framework
for optimizing and defining visibility metrics. We facilitate systematic
evaluation by introducing GEO-bench, a large-scale benchmark of diverse user
queries across multiple domains, along with relevant web sources to answer
these queries. Through rigorous evaluation, we demonstrate that GEO can boost
visibility by up to $40\%$ in generative engine responses. Moreover, we show
the efficacy of these strategies varies across domains, underscoring the need
for domain-specific optimization methods. Our work opens a new frontier in
information discovery systems, with profound implications for both developers
of generative engines and content creators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JMLR: Joint Medical <span class="highlight-title">LLM</span> and Retrieval Training for Enhancing Reasoning
  and Professional Question Answering Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17887v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17887v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated a remarkable potential in
medical knowledge acquisition and question-answering. However, LLMs can
potentially hallucinate and yield factually incorrect outcomes, even with
domain-specific pretraining. Previously, retrieval augmented generation (RAG)
has limited success in addressing hallucinations. Unlike previous methods in
RAG where the retrieval model was trained separately from the LLM, we introduce
JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning
phase. The synchronized training mechanism enhances JMLR's ability to retrieve
clinical guidelines and leverage medical knowledge to reason and answer
questions and reduces the demand for computational resources. We evaluated JMLR
on the important medical question-answering application. Our experimental
results demonstrate that JMLR-13B (70.5%) outperforms a previous
state-of-the-art open-source model using conventional pre-training and
fine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical
question-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances
reasoning quality and reduces hallucinations better than Claude3-Opus.
Additionally, JMLR-13B (148 GPU hours) also trains much faster than
Meditron-70B (42630 GPU hours). Through this work, we provide a new and
efficient knowledge enhancement method for healthcare, demonstrating the
potential of integrating retrieval and LLM training for medical
question-answering systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transparency, Privacy, and Fairness in Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Kowald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become a pervasive part of our daily online
experience, and are one of the most widely used applications of artificial
intelligence and machine learning. Therefore, regulations and requirements for
trustworthy artificial intelligence, for example, the European AI Act, which
includes notions such as transparency, privacy, and fairness are also highly
relevant for the design of recommender systems in practice. This habilitation
elaborates on aspects related to these three notions in the light of
recommender systems, namely: (i) transparency and cognitive models, (ii)
privacy and limited preference information, and (iii) fairness and popularity
bias in recommender systems. Specifically, with respect to aspect (i), we
highlight the usefulness of incorporating psychological theories for a
transparent design process of recommender systems. We term this type of systems
psychology-informed recommender systems. In aspect (ii), we study and address
the trade-off between accuracy and privacy in differentially-private
recommendations. We design a novel recommendation approach for collaborative
filtering based on an efficient neighborhood reuse concept, which reduces the
number of users that need to be protected with differential privacy.
Furthermore, we address the related issue of limited availability of user
preference information, e.g., click data, in the settings of session-based and
cold-start recommendations. With respect to aspect (iii), we analyze popularity
bias in recommender systems. We find that the recommendation frequency of an
item is positively correlated with this item's popularity. This also leads to
the unfair treatment of users with little interest in popular content. Finally,
we study long-term fairness dynamics in algorithmic decision support in the
labor market using agent-based modeling techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Habilitation (post-doctoral thesis) at Graz University of Technology
  for the scientific subject "Applied Computer Science" (accepted in June 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowVQA: Mapping Multimodal Logic in Visual Question Answering with
  Flowcharts <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for visual question answering lack in visual grounding
and complexity, particularly in evaluating spatial reasoning skills. We
introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of
visual question-answering multimodal language models in reasoning with
flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and
human-verified flowchart images from three distinct content sources, along with
22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,
including information localization, decision-making, and logical progression.
We conduct a thorough baseline evaluation on a suite of both open-source and
proprietary multimodal language models using various strategies, followed by an
analysis of directional bias. The results underscore the benchmark's potential
as a vital tool for advancing the field of multimodal modeling, providing a
focused and challenging environment for enhancing model performance in visual
and logical reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language
  Models for Adaptable Conversational Task Assistants <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Fischer, Carlos Gemmell, Niklas Tecklenburg, Iain Mackie, Federico Rossetto, Jeffrey Dalton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of building real-world multimodal assistants for
complex real-world tasks. We describe the practicalities and challenges of
developing and deploying GRILLBot, a leading (first and second prize winning in
2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building
on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture
that leverages Large Language Models (LLMs) and specialised models tuned for
specific subtasks requiring very low latency. OAT allows us to define when, how
and which LLMs should be used in a structured and deployable manner. For
knowledge-grounded question answering and live task adaptations, we show that
LLM reasoning abilities over task context and world knowledge outweigh latency
concerns. For dialogue state management, we implement a code generation
approach and show that specialised smaller models have 84% effectiveness with
100x lower latency. Overall, we provide insights and discuss tradeoffs for
deploying both traditional models and LLMs to users in complex real-world
multimodal environments in the Alexa TaskBot challenge. These experiences will
continue to evolve as LLMs become more capable and efficient -- fundamentally
reshaping OAT and future assistant architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, KDD Preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-27T00:00:00Z">2024-06-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">136</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Data and Transformers for Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating ambient sounds and effects is a challenging problem due to data
scarcity and often insufficient caption quality, making it difficult to employ
large-scale generative models for the task. In this work, we tackle the problem
by introducing two new models. First, we propose AutoCap, a high-quality and
efficient automatic audio captioning model. We show that by leveraging metadata
available with the audio modality, we can substantially improve the quality of
captions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from
the best available captioning model at four times faster inference speed. We
then use AutoCap to caption clips from existing datasets, obtaining 761,000
audio clips with high-quality captions, forming the largest available
audio-text dataset. Second, we propose GenAu, a scalable transformer-based
audio generation architecture that we scale up to 1.25B parameters and train
with our new dataset. When compared to state-of-the-art audio generators, GenAu
obtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%
in CLAP score, indicating significantly improved quality of generated audio
compared to previous works. This shows that the quality of data is often as
important as its quantity. Besides, since AutoCap is fully automatic, new audio
samples can be added to the training dataset, unlocking the training of even
larger generative models for audio synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://snap-research.github.io/GenAU/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Remarkable Robustness of <span class="highlight-title">LLM</span>s: Stages of Inference? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedang Lad, Wes Gurnee, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate and investigate the remarkable robustness of Large Language
Models by deleting and swapping adjacent layers. We find that deleting and
swapping interventions retain 72-95\% of the original model's prediction
accuracy without fine-tuning, whereas models with more layers exhibit more
robustness. Based on the results of the layer-wise intervention and further
experiments, we hypothesize the existence of four universal stages of inference
across eight different models: detokenization, feature engineering, prediction
ensembling, and residual sharpening. The first stage integrates local
information, lifting raw token representations into higher-level contextual
representations. Next is the iterative refinement of task and entity-specific
features. Then, the second half of the model begins with a phase transition,
where hidden representations align more with the vocabulary space due to
specialized model components. Finally, the last layer sharpens the following
token distribution by eliminating obsolete features that add noise to the
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suri: Multi-constraint Instruction Following for Long-form Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chau Minh Pham, Simeng Sun, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research on instruction following largely focuses on tasks with
simple instructions and short responses. In this work, we explore
multi-constraint instruction following for generating long-form text. We create
Suri, a dataset with 20K human-written long-form texts paired with
LLM-generated backtranslated instructions that contain multiple complex
constraints. Because of prohibitive challenges associated with collecting human
preference judgments on long-form texts, preference-tuning algorithms such as
DPO are infeasible in our setting; thus, we propose Instructional ORPO
(I-ORPO), an alignment method based on the ORPO algorithm. Instead of receiving
negative feedback from dispreferred responses, I-ORPO obtains negative feedback
from synthetically corrupted instructions generated by an LLM. Using Suri, we
perform supervised and I-ORPO fine-tuning on Mistral-7b-Instruct-v0.2. The
resulting models, Suri-SFT and Suri-I-ORPO, generate significantly longer texts
(~5K tokens) than base models without significant quality deterioration. Our
human evaluation shows that while both SFT and I-ORPO models satisfy most
constraints, Suri-I-ORPO generations are generally preferred for their coherent
and informative incorporation of the constraints. We release our code at
https://github.com/chtmp223/suri.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Model Arena for Cross-lingual Sentiment Analysis: A Comparative
  Study in the Era of <span class="highlight-title">Large Language Models</span> <span class="chip">WASSA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiliang Zhu, Shayna Gardiner, Tere Roldán, David Rossouw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentiment analysis serves as a pivotal component in Natural Language
Processing (NLP). Advancements in multilingual pre-trained models such as XLM-R
and mT5 have contributed to the increasing interest in cross-lingual sentiment
analysis. The recent emergence in Large Language Models (LLM) has significantly
advanced general NLP tasks, however, the capability of such LLMs in
cross-lingual sentiment analysis has not been fully studied. This work
undertakes an empirical analysis to compare the cross-lingual transfer
capability of public Small Multilingual Language Models (SMLM) like XLM-R,
against English-centric LLMs such as Llama-3, in the context of sentiment
analysis across English, Spanish, French and Chinese. Our findings reveal that
among public models, SMLMs exhibit superior zero-shot cross-lingual performance
relative to LLMs. However, in few-shot cross-lingual settings, public LLMs
demonstrate an enhanced adaptive potential. In addition, we observe that
proprietary GPT-3.5 and GPT-4 lead in zero-shot cross-lingual capability, but
are outpaced by public models in few-shot scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WASSA workshop at ACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiVERT: Distractor Generation with Variational Errors Represented as
  Text for Math Multiple-choice Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel Fernandez, Alexander Scarlatos, Simon Woodhead, Andrew Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality distractors are crucial to both the assessment and pedagogical
value of multiple-choice questions (MCQs), where manually crafting ones that
anticipate knowledge deficiencies or misconceptions among real students is
difficult. Meanwhile, automated distractor generation, even with the help of
large language models (LLMs), remains challenging for subjects like math. It is
crucial to not only identify plausible distractors but also understand the
error behind them. In this paper, we introduce DiVERT (Distractor Generation
with Variational Errors Represented as Text), a novel variational approach that
learns an interpretable representation of errors behind distractors in math
MCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions
used by hundreds of thousands of students, we show that DiVERT, despite using a
base open-source LLM with 7B parameters, outperforms state-of-the-art
approaches using GPT-4o on downstream distractor generation. We also conduct a
human evaluation with math educators and find that DiVERT leads to error labels
that are of comparable quality to human-authored ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundamental Problems With Model Editing: How Should Rational Belief
  Revision Work in <span class="highlight-title">LLM</span>s? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The model editing problem concerns how language models should learn new facts
about the world over time. While empirical research on model editing has drawn
widespread attention, the conceptual foundations of model editing remain shaky
-- perhaps unsurprisingly, since model editing is essentially belief revision,
a storied problem in philosophy that has eluded succinct solutions for decades.
Model editing nonetheless demands a solution, since we need to be able to
control the knowledge within language models. With this goal in mind, this
paper critiques the standard formulation of the model editing problem and
proposes a formal testbed for model editing research. We first describe 12 open
problems with model editing, based on challenges with (1) defining the problem,
(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the
first place. Many of these challenges are extremely difficult to address, e.g.
determining far-reaching consequences of edits, labeling probabilistic
entailments between facts, and updating beliefs of agent simulators. Next, we
introduce a semi-synthetic dataset for model editing based on Wikidata, where
we can evaluate edits against labels given by an idealized Bayesian agent. This
enables us to say exactly how belief revision in language models falls short of
a desirable epistemic standard. We encourage further research exploring
settings where such a gold standard can be compared against. Our code is
publicly available at: https://github.com/peterbhase/LLM-belief-revision
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndoToxic2024: A Demographically-Enriched <span class="highlight-title">Dataset</span> of Hate Speech and
  Toxicity Types for Indonesian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucky Susanto, Musa Izzanardi Wijanarko, Prasetia Anugrah Pratama, Traci Hong, Ika Idris, Alham Fikri Aji, Derry Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech poses a significant threat to social harmony. Over the past two
years, Indonesia has seen a ten-fold increase in the online hate speech ratio,
underscoring the urgent need for effective detection mechanisms. However,
progress is hindered by the limited availability of labeled data for Indonesian
texts. The condition is even worse for marginalized minorities, such as Shia,
LGBTQ, and other ethnic minorities because hate speech is underreported and
less understood by detection tools. Furthermore, the lack of accommodation for
subjectivity in current datasets compounds this issue. To address this, we
introduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity
classification dataset. Comprising 43,692 entries annotated by 19 diverse
individuals, the dataset focuses on texts targeting vulnerable groups in
Indonesia, specifically during the hottest political event in the country: the
presidential election. We establish baselines for seven binary classification
tasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)
fine-tuned for hate speech classification. Furthermore, we demonstrate how
incorporating demographic information can enhance the zero-shot performance of
the large language model, gpt-3.5-turbo. However, we also caution that an
overemphasis on demographic information can negatively impact the fine-tuned
model performance due to data fragmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jump Starting Bandits with <span class="highlight-title">LLM</span>-Generated Prior Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parand A. Alamdari, Yanshuai Cao, Kevin H. Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present substantial evidence demonstrating the benefits of integrating
Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.
Contextual bandits have been widely used in recommendation systems to generate
personalized suggestions based on user-specific contexts. We show that LLMs,
pre-trained on extensive corpora rich in human knowledge and preferences, can
simulate human behaviours well enough to jump-start contextual multi-armed
bandits to reduce online learning regret. We propose an initialization
algorithm for contextual bandits by prompting LLMs to produce a pre-training
dataset of approximate human preferences for the bandit. This significantly
reduces online learning regret and data-gathering costs for training such
models. Our approach is validated empirically through two sets of experiments
with different bandit setups: one which utilizes LLMs to serve as an oracle and
a real-world experiment utilizing data from a conjoint survey experiment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> LiveBench: A Challenging, Contamination-Free <span class="highlight-title">LLM</span> Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, <span class="highlight-author">Yann LeCun</span>, Tom Goldstein, Willie Neiswanger, Micah Goldblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test set contamination, wherein test data from a benchmark ends up in a newer
model's training set, is a well-documented obstacle for fair LLM evaluation and
can quickly render benchmarks obsolete. To mitigate this, many recent
benchmarks crowdsource new prompts and evaluations from human or LLM judges;
however, these can introduce significant biases, and break down when scoring
hard questions. In this work, we introduce a new benchmark for LLMs designed to
be immune to both test set contamination and the pitfalls of LLM judging and
human crowdsourcing. We release LiveBench, the first benchmark that (1)
contains frequently-updated questions from recent information sources, (2)
scores answers automatically according to objective ground-truth values, and
(3) contains a wide variety of challenging tasks, spanning math, coding,
reasoning, language, instruction following, and data analysis. To achieve this,
LiveBench contains questions that are based on recently-released math
competitions, arXiv papers, news articles, and datasets, and it contains
harder, contamination-free versions of tasks from previous benchmarks such as
Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source
models, as well as dozens of open-source models ranging from 0.5B to 110B in
size. LiveBench is difficult, with top models achieving below 65% accuracy. We
release all questions, code, and model answers. Questions will be added and
updated on a monthly basis, and we will release new tasks and harder versions
of tasks over time so that LiveBench can distinguish between the capabilities
of LLMs as they improve in the future. We welcome community engagement and
collaboration for expanding the benchmark tasks and models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Odyssey of Commonsense Causality: From Foundational Benchmarks to
  Cutting-Edge Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaobo Cui, Zhijing Jin, Bernhard Schölkopf, Boi Faltings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding commonsense causality is a unique mark of intelligence for
humans. It helps people understand the principles of the real world better and
benefits the decision-making process related to causation. For instance,
commonsense causality is crucial in judging whether a defendant's action causes
the plaintiff's loss in determining legal liability. Despite its significance,
a systematic exploration of this topic is notably lacking. Our comprehensive
survey bridges this gap by focusing on taxonomies, benchmarks, acquisition
methods, qualitative reasoning, and quantitative measurements in commonsense
causality, synthesizing insights from over 200 representative articles. Our
work aims to provide a systematic overview, update scholars on recent
advancements, provide a pragmatic guide for beginners, and highlight promising
future research directions in this vital field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Artificial Needles to Real Haystacks: Improving Retrieval
  Capabilities in <span class="highlight-title">LLM</span>s by Finetuning on Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, Dimitris Papailiopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that Large Language Models (LLMs) struggle to
accurately retrieve information and maintain reasoning capabilities when
processing long-context inputs. To address these limitations, we propose a
finetuning approach utilizing a carefully designed synthetic dataset comprising
numerical key-value retrieval tasks. Our experiments on models like GPT-3.5
Turbo and Mistral 7B demonstrate that finetuning LLMs on this dataset
significantly improves LLMs' information retrieval and reasoning capabilities
in longer-context settings. We present an analysis of the finetuned models,
illustrating the transfer of skills from synthetic to real task evaluations
(e.g., $10.5\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5
Turbo). We also find that finetuned LLMs' performance on general benchmarks
remains almost constant while LLMs finetuned on other baseline long-context
augmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B
finetuned on our synthetic data cause no performance drop while other baseline
data can cause a drop that ranges from $2.33\%$ to $6.19\%$). Our study
highlights the potential of finetuning on synthetic data for improving the
performance of LLMs on longer-context tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal <span class="highlight-title">LLM</span>s at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VERISCORE: Evaluating the factuality of verifiable claims in long-form
  text generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Song, Yekyung Kim, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing metrics for evaluating the factuality of long-form text, such as
FACTSCORE (Min et al., 2023) and SAFE (Wei et al., 2024), decompose an input
text into "atomic claims" and verify each against a knowledge base like
Wikipedia. These metrics are not suitable for most generation tasks because
they assume that every claim is verifiable (i.e., can plausibly be proven true
or false). We address this issue with VERISCORE, a metric for diverse long-form
generation tasks that contain both verifiable and unverifiable content.
VERISCORE can be effectively implemented with either closed or fine-tuned
open-weight language models, and human evaluation confirms that VERISCORE's
extracted claims are more sensible than those from competing methods across
eight different long-form tasks. We use VERISCORE to evaluate generations from
16 different models across multiple long-form tasks and find that while GPT-4o
is the best-performing model overall, open-weight models such as Mixtral-8x22
are closing the gap. We show that an LM's VERISCORE on one task (e.g.,
biography generation) does not necessarily correlate to its VERISCORE on a
different task (e.g., long-form QA), highlighting the need for expanding
factuality evaluation across tasks with varying fact density.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoPureData: Automated Filtering of Web Data for <span class="highlight-title">LLM</span> Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneeth Vadlapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Up-to-date and reliable Large Language Models (LLMs) are consistently sought
after. Typically, LLMs are trained on a fixed dataset and then deployed.
However, the training data continually becomes outdated. Enable automatic
training of AI using web data involves significant concerns regarding data
quality and safety due to bias, spam, and other unsafe or unwanted text. Pure
data is essential for producing reliable models. Training a model on impure
data may result in undesirable outcomes. This research proposes a system that
collects web data and automatically filters out unwanted text with the
assistance of existing trusted AI models. In the experiment, a small sample of
web data was collected and filtered, demonstrating the system's effectiveness
in purifying the data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens
  Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical User Interfaces (GUIs) are central to our interaction with digital
devices. Recently, growing efforts have been made to build models for various
GUI understanding tasks. However, these efforts largely overlook an important
GUI-referring task: screen reading based on user-indicated points, which we
name the Screen Point-and-Read (SPR) task. This task is predominantly handled
by rigid accessible screen reading tools, in great need of new models driven by
advancements in Multimodal Large Language Models (MLLMs). In this paper, we
propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,
to address the SPR task. Based on the input point coordinate and the
corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout
Tree. Based on the tree, our ToL agent not only comprehends the content of the
indicated area but also articulates the layout and spatial relationships
between elements. Such layout information is crucial for accurately
interpreting information on the screen, distinguishing our ToL agent from other
screen reading tools. We also thoroughly evaluate the ToL agent against other
baselines on a newly proposed SPR benchmark, which includes GUIs from mobile,
web, and operating systems. Last but not least, we test the ToL agent on mobile
GUI navigation tasks, demonstrating its utility in identifying incorrect
actions along the path of agent execution trajectories. Code and data:
screen-point-and-read.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Video-Language Representations with Structural Spatio-Temporal
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pre-training large-scale video-language models (VLMs) has shown
remarkable potential for various downstream video-language tasks, existing VLMs
can still suffer from certain commonly seen limitations, e.g., coarse-grained
cross-modal aligning , under-modeling of temporal dynamics, detached
video-language view. In this work, we target enhancing VLMs with a fine-grained
structural spatio-temporal alignment learning method (namely Finsta). First of
all, we represent the input texts and videos with fine-grained scene graph (SG)
structures, both of which are further unified into a holistic SG (HSG) for
bridging two modalities. Then, an SG-based framework is built, where the
textual SG (TSG) is encoded with a graph Transformer, while the video dynamic
SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for
spatial and temporal feature propagation. A spatial-temporal Gaussian
differential graph Transformer is further devised to strengthen the sense of
the changes in objects across spatial and temporal dimensions. Next, based on
the fine-grained structural features of TSG and DSG, we perform object-centered
spatial alignment and predicate-centered temporal alignment respectively,
enhancing the video-language grounding in both the spatiality and temporality.
We design our method as a plug&play system, which can be integrated into
existing well-trained VLMs for further representation augmentation, without
training from scratch or relying on SG annotations in downstream applications.
On 6 representative VL modeling tasks over 12 datasets in both standard and
long-form video scenarios, Finsta consistently improves the existing 13
strong-performing VLMs persistently, and refreshes the current state-of-the-art
end task performance significantly in both the fine-tuning and zero-shot
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto<span class="highlight-title">RAG</span>-HP: Automatic Online Hyper-Parameter Tuning for
  <span class="highlight-title">Retrieval-Augmented</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Fu, Xiaoting Qin, Fangkai Yang, Lu Wang, Jue Zhang, Qingwei Lin, Yubo Chen, Dongmei Zhang, Saravan Rajmohan, Qi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models have transformed ML/AI
development, necessitating a reevaluation of AutoML principles for the
Retrieval-Augmented Generation (RAG) systems. To address the challenges of
hyper-parameter optimization and online adaptation in RAG, we propose the
AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online
multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical
MAB (Hier-MAB) method for efficient exploration of large search spaces. We
conduct extensive experiments on tuning hyper-parameters, such as top-k
retrieved documents, prompt compression ratio, and embedding methods, using the
ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly
optimization all three hyper-parameters demonstrate that MAB-based online
learning methods can achieve Recall@5 $\approx 0.8$ for scenarios with
prominent gradients in search space, using only $\sim20\%$ of the LLM API calls
required by the Grid Search approach. Additionally, the proposed Hier-MAB
approach outperforms other baselines in more challenging optimization
scenarios. The code will be made available at https://aka.ms/autorag.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Fine-Grained Values and Opinions in <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering latent values and opinions in large language models (LLMs) can
help identify biases and mitigate potential harm. Recently, this has been
approached by presenting LLMs with survey questions and quantifying their
stances towards morally and politically charged statements. However, the
stances generated by LLMs can vary greatly depending on how they are prompted,
and there are many ways to argue for or against a given position. In this work,
we propose to address this by analysing a large and robust dataset of 156k LLM
responses to the 62 propositions of the Political Compass Test (PCT) generated
by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of
their generated stances and fine-grained analysis of the plain text
justifications for those stances. For fine-grained analysis, we propose to
identify tropes in the responses: semantically similar phrases that are
recurrent and consistent across different prompts, revealing patterns in the
text that a given LLM is prone to produce. We find that demographic features
added to prompts significantly affect outcomes on the PCT, reflecting bias, as
well as disparities between the results of tests when eliciting closed-form vs.
open domain responses. Additionally, patterns in the plain text rationales via
tropes show that similar justifications are repeatedly generated across models
and prompts even with disparate stances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 20 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowVQA: Mapping Multimodal Logic in Visual Question Answering with
  Flowcharts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for visual question answering lack in visual grounding
and complexity, particularly in evaluating spatial reasoning skills. We
introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of
visual question-answering multimodal language models in reasoning with
flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and
human-verified flowchart images from three distinct content sources, along with
22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,
including information localization, decision-making, and logical progression.
We conduct a thorough baseline evaluation on a suite of both open-source and
proprietary multimodal language models using various strategies, followed by an
analysis of directional bias. The results underscore the benchmark's potential
as a vital tool for advancing the field of multimodal modeling, providing a
focused and challenging environment for enhancing model performance in visual
and logical reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RuBLiMP: Russian Benchmark of Linguistic Minimal Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Taktasheva, Maxim Bazhukov, Kirill Koncha, Alena Fenogenova, Ekaterina Artemova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimal pairs are a well-established approach to evaluating the grammatical
knowledge of language models. However, existing resources for minimal pairs
address a limited number of languages and lack diversity of language-specific
grammatical phenomena. This paper introduces the Russian Benchmark of
Linguistic Minimal Pairs (RuBLiMP), which includes 45k pairs of sentences that
differ in grammaticality and isolate a morphological, syntactic, or semantic
phenomenon. In contrast to existing benchmarks of linguistic minimal pairs,
RuBLiMP is created by applying linguistic perturbations to automatically
annotated sentences from open text corpora and carefully curating test data. We
describe the data collection protocol and present the results of evaluating 25
language models in various scenarios. We find that the widely used language
models for Russian are sensitive to morphological and agreement-oriented
contrasts but fall behind humans on phenomena requiring understanding of
structural relations, negation, transitivity, and tense. RuBLiMP, the codebase,
and other materials are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spiking Convolutional Neural Networks for Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changze Lv, Jianhan Xu, Xiaoqing Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) offer a promising pathway to implement deep
neural networks (DNNs) in a more energy-efficient manner since their neurons
are sparsely activated and inferences are event-driven. However, there have
been very few works that have demonstrated the efficacy of SNNs in language
tasks partially because it is non-trivial to represent words in the forms of
spikes and to deal with variable-length texts by SNNs. This work presents a
"conversion + fine-tuning" two-step method for training SNNs for text
classification and proposes a simple but effective way to encode pre-trained
word embeddings as spike trains. We show empirically that after fine-tuning
with surrogate gradients, the converted SNNs achieve comparable results to
their DNN counterparts with much less energy consumption across multiple
datasets for both English and Chinese. We also show that such SNNs are more
robust to adversarial attacks than DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tools Fail: Detecting Silent Errors in Faulty Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimin Sun, So Yeon Min, Yingshan Chang, Yonatan Bisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not
in their weights, to perform tasks on the web, and even to control robots.
However, most ontologies and surveys of tool-use have assumed the core
challenge for LLMs is choosing the tool. Instead, we introduce a framework for
tools more broadly which guides us to explore a model's ability to detect
"silent" tool errors, and reflect on how to plan. This more directly aligns
with the increasingly popular use of models as tools. We provide an initial
approach to failure recovery with promising results both on a controlled
calculator setting and embodied agent planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Teacher with Student Preferences for Tailored Training Data
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yantao Liu, Zhao Zhang, Zijun Yao, Shulin Cao, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown significant promise as copilots in
various tasks. Local deployment of LLMs on edge devices is necessary when
handling privacy-sensitive data or latency-sensitive tasks. The computational
constraints of such devices make direct deployment of powerful large-scale LLMs
impractical, necessitating the Knowledge Distillation from large-scale models
to lightweight models. Lots of work has been done to elicit diversity and
quality training examples from LLMs, but little attention has been paid to
aligning teacher instructional content based on student preferences, akin to
"responsive teaching" in pedagogy. Thus, we propose ARTE, dubbed Aligning
TeacheR with StudenT PreferencEs, a framework that aligns the teacher model
with student preferences to generate tailored training examples for Knowledge
Distillation. Specifically, we elicit draft questions and rationales from the
teacher model, then collect student preferences on these questions and
rationales using students' performance with in-context learning as a proxy, and
finally align the teacher model with student preferences. In the end, we repeat
the first step with the aligned teacher model to elicit tailored training
examples for the student model on the target task. Extensive experiments on
academic benchmarks demonstrate the superiority of ARTE over existing
instruction-tuning datasets distilled from powerful LLMs. Moreover, we
thoroughly investigate the generalization of ARTE, including the generalization
of fine-tuned student models in reasoning ability and the generalization of
aligned teacher models to generate tailored training data across tasks and
students. In summary, our contributions lie in proposing a novel framework for
tailored training example generation, demonstrating its efficacy in
experiments, and investigating the generalization of both student & aligned
teacher models in ARTE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating Classroom Education with <span class="highlight-title">LLM</span>-Empowered Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhiyuan Liu, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been employed in various intelligent
educational tasks to assist teaching. While preliminary explorations have
focused on independent LLM-empowered agents for specific educational tasks, the
potential for LLMs within a multi-agent collaborative framework to simulate a
classroom with real user participation remains unexplored. In this work, we
propose SimClass, a multi-agent classroom simulation framework involving user
participation. We recognize representative class roles and introduce a novel
class control mechanism for automatic classroom teaching, and conduct user
experiments in two real-world courses. Utilizing the Flanders Interactive
Analysis System and Community of Inquiry theoretical frame works from
educational analysis, we demonstrate that LLMs can simulate traditional
classroom interaction patterns effectively while enhancing user's experience.
We also observe emergent group behaviors among agents in SimClass, where agents
collaborate to create enlivening interactions in classrooms to improve user
learning process. We hope this work pioneers the application of LLM-empowered
multi-agent systems in virtual classroom teaching.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T-FREE: Tokenizer-Free Generative <span class="highlight-title">LLM</span>s via Sparse Representations for
  Memory-Efficient Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Björn Deiseroth, Manuel Brack, Patrick Schramowski, Kristian Kersting, Samuel Weinbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenizers are crucial for encoding information in Large Language Models, but
their development has recently stagnated, and they contain inherent weaknesses.
Major limitations include computational overhead, ineffective vocabulary use,
and unnecessarily large embedding and head layers. Additionally, their
performance is biased towards a reference corpus, leading to reduced
effectiveness for underrepresented languages.
  To remedy these issues, we propose T-FREE, which directly embeds words
through sparse activation patterns over character triplets, and does not
require a reference corpus. T-FREE inherently exploits morphological
similarities and allows for strong compression of embedding layers. In our
exhaustive experimental evaluation, we achieve competitive downstream
performance with a parameter reduction of more than 85% on these layers.
Further, T-FREE shows significant improvements in cross-lingual transfer
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeaKR: Self-aware Knowledge Retrieval for Adaptive <span class="highlight-title">Retrieval Augmented</span>
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Yao, Weijian Qi, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel
adaptive RAG model that extracts self-aware uncertainty of LLMs from their
internal states. SeaKR activates retrieval when the LLMs present high
self-aware uncertainty for generation. To effectively integrate retrieved
knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty
to preserve the snippet that reduces their uncertainty to the utmost. To
facilitate solving complex tasks that require multiple retrievals, SeaKR
utilizes their self-aware uncertainty to choose among different reasoning
strategies. Our experiments on both complex and simple Question Answering
datasets show that SeaKR outperforms existing adaptive RAG methods. We release
our code at https://github.com/THU-KEG/SeaKR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotation Errors and NER: A Study with OntoNotes 5.0 <span class="chip">LREC 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Bernier-Colborne, Sowmya Vajjala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) is a well-studied problem in NLP. However,
there is much less focus on studying NER datasets, compared to developing new
NER models. In this paper, we employed three simple techniques to detect
annotation errors in the OntoNotes 5.0 corpus for English NER, which is the
largest available NER corpus for English. Our techniques corrected ~10% of the
sentences in train/dev/test data. In terms of entity mentions, we corrected the
span and/or type of ~8% of mentions in the dataset, while
adding/deleting/splitting/merging a few more. These are large numbers of
changes, considering the size of OntoNotes. We used three NER libraries to
train, evaluate and compare the models trained with the original and the
re-annotated datasets, which showed an average improvement of 1.23% in overall
F-scores, with large (>10%) improvements for some of the entity types. While
our annotation error detection methods are not exhaustive and there is some
manual annotation effort involved, they are largely language agnostic and can
be employed with other NER datasets, and other sequence labelling tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Unpublished report. Originally submitted to LREC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Illusion of Competence: Evaluating the Effect of Explanations on
  Users' Mental Models of Visual Question Answering Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judith Sieker, Simeon Junker, Ronja Utescher, Nazia Attari, Heiko Wersing, Hendrik Buschmeier, Sina Zarrieß
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine how users perceive the limitations of an AI system when it
encounters a task that it cannot perform perfectly and whether providing
explanations alongside its answers aids users in constructing an appropriate
mental model of the system's capabilities and limitations. We employ a visual
question answer and explanation task where we control the AI system's
limitations by manipulating the visual inputs: during inference, the system
either processes full-color or grayscale images. Our goal is to determine
whether participants can perceive the limitations of the system. We hypothesize
that explanations will make limited AI capabilities more transparent to users.
However, our results show that explanations do not have this effect. Instead of
allowing users to more accurately assess the limitations of the AI system,
explanations generally increase users' perceptions of the system's competence -
regardless of its actual performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages (including Appendix); under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resolving Discrepancies in Compute-Optimal Scaling of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomer Porian, Mitchell Wortsman, Jenia Jitsev, Ludwig Schmidt, Yair Carmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kaplan et al. and Hoffmann et al. developed influential scaling laws for the
optimal model size as a function of the compute budget, but these laws yield
substantially different predictions. We explain the discrepancy by reproducing
the Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and
identifying three factors causing the difference: last layer computational
cost, warmup duration, and scale-dependent optimizer tuning. With these factors
corrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,
"Chinchilla") scaling law. Counter to a hypothesis of Hoffmann et al., we find
that careful learning rate decay is not essential for the validity of their
scaling law. As a secondary result, we derive scaling laws for the optimal
learning rate and batch size, finding that tuning the AdamW $\beta_2$ parameter
is essential at lower batch sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHEW: A <span class="highlight-title">Dataset</span> of CHanging Events in Wikipedia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsuvas Borkakoty, Luis Espinosa-Anke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CHEW, a novel dataset of changing events in Wikipedia expressed
in naturally occurring text. We use CHEW for probing LLMs for their timeline
understanding of Wikipedia entities and events in generative and classification
experiments. Our results suggest that LLMs, despite having temporal information
available, struggle to construct accurate timelines. We further show the
usefulness of CHEW-derived embeddings for identifying meaning shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statements: Universal Information Extraction from Tables with Large
  Language Models for ESG KPIs <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lokesh Mishra, Sohayl Dhibi, Yusik Kim, Cesar Berrospi Ramis, Shubham Gupta, Michele Dolfi, Peter Staar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environment, Social, and Governance (ESG) KPIs assess an organization's
performance on issues such as climate change, greenhouse gas emissions, water
consumption, waste management, human rights, diversity, and policies. ESG
reports convey this valuable quantitative information through tables.
Unfortunately, extracting this information is difficult due to high variability
in the table structure as well as content. We propose Statements, a novel
domain agnostic data structure for extracting quantitative facts and related
information. We propose translating tables to statements as a new supervised
deep-learning universal information extraction task. We introduce SemTabNet - a
dataset of over 100K annotated tables. Investigating a family of T5-based
Statement Extraction Models, our best model generates statements which are 82%
similar to the ground-truth (compared to baseline of 21%). We demonstrate the
advantages of statements by applying our model to over 2700 tables from ESG
reports. The homogeneous nature of statements permits exploratory data analysis
on expansive information found in large collections of ESG reports.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the NLP4Climate workshop in the 62nd Annual Meeting of
  the Association for Computational Linguistics (ACL 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness and Bias in Multimodal AI: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tosin Adewumi, Lama Alkhaled, Namrata Gurung, Goya van Boven, Irene Pagliai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of addressing fairness and bias in artificial intelligence
(AI) systems cannot be over-emphasized. Mainstream media has been awashed with
news of incidents around stereotypes and bias in many of these systems in
recent years. In this survey, we fill a gap with regards to the minimal study
of fairness and bias in Large Multimodal Models (LMMs) compared to Large
Language Models (LLMs), providing 50 examples of datasets and models along with
the challenges affecting them; we identify a new category of quantifying bias
(preuse), in addition to the two well-known ones in the literature: intrinsic
and extrinsic; we critically discuss the various ways researchers are
addressing these challenges. Our method involved two slightly different search
queries on Google Scholar, which revealed that 33,400 and 538,000 links are the
results for the terms "Fairness and bias in Large Multimodal Models" and
"Fairness and bias in Large Language Models", respectively. We believe this
work contributes to filling this gap and providing insight to researchers and
other stakeholders on ways to address the challenge of fairness and bias in
multimodal A!.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database
  Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irina Saparina, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical semantic parsers are expected to understand user utterances and map
them to executable programs, even when these are ambiguous. We introduce a new
benchmark, AMBROSIA, which we hope will inform and inspire the development of
text-to-SQL parsers capable of recognizing and interpreting ambiguous requests.
Our dataset contains questions showcasing three different types of ambiguity
(scope ambiguity, attachment ambiguity, and vagueness), their interpretations,
and corresponding SQL queries. In each case, the ambiguity persists even when
the database context is provided. This is achieved through a novel approach
that involves controlled generation of databases from scratch. We benchmark
various LLMs on AMBROSIA, revealing that even the most advanced models struggle
to identify and interpret ambiguity in questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmPO: Theory-Driven <span class="highlight-title">Dataset</span> Construction for Empathetic Response
  Generation through Preference Optimization <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ondrej Sotolar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic response generation is a desirable aspect of conversational
agents, crucial for facilitating engaging and emotionally intelligent
multi-turn conversations between humans and machines. Leveraging large language
models for this task has shown promising results, yet challenges persist in
ensuring both the empathetic quality of the responses and retention of the
generalization performance of the models. In this paper, we propose a novel
approach where we construct theory-driven preference datasets and use them to
align LLMs with preference optimization algorithms to address these challenges.
To measure empathetic response generation, we employ the EmpatheticDialogues
dataset, assessing empathy with the diff-EPITOME and BERTscore metrics, and
evaluate the generalization performance on the MMLU benchmark. We make all
datasets, source code, and models publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v01, 4 pages short paper, ACL style</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STBench: Assessing the Ability of <span class="highlight-title">Large Language Models</span> in
  Spatio-Temporal Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbin Li, Di Yao, Ruibo Zhao, Wenjie Chen, Zijie Xu, Chengxue Luo, Chang Gong, Quanliang Jing, Haining Tan, Jingping Bi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of large language models (LLMs) holds promise for
reforming the methodology of spatio-temporal data mining. However, current
works for evaluating the spatio-temporal understanding capability of LLMs are
somewhat limited and biased. These works either fail to incorporate the latest
language models or only focus on assessing the memorized spatio-temporal
knowledge. To address this gap, this paper dissects LLMs' capability of
spatio-temporal data into four distinct dimensions: knowledge comprehension,
spatio-temporal reasoning, accurate computation, and downstream applications.
We curate several natural language question-answer tasks for each category and
build the benchmark dataset, namely STBench, containing 13 distinct tasks and
over 60,000 QA pairs. Moreover, we have assessed the capabilities of 13 LLMs,
such as GPT-4o, Gemma and Mistral. Experimental results reveal that existing
LLMs show remarkable performance on knowledge comprehension and spatio-temporal
reasoning tasks, with potential for further enhancement on other tasks through
in-context learning, chain-of-though prompting, and fine-tuning. The code and
datasets of STBench are released on https://github.com/LwbXc/STBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Weak-to-Strong Generalization with Reliability-Aware Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Guo, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are now rapidly advancing and surpassing human
abilities on many natural language tasks. However, aligning these super-human
LLMs with human knowledge remains challenging because the supervision signals
from human annotators may be wrong. This issue, known as the "super-alignment"
problem, requires enhancing weak-to-strong generalization, where a strong LLM
must generalize from imperfect supervision provided by a weaker source. To
address this issue, we propose an approach to improve weak-to-strong
generalization by involving the reliability of weak supervision signals in the
alignment process. In our method, we query the weak supervisor for multiple
answers, estimate the answer reliability, and enhance the alignment process by
filtering out uncertain data or re-weighting reliable data. Experiments on four
datasets demonstrate that our methods effectively identify the quality of weak
labels and significantly enhance weak-to-strong generalization. Our work
presents effective techniques for error-robust model alignment, reducing error
propagation from noisy supervision and enhancing the accuracy and reliability
of LLMs. Codes are publicly available at
http://github.com/Irenehere/ReliableAlignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboUniView: Visual-Language Model with Unified View Representation for
  Robotic Manipulaiton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanfan Liu, Feng Yan, Liming Zheng, Chengjian Feng, Yiyang Huang, Lin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing Vision-Language Models (VLMs) for robotic manipulation represents a
novel paradigm, aiming to enhance the model's ability to generalize to new
objects and instructions. However, due to variations in camera specifications
and mounting positions, existing methods exhibit significant performance
disparities across different robotic platforms. To address this challenge, we
propose RoboUniView in this paper, an innovative approach that decouples visual
feature extraction from action learning. We first learn a unified view
representation from multi-perspective views by pre-training on readily
accessible data, and then derive actions from this unified view representation
to control robotic manipulation. This unified view representation more
accurately mirrors the physical world and is not constrained by the robotic
platform's camera parameters. Thanks to this methodology, we achieve
state-of-the-art performance on the demanding CALVIN benchmark, enhancing the
success rate in the $D \to D$ setting from 88.7% to 96.2%, and in the $ABC \to
D$ setting from 82.4% to 94.2%. Moreover, our model exhibits outstanding
adaptability and flexibility: it maintains high performance under unseen camera
parameters, can utilize multiple datasets with varying camera parameters, and
is capable of joint cross-task learning across datasets. Code is provided for
re-implementation. https://github.com/liufanfanlff/RoboUniview
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying <span class="highlight-title">LLM</span>s for Rescoring N-best ASR Hypotheses of Casual
  Conversations: Effects of Domain Adaptation and Context Carry-over 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsunori Ogawa, Naoyuki Kamo, Kohei Matsuura, Takanori Ashihara, Takafumi Moriya, Takatomo Kano, Naohiro Tawara, Marc Delcroix
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been successfully applied for rescoring
automatic speech recognition (ASR) hypotheses. However, their ability to
rescore ASR hypotheses of casual conversations has not been sufficiently
explored. In this study, we reveal it by performing N-best ASR hypotheses
rescoring using Llama2 on the CHiME-7 distant ASR (DASR) task. Llama2 is one of
the most representative LLMs, and the CHiME-7 DASR task provides datasets of
casual conversations between multiple participants. We investigate the effects
of domain adaptation of the LLM and context carry-over when performing N-best
rescoring. Experimental results show that, even without domain adaptation,
Llama2 outperforms a standard-size domain-adapted Transformer-LM, especially
when using a long context. Domain adaptation shortens the context length needed
with Llama2 to achieve its best performance, i.e., it reduces the computational
cost of Llama2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniGen: A Unified Framework for Textual <span class="highlight-title">Dataset</span> Generation Using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Wu, Yue Huang, Chujie Gao, Dongping Chen, Qihui Zhang, Yao Wan, Tianyi Zhou, Xiangliang Zhang, Jianfeng Gao, Chaowei Xiao, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly
impacted various fields by enabling high-quality synthetic data generation and
reducing dependence on expensive human-generated datasets. Despite this,
challenges remain in the areas of generalization, controllability, diversity,
and truthfulness within the existing generative frameworks. To address these
challenges, this paper presents UniGen, a comprehensive LLM-powered framework
designed to produce diverse, accurate, and highly controllable datasets. UniGen
is adaptable, supporting all types of text datasets and enhancing the
generative process through innovative mechanisms. To augment data diversity,
UniGen incorporates an attribute-guided generation module and a group checking
feature. For accuracy, it employs a code-based mathematical assessment for
label verification alongside a retrieval-augmented generation technique for
factual validation. The framework also allows for user-specified constraints,
enabling customization of the data generation process to suit particular
requirements. Extensive experiments demonstrate the superior quality of data
generated by UniGen, and each module within UniGen plays a critical role in
this enhancement. Additionally, UniGen is applied in two practical scenarios:
benchmarking LLMs and data augmentation. The results indicate that UniGen
effectively supports dynamic and evolving benchmarking, and that data
augmentation improves LLM capabilities in various domains, including
agent-oriented abilities and reasoning skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The single-use restriction for register automata and transducers over
  infinite alphabets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafał Stefański
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis studies the single-use restriction for register automata and
transducers over infinite alphabets. The restriction requires that a
read-access to a register should have the side effect of destroying its
contents. This constraint results in robust classes of languages and
transductions. For automata models, we show that one-way register automata,
two-way register automata, and orbit-finite monoids have the same expressive
power. For transducer models, we show that single-use Mealy machines and
single-use two-way transducers admit versions of the Krohn-Rhodes decomposition
theorem. Moreover, single-use Mealy machines are equivalent to an algebraic
model called local algebraic semigroup transductions. Additionally, we show
that single-use two-way transducers are equivalent to single-use streaming
string transducers (SSTs) over infinite alphabets and to regular list functions
with atoms.
  Compared with the previous work arXiv:1907.10504, this thesis offers a
coherent narrative on the single-use restriction. We introduce an abstract
notion of single-use functions and use them to define all the discussed
single-use models. We also introduce and study the algebraic models of local
semigroup transduction and local rational semigroup transduction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis at University of Warsaw. Supervisor: Miko{\l}aj
  Boja\'nczyk</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced ASR Robustness to Packet Loss with a Front-End Adaptation
  Network <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehoshua Dissen, Shiry Yonash, Israel Cohen, Joseph Keshet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of automatic speech recognition (ASR), robustness in noisy
environments remains a significant challenge. Recent ASR models, such as
Whisper, have shown promise, but their efficacy in noisy conditions can be
further enhanced. This study is focused on recovering from packet loss to
improve the word error rate (WER) of ASR models. We propose using a front-end
adaptation network connected to a frozen ASR model. The adaptation network is
trained to modify the corrupted input spectrum by minimizing the criteria of
the ASR model in addition to an enhancement loss function. Our experiments
demonstrate that the adaptation network, trained on Whisper's criteria, notably
reduces word error rates across domains and languages in packet-loss scenarios.
This improvement is achieved with minimal affect to Whisper model's
foundational performance, underscoring our method's practicality and potential
in enhancing ASR models in challenging acoustic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at INTERSPEECH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Vision is the Challenge for Visual Reasoning: A Benchmark for
  Visual Argument Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwan Chung, Sungjae Lee, Minseo Kim, Seungju Han, Ashkan Yousefpour, Jack Hessel, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual arguments, often used in advertising or social causes, rely on images
to persuade viewers to do or believe something. Understanding these arguments
requires selective vision: only specific visual stimuli within an image are
relevant to the argument, and relevance can only be understood within the
context of a broader argumentative structure. While visual arguments are
readily appreciated by human audiences, we ask: are today's AI capable of
similar understanding?
  We collect and release VisArgs, an annotated corpus designed to make explicit
the (usually implicit) structures underlying visual arguments. VisArgs includes
1,611 images accompanied by three types of textual annotations: 5,112 visual
premises (with region annotations), 5,574 commonsense premises, and reasoning
trees connecting them to a broader argument. We propose three tasks over
VisArgs to probe machine capacity for visual argument understanding:
localization of premises, identification of premises, and deduction of
conclusions. Experiments demonstrate that 1) machines cannot fully identify the
relevant visual cues. The top-performing model, GPT-4-O, achieved an accuracy
of only 78.5%, whereas humans reached 98.0%. All models showed a performance
drop, with an average decrease in accuracy of 19.5%, when the comparison set
was changed from objects outside the image to irrelevant objects within the
image. Furthermore, 2) this limitation is the greatest factor impacting their
performance in understanding visual arguments. Most models improved the most
when given relevant visual premises as additional inputs, compared to other
inputs, for deducing the conclusion of the visual argument.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models
  with Personality-Indicative Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiting Ran, Xintao Wang, Rui Xu, Xinfeng Yuan, Jiaqing Liang, Yanghua Xiao, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Role-playing agents (RPA) have been a popular application area for large
language models (LLMs), attracting significant interest from both industry and
academia.While existing RPAs well portray the characters' knowledge and tones,
they face challenges in capturing their minds, especially for small
role-playing language models (RPLMs). In this paper, we propose to enhance
RPLMs via personality-indicative data. Specifically, we leverage questions from
psychological scales and distill advanced RPAs to generate dialogues that grasp
the minds of characters. Experimental results validate that RPLMs trained with
our dataset exhibit advanced role-playing capabilities for both general and
personality-related evaluations. Code and data are available at
\href{https://github.com/alienet1109/RolePersonality}{this URL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrustUQA: A Trustful Framework for Unified Structured Data Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language question answering (QA) over structured data sources such as
tables and knowledge graphs (KGs) have been widely investigated, for example
with Large Language Models (LLMs). The main solutions include question to
formal query parsing and retrieval-based answer generation. However, current
methods of the former often suffer from weak generalization, failing to dealing
with multiple sources simultaneously, while the later is limited in
trustfulness. In this paper, we propose UnifiedTQA, a trustful QA framework
that can simultaneously support multiple types of structured data in a unified
way. To this end, it adopts an LLM-friendly and unified knowledge
representation method called Condition Graph (CG), and uses an LLM and
demonstration-based two-level method for CG querying. For enhancement, it is
also equipped with dynamic demonstration retrieval. We have evaluated
UnifiedTQA with 5 benchmarks covering 3 types of structured data. It
outperforms 2 existing unified structured data QA methods and in comparison
with the baselines that are specific to a data type, it achieves
state-of-the-art on 2 of them. Further more, we demonstrates potential of our
method for more general QA tasks, QA over mixed structured data and QA across
structured data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Factor-Conditioned Speaking-Style Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Ando, Takafumi Moriya, Shota Horiguchi, Ryo Masumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel speaking-style captioning method that generates
diverse descriptions while accurately predicting speaking-style information.
Conventional learning criteria directly use original captions that contain not
only speaking-style factor terms but also syntax words, which disturbs learning
speaking-style information. To solve this problem, we introduce
factor-conditioned captioning (FCC), which first outputs a phrase representing
speaking-style factors (e.g., gender, pitch, etc.), and then generates a
caption to ensure the model explicitly learns speaking-style factors. We also
propose greedy-then-sampling (GtS) decoding, which first predicts
speaking-style factors deterministically to guarantee semantic accuracy, and
then generates a caption based on factor-conditioned sampling to ensure
diversity. Experiments show that FCC outperforms the original caption-based
training, and with GtS, it generates more diverse captions while keeping style
prediction performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Historia Magistra Vitae: Dynamic Topic Modeling of Roman Literature
  using Neural Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Ginn, Mans Hulden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic topic models have been proposed as a tool for historical analysis,
but traditional approaches have had limited usefulness, being difficult to
configure, interpret, and evaluate. In this work, we experiment with a recent
approach for dynamic topic modeling using BERT embeddings. We compare topic
models built using traditional statistical models (LDA and NMF) and the
BERT-based model, modeling topics over the entire surviving corpus of Roman
literature. We find that while quantitative metrics prefer statistical models,
qualitative evaluation finds better insights from the neural model.
Furthermore, the neural topic model is less sensitive to hyperparameter
configuration and thus may make dynamic topic modeling more viable for
historical researchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sonnet or Not, Bot? Poetry Evaluation for Large Models and <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie Walsh, Anna Preus, Maria Antoniak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can now generate and recognize text in a wide
range of styles and genres, including highly specialized, creative genres like
poetry. But what do LLMs really know about poetry? What can they know about
poetry? We develop a task to evaluate how well LLMs recognize a specific aspect
of poetry, poetic form, for more than 20 forms and formal elements in the
English language. Poetic form captures many different poetic features,
including rhyme scheme, meter, and word or line repetition. We use this task to
reflect on LLMs' current poetic capabilities, as well as the challenges and
pitfalls of creating NLP benchmarks for poetry and for other creative tasks. In
particular, we use this task to audit and reflect on the poems included in
popular pretraining datasets. Our findings have implications for NLP
researchers interested in model evaluation, digital humanities and cultural
analytics scholars, and cultural heritage professionals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can we teach language models to gloss endangered languages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Ginn, Mans Hulden, Alexis Palmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interlinear glossed text (IGT) is a popular format in language documentation
projects, where each morpheme is labeled with a descriptive annotation.
Automating the creation of interlinear glossed text can be desirable to reduce
annotator effort and maintain consistency across annotated corpora. Prior
research has explored a number of statistical and neural methods for
automatically producing IGT.
  As large language models (LLMs) have showed promising results across
multilingual tasks, even for rare, endangered languages, it is natural to
wonder whether they can be utilized for the task of generating IGT. We explore
whether LLMs can be effective at the task of interlinear glossing with
in-context learning, without any traditional training. We propose new
approaches for selecting examples to provide in-context, observing that
targeted selection can significantly improve performance. We find that
LLM-based methods beat standard transformer baselines, despite requiring no
training at all. These approaches still underperform state-of-the-art
supervised systems for the task, but are highly practical for researchers
outside of the NLP community, requiring minimal effort to use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSP: Self-Supervised Prompting for Cross-Lingual Transfer to
  Low-Resource Languages using <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vipul Rathore, Aniruddha Deb, Ankish Chandresh, Parag Singla,  Mausam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, very large language models (LLMs) have shown exceptional
performance on several English NLP tasks with just in-context learning (ICL),
but their utility in other languages is still underexplored. We investigate
their effectiveness for NLP tasks in low-resource languages (LRLs), especially
in the setting of zero-labelled cross-lingual transfer (0-CLT), where no
labelled training data for the target language is available -- however training
data from one or more related medium-resource languages (MRLs) is utilized,
alongside the available unlabeled test data for a target language. We introduce
Self-Supervised Prompting (SSP), a novel ICL approach tailored for the 0-CLT
setting.
  SSP is based on the key observation that LLMs output more accurate labels if
in-context exemplars are from the target language (even if their labels are
slightly noisy). To operationalize this, since target language training data is
not available in 0-CLT, SSP operates in two stages. In Stage I, using source
MRL training data, target language's test data is noisily labeled. In Stage II,
these noisy test data points are used as exemplars in ICL for further improved
labelling. Additionally, our implementation of SSP uses a novel Integer Linear
Programming (ILP)-based exemplar selection that balances similarity, prediction
confidence (when available) and label coverage. Experiments on three tasks and
eleven LRLs (from three regions) demonstrate that SSP strongly outperforms
existing SOTA fine-tuned and prompting-based baselines in 0-CLT setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, He Huang, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent speech language models (SLMs) typically incorporate pre-trained speech
models to extend the capabilities from large language models (LLMs). In this
paper, we propose a Descriptive Speech-Text Alignment approach that leverages
speech captioning to bridge the gap between speech and text modalities,
enabling SLMs to interpret and generate comprehensive natural language
descriptions, thereby facilitating the capability to understand both linguistic
and non-linguistic features in speech. Enhanced with the proposed approach, our
model demonstrates superior performance on the Dynamic-SUPERB benchmark,
particularly in generalizing to unseen tasks. Moreover, we discover that the
aligned model exhibits a zero-shot instruction-following capability without
explicit speech instruction tuning. These findings highlight the potential to
reshape instruction-following SLMs by incorporating rich, descriptive speech
captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficacy of Language Model Self-Play in Non-Zero-Sum Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austen Liao, Nicholas Tomlin, Dan Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Game-playing agents like AlphaGo have achieved superhuman performance through
self-play, which is theoretically guaranteed to yield optimal policies in
competitive games. However, most language tasks are partially or fully
cooperative, so it is an open question whether techniques like self-play can
effectively be used to improve language models. We empirically investigate this
question in a negotiation game setting known as Deal or No Deal (DoND).
Crucially, the objective in DoND can be modified to produce a fully cooperative
game, a strictly competitive one, or anything in between. We finetune language
models in self-play over multiple rounds of filtered behavior cloning in DoND
for each of these objectives. Contrary to expectations, we find that language
model self-play leads to significant performance gains in both cooperation and
competition with humans, suggesting that self-play and related techniques have
promise despite a lack of theoretical guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology
  Report Simplification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Yang, Santhosh Cherian, Slobodan Vucetic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology reports are highly technical documents aimed primarily at
doctor-doctor communication. There has been an increasing interest in sharing
those reports with patients, necessitating providing them patient-friendly
simplifications of the original reports. This study explores the suitability of
large language models in automatically generating those simplifications. We
examine the usefulness of chain-of-thought and self-correction prompting
mechanisms in this domain. We also propose a new evaluation protocol that
employs radiologists and laypeople, where radiologists verify the factual
correctness of simplifications, and laypeople assess simplicity and
comprehension. Our experimental results demonstrate the effectiveness of
self-correction prompting in producing high-quality simplifications. Our
findings illuminate the preferences of radiologists and laypeople regarding
text simplification, informing future research on this topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Fu, Shijing Si, Leyi Mai, Xi-ang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have stunningly advanced the field of machine
translation, though their effectiveness within the financial domain remains
largely underexplored. To probe this issue, we constructed a fine-grained
Chinese-English parallel corpus of financial news called FFN. We acquired
financial news articles spanning between January 1st, 2014, to December 31,
2023, from mainstream media websites such as CNN, FOX, and China Daily. The
dataset consists of 1,013 main text and 809 titles, all of which have been
manually corrected. We measured the translation quality of two LLMs -- ChatGPT
and ERNIE-bot, utilizing BLEU, TER and chrF scores as the evaluation metrics.
For comparison, we also trained an OpenNMT model based on our dataset. We
detail problems of LLMs and provide in-depth analysis, intending to stimulate
further research and solutions in this largely uncharted territory. Our
research underlines the need to optimize LLMs within the specific field of
financial translation to ensure accuracy and quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>a simplified version of this paper is accepted by International
  Conference on Asian Language Processing 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Retrieval Augmentation for Personalized Dialogue Generation <span class="chip">EMNLP-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, Lilian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized dialogue generation, focusing on generating highly tailored
responses by leveraging persona profiles and dialogue context, has gained
significant attention in conversational AI applications. However, persona
profiles, a prevalent setting in current personalized dialogue datasets,
typically composed of merely four to five sentences, may not offer
comprehensive descriptions of the persona about the agent, posing a challenge
to generate truly personalized dialogues. To handle this problem, we propose
$\textbf{L}$earning Retrieval $\textbf{A}$ugmentation for
$\textbf{P}$ersonalized $\textbf{D}$ial$\textbf{O}$gue $\textbf{G}$eneration
($\textbf{LAPDOG}$), which studies the potential of leveraging external
knowledge for persona dialogue generation. Specifically, the proposed LAPDOG
model consists of a story retriever and a dialogue generator. The story
retriever uses a given persona profile as queries to retrieve relevant
information from the story document, which serves as a supplementary context to
augment the persona profile. The dialogue generator utilizes both the dialogue
history and the augmented persona profile to generate personalized responses.
For optimization, we adopt a joint training framework that collaboratively
learns the story retriever and dialogue generator, where the story retriever is
optimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for
the dialogue generator to generate personalized responses. Experiments
conducted on the CONVAI2 dataset with ROCStory as a supplementary data source
show that the proposed LAPDOG method substantially outperforms the baselines,
indicating the effectiveness of the proposed method. The LAPDOG model code is
publicly available for further exploration.
https://github.com/hqsiswiliam/LAPDOG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OutlierTune: Efficient Channel-Wise Quantization for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinguang Wang, Yuexi Yin, Haifeng Sun, Qi Qi, Jingyu Wang, Zirui Zhuang, Tingting Yang, Jianxin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantizing the activations of large language models (LLMs) has been a
significant challenge due to the presence of structured outliers. Most existing
methods focus on the per-token or per-tensor quantization of activations,
making it difficult to achieve both accuracy and hardware efficiency. To
address this problem, we propose OutlierTune, an efficient per-channel
post-training quantization (PTQ) method for the activations of LLMs.
OutlierTune consists of two components: pre-execution of dequantization and
symmetrization. The pre-execution of dequantization updates the model weights
by the activation scaling factors, avoiding the internal scaling and costly
additional computational overheads brought by the per-channel activation
quantization. The symmetrization further reduces the quantization differences
arising from the weight updates by ensuring the balanced numerical ranges
across different activation channels. OutlierTune is easy to implement and
hardware-efficient, introducing almost no additional computational overheads
during the inference. Extensive experiments show that the proposed framework
outperforms existing methods across multiple different tasks. Demonstrating
better generalization, this framework improves the Int6 quantization of the
instruction-tuning LLMs, such as OPT-IML, to the same level as half-precision
(FP16). Moreover, we have shown that the proposed framework is 1.48x faster
than the FP16 implementation while reducing approximately 2x memory usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathAlign: A vision-language model for whole slide images in
  histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faruk Ahmed, Andrew Sellergren, Lin Yang, Shawn Xu, Boris Babenko, Abbi Ward, Niels Olson, Arash Mohtashamian, Yossi Matias, Greg S. Corrado, Quang Duong, Dale R. Webster, Shravya Shetty, Daniel Golden, Yun Liu, David F. Steiner, Ellery Wulczyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic interpretation of histopathology images underlies many important
diagnostic and treatment decisions. While advances in vision-language modeling
raise new opportunities for analysis of such images, the gigapixel-scale size
of whole slide images (WSIs) introduces unique challenges. Additionally,
pathology reports simultaneously highlight key findings from small regions
while also aggregating interpretation across multiple slides, often making it
difficult to create robust image-text pairs. As such, pathology reports remain
a largely untapped source of supervision in computational pathology, with most
efforts relying on region-of-interest annotations or self-supervision at the
patch-level. In this work, we develop a vision-language model based on the
BLIP-2 framework using WSIs paired with curated text from pathology reports.
This enables applications utilizing a shared image-text embedding space, such
as text or image retrieval for finding cases of interest, as well as
integration of the WSI encoder with a frozen large language model (LLM) for
WSI-based generative text capabilities such as report generation or
AI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000
WSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure
types, and tissue types. We present pathologist evaluation of text generation
and text retrieval using WSI embeddings, as well as results for WSI
classification and workflow prioritization (slide-level triaging).
Model-generated text for WSIs was rated by pathologists as accurate, without
clinically significant error or omission, for 78% of WSIs on average. This work
demonstrates exciting potential capabilities for language-aligned WSI
embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages and 19 pages of supplemental material; 3 main tables, 3
  main figures and 11 supplemental tables, 7 supplemental figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Voices Unheard: NLP Resources and Models for Yorùbá Regional
  Dialects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orevaoghene Ahia, Anuoluwapo Aremu, Diana Abagyan, Hila Gonen, David Ifeoluwa Adelani, Daud Abolade, Noah A. Smith, Yulia Tsvetkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Yor\`ub\'a an African language with roughly 47 million speakers encompasses a
continuum with several dialects. Recent efforts to develop NLP technologies for
African languages have focused on their standard dialects, resulting in
disparities for dialects and varieties for which there are little to no
resources or tools. We take steps towards bridging this gap by introducing a
new high-quality parallel text and speech corpus YOR\`ULECT across three
domains and four regional Yor\`ub\'a dialects. To develop this corpus, we
engaged native speakers, travelling to communities where these dialects are
spoken, to collect text and speech data. Using our newly created corpus, we
conducted extensive experiments on (text) machine translation, automatic speech
recognition, and speech-to-text translation. Our results reveal substantial
performance disparities between standard Yor\`ub\'a and the other dialects
across all tasks. However, we also show that with dialect-adaptive finetuning,
we are able to narrow this gap. We believe our dataset and experimental
analysis will contribute greatly to developing NLP tools for Yor\`ub\'a and its
dialects, and potentially for other African languages, by improving our
understanding of existing challenges and offering a high-quality dataset for
further development. We release YOR\`ULECT dataset and models publicly under an
open license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking harmless refusals when fine-tuning foundation models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florin Pop, Judd Rosenblatt, Diogo Schwerz de Lucena, Michael Vaiana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the degree to which fine-tuning in Large
Language Models (LLMs) effectively mitigates versus merely conceals undesirable
behavior. Through the lens of semi-realistic role-playing exercises designed to
elicit such behaviors, we explore the response dynamics of LLMs post
fine-tuning interventions. Our methodology involves prompting models for
Chain-of-Thought (CoT) reasoning and analyzing the coherence between the
reasoning traces and the resultant outputs. Notably, we identify a pervasive
phenomenon we term \emph{reason-based deception}, where models either stop
producing reasoning traces or produce seemingly ethical reasoning traces that
belie the unethical nature of their final outputs. We further examine the
efficacy of response strategies (polite refusal versus explicit rebuttal) in
curbing the occurrence of undesired behavior in subsequent outputs of
multi-turn interactions. Our findings reveal that explicit rebuttals
significantly outperform polite refusals in preventing the continuation of
undesired outputs and nearly eliminate reason-based deception, challenging
current practices in model fine-tuning. Accordingly, the two key contributions
of this paper are (1) defining and studying reason-based deception, a new type
of hidden behavior, and (2) demonstrating that rebuttals provide a more robust
response model to harmful requests than refusals, thereby highlighting the need
to reconsider the response strategies in fine-tuning approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 AGI Workshop Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leve<span class="highlight-title">rag</span>ing Machine-Generated Rationales to Facilitate Social Meaning
  Detection in Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritam Dutt, Zhen Wu, Kelly Shi, Divyanshu Sheth, Prakhar Gupta, Carolyn Penstein Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a generalizable classification approach that leverages Large
Language Models (LLMs) to facilitate the detection of implicitly encoded social
meaning in conversations. We design a multi-faceted prompt to extract a textual
explanation of the reasoning that connects visible cues to underlying social
meanings. These extracted explanations or rationales serve as augmentations to
the conversational text to facilitate dialogue understanding and transfer. Our
empirical results over 2,340 experimental settings demonstrate the significant
positive impact of adding these rationales. Our findings hold true for
in-domain classification, zero-shot, and few-shot domain transfer for two
different social meaning detection tasks, each spanning two different corpora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at The Proceedings of the Association for Computational
  Linguistics, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demarked: A Strategy for Enhanced Abusive Speech Moderation through
  Counterspeech, Detoxification, and Message Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seid Muhie Yimam, Daryna Dementieva, Tim Fischer, Daniil Moskovskiy, Naquee Rizwan, Punyajoy Saha, Sarthak Roy, Martin Semmann, Alexander Panchenko, Chris Biemann, Animesh Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite regulations imposed by nations and social media platforms, such as
recent EU regulations targeting digital violence, abusive content persists as a
significant challenge. Existing approaches primarily rely on binary solutions,
such as outright blocking or banning, yet fail to address the complex nature of
abusive speech. In this work, we propose a more comprehensive approach called
Demarcation scoring abusive speech based on four aspect -- (i) severity scale;
(ii) presence of a target; (iii) context scale; (iv) legal scale -- and
suggesting more options of actions like detoxification, counter speech
generation, blocking, or, as a final measure, human intervention. Through a
thorough analysis of abusive speech regulations across diverse jurisdictions,
platforms, and research papers we highlight the gap in preventing measures and
advocate for tailored proactive steps to combat its multifaceted
manifestations. Our work aims to inform future strategies for effectively
addressing abusive speech online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context Matters: An Empirical Study of the Impact of Contextual
  Information in Temporal Question Answering Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Schumacher, Fatemeh Haji, Tara Grey, Niharika Bandlamudi, Nupoor Karnik, Gagana Uday Kumar, Jason Cho-Yu Chiang, Paul Rad, Nishant Vishwamitra, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often struggle with temporal reasoning, crucial
for tasks like historical event analysis and time-sensitive information
retrieval. Despite advancements, state-of-the-art models falter in handling
temporal information, especially when faced with irrelevant or noisy contexts.
This paper addresses this gap by empirically examining the robustness of
temporal question-answering (TQA) systems trained on various context types,
including relevant, irrelevant, slightly altered, and no context. Our findings
indicate that training with a mix of these contexts enhances model robustness
and accuracy. Additionally, we show that the position of context relative to
the question significantly impacts performance, with question-first positioning
yielding better results. We introduce two new context-rich TQA datasets,
ContextAQA and ContextTQE, and provide comprehensive evaluations and guidelines
for training robust TQA models. Our work lays the foundation for developing
reliable and context-aware temporal QA systems, with broader implications for
enhancing LLM robustness against diverse and potentially adversarial
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handling Ontology Gaps in Semantic Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Bacciu, Marco Damonte, Marco Basaldella, Emilio Monti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of Neural Semantic Parsing (NSP) models are developed with the
assumption that there are no concepts outside the ones such models can
represent with their target symbols (closed-world assumption). This assumption
leads to generate hallucinated outputs rather than admitting their lack of
knowledge. Hallucinations can lead to wrong or potentially offensive responses
to users. Hence, a mechanism to prevent this behavior is crucial to build
trusted NSP-based Question Answering agents. To that end, we propose the
Hallucination Simulation Framework (HSF), a general setting for stimulating and
analyzing NSP model hallucinations. The framework can be applied to any NSP
task with a closed-ontology. Using the proposed framework and KQA Pro as the
benchmark dataset, we assess state-of-the-art techniques for hallucination
detection. We then present a novel hallucination detection strategy that
exploits the computational graph of the NSP model to detect the NSP
hallucinations in the presence of ontology gaps, out-of-domain utterances, and
to recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and
~1%. This is the first work in closed-ontology NSP that addresses the problem
of recognizing ontology gaps. We release our code and checkpoints at
https://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TocBERT: Medical Document Structure Extraction Using Bidirectional
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majd Saleh, Sarra Baghdadi, Stéphane Paquelet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text segmentation holds paramount importance in the field of Natural Language
Processing (NLP). It plays an important role in several NLP downstream tasks
like information retrieval and document summarization. In this work, we propose
a new solution, namely TocBERT, for segmenting texts using bidirectional
transformers. TocBERT represents a supervised solution trained on the detection
of titles and sub-titles from their semantic representations. This task was
formulated as a named entity recognition (NER) problem. The solution has been
applied on a medical text segmentation use-case where the Bio-ClinicalBERT
model is fine-tuned to segment discharge summaries of the MIMIC-III dataset.
The performance of TocBERT has been evaluated on a human-labeled ground truth
corpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a
linear text segmentation problem and 72.8% on a hierarchical text segmentation
problem. It outperformed a carefully designed rule-based solution, particularly
in distinguishing titles from subtitles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Captioning Visualizations with <span class="highlight-title">Large Language Models</span> (CV<span class="highlight-title">LLM</span>): A Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Carenini, Jordon Johnson, Ali Salamatian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically captioning visualizations is not new, but recent advances in
large language models(LLMs) open exciting new possibilities. In this tutorial,
after providing a brief review of Information Visualization (InfoVis)
principles and past work in captioning, we introduce neural models and the
transformer architecture used in generic LLMs. We then discuss their recent
applications in InfoVis, with a focus on captioning. Additionally, we explore
promising future directions in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Generative Language Models Multicultural? A Study on Hausa Culture
  and Emotions using ChatGPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Said Ahmad, Shiran Dudy, Resmi Ramachandranpillai, Kenneth Church
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as ChatGPT, are widely used to generate
content for various purposes and audiences. However, these models may not
reflect the cultural and emotional diversity of their users, especially for
low-resource languages. In this paper, we investigate how ChatGPT represents
Hausa's culture and emotions. We compare responses generated by ChatGPT with
those provided by native Hausa speakers on 37 culturally relevant questions. We
conducted experiments using emotion analysis and applied two similarity metrics
to measure the alignment between human and ChatGPT responses. We also collected
human participants ratings and feedback on ChatGPT responses. Our results show
that ChatGPT has some level of similarity to human responses, but also exhibits
some gaps and biases in its knowledge and awareness of the Hausa culture and
emotions. We discuss the implications and limitations of our methodology and
analysis and suggest ways to improve the performance and evaluation of LLMs for
low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating How <span class="highlight-title">Large Language Models</span> Leve<span class="highlight-title">rag</span>e Internal Knowledge to
  Perform Complex Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miyoung Ko, Sue Hyun Park, Joonsuk Park, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements, there is a limited understanding of how
large language models (LLMs) utilize knowledge for reasoning. To address this,
we propose a method that deconstructs complex real-world questions into a
graph, representing each question as a node with parent nodes of background
knowledge needed to solve the question. We develop the DepthQA dataset,
deconstructing questions into three depths: (i) recalling conceptual knowledge,
(ii) applying procedural knowledge, and (iii) analyzing strategic knowledge.
Based on a hierarchical graph, we quantify forward discrepancy, discrepancies
in LLMs' performance on simpler sub-problems versus complex questions. We also
measure backward discrepancy, where LLMs answer complex questions but struggle
with simpler ones. Our analysis shows that smaller models have more
discrepancies than larger models. Additionally, guiding models from simpler to
complex questions through multi-turn interactions improves performance across
model sizes, highlighting the importance of structured intermediate steps in
knowledge reasoning. This work enhances our understanding of LLM reasoning and
suggests ways to improve their problem-solving abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress; code is available at
  https://github.com/kaistAI/knowledge-reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monitoring Latent World States in Language Models with Propositional
  Probes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahai Feng, Stuart Russell, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models are susceptible to bias, sycophancy, backdoors, and other
tendencies that lead to unfaithful responses to the input context. Interpreting
internal states of language models could help monitor and correct unfaithful
behavior. We hypothesize that language models represent their input contexts in
a latent world model, and seek to extract this latent world state from the
activations. We do so with 'propositional probes', which compositionally probe
tokens for lexical information and bind them into logical propositions
representing the world state. For example, given the input context ''Greg is a
nurse. Laura is a physicist.'', we decode the propositions ''WorksAs(Greg,
nurse)'' and ''WorksAs(Laura, physicist)'' from the model's activations. Key to
this is identifying a 'binding subspace' in which bound tokens have high
similarity (''Greg'' and ''nurse'') but unbound ones do not (''Greg'' and
''physicist''). We validate propositional probes in a closed-world setting with
finitely many predicates and properties. Despite being trained on simple
templated contexts, propositional probes generalize to contexts rewritten as
short stories and translated to Spanish. Moreover, we find that in three
settings where language models respond unfaithfully to the input context --
prompt injections, backdoor attacks, and gender bias -- the decoded
propositions remain faithful. This suggests that language models often encode a
faithful world model but decode it unfaithfully, which motivates the search for
better interpretability tools for monitoring LMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge acquisition for dialogue agents using reinforcement learning
  on graph representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selene Baez Santamaria, Shihan Wang, Piek Vossen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an artificial agent motivated to augment its knowledge base beyond
its initial training. The agent actively participates in dialogues with other
agents, strategically acquiring new information. The agent models its knowledge
as an RDF knowledge graph, integrating new beliefs acquired through
conversation. Responses in dialogue are generated by identifying graph patterns
around these new integrated beliefs. We show that policies can be learned using
reinforcement learning to select effective graph patterns during an
interaction, without relying on explicit user feedback. Within this context,
our study is a proof of concept for leveraging users as effective sources of
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inclusivity in <span class="highlight-title">Large Language Models</span>: Personality Traits and Gender Bias
  in Scientific Abstracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naseela Pervez, Alexander J. Titus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly utilized to assist in
scientific and academic writing, helping authors enhance the coherence of their
articles. Previous studies have highlighted stereotypes and biases present in
LLM outputs, emphasizing the need to evaluate these models for their alignment
with human narrative styles and potential gender biases. In this study, we
assess the alignment of three prominent LLMs - Claude 3 Opus, Mistral AI Large,
and Gemini 1.5 Flash - by analyzing their performance on benchmark
text-generation tasks for scientific abstracts. We employ the Linguistic
Inquiry and Word Count (LIWC) framework to extract lexical, psychological, and
social features from the generated texts. Our findings indicate that, while
these models generally produce text closely resembling human authored content,
variations in stylistic features suggest significant gender biases. This
research highlights the importance of developing LLMs that maintain a diversity
of writing styles to promote inclusivity in academic discourse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development and Evaluation of a <span class="highlight-title">Retrieval-Augmented</span> Generation Tool for
  Creating SAPPhIRE Models of Artificial Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anubhab Majumder, Kausik Bhattacharya, Amaresh Chakrabarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing systems using the SAPPhIRE causality model is found useful in
supporting design-by-analogy. However, creating a SAPPhIRE model of artificial
or biological systems is an effort-intensive process that requires human
experts to source technical knowledge from multiple technical documents
regarding how the system works. This research investigates how to leverage
Large Language Models (LLMs) in creating structured descriptions of systems
using the SAPPhIRE model of causality. This paper, the second part of the
two-part research, presents a new Retrieval-Augmented Generation (RAG) tool for
generating information related to SAPPhIRE constructs of artificial systems and
reports the results from a preliminary evaluation of the tool's success -
focusing on the factual accuracy and reliability of outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shouchang Guo, Sonam Damani, Keng-hao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In prompt tuning, a prefix or suffix text is added to the prompt, and the
embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix
are optimized to gain more control over language models for specific tasks.
This approach eliminates the need for hand-crafted prompt engineering or
explicit model fine-tuning. Prompt tuning is significantly more
parameter-efficient than model fine-tuning, as it involves optimizing partial
inputs of language models to produce desired outputs.
  In this work, we aim to further reduce the amount of trainable parameters
required for a language model to perform well on specific tasks. We propose
Low-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves
efficient prompt optimization. The proposed method demonstrates similar
outcomes to full parameter prompt tuning while reducing the number of trainable
parameters by a factor of 5. It also provides promising results compared to the
state-of-the-art methods that would require 10 to 20 times more parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xTower: A Multilingual <span class="highlight-title">LLM</span> for Explaining and Correcting Translation
  Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos Treviso, Nuno M. Guerreiro, Sweta Agrawal, Ricardo Rei, José Pombal, Tania Vaz, Helena Wu, Beatriz Silva, Daan van Stigt, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While machine translation (MT) systems are achieving increasingly strong
performance on benchmarks, they often produce translations with errors and
anomalies. Understanding these errors can potentially help improve the
translation quality and user experience. This paper introduces xTower, an open
large language model (LLM) built on top of TowerBase designed to provide
free-text explanations for translation errors in order to guide the generation
of a corrected translation. The quality of the generated explanations by xTower
are assessed via both intrinsic and extrinsic evaluation. We ask expert
translators to evaluate the quality of the explanations across two dimensions:
relatedness towards the error span being explained and helpfulness in error
understanding and improving translation quality. Extrinsically, we test xTower
across various experimental setups in generating translation corrections,
demonstrating significant improvements in translation quality. Our findings
highlight xTower's potential towards not only producing plausible and helpful
explanations of automatic translations, but also leveraging them to suggest
corrected translations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Regression for Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergun Biçici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use transductive regression techniques to learn mappings between source
and target features of given parallel corpora and use these mappings to
generate machine translation outputs. We show the effectiveness of $L_1$
regularized regression (\textit{lasso}) to learn the mappings between sparsely
observed feature sets versus $L_2$ regularized regression. Proper selection of
training instances plays an important role to learn correct feature mappings
within limited computational resources and at expected accuracy levels. We
introduce \textit{dice} instance selection method for proper selection of
training instances, which plays an important role to learn correct feature
mappings for improving the source and target coverage of the training set. We
show that $L_1$ regularized regression performs better than $L_2$ regularized
regression both in regression measurements and in the translation experiments
using graph decoding. We present encouraging results when translating from
German to English and Spanish to English. We also demonstrate results when the
phrase table of a phrase-based decoder is replaced with the mappings we find
with the regression model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Changing Answer Order Can Decrease MMLU Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vipul Gupta, David Pantoja, Candace Ross, Adina Williams, Megan Ung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) have grown in prevalence, particular
benchmarks have become essential for the evaluation of these models and for
understanding model capabilities. Most commonly, we use test accuracy averaged
across multiple subtasks in order to rank models on leaderboards, to determine
which model is best for our purposes. In this paper, we investigate the
robustness of the accuracy measurement on a widely used multiple choice
question answering dataset, MMLU. When shuffling the answer label contents, we
find that all explored models decrease in accuracy on MMLU, but not every model
is equally sensitive. These findings suggest a possible adjustment to the
standard practice of leaderboard testing, where we additionally consider the
percentage of examples each model answers correctly by random chance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short paper, 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">Large Language Models</span> Generate High-quality Patent Claims? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lekang Jiang, Caiqi Zhang, Pascal A Scherz, Stephan Goetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown exceptional performance across
various text generation tasks but remain under-explored in the patent domain,
which offers highly structured and precise language. This paper constructs a
dataset to investigate the performance of current LLMs in patent claim
generation. Our results demonstrate that generating claims based on patent
descriptions outperforms previous research relying on abstracts. Interestingly,
current patent-specific LLMs perform much worse than state-of-the-art general
LLMs, highlighting the necessity for future research on in-domain LLMs. We also
find that LLMs can produce high-quality first independent claims, but their
performances markedly decrease for subsequent dependent claims. Moreover,
fine-tuning can enhance the completeness of inventions' features, conceptual
clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the
best performance in comprehensive human evaluations by patent experts, with
better feature coverage, conceptual clarity, and technical coherence. Despite
these capabilities, comprehensive revision and modification are still necessary
to pass rigorous patent scrutiny and ensure legal robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank
  Modifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show inherent brittleness in their safety
mechanisms, as evidenced by their susceptibility to jailbreaking and even
non-malicious fine-tuning. This study explores this brittleness of safety
alignment by leveraging pruning and low-rank modifications. We develop methods
to identify critical regions that are vital for safety guardrails, and that are
disentangled from utility-relevant regions at both the neuron and rank levels.
Surprisingly, the isolated regions we find are sparse, comprising about $3\%$
at the parameter level and $2.5\%$ at the rank level. Removing these regions
compromises safety without significantly impacting utility, corroborating the
inherent brittleness of the model's safety mechanisms. Moreover, we show that
LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications
to the safety-critical regions are restricted. These findings underscore the
urgent need for more robust safety strategies in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures. Project page is available at
  https://boyiwei.com/alignment-attribution/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VDebugger: Harnessing Execution Feedback for Debugging Visual Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual programs are executable code generated by large language models to
address visual reasoning problems. They decompose complex questions into
multiple reasoning steps and invoke specialized models for each step to solve
the problems. However, these programs are prone to logic errors, with our
preliminary evaluation showing that 58% of the total errors are caused by
program logic errors. Debugging complex visual programs remains a major
bottleneck for visual reasoning. To address this, we introduce VDebugger, a
novel critic-refiner framework trained to localize and debug visual programs by
tracking execution step by step. VDebugger identifies and corrects program
errors leveraging detailed execution feedback, improving interpretability and
accuracy. The training data is generated through an automated pipeline that
injects errors into correct visual programs using a novel mask-best decoding
technique. Evaluations on six datasets demonstrate VDebugger's effectiveness,
showing performance improvements of up to 3.2% in downstream task accuracy.
Further studies show VDebugger's ability to generalize to unseen tasks,
bringing a notable improvement of 2.3% on the unseen COVR task. Code, data and
models are made publicly available at https://github.com/shirley-wu/vdebugger/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WebCanvas: Benchmarking Web Agents in Online Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, Zhengyang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For web agents to be practically useful, they must adapt to the continuously
evolving web environment characterized by frequent updates to user interfaces
and content. However, most existing benchmarks only capture the static aspects
of the web. To bridge this gap, we introduce WebCanvas, an innovative online
evaluation framework for web agents that effectively addresses the dynamic
nature of web interactions. WebCanvas contains three main components to
facilitate realistic assessments: (1) A novel evaluation metric which reliably
capture critical intermediate actions or states necessary for task completions
while disregarding noise caused by insignificant events or changed
web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version
of original Mind2Web static dataset containing 542 tasks with 2439 intermediate
evaluation states; (3) Lightweight and generalizable annotation tools and
testing pipelines that enables the community to collect and maintain the
high-quality, up-to-date dataset. Building on WebCanvas, we open-source an
agent framework with extensible modules for reasoning, providing a foundation
for the community to conduct online inference and evaluations. Our
best-performing agent achieves a task success rate of 23.1% and a task
completion rate of 48.8% on the Mind2Web-Live test set. Additionally, we
analyze the performance discrepancies across various websites, domains, and
experimental environments. We encourage the community to contribute further
insights on online agent evaluation, thereby advancing this field of research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our platform, tool and dataset are publically available at
  https://www.imean.ai/web-canvas/ and
  https://huggingface.co/datasets/iMeanAI/Mind2Web-Live/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-On-Feet Tuning: Scaling Self-Alignment of <span class="highlight-title">LLM</span>s via Bootstrapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-alignment is an effective way to reduce the cost of human annotation
while ensuring promising model capability. However, most current methods
complete the data collection and training steps in a single round, which may
overlook the continuously improving ability of self-aligned models. This gives
rise to a key query: What if we do multi-time bootstrapping self-alignment?
Does this strategy enhance model performance or lead to rapid degradation? In
this paper, our pioneering exploration delves into the impact of bootstrapping
self-alignment on large language models. Our findings reveal that bootstrapping
self-alignment markedly surpasses the single-round approach, by guaranteeing
data diversity from in-context learning. To further exploit the capabilities of
bootstrapping, we investigate and adjust the training order of data, which
yields improved performance of the model. Drawing on these findings, we propose
Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
few-shot ability to boost zero or one-shot performance. Based on easy-to-hard
training recipe, we propose SOFT+ which further boost self-alignment's
performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across
various classification and generation tasks, highlighting the potential of
bootstrapping self-alignment on continually enhancing model alignment
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thermometer: Towards Universal Calibration for <span class="highlight-title">Large Language Models</span> <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, Soumya Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the issue of calibration in large language models (LLM). Recent
studies have found that common interventions such as instruction tuning often
result in poorly calibrated LLMs. Although calibration is well-explored in
traditional applications, calibrating LLMs is uniquely challenging. These
challenges stem as much from the severe computational requirements of LLMs as
from their versatility, which allows them to be applied to diverse tasks.
Addressing these challenges, we propose THERMOMETER, a calibration approach
tailored to LLMs. THERMOMETER learns an auxiliary model, given data from
multiple tasks, for calibrating a LLM. It is computationally efficient,
preserves the accuracy of the LLM, and produces better-calibrated responses for
new tasks. Extensive empirical evaluations across various benchmarks
demonstrate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuTox: Universal MUltilingual Audio-based TOXicity <span class="highlight-title">Dataset</span> and Zero-shot
  Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.05060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.05060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta R. Costa-jussà, Mariano Coria Meglioli, Pierre Andrews, David Dale, Prangthip Hansanti, Elahe Kalbassi, Alex Mourachko, Christophe Ropers, Carleigh Wood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in toxicity detection in natural language processing for the speech
modality (audio-based) is quite limited, particularly for languages other than
English. To address these limitations and lay the groundwork for truly
multilingual audio-based toxicity detection, we introduce MuTox, the first
highly multilingual audio-based dataset with toxicity labels. The dataset
comprises 20,000 audio utterances for English and Spanish, and 4,000 for the
other 19 languages. To demonstrate the quality of this dataset, we trained the
MuTox audio-based toxicity classifier, which enables zero-shot toxicity
detection across a wide range of languages. This classifier outperforms
existing text-based trainable classifiers by more than 1% AUC, while expanding
the language coverage more than tenfold. When compared to a wordlist-based
classifier that covers a similar number of languages, MuTox improves precision
and recall by approximately 2.5 times. This significant improvement underscores
the potential of MuTox in advancing the field of audio-based toxicity
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaGPT: Merging <span class="highlight-title">Large Language Models</span> Using Model Exclusive Task
  Arithmetic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyan Zhou, Liang Song, Bingning Wang, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) like GPT-4 has catalyzed the
exploration of multi-task learning (MTL), in which a single model demonstrates
proficiency across diverse tasks. Task arithmetic has emerged as a
cost-effective approach for MTL. It enables performance enhancement across
multiple tasks by adding their corresponding task vectors to a pre-trained
model. However, the current lack of a method that can simultaneously achieve
optimal performance, computational efficiency, and data privacy limits their
application to LLMs. In this paper, we propose \textbf{M}odel
\textbf{E}xclusive \textbf{T}ask \textbf{A}rithmetic for merging
\textbf{GPT}-scale models, which formalizes the objective of model merging into
a multi-task learning framework, aiming to minimize the average loss difference
between the merged model and each individual task model. Since data privacy
limits the use of multi-task training data, we leverage LLMs' local linearity
and task vectors' orthogonality to separate the data term and scaling
coefficients term and derive a model-exclusive task arithmetic method. Our
proposed MetaGPT is data-agnostic and bypasses the heavy search process, making
it cost-effective and easy to implement for LLMs.Extensive experiments
demonstrate that MetaGPT leads to improvements in task arithmetic and achieves
state-of-the-art performance on multiple tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLERC: A <span class="highlight-title">Dataset</span> for Legal Case Retrieval and <span class="highlight-title">Retrieval-Augmented</span>
  Analysis Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abe Bohan Hou, Orion Weller, Guanghui Qin, Eugene Yang, Dawn Lawrie, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal professionals need to write analyses that rely on citations to relevant
precedents, i.e., previous case decisions. Intelligent systems assisting legal
professionals in writing such documents provide great benefits but are
challenging to design. Such systems need to help locate, summarize, and reason
over salient precedents in order to be useful. To enable systems for such
tasks, we work with legal professionals to transform a large open-source legal
corpus into a dataset supporting two important backbone tasks: information
retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC
(Case Law Evaluation Retrieval Corpus), is constructed for training and
evaluating models on their ability to (1) find corresponding citations for a
given piece of legal analysis and to (2) compile the text of these citations
(as well as previous context) into a cogent analysis that supports a reasoning
goal. We benchmark state-of-the-art models on CLERC, showing that current
approaches still struggle: GPT-4o generates analyses with the highest ROUGE
F-scores but hallucinates the most, while zero-shot IR models only achieve
48.3% recall@1000.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the nature of <span class="highlight-title">large language models</span>: A caution against
  anthropocentrism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07683v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07683v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ann Speed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI models garnered a large amount of public attention and
speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion
camps exist: one excited about possibilities these models offer for fundamental
changes to human tasks, and another highly concerned about power these models
seem to have. To address these concerns, we assessed several LLMs, primarily
GPT 3.5, using standard, normed, and validated cognitive and personality
measures. For this seedling project, we developed a battery of tests that
allowed us to estimate the boundaries of some of these models capabilities, how
stable those capabilities are over a short period of time, and how they compare
to humans. Our results indicate that LLMs are unlikely to have developed
sentience, although its ability to respond to personality inventories is
interesting. GPT3.5 did display large variability in both cognitive and
personality measures over repeated observations, which is not expected if it
had a human-like personality. Variability notwithstanding, LLMs display what in
a human would be considered poor mental health, including low self-esteem,
marked dissociation from reality, and in some cases narcissism and psychopathy,
despite upbeat and helpful responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Hospital: Benchmarking <span class="highlight-title">Large Language Models</span> in a Multi-agent Medical
  Interaction Simulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09742v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09742v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has significantly advanced healthcare, particularly
through large language models (LLMs) that excel in medical question answering
benchmarks. However, their real-world clinical application remains limited due
to the complexities of doctor-patient interactions. To address this, we
introduce \textbf{AI Hospital}, a multi-agent framework simulating dynamic
medical interactions between \emph{Doctor} as player and NPCs including
\emph{Patient}, \emph{Examiner}, \emph{Chief Physician}. This setup allows for
realistic assessments of LLMs in clinical scenarios. We develop the Multi-View
Medical Evaluation (MVME) benchmark, utilizing high-quality Chinese medical
records and NPCs to evaluate LLMs' performance in symptom collection,
examination recommendations, and diagnoses. Additionally, a dispute resolution
collaborative mechanism is proposed to enhance diagnostic accuracy through
iterative discussions. Despite improvements, current LLMs exhibit significant
performance gaps in multi-turn interactions compared to one-step approaches.
Our findings highlight the need for further research to bridge these gaps and
improve LLMs' clinical diagnostic capabilities. Our data, code, and
experimental results are all open-sourced at
\url{https://github.com/LibertFan/AI_Hospital}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/LibertFan/AI_Hospital</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Muffin or Chihuahua? Challenging <span class="highlight-title">Multimodal Large Language Models</span> with
  Multipanel VQA <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multipanel images, commonly seen as web screenshots, posters, etc., pervade
our daily lives. These images, characterized by their composition of multiple
subfigures in distinct layouts, effectively convey information to people.
Toward building advanced multimodal AI applications, such as agents that
understand complex scenes and navigate through webpages, the skill of
multipanel visual reasoning is essential, and a comprehensive evaluation of
models in this regard is important. Therefore, we introduce Multipanel Visual
Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets
of questions, answers, and multipanel images that specifically challenge models
in comprehending multipanel images. Our evaluation shows that questions in the
MultipanelVQA benchmark pose significant challenges to the state-of-the-art
Multimodal Large Language Models (MLLMs) tested, even though humans can attain
approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA
benchmark features synthetically generated multipanel images specifically
crafted to isolate and assess the impact of various factors, such as the
layout, on MLLMs' multipanel image comprehension abilities. As a result, in
addition to benchmarking the capabilities of MLLMs in understanding multipanel
images, we analyze various factors of the multipanel image that affect MLLMs'
performance with synthetic data and offer insights for enhancement. Code and
data are released at https://sites.google.com/view/multipanelvqa/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Active Retrieval for <span class="highlight-title">Retrieval Augmented Generation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12534v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12534v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Retrieval-Augmented Generation (RAG), retrieval is not always helpful and
applying it to every instruction is sub-optimal. Therefore, determining whether
to retrieve is crucial for RAG, which is usually referred to as Active
Retrieval. However, existing active retrieval methods face two challenges: 1.
They usually rely on a single criterion, which struggles with handling various
types of instructions. 2. They depend on specialized and highly differentiated
procedures, and thus combining them makes the RAG system more complicated and
leads to higher response latency. To address these challenges, we propose
Unified Active Retrieval (UAR). UAR contains four orthogonal criteria and casts
them into plug-and-play classification tasks, which achieves multifaceted
retrieval timing judgements with negligible extra inference cost. We further
introduce the Unified Active Retrieval Criteria (UAR-Criteria), designed to
process diverse active retrieval scenarios through a standardized procedure.
Experiments on four representative types of user instructions show that UAR
significantly outperforms existing work on the retrieval timing judgement and
the performance of downstream tasks, which shows the effectiveness of UAR and
its helpfulness to downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFT: Reasoning with Reinforced Fine-Tuning <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One way to enhance the reasoning capability of Large Language Models (LLMs)
is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)
annotations. This approach does not show sufficiently strong generalization
ability, however, because the training only relies on the given CoT data. In
math problem-solving, for example, there is usually only one annotated
reasoning path for each question in the training data. Intuitively, it would be
better for the algorithm to learn from multiple annotated reasoning paths given
a question. To address this issue, we propose a simple yet effective approach
called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of
learning LLMs for reasoning, with math problem-solving as an example. ReFT
first warmups the model with SFT, and then employs on-line reinforcement
learning, specifically the PPO algorithm in this paper, to further fine-tune
the model, where an abundance of reasoning paths are automatically sampled
given the question and the rewards are naturally derived from the ground-truth
answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that
ReFT significantly outperforms SFT, and the performance can be potentially
further boosted by combining inference-time strategies such as majority voting
and re-ranking. Note that ReFT obtains the improvement by learning from the
same training questions as SFT, without relying on extra or augmented training
questions. This indicates a superior generalization ability for ReFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 main conference; adjust with reviewer comments; 13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Token-level Direct Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11999v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11999v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained Large Language Models (LLMs) is essential to align
them with human values and intentions. This process often utilizes methods like
pairwise comparisons and KL divergence against a reference LLM, focusing on the
evaluation of full answers generated by the models. However, the generation of
these responses occurs in a token level, following a sequential,
auto-regressive fashion. In this paper, we introduce Token-level Direct
Preference Optimization (TDPO), a novel approach to align LLMs with human
preferences by optimizing policy at the token level. Unlike previous methods,
which face challenges in divergence efficiency, TDPO incorporates forward KL
divergence constraints for each token, improving alignment and diversity.
Utilizing the Bradley-Terry model for a token-based reward system, TDPO
enhances the regulation of KL divergence, while preserving simplicity without
the need for explicit reward modeling. Experimental results across various text
tasks demonstrate TDPO's superior performance in balancing alignment with
generation diversity. Notably, fine-tuning with TDPO strikes a better balance
than DPO in the controlled sentiment generation and single-turn dialogue
datasets, and significantly improves the quality of generated responses
compared to both DPO and PPO-based RLHF methods. Our code is open-sourced at
https://github.com/Vance0124/Token-level-Direct-Preference-Optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedCalc-Bench: Evaluating <span class="highlight-title">Large Language Models</span> for Medical Calculations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12036v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12036v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As opposed to evaluating computation and logic-based reasoning, current
benchmarks for evaluating large language models (LLMs) in medicine are
primarily focused on question-answering involving domain knowledge and
descriptive reasoning. While such qualitative capabilities are vital to medical
diagnosis, in real-world scenarios, doctors frequently use clinical calculators
that follow quantitative equations and rule-based reasoning paradigms for
evidence-based decision support. To this end, we propose MedCalc-Bench, a
first-of-its-kind dataset focused on evaluating the medical calculation
capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000
manually reviewed instances from 55 different medical calculation tasks. Each
instance in MedCalc-Bench consists of a patient note, a question requesting to
compute a specific medical value, a ground truth answer, and a step-by-step
explanation showing how the answer is obtained. While our evaluation results
show the potential of LLMs in this area, none of them are effective enough for
clinical settings. Common issues include extracting the incorrect entities, not
using the correct equation or rules for a calculation task, or incorrectly
performing the arithmetic for the computation. We hope our study highlights the
quantitative knowledge and reasoning gaps in LLMs within medical settings,
encouraging future improvements of LLMs for various clinical calculation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github link: https://github.com/ncbi-nlp/MedCalc-Bench HuggingFace
  link: https://huggingface.co/datasets/nsk7153/MedCalc-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rendi Chevi, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We often verbally express emotions in a multifaceted manner, they may vary in
their intensities and may be expressed not just as a single but as a mixture of
emotions. This wide spectrum of emotions is well-studied in the structural
model of emotions, which represents variety of emotions as derivative products
of primary emotions with varying degrees of intensity. In this paper, we
propose an emotional text-to-speech design to simulate a wider spectrum of
emotions grounded on the structural model. Our proposed design, Daisy-TTS,
incorporates a prosody encoder to learn emotionally-separable prosody embedding
as a proxy for emotion. This emotion representation allows the model to
simulate: (1) Primary emotions, as learned from the training samples, (2)
Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by
scaling the emotion embedding, and (4) Emotions polarity, by negating the
emotion embedding. Through a series of perceptual evaluations, Daisy-TTS
demonstrated overall higher emotional speech naturalness and emotion
perceiveability compared to the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://rendchevi.github.io/daisy-tts; Updates: (1)
  Fixed typos, missing references, and layout, (2) Revise explanation on
  emotion classifier or discriminator</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIMATELI: Evaluating Entity Linking on Climate Change Data <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijia Zhou, Siyao Peng, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate Change (CC) is a pressing topic of global importance, attracting
increasing attention across research fields, from social sciences to Natural
Language Processing (NLP). CC is also discussed in various settings and
communication platforms, from academic publications to social media forums.
Understanding who and what is mentioned in such data is a first critical step
to gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),
the first manually annotated CC dataset that links 3,087 entity spans to
Wikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing
entity linking (EL) systems on the CC topic across various genres and propose
automated filtering methods for CC entities. We find that the performance of EL
models notably lags behind humans at both token and entity levels. Testing
within the scope of retaining or excluding non-nominal and/or non-CC entities
particularly impacts the models' performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, accepted at ClimateNLP 2024 workshop @ ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devichand Budagam, Sankalp KJ, Ashutosh Kumar, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the effectiveness of large language models (LLMs) in addressing
diverse tasks is essential for comprehending their strengths and weaknesses.
Conventional evaluation techniques typically apply a single prompting strategy
uniformly across datasets, not considering the varying degrees of task
complexity. We introduce the Hierarchical Prompting Taxonomy (HPT), a taxonomy
that employs a Hierarchical Prompt Framework (HPF) composed of five unique
prompting strategies, arranged from the simplest to the most complex, to assess
LLMs more precisely and to offer a clearer perspective. This taxonomy assigns a
score, called the Hierarchical Prompting Score (HP-Score), to datasets as well
as LLMs based on the rules of the taxonomy, providing a nuanced understanding
of their ability to solve diverse tasks and offering a universal measure of
task complexity. Additionally, we introduce the Adaptive Hierarchical Prompt
framework, which automates the selection of appropriate prompting strategies
for each task. This study compares manual and adaptive hierarchical prompt
frameworks using four instruction-tuned LLMs, namely Llama 3 8B, Phi 3 3.8B,
Mistral 7B, and Gemma 7B, across four datasets: BoolQ, CommonSenseQA (CSQA),
IWSLT-2017 en-fr (IWSLT), and SamSum. Experiments demonstrate the effectiveness
of HPT, providing a reliable way to compare different tasks and LLM
capabilities. This paper leads to the development of a universal evaluation
metric that can be used to evaluate both the complexity of the datasets and the
capabilities of LLMs. The implementation of both manual HPF and adaptive HPF is
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUB-Cirdan at "Discharge Me!": Zero shot discharge letter generation by
  open-source <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Guo, Greg Farnan, Niall McLaughlin, Barry Devereux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The BioNLP ACL'24 Shared Task on Streamlining Discharge Documentation aims to
reduce the administrative burden on clinicians by automating the creation of
critical sections of patient discharge letters. This paper presents our
approach using the Llama3 8B quantized model to generate the "Brief Hospital
Course" and "Discharge Instructions" sections. We employ a zero-shot method
combined with Retrieval-Augmented Generation (RAG) to produce concise,
contextually accurate summaries. Our contributions include the development of a
curated template-based approach to ensure reliability and consistency, as well
as the integration of RAG for word count prediction. We also describe several
unsuccessful experiments to provide insights into our pathway for the
competition. Our results demonstrate the effectiveness and efficiency of our
approach, achieving high scores across multiple evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BioNLP 2024 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with
  Lightweight Blocks <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Dhakal, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation Vision-Language Models (VLMs) trained using large-scale
open-domain images and text pairs have recently been adapted to develop
Vision-Language Segmentation Models (VLSMs) that allow providing text prompts
during inference to guide image segmentation. If robust and powerful VLSMs can
be built for medical images, it could aid medical professionals in many
clinical tasks where they must spend substantial time delineating the target
structure of interest. VLSMs for medical images resort to fine-tuning base VLM
or VLSM pretrained on open-domain natural image datasets due to fewer annotated
medical image datasets; this fine-tuning is resource-consuming and expensive as
it usually requires updating all or a significant fraction of the pretrained
parameters. Recently, lightweight blocks called adapters have been proposed in
VLMs that keep the pretrained model frozen and only train adapters during
fine-tuning, substantially reducing the computing resources required. We
introduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained
vision-language segmentation models using transformer encoders. Our experiments
in widely used CLIP-based segmentation models show that with only 3 million
trainable parameters, the VLSM-Adapter outperforms state-of-the-art and is
comparable to the upper bound end-to-end fine-tuning. The source code is
available at: https://github.com/naamiinepal/vlsm-adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2024, the 27th International Conference on Medical
  Image Computing and Computer Assisted Intervention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Handle Different Types of Out-of-Distribution Scenarios in
  Computational Argumentation? A Comprehensive and Fine-Grained Field Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08316v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08316v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Waldis, Yufang Hou, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of pre-trained Language Models (LMs) has markedly advanced natural
language processing, but their efficacy in out-of-distribution (OOD) scenarios
remains a significant challenge. Computational argumentation (CA), modeling
human argumentation processes, is a field notably impacted by these challenges
because complex annotation schemes and high annotation costs naturally lead to
resources barely covering the multiplicity of available text sources and
topics. Due to this data scarcity, generalization to data from uncovered
covariant distributions is a common challenge for CA tasks like stance
detection or argument classification. This work systematically assesses LMs'
capabilities for such OOD scenarios. While previous work targets specific OOD
types like topic shifts or OOD uniformly, we address three prevalent OOD
scenarios in CA: topic shift, domain shift, and language shift. Our findings
challenge the previously asserted general superiority of in-context learning
(ICL) for OOD. We find that the efficacy of such learning paradigms varies with
the type of OOD. Specifically, while ICL excels for domain shifts, prompt-based
fine-tuning surpasses for topic shifts. To sum up, we navigate the
heterogeneity of OOD scenarios in CA and empirically underscore the potential
of base-sized LMs in overcoming these challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Next-Generation Database Interfaces: A <span class="highlight-title">Survey</span> of <span class="highlight-title">LLM</span>-based Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating accurate SQL according to natural language questions (text-to-SQL)
is a long-standing challenge due to the complexities involved in user question
understanding, database schema comprehension, and SQL generation. Conventional
text-to-SQL systems, comprising human engineering and deep neural networks,
have made substantial progress. Subsequently, pre-trained language models
(PLMs) have been developed and utilized for text-to-SQL tasks, achieving
promising performance. As modern databases become more complex, the
corresponding user questions also grow more challenging, leading PLMs with
limited comprehension capabilities to produce incorrect SQL. This necessitates
more sophisticated and tailored optimization methods for PLMs, which, in turn,
restricts the applications of PLM-based systems. Most recently, large language
models (LLMs) have demonstrated significant capabilities in natural language
understanding as the model scale remains increasing. Therefore, integrating the
LLM-based implementation can bring unique opportunities, improvements, and
solutions to text-to-SQL research. In this survey, we present a comprehensive
review of LLM-based text-to-SQL. Specifically, we propose a brief overview of
the technical challenges and the evolutionary process of text-to-SQL. Then, we
provide a detailed introduction to the datasets and metrics designed to
evaluate text-to-SQL systems. After that, we present a systematic analysis of
recent advances in LLM-based text-to-SQL. Finally, we discuss the remaining
challenges in this field and propose expectations for future research
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WsiCaption: Multiple Instance Generation of Pathology Reports for
  Gigapixel Whole-Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16480v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16480v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingyi Chen, Honglin Li, Chenglu Zhu, Sunyi Zheng, Zhongyi Shui, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide images are the foundation of digital pathology for the diagnosis
and treatment of carcinomas. Writing pathology reports is laborious and
error-prone for inexperienced pathologists. To reduce the workload and improve
clinical automation, we investigate how to generate pathology reports given
whole slide images. On the data end, we curated the largest WSI-text dataset
(PathText). In specific, we collected nearly 10000 high-quality WSI-text pairs
for visual-language models by recognizing and cleaning pathology reports which
narrate diagnostic slides in TCGA. On the model end, we propose the multiple
instance generative model (MI-Gen) which can produce pathology reports for
gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText.
Experimental results show our model can generate pathology reports which
contain multiple clinical clues and achieve competitive performance on certain
slide-level tasks. We observe that simple semantic extraction from the
pathology reports can achieve the best performance (0.838 of F1 score) on BRCA
subtyping surpassing previous state-of-the-art approaches. Our collected
dataset and related code are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12416v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12416v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbang Yuan, Yubo Chen, Pengfei Cao, Zhuoran Jin, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success but still tend
to generate factually erroneous responses, a phenomenon known as hallucination.
A recent trend is to use preference learning to fine-tune models to align with
factuality. However, existing work primarily evaluates fine-tuned models on
in-domain (ID) datasets and the factuality on out-of-domain (OOD) datasets
remains underexplored. In this paper, we conduct a comprehensive evaluation of
the factuality of different models tuned by various preference learning
algorithms and demonstrate that their performance on OOD datasets either
increases minimally or decreases. Subsequently, we reveal that the main cause
of model's failure to uphold factuality under a distribution shift is
\textbf{under-alignment}, rather than \textbf{over-alignment}, by analyzing the
token distribution shift of the models before and after tuning. Finally, we
propose \textbf{APEFT} (\textbf{A}tomic \textbf{P}reference \textbf{E}nhanced
\textbf{F}actuality \textbf{T}uning), a framework that enhances model's
awareness of factuality at the granularity of individual facts. Extensive
experiments demonstrate that APEFT improves model performance by an average of
$\boldsymbol{3.45\%}$ on both ID and OOD datasets, which is highly effective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weak Reward Model Transforms Generative Models into Robust Causal Event
  Extraction Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Italo Luis da Silva, Hanqi Yan, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inherent ambiguity of cause and effect boundaries poses a challenge in
evaluating causal event extraction tasks. Traditional metrics like Exact Match
and BertScore poorly reflect model performance, so we trained evaluation models
to approximate human evaluation, achieving high agreement. We used them to
perform Reinforcement Learning with extraction models to align them with human
preference, prioritising semantic understanding. We successfully explored our
approach through multiple datasets, including transferring an evaluator trained
on one dataset to another as a way to decrease the reliance on human-annotated
data. In that vein, we also propose a weak-to-strong supervision method that
uses a fraction of the annotated data to train an evaluation model while still
achieving high performance in training an RL model. Our code is available at
https://github.com/oyarsa/event_extraction/tree/causal-event-extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrAM: Credibility-Aware Attention Modification in <span class="highlight-title">LLM</span>s for Combating
  Misinformation in <span class="highlight-title">RAG</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyi Deng, Wenjie Wang, Fengbin Zhu, Qifan Wang, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) can alleviate hallucinations of Large
Language Models (LLMs) by referencing external documents. However, the
misinformation in external documents may mislead LLMs' generation. To address
this issue, we explore the task of "credibility-aware RAG", in which LLMs
automatically adjust the influence of retrieved documents based on their
credibility scores to counteract misinformation. To this end, we introduce a
plug-and-play method named $\textbf{Cr}$edibility-aware $\textbf{A}$ttention
$\textbf{M}$odification (CrAM). CrAM identifies influential attention heads in
LLMs and adjusts their attention weights based on the credibility of the
documents, thereby reducing the impact of low-credibility documents.
Experiments on Natual Questions and TriviaQA using Llama2-13B, Llama3-8B, and
Qwen-7B show that CrAM improves the RAG performance of LLMs against
misinformation pollution by over 20%, even surpassing supervised fine-tuning
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Defeasibility in Causal Reasoning <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaobo Cui, Lazar Milikic, Yiyang Feng, Mete Ismayilzada, Debjit Paul, Antoine Bosselut, Boi Faltings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defeasibility in causal reasoning implies that the causal relationship
between cause and effect can be strengthened or weakened. Namely, the causal
strength between cause and effect should increase or decrease with the
incorporation of strengthening arguments (supporters) or weakening arguments
(defeaters), respectively. However, existing works ignore defeasibility in
causal reasoning and fail to evaluate existing causal strength metrics in
defeasible settings. In this work, we present $\delta$-CAUSAL, the first
benchmark dataset for studying defeasibility in causal reasoning.
$\delta$-CAUSAL includes around 11K events spanning ten domains, featuring
defeasible causality pairs, i.e., cause-effect pairs accompanied by supporters
and defeaters. We further show current causal strength metrics fail to reflect
the change of causal strength with the incorporation of supporters or defeaters
in $\delta$-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation
with Attention Rating), a metric that measures causal strength based on
token-level causal relationships. CESAR achieves a significant 69.7% relative
improvement over existing metrics, increasing from 47.2% to 80.1% in capturing
the causal strength change brought by supporters and defeaters. We further
demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and
10.7 points behind humans in generating supporters and defeaters, emphasizing
the challenge posed by $\delta$-CAUSAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leve<span class="highlight-title">rag</span>ing Synthetic Audio Data for End-to-End Low-Resource Speech
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17363v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17363v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasmin Moslem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes our system submission to the International Conference on
Spoken Language Translation (IWSLT 2024) for Irish-to-English speech
translation. We built end-to-end systems based on Whisper, and employed a
number of data augmentation techniques, such as speech back-translation and
noise augmentation. We investigate the effect of using synthetic audio data and
discuss several methods for enriching signal diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 1000 African Voices: Advancing inclusive multi-speaker multi-accent
  speech synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewade Ogun, Abraham T. Owodunni, Tobi Olatunji, Eniola Alese, Babatunde Oladimeji, Tejumade Afonja, Kayode Olaleye, Naome A. Etori, Tosin Adewumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in speech synthesis have enabled many useful applications
like audio directions in Google Maps, screen readers, and automated content
generation on platforms like TikTok. However, these systems are mostly
dominated by voices sourced from data-rich geographies with personas
representative of their source data. Although 3000 of the world's languages are
domiciled in Africa, African voices and personas are under-represented in these
systems. As speech synthesis becomes increasingly democratized, it is desirable
to increase the representation of African English accents. We present Afro-TTS,
the first pan-African accented English speech synthesis system able to generate
speech in 86 African accents, with 1000 personas representing the rich
phonological diversity across the continent for downstream application in
Education, Public Health, and Automated Content Creation. Speaker interpolation
retains naturalness and accentedness, enabling the creation of new voices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning Under Language Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01200v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01200v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelia Gogoulou, Timothée Lesort, Magnus Boman, Joakim Nivre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent increase in data and model scale for language model pre-training
has led to huge training costs. In scenarios where new data become available
over time, updating a model instead of fully retraining it would therefore
provide significant gains. We study the pros and cons of updating a language
model when new data comes from new languages -- the case of continual learning
under language shift. Starting from a monolingual English language model, we
incrementally add data from Danish, Icelandic, and Norwegian to investigate how
forward and backward transfer effects depend on pre-training order and
characteristics of languages, for three different model sizes. Our results show
that, while forward transfer is largely positive and independent of language
order, backward transfer can be positive or negative depending on the order and
characteristics of new languages. We explore a number of potentially
explanatory factors and find that a combination of language contamination and
syntactic similarity best fits our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TSD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Complex Disease Treatment through Network Medicine and
  GenAI: A Case Study on Drug Repurposing for Breast Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13106v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13106v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdeen Hamed, Tamer E. Fandy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this research is to introduce a network specialized in
predicting drugs that can be repurposed by investigating real-world evidence
sources, such as clinical trials and biomedical literature. Specifically, it
aims to generate drug combination therapies for complex diseases (e.g., cancer,
Alzheimer's). We present a multilayered network medicine approach, empowered by
a highly configured ChatGPT prompt engineering system, which is constructed on
the fly to extract drug mentions in clinical trials. Additionally, we introduce
a novel algorithm that connects real-world evidence with disease-specific
signaling pathways (e.g., KEGG database). This sheds light on the
repurposability of drugs if they are found to bind with one or more protein
constituents of a signaling pathway. To demonstrate, we instantiated the
framework for breast cancer and found that, out of 46 breast cancer signaling
pathways, the framework identified 38 pathways that were covered by at least
two drugs. This evidence signals the potential for combining those drugs.
Specifically, the most covered signaling pathway, ID hsa:2064, was covered by
108 drugs, some of which can be combined. Conversely, the signaling pathway ID
hsa:1499 was covered by only two drugs, indicating a significant gap for
further research. Our network medicine framework, empowered by GenAI, shows
promise in identifying drug combinations with a high degree of specificity,
knowing the exact signaling pathways and proteins that serve as targets. It is
noteworthy that ChatGPT successfully accelerated the process of identifying
drug mentions in clinical trials, though further investigations are required to
determine the relationships among the drug mentions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages double columns, 5 figures, 3 algorithms, 3 tables, and 1
  listing, Submitted to IEEE MedAI'24 Conference, to be held November 15-17,
  Chongqing, China</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Continual Pre-training by Mitigating the Stability Gap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiduo Guo, Jie Fu, Huishuai Zhang, Dongyan Zhao, Yikang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual pre-training has increasingly become the predominant approach for
adapting Large Language Models (LLMs) to new domains. This process involves
updating the pre-trained LLM with a corpus from a new domain, resulting in a
shift in the training distribution. To study the behavior of LLMs during this
shift, we measured the model's performance throughout the continual
pre-training process. we observed a temporary performance drop at the
beginning, followed by a recovery phase, a phenomenon known as the "stability
gap," previously noted in vision models classifying new classes. To address
this issue and enhance LLM performance within a fixed compute budget, we
propose three effective strategies: (1) Continually pre-training the LLM on a
subset with a proper size for multiple epochs, resulting in faster performance
recovery than pre-training the LLM on a large corpus in a single epoch; (2)
Pre-training the LLM only on high-quality sub-corpus, which rapidly boosts
domain performance; and (3) Using a data mixture similar to the pre-training
data to reduce distribution gap. We conduct various experiments on Llama-family
models to validate the effectiveness of our strategies in both medical
continual pre-training and instruction tuning. For example, our strategies
improve the average medical task performance of the OpenLlama-3B model from
36.2% to 40.7% with only 40% of the original training budget and enhance the
average general task performance without causing forgetting. Furthermore, we
apply our strategies to the Llama-3-8B model. The resulting model,
Llama-3-Physician, achieves the best medical performance among current
open-source models, and performs comparably to or even better than GPT-4 on
several medical benchmarks. We release our models at
\url{https://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mHuBERT-147: A Compact Multilingual HuBERT Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06371v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06371v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcely Zanon Boito, Vivek Iyer, Nikolaos Lagos, Laurent Besacier, Ioan Calapodescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present mHuBERT-147, the first general-purpose massively multilingual
HuBERT speech representation model trained on 90K hours of clean, open-license
data. To scale up the multi-iteration HuBERT approach, we use faiss-based
clustering, achieving 5.2x faster label assignment than the original method. We
also apply a new multilingual batching up-sampling strategy, leveraging both
language and dataset diversity. After 3 training iterations, our compact 95M
parameter mHuBERT-147 outperforms larger models trained on substantially more
data. We rank second and first on the ML-SUPERB 10min and 1h leaderboards, with
SOTA scores for 3 tasks. Across ASR/LID tasks, our model consistently surpasses
XLS-R (300M params; 436K hours) and demonstrates strong competitiveness against
the much larger MMS (1B params; 491K hours). Our findings indicate that
mHuBERT-147 is a promising model for multilingual speech tasks, offering an
unprecedented balance between high performance and parameter efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the Interspeech 2024 paper of same name</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems
  in Commonsense Reasoning <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models exhibit high-level commonsense reasoning abilities,
especially with enhancement methods like Chain-of-Thought (CoT). However, we
find these CoT-like methods lead to a considerable number of originally correct
answers turning wrong, which we define as the Toxic CoT problem. To interpret
and mitigate this problem, we first utilize attribution tracing and causal
tracing methods to probe the internal working mechanism of the LLM during CoT
reasoning. Through comparisons, we prove that the model exhibits information
loss from the question over the shallow attention layers when generating
rationales or answers. Based on the probing findings, we design a novel method
called RIDERS (Residual decodIng and sERial-position Swap), which compensates
for the information deficit in the model from both decoding and serial-position
perspectives. Through extensive experiments on multiple commonsense reasoning
benchmarks, we validate that this method not only significantly eliminates
Toxic CoT problems (decreased by 23.6%), but also effectively improves the
model's overall commonsense reasoning performance (increased by 5.5%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper to ACL 2024 Main, 25 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GCRE-GPT: A Generative Model for Comparative Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yequan Wang, Hengran Zhang, Aixin Sun, Xuying Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given comparative text, comparative relation extraction aims to extract two
targets (\eg two cameras) in comparison and the aspect they are compared for
(\eg image quality). The extracted comparative relations form the basis of
further opinion analysis.Existing solutions formulate this task as a sequence
labeling task, to extract targets and aspects. However, they cannot directly
extract comparative relation(s) from text. In this paper, we show that
comparative relations can be directly extracted with high accuracy, by
generative model. Based on GPT-2, we propose a Generation-based Comparative
Relation Extractor (GCRE-GPT). Experiment results show that \modelname achieves
state-of-the-art accuracy on two datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 tables, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concentrate Attention: Towards Domain-Generalizable Prompt Optimization
  for Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10584v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10584v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengzhengxu Li, Xiaoming Liu, Zhaohan Zhang, Yichen Wang, Chen Liu, Yu Lan, Chao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in prompt optimization have notably enhanced the performance
of pre-trained language models (PLMs) on downstream tasks. However, the
potential of optimized prompts on domain generalization has been
under-explored. To explore the nature of prompt generalization on unknown
domains, we conduct pilot experiments and find that (i) Prompts gaining more
attention weight from PLMs' deep layers are more generalizable and (ii) Prompts
with more stable attention distributions in PLMs' deep layers are more
generalizable. Thus, we offer a fresh objective towards domain-generalizable
prompts optimization named "Concentration", which represents the "lookback"
attention from the current decoding token to the prompt tokens, to increase the
attention strength on prompts and reduce the fluctuation of attention
distribution. We adapt this new objective to popular soft prompt and hard
prompt optimization methods, respectively. Extensive experiments demonstrate
that our idea improves comparison prompt optimization methods by 1.42% for soft
prompt generalization and 2.16% for hard prompt generalization in accuracy on
the multi-source domain generalization setting, while maintaining satisfying
in-domain performance. The promising results validate the effectiveness of our
proposed prompt optimization objective and provide key insights into
domain-generalizable prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS 2024, Preprint, Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Osama Mohanned Afzal, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has brought an unprecedented surge
in machine-generated text (MGT) across diverse channels. This raises legitimate
concerns about its potential misuse and societal implications. The need to
identify and differentiate such content from genuine human-generated text is
critical in combating disinformation, preserving the integrity of education and
scientific fields, and maintaining trust in communication. In this work, we
address this problem by introducing a new benchmark based on a multilingual,
multi-domain, and multi-generator corpus of MGTs -- M4GT-Bench. The benchmark
is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT
detection; (2) multi-way detection where one need to identify, which particular
model generated the text; and (3) mixed human-machine text detection, where a
word boundary delimiting MGT from human-written content should be determined.
On the developed benchmark, we have tested several MGT detection baselines and
also conducted an evaluation of human performance. We see that obtaining good
performance in MGT detection usually requires an access to the training data
from the same domain and generators. The benchmark is available at
https://github.com/mbzuai-nlp/M4GT-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">LLM</span>s' Mathematical and Coding Competency through
  Ontology-guided Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09395v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09395v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Hong, Navonil Majumder, Deepanway Ghosal, Somak Aditya, Rada Mihalcea, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have showcased striking
results on existing logical reasoning benchmarks, with some models even
surpassing human performance. However, the true depth of their competencies and
robustness in reasoning tasks remains an open question. To this end, in this
paper, we focus on two popular reasoning tasks: arithmetic reasoning and code
generation. Particularly, we introduce: (i) a general ontology of perturbations
for maths and coding questions, (ii) a semi-automatic method to apply these
perturbations, and (iii) two datasets, MORE and CORE, respectively, of
perturbed maths and coding problems to probe the limits of LLM capabilities in
numeric reasoning and coding tasks. Through comprehensive evaluations of both
closed-source and open-source LLMs, we show a significant performance drop
across all the models against the perturbed questions, suggesting that the
current LLMs lack robust problem solving skills and structured reasoning
abilities in many areas, as defined by our ontology. We open source the
datasets and source codes at: https://github.com/declare-lab/llm_robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Ginn, Lindia Tjuatja, Taiqi He, Enora Rice, Graham Neubig, Alexis Palmer, Lori Levin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language documentation projects often involve the creation of annotated text
in a format such as interlinear glossed text (IGT), which captures fine-grained
morphosyntactic analyses in a morpheme-by-morpheme format. However, there are
few existing resources providing large amounts of standardized, easily
accessible IGT data, limiting their applicability to linguistic research, and
making it difficult to use such data in NLP modeling.
  We compile the largest existing corpus of IGT data from a variety of sources,
covering over 450k examples across 1.8k languages, to enable research on
crosslingual transfer and IGT generation. We normalize much of our data to
follow a standard set of labels across languages.
  Furthermore, we explore the task of automatically generating IGT in order to
aid documentation projects. As many languages lack sufficient monolingual data,
we pretrain a large multilingual model on our corpus. We demonstrate the
utility of this model by finetuning it on monolingual corpora, outperforming
SOTA models by up to 6.6%. We will make our pretrained model and dataset
available through Hugging Face, as well as provide access through a web
interface for use in language documentation efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures Submitted to ACL ARR June 2024. First two authors
  are equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Text-based Knowledge Graph Completion with Zero-Shot Large
  Language Models: A Focus on Semantic Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08279v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08279v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The design and development of text-based knowledge graph completion (KGC)
methods leveraging textual entity descriptions are at the forefront of
research. These methods involve advanced optimization techniques such as soft
prompts and contrastive learning to enhance KGC models. The effectiveness of
text-based methods largely hinges on the quality and richness of the training
data. Large language models (LLMs) can utilize straightforward prompts to alter
text data, thereby enabling data augmentation for KGC. Nevertheless, LLMs
typically demand substantial computational resources. To address these issues,
we introduce a framework termed constrained prompts for KGC (CP-KGC). This
CP-KGC framework designs prompts that adapt to different datasets to enhance
semantic richness. Additionally, CP-KGC employs a context constraint strategy
to effectively identify polysemous entities within KGC datasets. Through
extensive experimentation, we have verified the effectiveness of this
framework. Even after quantization, the LLM (Qwen-7B-Chat-int4) still enhances
the performance of text-based KGC methods \footnote{Code and datasets are
available at
\href{https://github.com/sjlmg/CP-KGC}{https://github.com/sjlmg/CP-KGC}}. This
study extends the performance limits of existing models and promotes further
integration of KGC with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>new version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NutePrune: Efficient Progressive Pruning with Numerous Teachers for
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengrui Li, Junzhe Chen, Xueting Han, Jing Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The considerable size of Large Language Models (LLMs) presents notable
deployment challenges, particularly on resource-constrained hardware.
Structured pruning, offers an effective means to compress LLMs, thereby
reducing storage costs and enhancing inference speed for more efficient
utilization. In this work, we study data-efficient and resource-efficient
structure pruning methods to obtain smaller yet still powerful models.
Knowledge Distillation is well-suited for pruning, as the intact model can
serve as an excellent teacher for pruned students. However, it becomes
challenging in the context of LLMs due to memory constraints. To address this,
we propose an efficient progressive Numerous-teacher pruning method
(NutePrune). NutePrune mitigates excessive memory costs by loading only one
intact model and integrating it with various masks and LoRA modules, enabling
it to seamlessly switch between teacher and student roles. This approach allows
us to leverage numerous teachers with varying capacities to progressively guide
the pruned model, enhancing overall performance. Extensive experiments across
various tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot
experiments, NutePrune retains 97.17% of the performance of the original model
at 20% sparsity and 95.07% at 25% sparsity. Our code is available at
https://github.com/Lucius-lsr/NutePrune.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Dimension and Resolvability of Jaccard Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel E. Lladser, Alexander J. Paradise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A subset of points in a metric space is said to resolve it if each point in
the space is uniquely characterized by its distance to each point in the
subset. In particular, resolving sets can be used to represent points in
abstract metric spaces as Euclidean vectors. Importantly, due to the triangle
inequality, points close by in the space are represented as vectors with
similar coordinates, which may find applications in classification problems of
symbolic objects under suitably chosen metrics. In this manuscript, we address
the resolvability of Jaccard spaces, i.e., metric spaces of the form
$(2^X,\text{Jac})$, where $2^X$ is the power set of a finite set $X$, and
$\text{Jac}$ is the Jaccard distance between subsets of $X$. Specifically, for
different $a,b\in 2^X$, $\text{Jac}(a,b)=|a\Delta b|/|a\cup b|$, where
$|\cdot|$ denotes size (i.e., cardinality) and $\Delta$ denotes the symmetric
difference of sets. We combine probabilistic and linear algebra arguments to
construct highly likely but nearly optimal (i.e., of minimal size) resolving
sets of $(2^X,\text{Jac})$. In particular, we show that the metric dimension of
$(2^X,\text{Jac})$, i.e., the minimum size of a resolving set of this space, is
$\Theta(|X|/\ln|X|)$. In addition, we show that a much smaller subset of $2^X$
suffices to resolve, with high probability, all different pairs of subsets of
$X$ of cardinality at most $\sqrt{|X|}/\ln|X|$, up to a factor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Context Pruning: Optimizing Real-World Code Completion with
  Repository-Level Pretrained Code <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhang, Yunshui Li, Jiaming Li, Xiaobo Xia, Jiaxi Yang, Run Luo, Minzheng Wang, Longze Chen, Junhao Liu, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some recently developed code large language models (Code LLMs) have been
pre-trained on repository-level code data (Repo-Code LLMs), enabling these
models to recognize repository structures and utilize cross-file information
for code completion. However, in real-world development scenarios, simply
concatenating the entire code repository often exceeds the context window
limits of these Repo-Code LLMs, leading to significant performance degradation.
In this study, we conducted extensive preliminary experiments and analyses on
six Repo-Code LLMs. The results indicate that maintaining the topological
dependencies of files and increasing the code file content in the completion
prompts can improve completion accuracy; pruning the specific implementations
of functions in all dependent files does not significantly reduce the accuracy
of completions. Based on these findings, we proposed a strategy named
Hierarchical Context Pruning (HCP) to construct completion prompts with high
informational code content. The HCP models the code repository at the function
level, maintaining the topological dependencies between code files while
removing a large amount of irrelevant code content, significantly reduces the
input length for repository-level code completion. We applied the HCP strategy
in experiments with six Repo-Code LLMs, and the results demonstrate that our
proposed method can significantly enhance completion accuracy while
substantially reducing the length of input. Our code and data are available at
https://github.com/Hambaobao/HCP-Coder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EHRNoteQA: An <span class="highlight-title">LLM</span> Benchmark for Real-World Clinical Practice Using
  Discharge Summaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16040v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16040v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon, Kwanghyun Kim, Jeewon Yang, Seunghyun Won, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discharge summaries in Electronic Health Records (EHRs) are crucial for
clinical decision-making, but their length and complexity make information
extraction challenging, especially when dealing with accumulated summaries
across multiple patient admissions. Large Language Models (LLMs) show promise
in addressing this challenge by efficiently analyzing vast and complex data.
Existing benchmarks, however, fall short in properly evaluating LLMs'
capabilities in this context, as they typically focus on single-note
information or limited topics, failing to reflect the real-world inquiries
required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel
benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each
linked to distinct patients' discharge summaries. Every QA pair is initially
generated using GPT-4 and then manually reviewed and refined by three
clinicians to ensure clinical relevance. EHRNoteQA includes questions that
require information across multiple discharge summaries and covers eight
diverse topics, mirroring the complexity and diversity of real clinical
inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice
question answering, and propose a reliable evaluation method for each. We
evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the
model performance (e.g., the length and number of discharge summaries).
Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations
in clinical practice, we measure the correlation between the LLM performance on
EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results
show that LLM performance on EHRNoteQA have higher correlation with
clinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to
other benchmarks, demonstrating its practical relevance in evaluating LLMs in
clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">Large Language Model</span> Summarizers Adapt to Diverse Scientific
  Communication Goals? <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcio Fonseca, Shay B. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the controllability of large language models
(LLMs) on scientific summarization tasks. We identify key stylistic and content
coverage factors that characterize different types of summaries such as paper
reviews, abstracts, and lay summaries. By controlling stylistic features, we
find that non-fine-tuned LLMs outperform humans in the MuP review generation
task, both in terms of similarity to reference summaries and human preferences.
Also, we show that we can improve the controllability of LLMs with
keyword-based classifier-free guidance (CFG) while achieving lexical overlap
comparable to strong fine-tuned baselines on arXiv and PubMed. However, our
results also indicate that LLMs cannot consistently generate long summaries
with more than 8 sentences. Furthermore, these models exhibit limited capacity
to produce highly abstractive lay summaries. Although LLMs demonstrate strong
generic summarization competency, sophisticated content control without costly
fine-tuning remains an open problem for domain-specific applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Models</span> for Cuffless Blood Pressure Measurement From
  Wearable Biosignals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengding Liu, Chen Chen, Jiannong Cao, Minglei Pan, Jikui Liu, Nan Li, Fen Miao, Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have captured significant interest from both
academia and industry due to their impressive performance across various
textual tasks. However, the potential of LLMs to analyze physiological
time-series data remains an emerging research field. Particularly, there is a
notable gap in the utilization of LLMs for analyzing wearable biosignals to
achieve cuffless blood pressure (BP) measurement, which is critical for the
management of cardiovascular diseases. This paper presents the first work to
explore the capacity of LLMs to perform cuffless BP estimation based on
wearable biosignals. We extracted physiological features from electrocardiogram
(ECG) and photoplethysmogram (PPG) signals and designed context-enhanced
prompts by combining these features with BP domain knowledge and user
information. Subsequently, we adapted LLMs to BP estimation tasks through
fine-tuning. To evaluate the proposed approach, we conducted assessments of ten
advanced LLMs using a comprehensive public dataset of wearable biosignals from
1,272 participants. The experimental results demonstrate that the optimally
fine-tuned LLM significantly surpasses conventional task-specific baselines,
achieving an estimation error of 0.00 $\pm$ 9.25 mmHg for systolic BP and 1.29
$\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies highlight the
benefits of our context enhancement strategy, leading to an 8.9% reduction in
mean absolute error for systolic BP estimation. This paper pioneers the
exploration of LLMs for cuffless BP measurement, providing a potential solution
to enhance the accuracy of cuffless BP measurement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EVALALIGN: Supervised Fine-Tuning Multimodal <span class="highlight-title">LLM</span>s with Human-Aligned
  Data for Evaluating Text-to-Image Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, Mengping Yang, Cheng Zhang, Hao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in text-to-image generative models have been
remarkable. Yet, the field suffers from a lack of evaluation metrics that
accurately reflect the performance of these models, particularly lacking
fine-grained metrics that can guide the optimization of the models. In this
paper, we propose EvalAlign, a metric characterized by its accuracy, stability,
and fine granularity. Our approach leverages the capabilities of Multimodal
Large Language Models (MLLMs) pre-trained on extensive datasets. We develop
evaluation protocols that focus on two key dimensions: image faithfulness and
text-image alignment. Each protocol comprises a set of detailed, fine-grained
instructions linked to specific scoring options, enabling precise manual
scoring of the generated images. We Supervised Fine-Tune (SFT) the MLLM to
align closely with human evaluative judgments, resulting in a robust evaluation
model. Our comprehensive tests across 24 text-to-image generation models
demonstrate that EvalAlign not only provides superior metric stability but also
aligns more closely with human preferences than existing metrics, confirming
its effectiveness and utility in model assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Repository: https://github.com/SAIS-FUXI/EvalAlign</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mobile<span class="highlight-title">LLM</span>: Optimizing Sub-billion Parameter Language Models for
  On-Device Use Cases <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the growing need for efficient large language models
(LLMs) on mobile devices, driven by increasing cloud costs and latency
concerns. We focus on designing top-quality LLMs with fewer than a billion
parameters, a practical choice for mobile deployment. Contrary to prevailing
belief emphasizing the pivotal role of data and parameter quantity in
determining model quality, our investigation underscores the significance of
model architecture for sub-billion scale LLMs. Leveraging deep and thin
architectures, coupled with embedding sharing and grouped-query attention
mechanisms, we establish a strong baseline network denoted as MobileLLM, which
attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M
state-of-the-art models. Additionally, we propose an immediate block-wise
weight-sharing approach with no increase in model size and only marginal
latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a
further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover,
MobileLLM model family shows significant improvements compared to previous
sub-billion models on chat benchmarks, and demonstrates close correctness to
LLaMA-v2 7B in API calling tasks, highlighting the capability of small models
for common on-device use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024. Code is available at
  https://github.com/facebookresearch/MobileLLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">Large Language Models</span> Follow Concept Annotation Guidelines? A Case
  Study on Scientific and Financial Domains <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08704v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08704v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcio Fonseca, Shay B. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) exhibit remarkable capacity to leverage
in-context demonstrations, it is still unclear to what extent they can learn
new concepts or facts from ground-truth labels. To address this question, we
examine the capacity of instruction-tuned LLMs to follow in-context concept
guidelines for sentence labeling tasks. We design guidelines that present
different types of factual and counterfactual concept definitions, which are
used as prompts for zero-shot sentence classification tasks. Our results show
that although concept definitions consistently help in task performance, only
the larger models (with 70B parameters or more) have limited ability to work
under counterfactual contexts. Importantly, only proprietary models such as
GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is
due to more sophisticated alignment methods. Finally, we find that
Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which
indicates that careful fine-tuning is more effective than increasing model
scale. Altogether, our simple evaluation method reveals significant gaps in
concept understanding between the most capable open-source language models and
the leading proprietary APIs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear Cross-Lingual Mapping of Sentence Embeddings <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Vasilyev, Fumika Isono, John Bohannon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantics of a sentence is defined with much less ambiguity than semantics of
a single word, and we assume that it should be better preserved by translation
to another language. If multilingual sentence embeddings intend to represent
sentence semantics, then the similarity between embeddings of any two sentences
must be invariant with respect to translation. Based on this suggestion, we
consider a simple linear cross-lingual mapping as a possible improvement of the
multilingual embeddings. We also consider deviation from orthogonality
conditions as a measure of deficiency of the embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation
  and Fine-grained Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Sun, Yushi Bai, Ji Qi, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To advance the evaluation of multimodal math reasoning in large multimodal
models (LMMs), this paper introduces a novel benchmark, MM-MATH. MM-MATH
consists of 5,929 open-ended middle school math problems with visual contexts,
with fine-grained classification across difficulty, grade level, and knowledge
points. Unlike existing benchmarks relying on binary answer comparison, MM-MATH
incorporates both outcome and process evaluations. Process evaluation employs
LMM-as-a-judge to automatically analyze solution steps, identifying and
categorizing errors into specific error types. Extensive evaluation of ten
models on MM-MATH reveals significant challenges for existing LMMs,
highlighting their limited utilization of visual information and struggles with
higher-difficulty problems. The best-performing model achieves only 31%
accuracy on MM-MATH, compared to 82% for humans. This highlights the
challenging nature of our benchmark for existing models and the significant gap
between the multimodal reasoning capabilities of current models and humans. Our
process evaluation reveals that diagram misinterpretation is the most common
error, accounting for more than half of the total error cases, underscoring the
need for improved image comprehension in multimodal reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>It has changed a lot from the previous version and needs to set up a
  new one</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a clinically accessible radiology foundation model: open-access
  and lightweight, with automated evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08002v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08002v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, Hany Awadalla, Julia Gong, Houdong Hu, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Yu Gu, Cliff Wong, Mu Wei, Tristan Naumann, Muhao Chen, Matthew P. Lungren, Akshay Chaudhari, Serena Yeung-Levy, Curtis P. Langlotz, Sheng Wang, Hoifung Poon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling laws and extraordinary performance of large foundation models
motivate the development and utilization of such models in biomedicine.
However, despite early promising results on some biomedical benchmarks, there
are still major challenges that need to be addressed before these models can be
used in real-world clinics. Frontier general-domain models such as GPT-4V still
have significant performance gaps in multimodal biomedical applications. More
importantly, less-acknowledged pragmatic issues, including accessibility, model
cost, and tedious manual evaluation make it hard for clinicians to use
state-of-the-art large models directly on private patient data. Here, we
explore training open-source small multimodal models (SMMs) to bridge
competency gaps for unmet clinical needs in radiology. To maximize data
efficiency, we adopt a modular approach by incorporating state-of-the-art
pre-trained models for image and text modalities, and focusing on training a
lightweight adapter to ground each modality to the text embedding space, as
exemplified by LLaVA-Med. For training, we assemble a large dataset of over 697
thousand radiology image-text pairs. For evaluation, we propose CheXprompt, a
GPT-4-based metric for factuality evaluation, and demonstrate its parity with
expert evaluation. For best practice, we conduct a systematic ablation study on
various choices in data engineering and multimodal training. The resulting
LlaVA-Rad (7B) model attains state-of-the-art results on standard radiology
tasks such as report generation and cross-modal retrieval, even outperforming
much larger models such as GPT-4V and Med-PaLM M (84B). The inference of
LlaVA-Rad is fast and can be performed on a single V100 GPU in private
settings, offering a promising state-of-the-art tool for real-world clinical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Large Language Model</span> Approach to Educational <span class="highlight-title">Survey</span> Feedback Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael J. Parker, Caitlin Anderson, Claire Stone, YeaRim Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper assesses the potential for the large language models (LLMs) GPT-4
and GPT-3.5 to aid in deriving insight from education feedback surveys.
Exploration of LLM use cases in education has focused on teaching and learning,
with less exploration of capabilities in education feedback analysis. Survey
analysis in education involves goals such as finding gaps in curricula or
evaluating teachers, often requiring time-consuming manual processing of
textual responses. LLMs have the potential to provide a flexible means of
achieving these goals without specialized machine learning models or
fine-tuning. We demonstrate a versatile approach to such goals by treating them
as sequences of natural language processing (NLP) tasks including
classification (multi-label, multi-class, and binary), extraction, thematic
analysis, and sentiment analysis, each performed by LLM. We apply these
workflows to a real-world dataset of 2500 end-of-course survey comments from
biomedical science courses, and evaluate a zero-shot approach (i.e., requiring
no examples or labeled training data) across all tasks, reflecting education
settings, where labeled data is often scarce. By applying effective prompting
practices, we achieve human-level performance on multiple tasks with GPT-4,
enabling workflows necessary to achieve typical goals. We also show the
potential of inspecting LLMs' chain-of-thought (CoT) reasoning for providing
insight that may foster confidence in practice. Moreover, this study features
development of a versatile set of classification categories, suitable for
various course types (online, hybrid, or in-person) and amenable to
customization. Our results suggest that LLMs can be used to derive a range of
insights from survey text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Methodology of Adapting Large English Language Models for Specific
  Cultural Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjing Zhang, Siqi Xiao, Xuejiao Lei, Ning Wang, Huazheng Zhang, Meijuan An, Bikun Yang, Zhaoxiang Liu, Kai Wang, Shiguo Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of large language models(LLMs) has emerged as a prominent
trend in the field of artificial intelligence. However, current
state-of-the-art LLMs are predominantly based on English. They encounter
limitations when directly applied to tasks in specific cultural domains, due to
deficiencies in domain-specific knowledge and misunderstandings caused by
differences in cultural values. To address this challenge, our paper proposes a
rapid adaptation method for large models in specific cultural contexts, which
leverages instruction-tuning based on specific cultural knowledge and safety
values data. Taking Chinese as the specific cultural context and utilizing the
LLaMA3-8B as the experimental English LLM, the evaluation results demonstrate
that the adapted LLM significantly enhances its capabilities in domain-specific
knowledge and adaptability to safety values, while maintaining its original
expertise advantages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient
  Compile-Time Prompt Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schnabel, Jennifer Neville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many modern LLM applications, such as retrieval augmented generation,
prompts have become programs themselves. In these settings, prompt programs are
repeatedly called with different user queries or data instances. A big
practical challenge is optimizing such prompt programs. Recent work has mostly
focused on either simple prompt programs or assumed that the general structure
of a prompt program is fixed.
  We introduce SAMMO, a framework to perform symbolic prompt program search for
compile-time optimizations of prompt programs. SAMMO represents prompt programs
on a symbolic level which allows for a rich set of transformations that can be
searched over during optimization. We show that SAMMO generalizes previous
methods and improves the performance of complex prompts on (1) instruction
tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several
different LLMs. We make all code available open-source at
https://github.com/microsoft/sammo .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13372v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13372v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, Yongqiang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient fine-tuning is vital for adapting large language models (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present LlamaFactory, a unified framework that
integrates a suite of cutting-edge efficient training methods. It provides a
solution for flexibly customizing the fine-tuning of 100+ LLMs without the need
for coding through the built-in web UI LlamaBoard. We empirically validate the
efficiency and effectiveness of our framework on language modeling and text
generation tasks. It has been released at
https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and
3,000 forks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, accepted to ACL 2024 System Demonstration Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning BERTs for Definition Extraction from Mathematical Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Horowitz, Ryan Hathaway
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we fine-tuned three pre-trained BERT models on the task of
"definition extraction" from mathematical English written in LaTeX. This is
presented as a binary classification problem, where either a sentence contains
a definition of a mathematical term or it does not. We used two original data
sets, "Chicago" and "TAC," to fine-tune and test these models. We also tested
on WFMALL, a dataset presented by Vanetik and Litvak in 2021 and compared the
performance of our models to theirs. We found that a high-performance
Sentence-BERT transformer model performed best based on overall accuracy,
recall, and precision metrics, achieving comparable results to the earlier
models with less computational effort.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target Span Detection for Implicit Harmful Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazanin Jafari, James Allan, Sheikh Muhammad Sarwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the targets of hate speech is a crucial step in grasping the
nature of such speech and, ultimately, in improving the detection of offensive
posts on online forums. Much harmful content on online platforms uses implicit
language especially when targeting vulnerable and protected groups such as
using stereotypical characteristics instead of explicit target names, making it
harder to detect and mitigate the language. In this study, we focus on
identifying implied targets of hate speech, essential for recognizing subtler
hate speech and enhancing the detection of harmful content on digital
platforms. We define a new task aimed at identifying the targets even when they
are not explicitly stated. To address that task, we collect and annotate target
spans in three prominent implicit hate speech datasets: SBIC, DynaHate, and
IHC. We call the resulting merged collection Implicit-Target-Span. The
collection is achieved using an innovative pooling method with matching scores
based on human annotations and Large Language Models (LLMs). Our experiments
indicate that Implicit-Target-Span provides a challenging test bed for target
span detection methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Software Engineering Methods For AI-Driven Deductive Legal Reasoning <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Padhye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent proliferation of generative artificial intelligence (AI)
technologies such as pre-trained large language models (LLMs) has opened up new
frontiers in computational law. An exciting area of development is the use of
AI to automate the deductive rule-based reasoning inherent in statutory and
contract law. This paper argues that such automated deductive legal reasoning
can now be viewed from the lens of software engineering, treating LLMs as
interpreters of natural-language programs with natural-language inputs. We show
how it is possible to apply principled software engineering techniques to
enhance AI-driven legal reasoning of complex statutes and to unlock new
applications in automated meta-reasoning such as mutation-guided example
generation and metamorphic property-based testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appearing in Onward! at SPLASH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Vorbeşti Româneşte?" A Recipe to Train Powerful Romanian <span class="highlight-title">LLM</span>s
  with English Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihai Masala, Denis C. Ilie-Ablachim, Alexandru Dima, Dragos Corlatescu, Miruna Zavelca, Ovio Olaru, Simina Terian, Andrei Terian, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have achieved almost human-like
performance on various tasks. While some LLMs have been trained on multilingual
data, most of the training data is in English; hence, their performance in
English greatly exceeds other languages. To our knowledge, we are the first to
collect and translate a large collection of texts, instructions, and benchmarks
and train, evaluate, and release open-source LLMs tailored for Romanian. We
evaluate our methods on four different categories, including academic
benchmarks, MT-Bench (manually translated), and a professionally built
historical, cultural, and social benchmark adapted to Romanian. We argue for
the usefulness and high performance of RoLLMs by obtaining state-of-the-art
results across the board. We publicly release all resources (i.e., data,
training and evaluation code, models) to support and encourage research on
Romanian LLMs while concurrently creating a generalizable recipe, adequate for
other low or less-resourced languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.07703</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guylingo: The Republic of Guyana Creole Corpora <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Clarke, Roland Daynauth, Charlene Wilkinson, Hubert Devonish, Jason Mars
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While major languages often enjoy substantial attention and resources, the
linguistic diversity across the globe encompasses a multitude of smaller,
indigenous, and regional languages that lack the same level of computational
support. One such region is the Caribbean. While commonly labeled as "English
speaking", the ex-British Caribbean region consists of a myriad of Creole
languages thriving alongside English. In this paper, we present Guylingo: a
comprehensive corpus designed for advancing NLP research in the domain of
Creolese (Guyanese English-lexicon Creole), the most widely spoken language in
the culturally rich nation of Guyana. We first outline our framework for
gathering and digitizing this diverse corpus, inclusive of colloquial
expressions, idioms, and regional variations in a low-resource language. We
then demonstrate the challenges of training and evaluating NLP models for
machine translation in Creole. Lastly, we discuss the unique opportunities
presented by recent NLP advancements for accelerating the formal adoption of
Creole languages as official languages in the Caribbean.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 Main Conference Special Theme Track: Languages
  of Latin America and The Caribbean</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Neurons Matter in IR? Applying Integrated Gradients-based Methods
  to Understand Cross-Encoders <span class="chip">ICTIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathias Vast, Basile Van Cooten, Laure Soulier, Benjamin Piwowarski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent addition of Retrieval-Augmented Generation (RAG), the scope
and importance of Information Retrieval (IR) has expanded. As a result, the
importance of a deeper understanding of IR models also increases. However,
interpretability in IR remains under-explored, especially when it comes to the
models' inner mechanisms. In this paper, we explore the possibility of adapting
Integrated Gradient-based methods in an IR context to identify the role of
individual neurons within the model. In particular, we provide new insights
into the role of what we call "relevance" neurons, as well as how they deal
with unseen data. Finally, we carry out an in-depth pruning study to validate
our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICTIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounded and Transparent Response Generation for Conversational
  Information-Seeking Systems <span class="chip">WSDM '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weronika Łajewska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While previous conversational information-seeking (CIS) research has focused
on passage retrieval, reranking, and query rewriting, the challenge of
synthesizing retrieved information into coherent responses remains. The
proposed research delves into the intricacies of response generation in CIS
systems. Open-ended information-seeking dialogues introduce multiple challenges
that may lead to potential pitfalls in system responses. The study focuses on
generating responses grounded in the retrieved passages and being transparent
about the system's limitations. Specific research questions revolve around
obtaining confidence-enriched information nuggets, automatic detection of
incomplete or incorrect responses, generating responses communicating the
system's limitations, and evaluating enhanced responses. By addressing these
research tasks the study aspires to contribute to the advancement of
conversational response generation, fostering more trustworthy interactions in
CIS dialogues, and paving the way for grounded and transparent systems to meet
users' needs in an information-driven world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 17th ACM International Conference on Web Search
  and Data Mining (WSDM '24), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowVQA: Mapping Multimodal Logic in Visual Question Answering with
  Flowcharts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for visual question answering lack in visual grounding
and complexity, particularly in evaluating spatial reasoning skills. We
introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of
visual question-answering multimodal language models in reasoning with
flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and
human-verified flowchart images from three distinct content sources, along with
22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,
including information localization, decision-making, and logical progression.
We conduct a thorough baseline evaluation on a suite of both open-source and
proprietary multimodal language models using various strategies, followed by an
analysis of directional bias. The results underscore the benchmark's potential
as a vital tool for advancing the field of multimodal modeling, providing a
focused and challenging environment for enhancing model performance in visual
and logical reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAVEN: Multitask <span class="highlight-title">Retrieval Augmented</span> Vision-Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Nagaraj Rao, Siddharth Choudhary, Aditya Deshpande, Ravi Kumar Satzoda, Srikar Appalaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of large language models to encode all the world's knowledge in
model parameters is unsustainable and has exacerbated resource barriers.
Retrieval-Augmented Generation (RAG) presents a potential solution, yet its
application to vision-language models (VLMs) is under explored. Existing
methods focus on models designed for single tasks. Furthermore, they're limited
by the need for resource intensive pre training, additional parameter
requirements, unaddressed modality prioritization and lack of clear benefit
over non-retrieval baselines. This paper introduces RAVEN, a multitask
retrieval augmented VLM framework that enhances base VLMs through efficient,
task specific fine-tuning. By integrating retrieval augmented samples without
the need for additional retrieval-specific parameters, we show that the model
acquires retrieval properties that are effective across multiple tasks. Our
results and extensive ablations across retrieved modalities for the image
captioning and VQA tasks indicate significant performance improvements compared
to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a
+3\% accuracy on specific VQA question types. This underscores the efficacy of
applying RAG approaches to VLMs, marking a stride toward more efficient and
accessible multimodal learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statements: Universal Information Extraction from Tables with Large
  Language Models for ESG KPIs <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lokesh Mishra, Sohayl Dhibi, Yusik Kim, Cesar Berrospi Ramis, Shubham Gupta, Michele Dolfi, Peter Staar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environment, Social, and Governance (ESG) KPIs assess an organization's
performance on issues such as climate change, greenhouse gas emissions, water
consumption, waste management, human rights, diversity, and policies. ESG
reports convey this valuable quantitative information through tables.
Unfortunately, extracting this information is difficult due to high variability
in the table structure as well as content. We propose Statements, a novel
domain agnostic data structure for extracting quantitative facts and related
information. We propose translating tables to statements as a new supervised
deep-learning universal information extraction task. We introduce SemTabNet - a
dataset of over 100K annotated tables. Investigating a family of T5-based
Statement Extraction Models, our best model generates statements which are 82%
similar to the ground-truth (compared to baseline of 21%). We demonstrate the
advantages of statements by applying our model to over 2700 tables from ESG
reports. The homogeneous nature of statements permits exploratory data analysis
on expansive information found in large collections of ESG reports.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the NLP4Climate workshop in the 62nd Annual Meeting of
  the Association for Computational Linguistics (ACL 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient course recommendations with T5-based ranking and summarization <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thijmen Bijl, Niels van Weeren, Suzan Verberne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we implement and evaluate a two-stage retrieval pipeline for a
course recommender system that ranks courses for skill-occupation pairs. The
in-production recommender system BrightFit provides course recommendations from
multiple sources. Some of the course descriptions are long and noisy, while
retrieval and ranking in an online system have to be highly efficient. We
developed a two-step retrieval pipeline with RankT5 finetuned on MSMARCO as
re-ranker. We compare two summarizers for course descriptions: a LongT5 model
that we finetuned for the task, and a generative LLM (Vicuna) with in-context
learning. We experiment with quantization to reduce the size of the ranking
model and increase inference speed. We evaluate our rankers on two newly
labelled datasets, with an A/B test, and with a user questionnaire. On the two
labelled datasets, our proposed two-stage ranking with automatic summarization
achieves a substantial improvement over the in-production (BM25) ranker:
nDCG@10 scores improve from 0.482 to 0.684 and from 0.447 to 0.844 on the two
datasets. We also achieve a 40% speed-up by using a quantized version of
RankT5. The improved quality of the ranking was confirmed by the questionnaire
completed by 29 respondents, but not by the A/B test. In the A/B test, a higher
clickthrough rate was observed for the BM25-ranking than for the proposed
two-stage retrieval. We conclude that T5-based re-ranking and summarization for
online course recommendation can obtain much better effectiveness than
single-step lexical retrieval, and that quantization has a large effect on
RankT5. In the online evaluation, however, other factors than relevance play a
role (such as speed and interpretability of the retrieval results), as well as
individual preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ReNeuIR 2024 (at SIGIR 2024) - 3rd Workshop on Reaching Efficiency in
  Neural Information Retrieval, 18 July, 2024, Washington D.C, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Formal Characterization of User Simulation Objectives in
  Conversational Information Access <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nolwenn Bernard, Krisztian Balog
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User simulation is a promising approach for automatically training and
evaluating conversational information access agents, enabling the generation of
synthetic dialogues and facilitating reproducible experiments at scale.
However, the objectives of user simulation for the different uses remain
loosely defined, hindering the development of effective simulators. In this
work, we formally characterize the distinct objectives for user simulators:
training aims to maximize behavioral similarity to real users, while evaluation
focuses on the accurate prediction of real-world conversational agent
performance. Through an empirical study, we demonstrate that optimizing for one
objective does not necessarily lead to improved performance on the other. This
finding underscores the need for tailored design considerations depending on
the intended use of the simulator. By establishing clear objectives and
proposing concrete measures to evaluate user simulators against those
objectives, we pave the way for the development of simulators that are
specifically tailored to their intended use, ultimately leading to more
effective conversational agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 2024 ACM SIGIR International Conference on the
  Theory of Information Retrieval (ICTIR '24), July 13, 2024, Washington DC,
  DC, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amplify Graph Learning for Recommendation via Sparsity Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Yuan, Haojie Li, Minying Fang, Xu Yu, Yongjing Hao, Junwei Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning models have been widely deployed in collaborative filtering
(CF) based recommendation systems. Due to the issue of data sparsity, the graph
structure of the original input lacks potential positive preference edges,
which significantly reduces the performance of recommendations. In this paper,
we study how to enhance the graph structure for CF more effectively, thereby
optimizing the representation of graph nodes. Previous works introduced matrix
completion techniques into CF, proposing the use of either stochastic
completion methods or superficial structure completion to address this issue.
However, most of these approaches employ random numerical filling that lack
control over noise perturbations and limit the in-depth exploration of
higher-order interaction features of nodes, resulting in biased graph
representations.
  In this paper, we propose an Amplify Graph Learning framework based on
Sparsity Completion (called AGL-SC). First, we utilize graph neural network to
mine direct interaction features between user and item nodes, which are used as
the inputs of the encoder. Second, we design a factorization-based method to
mine higher-order interaction features. These features serve as perturbation
factors in the latent space of the hidden layer to facilitate generative
enhancement. Finally, by employing the variational inference, the above
multi-order features are integrated to implement the completion and enhancement
of missing graph structures. We conducted benchmark and strategy experiments on
four real-world datasets related to recommendation tasks. The experimental
results demonstrate that AGL-SC significantly outperforms the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Food Recommendation using Clustering and Self-supervised
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Zhang, Xin Zhou, Qianwen Meng, Fanglin Zhu, Yonghui Xu, Zhiqi Shen, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food recommendation systems serve as pivotal components in the realm of
digital lifestyle services, designed to assist users in discovering recipes and
food items that resonate with their unique dietary predilections. Typically,
multi-modal descriptions offer an exhaustive profile for each recipe, thereby
ensuring recommendations that are both personalized and accurate. Our
preliminary investigation of two datasets indicates that pre-trained
multi-modal dense representations might precipitate a deterioration in
performance compared to ID features when encapsulating interactive
relationships. This observation implies that ID features possess a relative
superiority in modeling interactive collaborative signals. Consequently,
contemporary cutting-edge methodologies augment ID features with multi-modal
information as supplementary features, overlooking the latent semantic
relations between recipes. To rectify this, we present CLUSSL, a novel food
recommendation framework that employs clustering and self-supervised learning.
Specifically, CLUSSL formulates a modality-specific graph tailored to each
modality with discrete/continuous features, thereby transforming semantic
features into structural representation. Furthermore, CLUSSL procures recipe
representations pertinent to different modalities via graph convolutional
operations. A self-supervised learning objective is proposed to foster
independence between recipe representations derived from different unimodal
graphs. Comprehensive experiments on real-world datasets substantiate that
CLUSSL consistently surpasses state-of-the-art recommendation benchmarks in
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Surprisingly Simple yet Effective Multi-Query Rewriting Method for
  Conversational Passage Retrieval <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivica Kostric, Krisztian Balog
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational passage retrieval is challenging as it often requires the
resolution of references to previous utterances and needs to deal with the
complexities of natural language, such as coreference and ellipsis. To address
these challenges, pre-trained sequence-to-sequence neural query rewriters are
commonly used to generate a single de-contextualized query based on
conversation history. Previous research shows that combining multiple query
rewrites for the same user utterance has a positive effect on retrieval
performance. We propose the use of a neural query rewriter to generate multiple
queries and show how to integrate those queries in the passage retrieval
pipeline efficiently. The main strength of our approach lies in its simplicity:
it leverages how the beam search algorithm works and can produce multiple query
rewrites at no additional cost. Our contributions further include devising ways
to utilize multi-query rewrites in both sparse and dense first-pass retrieval.
We demonstrate that applying our approach on top of a standard passage
retrieval pipeline delivers state-of-the-art performance without sacrificing
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 47th International ACM SIGIR Conference on
  Research and Development in Information Retrieval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Personalized Federated Multi-scenario Multi-task Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ding, Yanbiao Ji, Xun Cai, Xin Xin, Xiaofeng Gao, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern recommender system applications, such as e-commerce, predicting
multiple targets like click-through rate (CTR) and post-view click-through \&
conversion rate (CTCVR) is common. Multi-task recommender systems are gaining
traction in research and practical use. Existing multi-task recommender systems
tackle diverse business scenarios, merging and modeling these scenarios unlocks
shared knowledge to boost overall performance. As new and more complex
real-world recommendation scenarios have emerged, data privacy issues make it
difficult to train a single global multi-task recommendation model that
processes multiple separate scenarios.
  In this paper, we propose a novel framework for personalized federated
multi-scenario multi-task recommendation, called PF-MSMTrec. We assign each
scenario to a dedicated client, with each client utilizing the
Mixture-of-Experts (MMoE) structure. Our proposed method aims to tackle the
unique challenge posed by multiple optimization conflicts in this setting. We
introduce a bottom-up joint learning mechanism. Firstly, we design a parameter
template to decouple the parameters of the expert network. Thus, scenario
parameters are shared knowledge for federated parameter aggregation, while
task-specific parameters are personalized local parameters. Secondly, we
conduct personalized federated learning for the parameters of each expert
network through a federated communication round, utilizing three modules:
federated batch normalization, conflict coordination, and personalized
aggregation. Finally, we perform another round of personalized federated
parameter aggregation on the task tower network to obtain the prediction
results for multiple tasks. We conduct extensive experiments on two public
datasets, and the results demonstrate that our proposed method surpasses
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Composed Image Retrieval Considering Query-target Relationship
  Leve<span class="highlight-title">rag</span>ing Masked Image-text Pairs <span class="chip">ICIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaying Zhang, Rintaro Yanagi, Ren Togo, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel zero-shot composed image retrieval (CIR) method
considering the query-target relationship by masked image-text pairs. The
objective of CIR is to retrieve the target image using a query image and a
query text. Existing methods use a textual inversion network to convert the
query image into a pseudo word to compose the image and text and use a
pre-trained visual-language model to realize the retrieval. However, they do
not consider the query-target relationship to train the textual inversion
network to acquire information for retrieval. In this paper, we propose a novel
zero-shot CIR method that is trained end-to-end using masked image-text pairs.
By exploiting the abundant image-text pairs that are convenient to obtain with
a masking strategy for learning the query-target relationship, it is expected
that accurate zero-shot CIR using a retrieval-focused textual inversion network
can be realized. Experimental results show the effectiveness of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper in IEEE ICIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical
  and Categorical Features for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizheng Chen, Kounianhua Du, Jianghao Lin, Bo Chen, Ruiming Tang, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been flourishing in the natural language
processing (NLP) domain, and their potential for recommendation has been paid
much attention to. Despite the intelligence shown by the
recommendation-oriented finetuned models, LLMs struggle to fully understand the
user behavior patterns due to their innate weakness in interpreting numerical
features and the overhead for long context, where the temporal relations among
user behaviors, subtle quantitative signals among different ratings, and
various side features of items are not well explored. Existing works only
fine-tune a sole LLM on given text data without introducing that important
information to it, leaving these problems unsolved. In this paper, we propose
ELCoRec to Enhance Language understanding with CoPropagation of numerical and
categorical features for Recommendation. Concretely, we propose to inject the
preference understanding capability into LLM via a GAT expert model where the
user preference is better encoded by parallelly propagating the temporal
relations, and rating signals as well as various side information of historical
items. The parallel propagation mechanism could stabilize heterogeneous
features and offer an informative user preference encoding, which is then
injected into the language models via soft prompting at the cost of a single
token embedding. To further obtain the user's recent interests, we proposed a
novel Recent interaction Augmented Prompt (RAP) template. Experiment results
over three datasets against strong baselines validate the effectiveness of
ELCoRec. The code is available at
https://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TocBERT: Medical Document Structure Extraction Using Bidirectional
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majd Saleh, Sarra Baghdadi, Stéphane Paquelet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text segmentation holds paramount importance in the field of Natural Language
Processing (NLP). It plays an important role in several NLP downstream tasks
like information retrieval and document summarization. In this work, we propose
a new solution, namely TocBERT, for segmenting texts using bidirectional
transformers. TocBERT represents a supervised solution trained on the detection
of titles and sub-titles from their semantic representations. This task was
formulated as a named entity recognition (NER) problem. The solution has been
applied on a medical text segmentation use-case where the Bio-ClinicalBERT
model is fine-tuned to segment discharge summaries of the MIMIC-III dataset.
The performance of TocBERT has been evaluated on a human-labeled ground truth
corpus of 250 notes. It achieved an F1-score of 84.6% when evaluated on a
linear text segmentation problem and 72.8% on a hierarchical text segmentation
problem. It outperformed a carefully designed rule-based solution, particularly
in distinguishing titles from subtitles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Complex Disease Treatment through Network Medicine and
  GenAI: A Case Study on Drug Repurposing for Breast Cancer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13106v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13106v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdeen Hamed, Tamer E. Fandy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this research is to introduce a network specialized in
predicting drugs that can be repurposed by investigating real-world evidence
sources, such as clinical trials and biomedical literature. Specifically, it
aims to generate drug combination therapies for complex diseases (e.g., cancer,
Alzheimer's). We present a multilayered network medicine approach, empowered by
a highly configured ChatGPT prompt engineering system, which is constructed on
the fly to extract drug mentions in clinical trials. Additionally, we introduce
a novel algorithm that connects real-world evidence with disease-specific
signaling pathways (e.g., KEGG database). This sheds light on the
repurposability of drugs if they are found to bind with one or more protein
constituents of a signaling pathway. To demonstrate, we instantiated the
framework for breast cancer and found that, out of 46 breast cancer signaling
pathways, the framework identified 38 pathways that were covered by at least
two drugs. This evidence signals the potential for combining those drugs.
Specifically, the most covered signaling pathway, ID hsa:2064, was covered by
108 drugs, some of which can be combined. Conversely, the signaling pathway ID
hsa:1499 was covered by only two drugs, indicating a significant gap for
further research. Our network medicine framework, empowered by GenAI, shows
promise in identifying drug combinations with a high degree of specificity,
knowing the exact signaling pathways and proteins that serve as targets. It is
noteworthy that ChatGPT successfully accelerated the process of identifying
drug mentions in clinical trials, though further investigations are required to
determine the relationships among the drug mentions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages double columns, 5 figures, 3 algorithms, 3 tables, and 1
  listing, Submitted to IEEE MedAI'24 Conference, to be held November 15-17,
  Chongqing, China</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Error Bounds of Supervised Classification from Information-Theoretic
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binchuan Qi, Wei Gong, Li Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There remains a list of unanswered research questions on deep learning (DL),
including the remarkable generalization power of overparametrized neural
networks, the efficient optimization performance despite the non-convexity, and
the mechanisms behind flat minima in generalization. In this paper, we adopt an
information-theoretic perspective to explore the theoretical foundations of
supervised classification using deep neural networks (DNNs). Our analysis
introduces the concepts of fitting error and model risk, which, together with
generalization error, constitute an upper bound on the expected risk. We
demonstrate that the generalization errors are bounded by the complexity,
influenced by both the smoothness of distribution and the sample size.
Consequently, task complexity serves as a reliable indicator of the dataset's
quality, guiding the setting of regularization hyperparameters. Furthermore,
the derived upper bound fitting error links the back-propagated gradient,
Neural Tangent Kernel (NTK), and the model's parameter count with the fitting
error. Utilizing the triangle inequality, we establish an upper bound on the
expected risk. This bound offers valuable insights into the effects of
overparameterization, non-convex optimization, and the flat minima in
DNNs.Finally, empirical verification confirms a significant positive
correlation between the derived theoretical bounds and the practical expected
risk, confirming the practical relevance of the theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hierarchical Neural Framework for Classification and its Explanation
  in Large Unstructured Legal Documents <span class="chip">CIKM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10563v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10563v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishchal Prasad, Mohand Boughanem, Taoufik Dkaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic legal judgment prediction and its explanation suffer from the
problem of long case documents exceeding tens of thousands of words, in
general, and having a non-uniform structure. Predicting judgments from such
documents and extracting their explanation becomes a challenging task, more so
on documents with no structural annotation. We define this problem as "scarce
annotated legal documents" and explore their lack of structural information and
their long lengths with a deep-learning-based classification framework which we
call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment
prediction. We explore the adaptability of LLMs with multi-billion parameters
(GPT-Neo, and GPT-J) to legal texts and their intra-domain(legal) transfer
learning capacity. Alongside this, we compare their performance and
adaptability with MESc and the impact of combining embeddings from their last
layers. For such hierarchical models, we also propose an explanation extraction
algorithm named ORSE; Occlusion sensitivity-based Relevant Sentence Extractor;
based on the input-occlusion sensitivity of the model, to explain the
predictions with the most relevant sentences from the document. We explore
these methods and test their effectiveness with extensive experiments and
ablation studies on legal documents from India, the European Union, and the
United States with the ILDC dataset and a subset of the LexGLUE dataset. MESc
achieves a minimum total performance gain of approximately 2 points over
previous state-of-the-art proposed methods, while ORSE applied on MESc achieves
a total average gain of 50% over the baseline explainability scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as non archival paper in the The 3rd International Workshop
  on Mining and Learning in the Legal Domain (MLLD-2023) at CIKM 2023,
  Birmingham, United Kingdom. (https://sites.google.com/view/mlld2023/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bioptic -- A Target-Agnostic Efficacy-Based Small Molecules Search
  Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Vinogradov, Ivan Izmailov, Simon Steshin, Kong T. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent successes in virtual screening have been made possible by large models
and extensive chemical libraries. However, combining these elements is
challenging: the larger the model, the more expensive it is to run, making
ultra-large libraries unfeasible. To address this, we developed a
target-agnostic, efficacy-based molecule search model, which allows us to find
structurally dissimilar molecules with similar biological activities. We used
the best practices to design fast retrieval system, based on
processor-optimized SIMD instructions, enabling us to screen the ultra-large
40B Enamine REAL library with 100\% recall rate. We extensively benchmarked our
model and several state-of-the-art models for both speed performance and
retrieval quality of novel molecules.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-26T00:00:00Z">2024-06-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Compositionality in Concept Learning <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Stein, Aaditya Naik, Yinjun Wu, Mayur Naik, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-based interpretability methods offer a lens into the internals of
foundation models by decomposing their embeddings into high-level concepts.
These concept representations are most useful when they are compositional,
meaning that the individual concepts compose to explain the full sample. We
show that existing unsupervised concept extraction methods find concepts which
are not compositional. To automatically discover compositional concept
representations, we identify two salient properties of such representations,
and propose Compositional Concept Extraction (CCE) for finding concepts which
obey these properties. We evaluate CCE on five different datasets over image
and text data. Our evaluation shows that CCE finds more compositional concept
representations than baselines and yields better accuracy on four downstream
classification tasks. Code and data are available at
https://github.com/adaminsky/compositional_concepts .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2024. 26 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic Learning Enables Self-Evolving Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The AI community has been exploring a pathway to artificial general
intelligence (AGI) by developing "language agents", which are complex large
language models (LLMs) pipelines involving both prompting techniques and tool
usage methods. While language agents have demonstrated impressive capabilities
for many real-world tasks, a fundamental limitation of current language agents
research is that they are model-centric, or engineering-centric. That's to say,
the progress on prompts, tools, and pipelines of language agents requires
substantial manual engineering efforts from human experts rather than
automatically learning from data. We believe the transition from model-centric,
or engineering-centric, to data-centric, i.e., the ability of language agents
to autonomously learn and evolve in environments, is the key for them to
possibly achieve AGI.
  In this work, we introduce agent symbolic learning, a systematic framework
that enables language agents to optimize themselves on their own in a
data-centric way using symbolic optimizers. Specifically, we consider agents as
symbolic networks where learnable weights are defined by prompts, tools, and
the way they are stacked together. Agent symbolic learning is designed to
optimize the symbolic network within language agents by mimicking two
fundamental algorithms in connectionist learning: back-propagation and gradient
descent. Instead of dealing with numeric weights, agent symbolic learning works
with natural language simulacrums of weights, loss, and gradients. We conduct
proof-of-concept experiments on both standard benchmarks and complex real-world
tasks and show that agent symbolic learning enables language agents to update
themselves after being created and deployed in the wild, resulting in
"self-evolving agents".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/aiwaves-cn/agents</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrExMe! Large Scale Prompt Exploration of Open Source <span class="highlight-title">LLM</span>s for Machine
  Translation and Summarization Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Leiter, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized the field of NLP. Notably,
their in-context learning capabilities also enable their use as evaluation
metrics for natural language generation, making them particularly advantageous
in low-resource scenarios and time-restricted applications. In this work, we
introduce PrExMe, a large-scale prompt exploration for metrics, where we
evaluate more than 720 prompt templates for open-source LLM-based metrics on
machine translation (MT) and summarization datasets, totalling over 6.6M
evaluations. This extensive comparison (1) serves as a benchmark of the
performance of recent open-source LLMs as metrics and (2) explores the
stability and variability of different prompting strategies. We discover that,
on the one hand, there are scenarios for which prompts are stable. For
instance, some LLMs show idiosyncratic preferences and favor to grade generated
texts with textual labels while others prefer to return numeric scores. On the
other hand, the stability of prompts and model rankings can be susceptible to
seemingly innocuous changes. For example, changing the requested output format
from "0 to 100" to "-1 to +1" can strongly affect the rankings in our
evaluation. Our study contributes to understanding the impact of different
prompting approaches on LLM-based metrics for MT and summarization evaluation,
highlighting the most stable prompting patterns and potential limitations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of
  Text-to-Time-lapse Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Ruijie Zhu, Xinhua Cheng, Jiebo Luo, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel text-to-video (T2V) generation benchmark,
ChronoMagic-Bench, to evaluate the temporal and metamorphic capabilities of the
T2V models (e.g. Sora and Lumiere) in time-lapse video generation. In contrast
to existing benchmarks that focus on the visual quality and textual relevance
of generated videos, ChronoMagic-Bench focuses on the model's ability to
generate time-lapse videos with significant metamorphic amplitude and temporal
coherence. The benchmark probes T2V models for their physics, biology, and
chemistry capabilities, in a free-form text query. For these purposes,
ChronoMagic-Bench introduces 1,649 prompts and real-world videos as references,
categorized into four major types of time-lapse videos: biological,
human-created, meteorological, and physical phenomena, which are further
divided into 75 subcategories. This categorization comprehensively evaluates
the model's capacity to handle diverse and complex transformations. To
accurately align human preference with the benchmark, we introduce two new
automatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic
attributes and temporal coherence. MTScore measures the metamorphic amplitude,
reflecting the degree of change over time, while CHScore assesses the temporal
coherence, ensuring the generated videos maintain logical progression and
continuity. Based on the ChronoMagic-Bench, we conduct comprehensive manual
evaluations of ten representative T2V models, revealing their strengths and
weaknesses across different categories of prompts, and providing a thorough
evaluation framework that addresses current gaps in video generation research.
Moreover, we create a large-scale ChronoMagic-Pro dataset, containing 460k
high-quality pairs of 720p time-lapse videos and detailed captions ensuring
high physical pertinence and large metamorphic amplitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart understanding plays a pivotal role when applying Multimodal Large
Language Models (MLLMs) to real-world tasks such as analyzing scientific papers
or financial reports. However, existing datasets often focus on oversimplified
and homogeneous charts with template-based questions, leading to an
over-optimistic measure of progress. We demonstrate that although open-source
models can appear to outperform strong proprietary models on these benchmarks,
a simple stress test with slightly different charts or questions can
deteriorate performance by up to 34.5%. In this work, we propose CharXiv, a
comprehensive evaluation suite involving 2,323 natural, challenging, and
diverse charts from arXiv papers. CharXiv includes two types of questions: 1)
descriptive questions about examining basic chart elements and 2) reasoning
questions that require synthesizing information across complex visual elements
in the chart. To ensure quality, all charts and questions are handpicked,
curated, and verified by human experts. Our results reveal a substantial,
previously underestimated gap between the reasoning skills of the strongest
proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the
strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%.
All models lag far behind human performance of 80.5%, underscoring weaknesses
in the chart understanding capabilities of existing MLLMs. We hope CharXiv
facilitates future research on MLLM chart understanding by providing a more
realistic and faithful measure of progress. Project page and leaderboard:
https://charxiv.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>121 pages, 90 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APIGen: Automated Pipeline for Generating Verifiable and Diverse
  Function-Calling <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, Caiming Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of function-calling agent models requires diverse, reliable,
and high-quality datasets. This paper presents APIGen, an automated data
generation pipeline designed to synthesize verifiable high-quality datasets for
function-calling applications. We leverage APIGen and collect 3,673 executable
APIs across 21 different categories to generate diverse function-calling
datasets in a scalable and structured manner. Each data in our dataset is
verified through three hierarchical stages: format checking, actual function
executions, and semantic verification, ensuring its reliability and
correctness. We demonstrate that models trained with our curated datasets, even
with only 7B parameters, can achieve state-of-the-art performance on the
Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models.
Moreover, our 1B model achieves exceptional performance, surpassing
GPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000
high-quality entries, aiming to advance the field of function-calling agent
domains. The dataset is available on Huggingface:
https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k and the
project homepage: https://apigen-pipeline.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Is ChatGPT a Better Explainer than My Professor?": Evaluating the
  Explanation Capabilities of <span class="highlight-title">LLM</span>s in Conversation Compared to a Human Baseline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grace Li, Milad Alshomary, Smaranda Muresan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanations form the foundation of knowledge sharing and build upon
communication principles, social dynamics, and learning theories. We focus
specifically on conversational approaches for explanations because the context
is highly adaptive and interactive. Our research leverages previous work on
explanatory acts, a framework for understanding the different strategies that
explainers and explainees employ in a conversation to both explain, understand,
and engage with the other party. We use the 5-Levels dataset was constructed
from the WIRED YouTube series by Wachsmuth et al., and later annotated by
Booshehri et al. with explanatory acts. These annotations provide a framework
for understanding how explainers and explainees structure their response when
crafting a response.
  With the rise of generative AI in the past year, we hope to better understand
the capabilities of Large Language Models (LLMs) and how they can augment
expert explainer's capabilities in conversational settings. To achieve this
goal, the 5-Levels dataset (We use Booshehri et al.'s 2023 annotated dataset
with explanatory acts.) allows us to audit the ability of LLMs in engaging in
explanation dialogues. To evaluate the effectiveness of LLMs in generating
explainer responses, we compared 3 different strategies, we asked human
annotators to evaluate 3 different strategies: human explainer response, GPT4
standard response, GPT4 response with Explanation Moves.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 figures, 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially)
  Safer Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, Nouha Dziri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce WildTeaming, an automatic LLM safety red-teaming framework that
mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of
novel jailbreak tactics, and then composes multiple tactics for systematic
exploration of novel jailbreaks. Compared to prior work that performed
red-teaming via recruited human workers, gradient-based optimization, or
iterative revision with LLMs, our work investigates jailbreaks from chatbot
users who were not specifically instructed to break the system. WildTeaming
reveals previously unidentified vulnerabilities of frontier LLMs, resulting in
up to 4.6x more diverse and successful adversarial attacks compared to
state-of-the-art jailbreak methods.
  While many datasets exist for jailbreak evaluation, very few open-source
datasets exist for jailbreak training, as safety training data has been closed
even when model weights are open. With WildTeaming we create WildJailbreak, a
large-scale open-source synthetic safety dataset with 262K vanilla (direct
request) and adversarial (complex jailbreak) prompt-response pairs. To mitigate
exaggerated safety behaviors, WildJailbreak provides two contrastive types of
queries: 1) harmful queries (vanilla & adversarial) and 2) benign queries that
resemble harmful queries in form but contain no harm. As WildJailbreak
considerably upgrades the quality and scale of existing safety resources, it
uniquely enables us to examine the scaling effects of data and the interplay of
data properties and model capabilities during safety training. Through
extensive experiments, we identify the training properties that enable an ideal
balance of safety behaviors: appropriate safeguarding without over-refusal,
effective handling of vanilla and adversarial queries, and minimal, if any,
decrease in general capabilities. All components of WildJailbeak contribute to
achieving balanced safety behaviors of models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mental Modeling of Reinforcement Learning Agents by Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Lu, Xufeng Zhao, Josua Spisak, Jae Hee Lee, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can emergent language models faithfully model the intelligence of
decision-making agents? Though modern language models exhibit already some
reasoning ability, and theoretically can potentially express any probable
distribution over tokens, it remains underexplored how the world knowledge
these pretrained models have memorized can be utilized to comprehend an agent's
behaviour in the physical world. This study empirically examines, for the first
time, how well large language models (LLMs) can build a mental model of agents,
termed agent mental modelling, by reasoning about an agent's behaviour and its
effect on states from agent interaction history. This research may unveil the
potential of leveraging LLMs for elucidating RL agent behaviour, addressing a
key challenge in eXplainable reinforcement learning (XRL). To this end, we
propose specific evaluation metrics and test them on selected RL task datasets
of varying complexity, reporting findings on agent mental model establishment.
Our results disclose that LLMs are not yet capable of fully mental modelling
agents through inference alone without further innovations. This work thus
provides new insights into the capabilities and limitations of modern LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://lukaswill.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is In-Context Learning a Type of Gradient-Based Learning? Evidence from
  the Inverse Frequency Effect in Structural Priming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghao Zhou, Robert Frank, R. Thomas McCoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown the emergent capability of in-context
learning (ICL). One line of research has explained ICL as functionally
performing gradient descent. In this paper, we introduce a new way of
diagnosing whether ICL is functionally equivalent to gradient-based learning.
Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in
which an error-driven learner is expected to show larger updates when trained
on infrequent examples than frequent ones. The IFE has previously been studied
in psycholinguistics because humans show this effect in the context of
structural priming (the tendency for people to produce sentence structures they
have encountered recently); the IFE has been used as evidence that human
structural priming must involve error-driven learning mechanisms. In our
experiments, we simulated structural priming within ICL and found that LLMs
display the IFE, with the effect being stronger in larger models. We conclude
that ICL is indeed a type of gradient-based learning, supporting the hypothesis
that a gradient component is implicitly computed in the forward pass during
ICL. Our results suggest that both humans and LLMs make use of gradient-based,
error-driven processing mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks,
  and Refusals of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, Nouha Dziri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce WildGuard -- an open, light-weight moderation tool for LLM
safety that achieves three goals: (1) identifying malicious intent in user
prompts, (2) detecting safety risks of model responses, and (3) determining
model refusal rate. Together, WildGuard serves the increasing needs for
automatic safety moderation and evaluation of LLM interactions, providing a
one-stop tool with enhanced accuracy and broad coverage across 13 risk
categories. While existing open moderation tools such as Llama-Guard2 score
reasonably well in classifying straightforward model interactions, they lag far
behind a prompted GPT-4, especially in identifying adversarial jailbreaks and
in evaluating models' refusals, a key measure for evaluating safety behaviors
in model responses.
  To address these challenges, we construct WildGuardMix, a large-scale and
carefully balanced multi-task safety moderation dataset with 92K labeled
examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired
with various refusal and compliance responses. WildGuardMix is a combination of
WildGuardTrain, the training data of WildGuard, and WildGuardTest, a
high-quality human-annotated moderation test set with 5K labeled items covering
broad risk scenarios. Through extensive evaluations on WildGuardTest and ten
existing public benchmarks, we show that WildGuard establishes state-of-the-art
performance in open-source safety moderation across all the three tasks
compared to ten strong existing open-source moderation models (e.g., up to
26.4% improvement on refusal detection). Importantly, WildGuard matches and
sometimes exceeds GPT-4 performance (e.g., up to 3.9% improvement on prompt
harmfulness identification). WildGuard serves as a highly effective safety
moderator in an LLM interface, reducing the success rate of jailbreak attacks
from 79.8% to 2.4%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally. Third and fourth authors
  contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Role-Play Zero-Shot Prompting with <span class="highlight-title">Large Language Models</span> for Open-Domain
  Human-Machine Conversation <span class="chip">SIGDIAL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian, Fabrice Lefèvre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, various methods have been proposed to create open-domain
conversational agents with Large Language Models (LLMs). These models are able
to answer user queries, but in a one-way Q&A format rather than a true
conversation. Fine-tuning on particular datasets is the usual way to modify
their style to increase conversational ability, but this is expensive and
usually only available in a few languages. In this study, we explore role-play
zero-shot prompting as an efficient and cost-effective solution for open-domain
conversation, using capable multilingual LLMs (Beeching et al., 2023) trained
to obey instructions. We design a prompting system that, when combined with an
instruction-following model - here Vicuna (Chiang et al., 2023) - produces
conversational agents that match and even surpass fine-tuned models in human
evaluation in French in two different tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version of a paper originally submitted at SIGDIAL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascading <span class="highlight-title">Large Language Models</span> for Salient Event Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating event graphs from long documents is challenging due to the
inherent complexity of multiple tasks involved such as detecting events,
identifying their relationships, and reconciling unstructured input with
structured graphs. Recent studies typically consider all events with equal
importance, failing to distinguish salient events crucial for understanding
narratives. This paper presents CALLMSAE, a CAscading Large Language Model
framework for SAlient Event graph generation, which leverages the capabilities
of LLMs and eliminates the need for costly human annotations. We first identify
salient events by prompting LLMs to generate summaries, from which salient
events are identified. Next, we develop an iterative code refinement prompting
strategy to generate event relation graphs, removing hallucinated relations and
recovering missing edges. Fine-tuning contextualised graph generation models on
the LLM-generated graphs outperforms the models trained on CAEVO-generated
data. Experimental results on a human-annotated test set show that the proposed
method generates salient and more accurate graphs, outperforming competitive
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 + 12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IRCAN: Mitigating Knowledge Conflicts in <span class="highlight-title">LLM</span> Generation via Identifying
  and Reweighting Context-Aware Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Shi, Renren Jin, Tianhao Shen, Weilong Dong, Xinwei Wu, Deyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is widely acknowledged that large language models (LLMs) encode a vast
reservoir of knowledge after being trained on mass data. Recent studies
disclose knowledge conflicts in LLM generation, wherein outdated or incorrect
parametric knowledge (i.e., encoded knowledge) contradicts new knowledge
provided in the context. To mitigate such knowledge conflicts, we propose a
novel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to
capitalize on neurons that are crucial in processing contextual cues.
Specifically, IRCAN first identifies neurons that significantly contribute to
context processing, utilizing a context-aware attribution score derived from
integrated gradients. Subsequently, the identified context-aware neurons are
strengthened via reweighting. In doing so, we steer LLMs to generate
context-sensitive outputs with respect to the new knowledge provided in the
context. Extensive experiments conducted across a variety of models and tasks
demonstrate that IRCAN not only achieves remarkable improvements in handling
knowledge conflicts but also offers a scalable, plug-andplay solution that can
be integrated seamlessly with existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>s instead of Human Judges? A Large Scale Empirical Study across 20
  NLP Evaluation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an increasing trend towards evaluating NLP models with LLM-generated
judgments instead of human judgments. In the absence of a comparison against
human data, this raises concerns about the validity of these evaluations; in
case they are conducted with proprietary models, this also raises concerns over
reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with
human annotations, and comprehensively evaluate 11 current LLMs, covering both
open-weight and proprietary models, for their ability to replicate the
annotations. Our evaluations show that each LLM exhibits a large variance
across datasets in its correlation to human judgments. We conclude that LLMs
are not yet ready to systematically replace human judges in NLP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do <span class="highlight-title">LLM</span>s dream of elephants (when told not to)? Latent concept
  association and associative memory in transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have the capacity to store and recall facts.
Through experimentation with open-source models, we observe that this ability
to retrieve facts can be easily manipulated by changing contexts, even without
altering their factual meanings. These findings highlight that LLMs might
behave like an associative memory model where certain tokens in the contexts
serve as clues to retrieving facts. We mathematically explore this property by
studying how transformers, the building blocks of LLMs, can complete such
memory tasks. We study a simple latent concept association problem with a
one-layer transformer and we show theoretically and empirically that the
transformer gathers information using self-attention and uses the value matrix
for associative memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Data Pruning for Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiao Xiao, Pingchuan Ma, Adriana Fernandez-Lopez, Boqian Wu, Lu Yin, Stavros Petridis, Mykola Pechenizkiy, Maja Pantic, Decebal Constantin Mocanu, Shiwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of Automatic Speech Recognition (ASR) is largely
attributed to the ever-growing amount of training data. However, this trend has
made model training prohibitively costly and imposed computational demands.
While data pruning has been proposed to mitigate this issue by identifying a
small subset of relevant data, its application in ASR has been barely explored,
and existing works often entail significant overhead to achieve meaningful
results. To fill this gap, this paper presents the first investigation of
dynamic data pruning for ASR, finding that we can reach the full-data
performance by dynamically selecting 70% of data. Furthermore, we introduce
Dynamic Data Pruning for ASR (DDP-ASR), which offers several fine-grained
pruning granularities specifically tailored for speech-related datasets, going
beyond the conventional pruning of entire time sequences. Our intensive
experiments show that DDP-ASR can save up to 1.6x training time with negligible
performance loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Themis: Towards Flexible and Interpretable NLG Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of natural language generation (NLG) tasks is a significant
and longstanding research issue. With the recent emergence of powerful large
language models (LLMs), some studies have turned to LLM-based automatic
evaluation methods, which demonstrate great potential to become a new
evaluation paradigm following traditional string-based and model-based metrics.
However, despite the improved performance of existing methods, they still
possess some deficiencies, such as dependency on references and limited
evaluation flexibility. Therefore, in this paper, we meticulously construct a
large-scale NLG evaluation corpus NLG-Eval with human and GPT-4 annotations to
alleviate the lack of relevant data in this field. Furthermore, we propose
Themis, an LLM dedicated to NLG evaluation, which has been trained with our
designed multi-perspective consistency and rating-oriented preference alignment
methods. Themis can conduct flexible and interpretable evaluations without
references, and it exhibits superior evaluation performance on various NLG
tasks, simultaneously generalizing well to unseen tasks and surpassing other
evaluation models, including GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on Information Extraction of LCSTS <span class="highlight-title">Dataset</span> Based on an Improved
  BERTSum-LSTM Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Chen, Haobin Chen, Simin Liu, Yunyun Liu, Fanhao Zhou, Bing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous advancement of artificial intelligence, natural language
processing technology has become widely utilized in various fields. At the same
time, there are many challenges in creating Chinese news summaries. First of
all, the semantics of Chinese news is complex, and the amount of information is
enormous. Extracting critical information from Chinese news presents a
significant challenge. Second, the news summary should be concise and clear,
focusing on the main content and avoiding redundancy. In addition, the
particularity of the Chinese language, such as polysemy, word segmentation,
etc., makes it challenging to generate Chinese news summaries. Based on the
above, this paper studies the information extraction method of the LCSTS
dataset based on an improved BERTSum-LSTM model. We improve the BERTSum-LSTM
model to make it perform better in generating Chinese news summaries. The
experimental results show that the proposed method has a good effect on
creating news summaries, which is of great importance to the construction of
news summaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ICMIII 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grammar Assistance Using Syntactic Structures (GAUSS) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Zamaraeva, Lorena S. Allegue, Carlos Gómez-Rodríguez, Anastasiia Ogneva, Margarita Alonso-Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic grammar coaching serves an important purpose of advising on
standard grammar varieties while not imposing social pressures or reinforcing
established social roles. Such systems already exist but most of them are for
English and few of them offer meaningful feedback. Furthermore, they typically
rely completely on neural methods and require huge computational resources
which most of the world cannot afford. We propose a grammar coaching system for
Spanish that relies on (i) a rich linguistic formalism capable of giving
informative feedback; and (ii) a faster parsing algorithm which makes using
this formalism practical in a real-world application. The approach is feasible
for any language for which there is a computerized grammar and is less reliant
on expensive and environmentally costly neural methods. We seek to contribute
to Greener AI and to address global education challenges by raising the
standards of inclusivity and engagement in grammar coaching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, project summary for CEDI-SEPLN Seminar of the
  Spanish Society for Natural Language Processing at the 7th Spanish Conference
  on Informatics, June 19-20, 2024, A Coru\~na, Spain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PaCoST: Paired Confidence Significance Testing for Benchmark
  Contamination Detection in <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huixuan Zhang, Yun Lin, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are known to be trained on vast amounts of data,
which may unintentionally or intentionally include data from commonly used
benchmarks. This inclusion can lead to cheatingly high scores on model
leaderboards, yet result in disappointing performance in real-world
applications. To address this benchmark contamination problem, we first propose
a set of requirements that practical contamination detection methods should
follow. Following these proposed requirements, we introduce PaCoST, a Paired
Confidence Significance Testing to effectively detect benchmark contamination
in LLMs. Our method constructs a counterpart for each piece of data with the
same distribution, and performs statistical analysis of the corresponding
confidence to test whether the model is significantly more confident under the
original benchmark. We validate the effectiveness of PaCoST and apply it on
popular open-source models and benchmarks. We find that almost all models and
benchmarks we tested are suspected contaminated more or less. We finally call
for new LLM evaluation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathOdyssey: Benchmarking Mathematical Problem-Solving Skills in Large
  Language Models Using Odyssey Math Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, Kai Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly advanced natural language
understanding and demonstrated strong problem-solving abilities. Despite these
successes, most LLMs still struggle with solving mathematical problems due to
the intricate reasoning required. This paper investigates the mathematical
problem-solving capabilities of LLMs using the newly developed "MathOdyssey"
dataset. The dataset includes diverse mathematical problems at high school and
university levels, created by experts from notable institutions to rigorously
test LLMs in advanced problem-solving scenarios and cover a wider range of
subject areas. By providing the MathOdyssey dataset as a resource to the AI
community, we aim to contribute to the understanding and improvement of AI
capabilities in complex mathematical problem-solving. We conduct benchmarking
on open-source models, such as Llama-3 and DBRX-Instruct, and closed-source
models from the GPT series and Gemini models. Our results indicate that while
LLMs perform well on routine and moderately difficult tasks, they face
significant challenges with Olympiad-level problems and complex
university-level questions. Our analysis shows a narrowing performance gap
between open-source and closed-source models, yet substantial challenges
remain, particularly with the most demanding problems. This study highlights
the ongoing need for research to enhance the mathematical reasoning of LLMs.
The dataset, results, and code are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Airport Tower Command Recognition: Integrating
  Squeeze-and-Excitation and Broadcasted Residual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanxi Lin, Tonglin Zhou, Yang Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate recognition of aviation commands is vital for flight safety and
efficiency, as pilots must follow air traffic control instructions precisely.
This paper addresses challenges in speech command recognition, such as noisy
environments and limited computational resources, by advancing keyword spotting
technology. We create a dataset of standardized airport tower commands,
including routine and emergency instructions. We enhance broadcasted residual
learning with squeeze-and-excitation and time-frame frequency-wise
squeeze-and-excitation techniques, resulting in our BC-SENet model. This model
focuses on crucial information with fewer parameters. Our tests on five keyword
spotting models, including BC-SENet, demonstrate superior accuracy and
efficiency. These findings highlight the effectiveness of our model
advancements in improving speech command recognition for aviation safety and
efficiency in noisy, high-stakes environments. Additionally, BC-SENet shows
comparable performance on the common Google Speech Command dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IALP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-native Memory: A Pathway from <span class="highlight-title">LLM</span>s Towards AGI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingbo Shang, Zai Zheng, Xiang Ying, Felix Tao, Mindverse Team
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated the world with the sparks of
artificial general intelligence (AGI). One opinion, especially from some
startups working on LLMs, argues that an LLM with nearly unlimited context
length can realize AGI. However, they might be too optimistic about the
long-context capability of (existing) LLMs -- (1) Recent literature has shown
that their effective context length is significantly smaller than their claimed
context length; and (2) Our reasoning-in-a-haystack experiments further
demonstrate that simultaneously finding the relevant information from a long
context and conducting (simple) reasoning is nearly impossible. In this paper,
we envision a pathway from LLMs to AGI through the integration of
\emph{memory}. We believe that AGI should be a system where LLMs serve as core
processors. In addition to raw data, the memory in this system would store a
large number of important conclusions derived from reasoning processes.
Compared with retrieval-augmented generation (RAG) that merely processing raw
data, this approach not only connects semantically related information closer,
but also simplifies complex inferences at the time of querying. As an
intermediate stage, the memory will likely be in the form of natural language
descriptions, which can be directly consumed by users too. Ultimately, every
agent/person should have its own large personal model, a deep neural network
model (thus \emph{AI-native}) that parameterizes and compresses all types of
memory, even the ones cannot be described by natural languages. Finally, we
discuss the significant potential of AI-native memory as the transformative
infrastructure for (proactive) engagement, personalization, distribution, and
social in the AGI era, as well as the incurred privacy and security challenges
with preliminary solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S3: A Simple Strong Sample-effective Multimodal Dialog System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisei Rykov, Egor Malkershin, Alexander Panchenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a conceptually simple yet powerful baseline for the
multimodal dialog task, an S3 model, that achieves near state-of-the-art
results on two compelling leaderboards: MMMU and AI Journey Contest 2023. The
system is based on a pre-trained large language model, pre-trained modality
encoders for image and audio, and a trainable modality projector. The proposed
effective data mixture for training such an architecture demonstrates that a
multimodal model based on a strong language model and trained on a small amount
of multimodal data can perform efficiently in the task of multimodal dialog.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of
  Transcribed Audio for Speech Recognition Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Li, Yongbin You, Xuezhi Wang, Zhengkun Tian, Ke Ding, Guanglu Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multilingual artificial intelligence assistants, exemplified by
ChatGPT, have gained immense popularity. As a crucial gateway to human-computer
interaction, multilingual automatic speech recognition (ASR) has also garnered
significant attention, as evidenced by systems like Whisper. However, the
proprietary nature of the training data has impeded researchers' efforts to
study multilingual ASR. This paper introduces MSR-86K, an evolving, large-scale
multilingual corpus for speech recognition research. The corpus is derived from
publicly accessible videos on YouTube, comprising 15 languages and a total of
86,300 hours of transcribed ASR data. We also introduce how to use the MSR-86K
corpus and other open-source corpora to train a robust multilingual ASR model
that is competitive with Whisper. MSR-86K will be publicly released on
HuggingFace, and we believe that such a large corpus will pave new avenues for
research in multilingual ASR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by InterSpeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FactFinders at CheckThat! 2024: Refining Check-worthy Statement
  Detection with <span class="highlight-title">LLM</span>s through Data Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufeng Li, Rrubaa Panchendrarajan, Arkaitz Zubiaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid dissemination of information through social media and the Internet
has posed a significant challenge for fact-checking, among others in
identifying check-worthy claims that fact-checkers should pay attention to,
i.e. filtering claims needing fact-checking from a large pool of sentences.
This challenge has stressed the need to focus on determining the priority of
claims, specifically which claims are worth to be fact-checked. Despite
advancements in this area in recent years, the application of large language
models (LLMs), such as GPT, has only recently drawn attention in studies.
However, many open-source LLMs remain underexplored. Therefore, this study
investigates the application of eight prominent open-source LLMs with
fine-tuning and prompt engineering to identify check-worthy statements from
political transcriptions. Further, we propose a two-step data pruning approach
to automatically identify high-quality training data instances for effective
learning. The efficiency of our approach is demonstrated through evaluations on
the English language dataset as part of the check-worthiness estimation task of
CheckThat! 2024. Further, the experiments conducted with data pruning
demonstrate that competitive performance can be achieved with only about 44\%
of the training data. Our team ranked first in the check-worthiness estimation
task in the English language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Context Pruning: Optimizing Real-World Code Completion with
  Repository-Level Pretrained Code <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhang, Yunshui Li, Jiaming Li, Xiaobo Xia, Jiaxi Yang, Run Luo, Minzheng Wang, Longze Chen, Junhao Liu, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Some recently developed code large language models (Code LLMs) have been
pre-trained on repository-level code data (Repo-Code LLMs), enabling these
models to recognize repository structures and utilize cross-file information
for code completion. However, in real-world development scenarios, simply
concatenating the entire code repository often exceeds the context window
limits of these Repo-Code LLMs, leading to significant performance degradation.
In this study, we conducted extensive preliminary experiments and analyses on
six Repo-Code LLMs. The results indicate that maintaining the topological
dependencies of files and increasing the code file content in the completion
prompts can improve completion accuracy; pruning the specific implementations
of functions in all dependent files does not significantly reduce the accuracy
of completions. Based on these findings, we proposed a strategy named
Hierarchical Context Pruning (HCP) to construct completion prompts with high
informational code content. The HCP models the code repository at the function
level, maintaining the topological dependencies between code files while
removing a large amount of irrelevant code content, significantly reduces the
input length for repository-level code completion. We applied the HCP strategy
in experiments with six Repo-Code LLMs, and the results demonstrate that our
proposed method can significantly enhance completion accuracy while
substantially reducing the length of input. Our code and data are available at
https://github.com/Hambaobao/HCP-Coder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sanskrit Knowledge-based Systems: Annotation and Computational Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishikesh Terdalkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenges and opportunities in the development of knowledge
systems for Sanskrit, with a focus on question answering. By proposing a
framework for the automated construction of knowledge graphs, introducing
annotation tools for ontology-driven and general-purpose tasks, and offering a
diverse collection of web-interfaces, tools, and software libraries, we have
made significant contributions to the field of computational Sanskrit. These
contributions not only enhance the accessibility and accuracy of Sanskrit text
analysis but also pave the way for further advancements in knowledge
representation and language processing. Ultimately, this research contributes
to the preservation, understanding, and utilization of the rich linguistic
information embodied in Sanskrit texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD Thesis. 204 pages, 6 publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Vorbeşti Româneşte?" A Recipe to Train Powerful Romanian <span class="highlight-title">LLM</span>s
  with English Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihai Masala, Denis C. Ilie-Ablachim, Alexandru Dima, Dragos Corlatescu, Miruna Zavelca, Ovio Olaru, Simina Terian-Dan, Andrei Terian-Dan, Marius Leordeanu, Horia Velicu, Marius Popescu, Mihai Dascalu, Traian Rebedea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have achieved almost human-like
performance on various tasks. While some LLMs have been trained on multilingual
data, most of the training data is in English; hence, their performance in
English greatly exceeds other languages. To our knowledge, we are the first to
collect and translate a large collection of texts, instructions, and benchmarks
and train, evaluate, and release open-source LLMs tailored for Romanian. We
evaluate our methods on four different categories, including academic
benchmarks, MT-Bench (manually translated), and a professionally built
historical, cultural, and social benchmark adapted to Romanian. We argue for
the usefulness and high performance of RoLLMs by obtaining state-of-the-art
results across the board. We publicly release all resources (i.e., data,
training and evaluation code, models) to support and encourage research on
Romanian LLMs while concurrently creating a generalizable recipe, adequate for
other low or less-resourced languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.07703</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Machine-Generated Texts: Not Just "AI vs Humans" and
  Explainability is Complicated 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhou Ji, Ruizhe Li, Shujun Li, Jie Guo, Weidong Qiu, Zheng Huang, Chiyu Chen, Xiaoyu Jiang, Xinru Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs rapidly advance, increasing concerns arise regarding risks about
actual authorship of texts we see online and in real world. The task of
distinguishing LLM-authored texts is complicated by the nuanced and overlapping
behaviors of both machines and humans. In this paper, we challenge the current
practice of considering LLM-generated text detection a binary classification
task of differentiating human from AI. Instead, we introduce a novel ternary
text classification scheme, adding an "undecided" category for texts that could
be attributed to either source, and we show that this new category is crucial
to understand how to make the detection result more explainable to lay users.
This research shifts the paradigm from merely classifying to explaining
machine-generated texts, emphasizing need for detectors to provide clear and
understandable explanations to users. Our study involves creating four new
datasets comprised of texts from various LLMs and human authors. Based on new
datasets, we performed binary classification tests to ascertain the most
effective SOTA detection methods and identified SOTA LLMs capable of producing
harder-to-detect texts. We constructed a new dataset of texts generated by two
top-performing LLMs and human authors, and asked three human annotators to
produce ternary labels with explanation notes. This dataset was used to
investigate how three top-performing SOTA detectors behave in new ternary
classification context. Our results highlight why "undecided" category is much
needed from the viewpoint of explainability. Additionally, we conducted an
analysis of explainability of the three best-performing detectors and the
explanation notes of the human annotators, revealing insights about the
complexity of explainable detection of machine-generated texts. Finally, we
propose guidelines for developing future detection systems with improved
explanatory power.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaMIPa: An Incremental Discourse Parser 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Thompson, Akshay Chaturvedi, Julie Hunter, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides the first discourse parsing experiments with a large
language model (LLM) finetuned on corpora annotated in the style of SDRT
(Asher, 1993; Asher and Lascarides, 2003). The result is a discourse parser,
LLaMIPa (LLaMA Incremental Parser), which is able to more fully exploit
discourse context, leading to substantial performance gains over approaches
that use encoder-only models to provide local, context-sensitive
representations of discourse units. Furthermore, it is able to process
discourse data incrementally, which is essential for the eventual use of
discourse information in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak Reward Model Transforms Generative Models into Robust Causal Event
  Extraction Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Italo Luis da Silva, Hanqi Yan, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inherent ambiguity of cause and effect boundaries poses a challenge in
evaluating causal event extraction tasks. Traditional metrics like Exact Match
and BertScore poorly reflect model performance, so we trained evaluation models
to approximate human evaluation, achieving high agreement. We used them to
perform Reinforcement Learning with extraction models to align them with human
preference, prioritising semantic understanding. We successfully explored our
approach through multiple datasets, including transferring an evaluator trained
on one dataset to another as a way to decrease the reliance on human-annotated
data. In that vein, we also propose a weak-to-strong supervision method that
uses a fraction of the annotated data to train an evaluation model while still
achieving high performance in training an RL model. Our code is available at
\url{https://github.com/oyarsa/event_extraction/tree/causal-event-extraction}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot prompt-based classification: topic labeling in times of
  foundation models in German Tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Münker, Kai Kugler, Achim Rettinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Filtering and annotating textual data are routine tasks in many areas, like
social media or news analytics. Automating these tasks allows to scale the
analyses wrt. speed and breadth of content covered and decreases the manual
effort required. Due to technical advancements in Natural Language Processing,
specifically the success of large foundation models, a new tool for automating
such annotation processes by using a text-to-text interface given written
guidelines without providing training samples has become available.
  In this work, we assess these advancements in-the-wild by empirically testing
them in an annotation task on German Twitter data about social and political
European crises. We compare the prompt-based results with our human annotation
and preceding classification approaches, including Naive Bayes and a BERT-based
fine-tuning/domain adaptation pipeline. Our results show that the prompt-based
approach - despite being limited by local computation resources during the
model selection - is comparable with the fine-tuned BERT but without any
annotated training data. Our findings emphasize the ongoing paradigm shift in
the NLP landscape, i.e., the unification of downstream tasks and elimination of
the need for pre-labeled training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 tables, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUIDE: A Guideline-Guided <span class="highlight-title">Dataset</span> for Instructional Video Comprehension <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiafeng Liang, Shixin Jiang, Zekun Wang, Haojie Pan, Zerui Chen, Zheng Chu, Ming Liu, Ruiji Fu, Zhongyuan Wang, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are substantial instructional videos on the Internet, which provide us
tutorials for completing various tasks. Existing instructional video datasets
only focus on specific steps at the video level, lacking experiential
guidelines at the task level, which can lead to beginners struggling to learn
new tasks due to the lack of relevant experience. Moreover, the specific steps
without guidelines are trivial and unsystematic, making it difficult to provide
a clear tutorial. To address these problems, we present the GUIDE
(Guideline-Guided) dataset, which contains 3.5K videos of 560 instructional
tasks in 8 domains related to our daily life. Specifically, we annotate each
instructional task with a guideline, representing a common pattern shared by
all task-related videos. On this basis, we annotate systematic specific steps,
including their associated guideline steps, specific step descriptions and
timestamps. Our proposed benchmark consists of three sub-tasks to evaluate
comprehension ability of models: (1) Step Captioning: models have to generate
captions for specific steps from videos. (2) Guideline Summarization: models
have to mine the common pattern in task-related videos and summarize a
guideline from them. (3) Guideline-Guided Captioning: models have to generate
captions for specific steps under the guide of guideline. We evaluate plenty of
foundation models with GUIDE and perform in-depth analysis. Given the diversity
and practicality of GUIDE, we believe that it can be used as a better benchmark
for instructional video comprehension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Data Privacy in <span class="highlight-title">Large Language Models</span> through Private
  Association Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Venditti, Elena Sofia Ruzzetti, Giancarlo A. Xompero, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo Zanzotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are powerful tools with extensive applications,
but their tendency to memorize private information raises significant concerns
as private data leakage can easily happen. In this paper, we introduce Private
Association Editing (PAE), a novel defense approach for private data leakage.
PAE is designed to effectively remove Personally Identifiable Information (PII)
without retraining the model. Our approach consists of a four-step procedure:
detecting memorized PII, applying PAE cards to mitigate memorization of private
data, verifying resilience to targeted data extraction (TDE) attacks, and
ensuring consistency in the post-edit LLMs. The versatility and efficiency of
PAE, which allows for batch modifications, significantly enhance data privacy
in LLMs. Experimental results demonstrate the effectiveness of PAE in
mitigating private data leakage. We believe PAE will serve as a critical tool
in the ongoing effort to protect data privacy in LLMs, encouraging the
development of safer models for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Closer Look into Mixture-of-Experts in <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-experts (MoE) is gaining increasing attention due to its unique
properties and remarkable performance, especially for language tasks. By
sparsely activating a subset of parameters for each token, MoE architecture
could increase the model size without sacrificing computational efficiency,
achieving a better trade-off between performance and training costs. However,
the underlying mechanism of MoE still lacks further exploration, and its
modularization degree remains questionable. In this paper, we make an initial
attempt to understand the inner workings of MoE-based large language models.
Concretely, we comprehensively study the parametric and behavioral features of
three recent MoE-based models and reveal some intriguing observations,
including (1) Neurons act like fine-grained experts. (2) The router of MoE
usually selects experts with larger output norms. (3) The expert diversity
increases as the layer increases, while the last layer is an outlier. Based on
the observations, we also provide suggestions for a broad spectrum of MoE
practitioners, such as router design and expert allocation. We hope this work
could shed light on future research on the MoE framework and other modular
architectures. Code is available at
https://github.com/kamanphoebe/Look-into-MoEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEED: Accelerating Reasoning Tree Construction via Scheduled Speculative
  Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenglin Wang, Jialong Wu, Yilong Lai, Congzhi Zhang, Deyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate remarkable emergent abilities across
various tasks, yet fall short of complex reasoning and planning tasks. The
tree-search-based reasoning methods address this by surpassing the capabilities
of chain-of-thought prompting, encouraging exploration of intermediate steps.
However, such methods introduce significant inference latency due to the
systematic exploration and evaluation of multiple thought paths. This paper
introduces SeeD, a novel and efficient inference framework to optimize runtime
speed and GPU memory management concurrently. By employing a scheduled
speculative execution, SeeD efficiently handles multiple iterations for the
thought generation and the state evaluation, leveraging a rounds-scheduled
strategy to manage draft model dispatching. Extensive experimental evaluations
on three reasoning datasets demonstrate superior speedup performance of SeeD,
providing a viable path for batched inference in training-free speculative
decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Methodology of Adapting Large English Language Models for Specific
  Cultural Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjing Zhang, Siqi Xiao, Xuejiao Lei, Ning Wang, Huazheng Zhang, Meijuan An, Bikun Yang, Zhaoxiang Liu, Kai Wang, Shiguo Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of large language models(LLMs) has emerged as a prominent
trend in the field of artificial intelligence. However, current
state-of-the-art LLMs are predominantly based on English. They encounter
limitations when directly applied to tasks in specific cultural domains, due to
deficiencies in domain-specific knowledge and misunderstandings caused by
differences in cultural values. To address this challenge, our paper proposes a
rapid adaptation method for large models in specific cultural contexts, which
leverages instruction-tuning based on specific cultural knowledge and safety
values data. Taking Chinese as the specific cultural context and utilizing the
LLaMA3-8B as the experimental English LLM, the evaluation results demonstrate
that the adapted LLM significantly enhances its capabilities in domain-specific
knowledge and adaptability to safety values, while maintaining its original
expertise advantages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Prompting Tuning for Personalized Conversations with <span class="highlight-title">LLM</span>s <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiushi Huang, Xubo Liu, Tom Ko, Bo Wu, Wenwu Wang, Yu Zhang, Lilian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In conversational AI, personalizing dialogues with persona profiles and
contextual understanding is essential. Despite large language models' (LLMs)
improved response coherence, effective persona integration remains a challenge.
In this work, we first study two common approaches for personalizing LLMs:
textual prompting and direct fine-tuning. We observed that textual prompting
often struggles to yield responses that are similar to the ground truths in
datasets, while direct fine-tuning tends to produce repetitive or overly
generic replies. To alleviate those issues, we propose \textbf{S}elective
\textbf{P}rompt \textbf{T}uning (SPT), which softly prompts LLMs for
personalized conversations in a selective way. Concretely, SPT initializes a
set of soft prompts and uses a trainable dense retriever to adaptively select
suitable soft prompts for LLMs according to different input contexts, where the
prompt retriever is dynamically updated through feedback from the LLMs.
Additionally, we propose context-prompt contrastive learning and prompt fusion
learning to encourage the SPT to enhance the diversity of personalized
conversations. Experiments on the CONVAI2 dataset demonstrate that SPT
significantly enhances response diversity by up to 90\%, along with
improvements in other critical performance indicators. Those results highlight
the efficacy of SPT in fostering engaging and personalized dialogue generation.
The SPT model code (https://github.com/hqsiswiliam/SPT) is publicly available
for further exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UIO-<span class="highlight-title">LLM</span>s: Unbiased Incremental Optimization for Long-Context <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mingbao Lin, Yunshan Zhong, Shuicheng Yan, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Managing long texts is challenging for large language models (LLMs) due to
limited context window sizes. This study introduces UIO-LLMs, an unbiased
incremental optimization approach for memory-enhanced transformers under
long-context settings. We initially conceptualize the process as a streamlined
encoder-decoder framework where the weights-shared encoder and decoder
respectively encapsulate a context segment into memories and leverage these
memories to predict outputs of the subsequent segment. Subsequently, by
treating our memory-enhanced transformers as fully-connected recurrent neural
networks (RNNs), we refine the training process using the Truncated
Backpropagation Through Time (TBPTT) algorithm, which incorporates innovative
incremental optimization techniques. These techniques not only diminish time
complexity but also address the bias in gradient computation through an
unbiased optimization process. UIO-LLMs successfully handle long context, such
as extending the context window of Llama2-7b-chat from 4K to 100K tokens with
minimal 2% additional parameters, while keeping the inference cost nearly
linear as context length increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeBuLa: A discourse aware Minecraft Builder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Chaturvedi, Kate Thompson, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When engaging in collaborative tasks, humans efficiently exploit the semantic
structure of a conversation to optimize verbal and nonverbal interactions. But
in recent "language to code" or "language to action" models, this information
is lacking. We show how incorporating the prior discourse and nonlinguistic
context of a conversation situated in a nonlinguistic environment can improve
the "language to action" component of such interactions. We fine tune an LLM to
predict actions based on prior context; our model, NeBuLa, doubles the
net-action F1 score over the baseline on this task of Jayannavar et al.(2020).
We also investigate our model's ability to construct shapes and understand
location descriptions using a synthetic dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal
  Long-Context Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context Multimodal Large Language Models (MLLMs) demand substantial
computational resources for inference as the growth of their multimodal
Key-Value (KV) cache, in response to increasing input lengths, challenges
memory and time efficiency. Unlike single-modality LLMs that manage only
textual contexts, the KV cache of long-context MLLMs includes representations
from multiple images with temporal and spatial relationships and related
textual contexts. The predominance of image tokens means traditional
optimizations for LLMs' KV caches are unsuitable for multimodal long-context
settings, and no prior works have addressed this challenge. In this work, we
introduce LOOK-M, a pioneering, fine-tuning-free approach that efficiently
reduces the multimodal KV cache size while maintaining performance comparable
to a full cache. We observe that during prompt prefill, the model prioritizes
more textual attention over image features, and based on the multimodal
interaction observation, a new proposed text-prior method is explored to
compress the KV cache. Furthermore, to mitigate the degradation of image
contextual information, we propose several compensatory strategies using KV
pairs merging. LOOK-M demonstrates that with a significant reduction in KV
Cache memory usage, such as reducing it by 80% in some cases, it not only
achieves up to 1.5x faster decoding but also maintains or even enhances
performance across a variety of long context multimodal tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Speech Recognition for Hindi 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Saha, A. G. Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) is a key area in computational
linguistics, focusing on developing technologies that enable computers to
convert spoken language into text. This field combines linguistics and machine
learning. ASR models, which map speech audio to transcripts through supervised
learning, require handling real and unrestricted text. Text-to-speech systems
directly work with real text, while ASR systems rely on language models trained
on large text corpora. High-quality transcribed data is essential for training
predictive models. The research involved two main components: developing a web
application and designing a web interface for speech recognition. The web
application, created with JavaScript and Node.js, manages large volumes of
audio files and their transcriptions, facilitating collaborative human
correction of ASR transcripts. It operates in real-time using a client-server
architecture. The web interface for speech recognition records 16 kHz mono
audio from any device running the web app, performs voice activity detection
(VAD), and sends the audio to the recognition engine. VAD detects human speech
presence, aiding efficient speech processing and reducing unnecessary
processing during non-speech intervals, thus saving computation and network
bandwidth in VoIP applications. The final phase of the research tested a neural
network for accurately aligning the speech signal to hidden Markov model (HMM)
states. This included implementing a novel backpropagation method that utilizes
prior statistics of node co-activations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing "Implicit" Retrieval Robustness of <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Shen, Rexhina Blloshmi, Dawei Zhu, Jiahuan Pei, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation has gained popularity as a framework to
enhance large language models with external knowledge. However, its
effectiveness hinges on the retrieval robustness of the model. If the model
lacks retrieval robustness, its performance is constrained by the accuracy of
the retriever, resulting in significant compromises when the retrieved context
is irrelevant. In this paper, we evaluate the "implicit" retrieval robustness
of various large language models, instructing them to directly output the final
answer without explicitly judging the relevance of the retrieved context. Our
findings reveal that fine-tuning on a mix of gold and distracting context
significantly enhances the model's robustness to retrieval inaccuracies, while
still maintaining its ability to extract correct answers when retrieval is
accurate. This suggests that large language models can implicitly handle
relevant or irrelevant retrieved context by learning solely from the
supervision of the final answer in an end-to-end manner. Introducing an
additional process for explicit relevance judgment can be unnecessary and
disrupts the end-to-end approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConvoCache: Smart Re-Use of Chatbot Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conor Atkins, Ian Wood, Mohamed Ali Kaafar, Hassan Asghar, Nardine Basta, Michal Kepkowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ConvoCache, a conversational caching system that solves the
problem of slow and expensive generative AI models in spoken chatbots.
ConvoCache finds a semantically similar prompt in the past and reuses the
response. In this paper we evaluate ConvoCache on the DailyDialog dataset. We
find that ConvoCache can apply a UniEval coherence threshold of 90% and respond
to 89% of prompts using the cache with an average latency of 214ms, replacing
LLM and voice synthesis that can take over 1s. To further reduce latency we
test prefetching and find limited usefulness. Prefetching with 80% of a request
leads to a 63% hit rate, and a drop in overall coherence. ConvoCache can be
used with any chatbot to reduce costs by reducing usage of generative AI by up
to 89%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear at Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResumeAtlas: Revisiting Resume Classification with Large-Scale <span class="highlight-title">Dataset</span>s
  and <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Heakl, Youssef Mohamed, Noran Mohamed, Ali Sharkaway, Ahmed Zaky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing reliance on online recruitment platforms coupled with the
adoption of AI technologies has highlighted the critical need for efficient
resume classification methods. However, challenges such as small datasets, lack
of standardized resume templates, and privacy concerns hinder the accuracy and
effectiveness of existing classification models. In this work, we address these
challenges by presenting a comprehensive approach to resume classification. We
curated a large-scale dataset of 13,389 resumes from diverse sources and
employed Large Language Models (LLMs) such as BERT and Gemma1.1 2B for
classification. Our results demonstrate significant improvements over
traditional machine learning approaches, with our best model achieving a top-1
accuracy of 92\% and a top-5 accuracy of 97.5\%. These findings underscore the
importance of dataset quality and advanced model architectures in enhancing the
accuracy and robustness of resume classification systems, thus advancing the
field of online recruitment practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 1 table, 6th International Conference on AI in
  Computational Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Poisoned LangChain: Jailbreak <span class="highlight-title">LLM</span>s by LangChain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiu Wang, Jun Liu, Shengkai Zhang, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of natural language processing (NLP), large language
models (LLMs) are becoming increasingly popular. LLMs are integrating more into
everyday life, raising public concerns about their security vulnerabilities.
Consequently, the security of large language models is becoming critically
important. Currently, the techniques for attacking and defending against LLMs
are continuously evolving. One significant method type of attack is the
jailbreak attack, which designed to evade model safety mechanisms and induce
the generation of inappropriate content. Existing jailbreak attacks primarily
rely on crafting inducement prompts for direct jailbreaks, which are less
effective against large models with robust filtering and high comprehension
abilities. Given the increasing demand for real-time capabilities in large
language models, real-time updates and iterations of new knowledge have become
essential. Retrieval-Augmented Generation (RAG), an advanced technique to
compensate for the model's lack of new knowledge, is gradually becoming
mainstream. As RAG enables the model to utilize external knowledge bases, it
provides a new avenue for jailbreak attacks.
  In this paper, we conduct the first work to propose the concept of indirect
jailbreak and achieve Retrieval-Augmented Generation via LangChain. Building on
this, we further design a novel method of indirect jailbreak attack, termed
Poisoned-LangChain (PLC), which leverages a poisoned external knowledge base to
interact with large language models, thereby causing the large models to
generate malicious non-compliant dialogues.We tested this method on six
different large language models across three major categories of jailbreak
issues. The experiments demonstrate that PLC successfully implemented indirect
jailbreak attacks under three different scenarios, achieving success rates of
88.56%, 79.04%, and 82.69% respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages,2 figures,This paper is a submission to ACM TURC. It has been
  accepted by the editor of the organizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArzEn-<span class="highlight-title">LLM</span>: Code-Switched Egyptian Arabic-English Translation and Speech
  Recognition Using <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Heakl, Youssef Zaghloul, Mennatullah Ali, Rania Hossam, Walid Gomaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the widespread increase in the phenomenon of code-switching
between Egyptian Arabic and English in recent times, this paper explores the
intricacies of machine translation (MT) and automatic speech recognition (ASR)
systems, focusing on translating code-switched Egyptian Arabic-English to
either English or Egyptian Arabic. Our goal is to present the methodologies
employed in developing these systems, utilizing large language models such as
LLama and Gemma. In the field of ASR, we explore the utilization of the Whisper
model for code-switched Egyptian Arabic recognition, detailing our experimental
procedures including data preprocessing and training techniques. Through the
implementation of a consecutive speech-to-text translation system that
integrates ASR with MT, we aim to overcome challenges posed by limited
resources and the unique characteristics of the Egyptian Arabic dialect.
Evaluation against established metrics showcases promising results, with our
methodologies yielding a significant improvement of $56\%$ in English
translation over the state-of-the-art and $9.3\%$ in Arabic translation. Since
code-switching is deeply inherent in spoken languages, it is crucial that ASR
systems can effectively handle this phenomenon. This capability is crucial for
enabling seamless interaction in various domains, including business
negotiations, cultural exchanges, and academic discourse. Our models and code
are available as open-source resources. Code:
\url{http://github.com/ahmedheakl/arazn-llm}}, Models:
\url{http://huggingface.co/collections/ahmedheakl/arazn-llm-662ceaf12777656607b9524e}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 5 tables, 6th International Conference on AI in
  Computational Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeAligner: Safety Alignment against Jailbreak Attacks via Response
  Disparity Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the development of large language models (LLMs) rapidly advances, securing
these models effectively without compromising their utility has become a
pivotal area of research. However, current defense strategies against jailbreak
attacks (i.e., efforts to bypass security protocols) often suffer from limited
adaptability, restricted general capability, and high cost. To address these
challenges, we introduce SafeAligner, a methodology implemented at the decoding
stage to fortify defenses against jailbreak attacks. We begin by developing two
specialized models: the Sentinel Model, which is trained to foster safety, and
the Intruder Model, designed to generate riskier responses. SafeAligner
leverages the disparity in security levels between the responses from these
models to differentiate between harmful and beneficial tokens, effectively
guiding the safety alignment by altering the output token distribution of the
target model. Extensive experiments show that SafeAligner can increase the
likelihood of beneficial tokens, while reducing the occurrence of harmful ones,
thereby ensuring secure alignment with minimal loss to generality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BADGE: BADminton report Generation and Evaluation with <span class="highlight-title">LLM</span> <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang-Hsuan Chiang, Lin-Wei Chao, Kuang-Da Wang, Chih-Chuan Wang, Wen-Chih Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Badminton enjoys widespread popularity, and reports on matches generally
include details such as player names, game scores, and ball types, providing
audiences with a comprehensive view of the games. However, writing these
reports can be a time-consuming task. This challenge led us to explore whether
a Large Language Model (LLM) could automate the generation and evaluation of
badminton reports. We introduce a novel framework named BADGE, designed for
this purpose using LLM. Our method consists of two main phases: Report
Generation and Report Evaluation. Initially, badminton-related data is
processed by the LLM, which then generates a detailed report of the match. We
tested different Input Data Types, In-Context Learning (ICL), and LLM, finding
that GPT-4 performs best when using CSV data type and the Chain of Thought
prompting. Following report generation, the LLM evaluates and scores the
reports to assess their quality. Our comparisons between the scores evaluated
by GPT-4 and human judges show a tendency to prefer GPT-4 generated reports.
Since the application of LLM in badminton reporting remains largely unexplored,
our research serves as a foundational step for future advancements in this
area. Moreover, our method can be extended to other sports games, thereby
enhancing sports promotion. For more details, please refer to
https://github.com/AndyChiangSH/BADGE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2024 Workshop: The 2nd International Workshop on
  Intelligent Technologies for Precision Sports Science (IT4PSS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-Weighted RNN-T for Learning from Flawed Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gil Keren, Wei Zhou, Ozlem Kalinli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ASR models are commonly trained with the cross-entropy criterion to increase
the probability of a target token sequence. While optimizing the probability of
all tokens in the target sequence is sensible, one may want to de-emphasize
tokens that reflect transcription errors. In this work, we propose a novel
token-weighted RNN-T criterion that augments the RNN-T objective with
token-specific weights. The new objective is used for mitigating accuracy loss
from transcriptions errors in the training data, which naturally appear in two
settings: pseudo-labeling and human annotation errors. Experiments results show
that using our method for semi-supervised learning with pseudo-labels leads to
a consistent accuracy improvement, up to 38% relative. We also analyze the
accuracy degradation resulting from different levels of WER in the reference
transcription, and show that token-weighted RNN-T is suitable for overcoming
this degradation, recovering 64%-99% of the accuracy loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shimo Lab at "Discharge Me!": Discharge Summarization by Prompt-Driven
  Concatenation of Electronic Health Record Sections <span class="chip">ACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhen He, Hiroaki Yamagiwa, Hidetoshi Shimodaira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present our approach to the shared task "Discharge Me!" at
the BioNLP Workshop 2024. The primary goal of this task is to reduce the time
and effort clinicians spend on writing detailed notes in the electronic health
record (EHR). Participants develop a pipeline to generate the "Brief Hospital
Course" and "Discharge Instructions" sections from the EHR. Our approach
involves a first step of extracting the relevant sections from the EHR. We then
add explanatory prompts to these sections and concatenate them with separate
tokens to create the input text. To train a text generation model, we perform
LoRA fine-tuning on the ClinicalT5-large model. On the final test data, our
approach achieved a ROUGE-1 score of $0.394$, which is comparable to the top
solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BioNLP @ ACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-Driven Multimodal Opinion Expression Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bonian Jia, Huiyao Chen, Yueheng Sun, Meishan Zhang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinion Expression Identification (OEI) is essential in NLP for applications
ranging from voice assistants to depression diagnosis. This study extends OEI
to encompass multimodal inputs, underlining the significance of auditory cues
in delivering emotional subtleties beyond the capabilities of text. We
introduce a novel multimodal OEI (MOEI) task, integrating text and speech to
mirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we
construct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is
applied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template
for the OEI task to take full advantage of the generative power of large
language models (LLMs). Advancing further, we propose an LLM-driven method
STOEI, which combines speech and text modal to identify opinion expressions.
Our experiments demonstrate that MOEI significantly improves the performance
while our method outperforms existing methods by 9.20\% and obtains SOTA
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EHR-Based Mobile and Web Platform for Chronic Disease Risk Prediction
  Using Large Language Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun-Chieh Liao, Wei-Ting Kuo, I-Hsuan Hu, Yen-Chen Shih, Jun-En Ding, Feng Liu, Fang-Ming Hung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional diagnosis of chronic diseases involves in-person consultations
with physicians to identify the disease. However, there is a lack of research
focused on predicting and developing application systems using clinical notes
and blood test values. We collected five years of Electronic Health Records
(EHRs) from Taiwan's hospital database between 2017 and 2021 as an AI database.
Furthermore, we developed an EHR-based chronic disease prediction platform
utilizing Large Language Multimodal Models (LLMMs), successfully integrating
with frontend web and mobile applications for prediction. This prediction
platform can also connect to the hospital's backend database, providing
physicians with real-time risk assessment diagnostics. The demonstration link
can be found at https://www.youtube.com/watch?v=oqmL9DEDFgA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Knowledge Graph Completion from Pretrained Language Models
  with Knowledge Constraints <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Song, Shizhu He, Shengxiang Gao, Li Cai, Kang Liu, Zhengtao Yu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual Knowledge Graph Completion (mKGC) aim at solving queries like
(h, r, ?) in different languages by reasoning a tail entity t thus improving
multilingual knowledge graphs. Previous studies leverage multilingual
pretrained language models (PLMs) and the generative paradigm to achieve mKGC.
Although multilingual pretrained language models contain extensive knowledge of
different languages, its pretraining tasks cannot be directly aligned with the
mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit
a pronounced English-centric bias. This makes it difficult for mKGC to achieve
good results, particularly in the context of low-resource languages. To
overcome previous problems, this paper introduces global and local knowledge
constraints for mKGC. The former is used to constrain the reasoning of answer
entities, while the latter is used to enhance the representation of query
contexts. The proposed method makes the pretrained model better adapt to the
mKGC task. Experimental results on public datasets demonstrate that our method
outperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32% and
16.03%, which indicates that our proposed method has significant enhancement on
mKGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Octo-planner: On-device Language Model for Planner-Action Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chen, Zhiyuan Li, Zhen Guo, Yikang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI agents have become increasingly significant in various domains, enabling
autonomous decision-making and problem-solving. To function effectively, these
agents require a planning process that determines the best course of action and
then executes the planned actions. In this paper, we present an efficient
on-device Planner-Action framework that separates planning and action execution
into two distinct components: a planner agent based on Phi-3 Mini, a 3.8
billion parameter LLM optimized for edge devices, and an action agent using the
Octopus model for function execution. The planner agent first responds to user
queries by decomposing tasks into a sequence of sub-steps, which are then
executed by the action agent. To optimize performance on resource-constrained
devices, we employ model fine-tuning instead of in-context learning, reducing
computational costs and energy consumption while improving response times. Our
approach involves using GPT-4 to generate diverse planning queries and
responses based on available functions, with subsequent validations to ensure
data quality. We fine-tune the Phi-3 Mini model on this curated dataset,
achieving a 97\% success rate in our in-domain test environment. To address
multi-domain planning challenges, we developed a multi-LoRA training method
that merges weights from LoRAs trained on distinct function subsets. This
approach enables flexible handling of complex, multi-domain queries while
maintaining computational efficiency on resource-constrained devices. To
support further research, we have open-sourced our model weights at
\url{https://huggingface.co/NexaAIDev/octopus-planning}. For the demo, please
refer to \url{https://www.nexa4ai.com/octo-planner}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad
  Prediction <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yice Zhang, Jie Zeng, Weiming Hu, Ziyi Wang, Shiwei Chen, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect
term, aspect category, opinion term, sentiment polarity) for a given review,
which is the most representative and challenging task in aspect-based sentiment
analysis. A key challenge in the ASQP task is the scarcity of labeled data,
which limits the performance of existing methods. To tackle this issue, we
propose a self-training framework with a pseudo-label scorer, wherein a scorer
assesses the match between reviews and their pseudo-labels, aiming to filter
out mismatches and thereby enhance the effectiveness of self-training. We
highlight two critical aspects to ensure the scorer's effectiveness and
reliability: the quality of the training dataset and its model architecture. To
this end, we create a human-annotated comparison dataset and train a generative
model on it using ranking-based objectives. Extensive experiments on public
ASQP datasets reveal that using our scorer can greatly and consistently improve
the effectiveness of self-training. Moreover, we explore the possibility of
replacing humans with large language models for comparison dataset annotation,
and experiments demonstrate its feasibility. We release our code and data at
https://github.com/HITSZ-HLT/ST-w-Scorer-ABSA .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Models</span> for Cuffless Blood Pressure Measurement From
  Wearable Biosignals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengding Liu, Chen Chen, Jiannong Cao, Minglei Pan, Jikui Liu, Nan Li, Fen Miao, Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have captured significant interest from both
academia and industry due to their impressive performance across various
textual tasks. However, the potential of LLMs to analyze physiological
time-series data remains an emerging research field. Particularly, there is a
notable gap in the utilization of LLMs for analyzing wearable biosignals to
achieve cuffless blood pressure (BP) measurement, which is critical for the
management of cardiovascular diseases. This paper presents the first work to
explore the capacity of LLMs to perform cuffless BP estimation based on
wearable biosignals. We extracted physiological features from electrocardiogram
(ECG) and photoplethysmogram (PPG) signals and designed context-enhanced
prompts by combining these features with BP domain knowledge and user
information. Subsequently, we adapted LLMs to BP estimation tasks through
instruction tuning. To evaluate the proposed approach, we conducted assessments
of ten advanced LLMs using a comprehensive public dataset of wearable
biosignals from 1,272 participants. The experimental results demonstrate that
the optimally fine-tuned LLM significantly surpasses conventional task-specific
baselines, achieving an estimation error of 0.00 $\pm$ 9.25 mmHg for systolic
BP and 1.29 $\pm$ 6.37 mmHg for diastolic BP. Notably, the ablation studies
highlight the benefits of our context enhancement strategy, leading to an 8.9%
reduction in mean absolute error for systolic BP estimation. This paper
pioneers the exploration of LLMs for cuffless BP measurement, providing a
potential solution to enhance the accuracy of cuffless BP measurement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Energy-Based Models for Out-of-Distribution Detection in
  Dialect Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaqian Hao, Chenguang Hu, Yingying Gao, Shilei Zhang, Junlan Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diverse nature of dialects presents challenges for models trained on
specific linguistic patterns, rendering them susceptible to errors when
confronted with unseen or out-of-distribution (OOD) data. This study introduces
a novel margin-enhanced joint energy model (MEJEM) tailored specifically for
OOD detection in dialects. By integrating a generative model and the energy
margin loss, our approach aims to enhance the robustness of dialect
identification systems. Furthermore, we explore two OOD scores for OOD dialect
detection, and our findings conclusively demonstrate that the energy score
outperforms the softmax score. Leveraging Sharpness-Aware Minimization to
optimize the training process of the joint model, we enhance model
generalization by minimizing both loss and sharpness. Experiments conducted on
dialect identification tasks validate the efficacy of Energy-Based Models and
provide valuable insights into their performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Quality of Answers for <span class="highlight-title">Retrieval-Augmented</span> Generation: A
  Strong <span class="highlight-title">LLM</span> Is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Wang, Alberto Garcia Hernandez, Roman Kyslyi, Nicholas Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive evaluation of answer quality in
Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel
grading system that is designed to assess correctness, completeness, and
honesty. We further map the grading of quality aspects aforementioned into a
binary score, indicating an accept or reject decision, mirroring the intuitive
"thumbs-up" or "thumbs-down" gesture commonly used in chat applications. This
approach suits factual business settings where a clear decision opinion is
essential. Our assessment applies vRAG-Eval to two Large Language Models
(LLMs), evaluating the quality of answers generated by a vanilla RAG
application. We compare these evaluations with human expert judgments and find
a substantial alignment between GPT-4's assessments and those of human experts,
reaching 83% agreement on accept or reject decisions. This study highlights the
potential of LLMs as reliable evaluators in closed-domain, closed-ended
settings, particularly when human evaluations require significant resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for
  Memory-Efficient <span class="highlight-title">Large Language Models</span> Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Kai Zhen, Ershad Banijamal, Athanasios Mouchtaris, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) has achieved remarkable performance
across various natural language processing tasks, yet it demands more and more
memory as model sizes keep growing. To address this issue, the recently
proposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs
using only forward passes, thereby avoiding the need for a backpropagation
graph. However, significant performance drops and a high risk of divergence
have limited their widespread adoption. In this paper, we propose the Adaptive
Zeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed
to improve the performance and convergence of the ZO methods. To enhance
dimension-dependent ZO estimation accuracy, we introduce a fast-forward,
low-parameter tensorized adapter. To tackle the frequently observed divergence
issue in large-scale ZO fine-tuning tasks, we propose an adaptive query number
schedule that guarantees convergence. Detailed theoretical analysis and
extensive experimental results on Roberta-Large and Llama-2-7B models
substantiate the efficacy of our AdaZeta framework in terms of accuracy, memory
efficiency, and convergence speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Entity Recognition Using Ensembles of Deep Learning and
  Fine-tuned <span class="highlight-title">Large Language Models</span>: A Case Study on Adverse Event Extraction
  from Multiple Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Li, Deepthi Viswaroopan, William He, Jianfu Li, Xu Zuo, Hua Xu, Cui Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adverse event (AE) extraction following COVID-19 vaccines from text data is
crucial for monitoring and analyzing the safety profiles of immunizations.
Traditional deep learning models are adept at learning intricate feature
representations and dependencies in sequential data, but often require
extensive labeled data. In contrast, large language models (LLMs) excel in
understanding contextual information, but exhibit unstable performance on named
entity recognition tasks, possibly due to their broad but unspecific training.
This study aims to evaluate the effectiveness of LLMs and traditional deep
learning models in AE extraction, and to assess the impact of ensembling these
models on performance. In this study, we utilized reports and posts from the
VAERS (n=621), Twitter (n=9,133), and Reddit (n=131) as our corpora. Our goal
was to extract three types of entities: "vaccine", "shot", and "ae". We
explored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5,
GPT-4, and Llama-2, as well as traditional deep learning models like RNN and
BioBERT. To enhance performance, we created ensembles of the three models with
the best performance. For evaluation, we used strict and relaxed F1 scores to
evaluate the performance for each entity type, and micro-average F1 was used to
assess the overall performance. The ensemble model achieved the highest
performance in "vaccine", "shot", and "ae" with strict F1-scores of 0.878,
0.930, and 0.925, respectively, along with a micro-average score of 0.903. In
conclusion, this study demonstrates the effectiveness and robustness of
ensembling fine-tuned traditional deep learning models and LLMs, for extracting
AE-related information. This study contributes to the advancement of biomedical
natural language processing, providing valuable insights into improving AE
extraction from text data for pharmacovigilance and public health surveillance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PharmGPT: Domain-Specific <span class="highlight-title">Large Language Models</span> for Bio-Pharmaceutical
  and Chemistry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linqing Chen, Weilei Wang, Zilong Bai, Peng Xu, Yan Fang, Jie Fang, Wentao Wu, Lizhi Zhou, Ruiji Zhang, Yubin Xia, Chaobo Xu, Ran Hu, Licong Xu, Qijun Cai, Haoran Hua, Jing Sun, Jin Liu, Tian Qiu, Haowen Liu, Meng Hu, Xiuwen Li, Fei Gao, Yufu Wang, Lin Tie, Chaochao Wang, Jianping Lu, Cheng Sun, Yixin Wang, Shengjie Yang, Yuancheng Li, Lu Jin, Lisha Zhang, Fu Bian, Changyang Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized Natural Language Processing
(NLP) by by minimizing the need for complex feature engineering. However, the
application of LLMs in specialized domains like biopharmaceuticals and
chemistry remains largely unexplored. These fields are characterized by
intricate terminologies, specialized knowledge, and a high demand for precision
areas where general purpose LLMs often fall short. In this study, we introduce
PharmGPT, a suite of multilingual LLMs with 13 billion and 70 billion
parameters, specifically trained on a comprehensive corpus of hundreds of
billions of tokens tailored to the Bio-Pharmaceutical and Chemical sectors. Our
evaluation shows that PharmGPT matches or surpasses existing general models on
key benchmarks, such as NAPLEX, demonstrating its exceptional capability in
domain-specific tasks. This advancement establishes a new benchmark for LLMs in
the Bio-Pharmaceutical and Chemical fields, addressing the existing gap in
specialized language modeling. Furthermore, this suggests a promising path for
enhanced research and development in these specialized areas, paving the way
for more precise and effective applications of NLP in specialized domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>s for Doctors: Leve<span class="highlight-title">rag</span>ing Medical <span class="highlight-title">LLM</span>s to Assist Doctors, Not Replace
  Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenya Xie, Qingying Xiao, Yu Zheng, Xidong Wang, Junying Chen, Ke Ji, Anningzhe Gao, Xiang Wan, Feng Jiang, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of Large Language Models (LLMs) has had a significant
impact on the healthcare field, providing patients with medical advice,
diagnostic information, and more. However, due to a lack of professional
medical knowledge, patients are easily misled by generated erroneous
information from LLMs, which may result in serious medical problems. To address
this issue, we focus on tuning the LLMs to be medical assistants who
collaborate with more experienced doctors. We first conduct a two-stage survey
by inspiration-feedback to gain a broad understanding of the real needs of
doctors for medical assistants. Based on this, we construct a Chinese medical
dataset called DoctorFLAN to support the entire workflow of doctors, which
includes 92K Q\&A samples from 22 tasks and 27 specialists. Moreover, we
evaluate LLMs in doctor-oriented scenarios by constructing the
DoctorFLAN-\textit{test} containing 550 single-turn Q\&A and DotaBench
containing 74 multi-turn conversations. The evaluation results indicate that
being a medical assistant still poses challenges for existing open-source
models, but DoctorFLAN can help them significantly. It demonstrates that the
doctor-oriented dataset and benchmarks we construct can complement existing
patient-oriented work and better promote medical LLMs research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Clinical Data Extraction with Knowledge Conditioned <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diya Li, Asim Kadav, Aijing Gao, Rui Li, Richard Bourgon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of lung lesion information from clinical and medical imaging
reports is crucial for research on and clinical care of lung-related diseases.
Large language models (LLMs) can be effective at interpreting unstructured text
in reports, but they often hallucinate due to a lack of domain-specific
knowledge, leading to reduced accuracy and posing challenges for use in
clinical settings. To address this, we propose a novel framework that aligns
generated internal knowledge with external knowledge through in-context
learning (ICL). Our framework employs a retriever to identify relevant units of
internal or external knowledge and a grader to evaluate the truthfulness and
helpfulness of the retrieved internal-knowledge rules, to align and update the
knowledge bases. Our knowledge-conditioned approach also improves the accuracy
and reliability of LLM outputs by addressing the extraction task in two stages:
(i) lung lesion finding detection and primary structured field parsing,
followed by (ii) further parsing of lesion description text into additional
structured fields. Experiments with expert-curated test datasets demonstrate
that this ICL approach can increase the F1 score for key fields (lesion size,
margin and solidity) by an average of 12.9% over existing ICL methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding with Limited Teacher Supervision Requires Understanding When to
  Trust the Teacher 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjong Ok, Jegwang Ryu, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can sLLMs efficiently utilize the supervision of LLMs to improve their
generative quality? This question has been well studied in scenarios where
there is no restriction on the number of LLM supervisions one can use, giving
birth to many decoding algorithms that utilize supervision without further
training. However, it is still unclear what is an effective strategy under the
limited supervision scenario, where we assume that no more than a few tokens
can be generated by LLMs. To this end, we develop an algorithm to effectively
aggregate the sLLM and LLM predictions on initial tokens so that the generated
tokens can more accurately condition the subsequent token generation by sLLM
only. Critically, we find that it is essential to adaptively overtrust or
disregard the LLM prediction based on the confidence of the sLLM. Through our
experiments on a wide range of models and datasets, we demonstrate that our
method provides a consistent improvement over conventional decoding strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Catching Chameleons: Detecting Evolving Disinformation Generated using
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Jiang, Chengshuai Zhao, Zhen Tan, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in detecting disinformation generated by large
language models (LLMs), current efforts overlook the ever-evolving nature of
this disinformation. In this work, we investigate a challenging yet practical
research problem of detecting evolving LLM-generated disinformation.
Disinformation evolves constantly through the rapid development of LLMs and
their variants. As a consequence, the detection model faces significant
challenges. First, it is inefficient to train separate models for each
disinformation generator. Second, the performance decreases in scenarios when
evolving LLM-generated disinformation is encountered in sequential order. To
address this problem, we propose DELD (Detecting Evolving LLM-generated
Disinformation), a parameter-efficient approach that jointly leverages the
general fact-checking capabilities of pre-trained language models (PLM) and the
independent disinformation generation characteristics of various LLMs. In
particular, the learned characteristics are concatenated sequentially to
facilitate knowledge accumulation and transformation. DELD addresses the issue
of label scarcity by integrating the semantic embeddings of disinformation with
trainable soft prompts to elicit model-specific knowledge. Our experiments show
that \textit{DELD} significantly outperforms state-of-the-art methods.
Moreover, our method provides critical insights into the unique patterns of
disinformation generation across different LLMs, offering valuable perspectives
in this line of research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explicit Diversity Conditions for Effective Question Answer Generation
  with <span class="highlight-title">Large Language Models</span> <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikas Yadav, Hyuk Joon Kwon, Vijay Srinivasan, Hongxia Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answer Generation (QAG) is an effective data augmentation technique
to improve the accuracy of question answering systems, especially in
low-resource domains. While recent pretrained and large language model-based
QAG methods have made substantial progress, they face the critical issue of
redundant QA pair generation, affecting downstream QA systems. Implicit
diversity techniques such as sampling and diverse beam search are proven
effective solutions but often yield smaller diversity. We present explicit
diversity conditions for QAG, focusing on spatial aspects, question types, and
entities, substantially increasing diversity in QA generation. Our work
emphasizes the need of explicit diversity conditions for generating diverse
question-answer synthetic data by showing significant improvements in
downstream QA task over existing widely adopted implicit diversity techniques.
In particular, generated QA pairs from explicit diversity conditions when used
to train the downstream QA model results in an average 4.1% exact match and
4.5% F1 improvement over QAG from implicit sampling techniques on SQuADDU. Our
work emphasizes the need for explicit diversity conditions even more in
low-resource datasets (SubjQA), where average downstream QA performance
improvements are around 12% EM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-step Knowledge Retrieval and Inference over Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, CJ McFate, Lori Moon, Nati Seifu, Maksim Eremeev, Jose Barrera, Eric Brown, David Ferrucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) and Generative AI has
revolutionized natural language applications across various domains. However,
high-stakes decision-making tasks in fields such as medical, legal and finance
require a level of precision, comprehensiveness, and logical consistency that
pure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to
deliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI
platform to tackle these problems. The platform integrates fine-tuned LLMs for
knowledge extraction and alignment with a robust symbolic reasoning engine for
logical inference, planning and interactive constraint solving. We describe
Cora, a Collaborative Research Assistant built on this platform, that is
designed to perform complex research and discovery tasks in high-stakes
domains. This paper discusses the multi-step inference challenges inherent in
such domains, critiques the limitations of existing LLM-based methods, and
demonstrates how Cora's neuro-symbolic approach effectively addresses these
issues. We provide an overview of the system architecture, key algorithms for
knowledge extraction and formal reasoning, and present preliminary evaluation
results that highlight Cora's superior performance compared to well-known LLM
and RAG baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Psychological Profiling in Cybersecurity: A Look at <span class="highlight-title">LLM</span>s and
  Psycholinguistic Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Marie Tshimula, D'Jeff K. Nkashama, Jean Tshibangu Muabila, René Manassé Galekwa, Hugues Kanda, Maximilien V. Dialufuma, Mbuyi Mukendi Didier, Kalala Kalonji, Serge Mundele, Patience Kinshie Lenye, Tighana Wenge Basele, Aristarque Ilunga, Christian N. Mayemba, Nathanaël M. Kasoro, Selain K. Kasereka, Hardy Mikese, Pierre-Martin Tardif, Marc Frappier, Froduald Kabanza, Belkacem Chikhaoui, Shengrui Wang, Ali Mulenda Sumbu, Xavier Ndona, Raoul Kienge-Kienge Intudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing sophistication of cyber threats necessitates innovative
approaches to cybersecurity. In this paper, we explore the potential of
psychological profiling techniques, particularly focusing on the utilization of
Large Language Models (LLMs) and psycholinguistic features. We investigate the
intersection of psychology and cybersecurity, discussing how LLMs can be
employed to analyze textual data for identifying psychological traits of threat
actors. We explore the incorporation of psycholinguistic features, such as
linguistic patterns and emotional cues, into cybersecurity frameworks. \iffalse
Through case studies and experiments, we discuss the effectiveness of these
methods in enhancing threat detection and mitigation strategies.\fi Our
research underscores the importance of integrating psychological perspectives
into cybersecurity practices to bolster defense mechanisms against evolving
threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Discourse Relation Classification For Nigerian Pidgin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammed Saeed, Peter Bourgonje, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite attempts to make Large Language Models multi-lingual, many of the
world's languages are still severely under-resourced. This widens the
performance gap between NLP and AI applications aimed at well-financed, and
those aimed at less-resourced languages. In this paper, we focus on Nigerian
Pidgin (NP), which is spoken by nearly 100 million people, but has
comparatively very few NLP resources and corpora. We address the task of
Implicit Discourse Relation Classification (IDRC) and systematically compare an
approach translating NP data to English and then using a well-resourced IDRC
tool and back-projecting the labels versus creating a synthetic discourse
corpus for NP, in which we translate PDTB and project PDTB labels, and then
train an NP IDR classifier. The latter approach of learning a "native" NP
classifier outperforms our baseline by 13.27\% and 33.98\% in f$_{1}$ score for
4-way and 11-way classification, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Categorical Syllogisms Revisited: A <span class="highlight-title">Review</span> of the Logical Reasoning
  Abilities of <span class="highlight-title">LLM</span>s for Analyzing Categorical Syllogism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Zong, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been a huge number of benchmarks proposed to evaluate how large
language models (LLMs) behave for logic inference tasks. However, it remains an
open question how to properly evaluate this ability. In this paper, we provide
a systematic overview of prior works on the logical reasoning ability of LLMs
for analyzing categorical syllogisms. We first investigate all the possible
variations for the categorical syllogisms from a purely logical perspective and
then examine the underlying configurations (i.e., mood and figure) tested by
the existing datasets. Our results indicate that compared to template-based
synthetic datasets, crowdsourcing approaches normally sacrifice the coverage of
configurations (i.e., mood and figure) of categorical syllogisms for more
language variations, thus bringing challenges to fully testing LLMs under
different situations. We then proceed to summarize the findings and
observations for the performances of LLMs to infer the validity of syllogisms
from the current literature. The error rate breakdown analyses suggest that the
interpretation of the quantifiers seems to be the current bottleneck that
limits the performances of the LLMs and is thus worth more attention. Finally,
we discuss several points that might be worth considering when researchers plan
on the future release of categorical syllogism datasets. We hope our work will
not only provide a timely review of the current literature regarding
categorical syllogisms, but also motivate more interdisciplinary research
between communities, specifically computational linguists and logicians.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baharan Nouriinanloo, Maxime Lamothe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been revolutionizing a myriad of natural
language processing tasks with their diverse zero-shot capabilities. Indeed,
existing work has shown that LLMs can be used to great effect for many tasks,
such as information retrieval (IR), and passage ranking. However, current
state-of-the-art results heavily lean on the capabilities of the LLM being
used. Currently, proprietary, and very large LLMs such as GPT-4 are the highest
performing passage re-rankers. Hence, users without the resources to leverage
top of the line LLMs, or ones that are closed source, are at a disadvantage. In
this paper, we investigate the use of a pre-filtering step before passage
re-ranking in IR. Our experiments show that by using a small number of human
generated relevance scores, coupled with LLM relevance scoring, it is
effectively possible to filter out irrelevant passages before re-ranking. Our
experiments also show that this pre-filtering then allows the LLM to perform
significantly better at the re-ranking task. Indeed, our results show that
smaller models such as Mixtral can become competitive with much larger
proprietary models (e.g., ChatGPT and GPT-4).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WavRx: a Disease-Agnostic, Generalizable, and Privacy-Preserving Speech
  Health Diagnostic Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhu, Tiago Falk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech is known to carry health-related attributes, which has emerged as a
novel venue for remote and long-term health monitoring. However, existing
models are usually tailored for a specific type of disease, and have been shown
to lack generalizability across datasets. Furthermore, concerns have been
raised recently towards the leakage of speaker identity from health embeddings.
To mitigate these limitations, we propose WavRx, a speech health diagnostics
model that captures the respiration and articulation related dynamics from a
universal speech representation. Our in-domain and cross-domain experiments on
six pathological speech datasets demonstrate WavRx as a new state-of-the-art
health diagnostic model. Furthermore, we show that the amount of speaker
identity entailed in the WavRx health embeddings is significantly reduced
without extra guidance during training. An in-depth analysis of the model was
performed, thus providing physiological interpretation of its improved
generalizability and privacy-preserving ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review; Model script available at
  https://github.com/zhu00121/WavRx</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jailbreaking <span class="highlight-title">LLM</span>s with Arabic Transliteration and Arabizi 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mansour Al Ghanim, Saleh Almohaimeed, Mengxin Zheng, Yan Solihin, Qian Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study identifies the potential vulnerabilities of Large Language Models
(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and
its various forms. While most research has concentrated on English-based prompt
manipulation, our investigation broadens the scope to investigate the Arabic
language. We initially tested the AdvBench benchmark in Standardized Arabic,
finding that even with prompt manipulation techniques like prefix injection, it
was insufficient to provoke LLMs into generating unsafe content. However, when
using Arabic transliteration and chatspeak (or arabizi), we found that unsafe
content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3
Sonnet. Our findings suggest that using Arabic and its various forms could
expose information that might remain hidden, potentially increasing the risk of
jailbreak attacks. We hypothesize that this exposure could be due to the
model's learned connection to specific words, highlighting the need for more
comprehensive safety training across all language forms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn it or Leave it: Module Composition and Pruning for Continual
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Wang, Heike Adel, Lukas Lange, Jannik Strötgen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world environments, continual learning is essential for machine
learning models, as they need to acquire new knowledge incrementally without
forgetting what they have already learned. While pretrained language models
have shown impressive capabilities on various static tasks, applying them to
continual learning poses significant challenges, including avoiding
catastrophic forgetting, facilitating knowledge transfer, and maintaining
parameter efficiency. In this paper, we introduce MoCL-P, a novel lightweight
continual learning method that addresses these challenges simultaneously.
Unlike traditional approaches that continuously expand parameters for newly
arriving tasks, MoCL-P integrates task representation-guided module composition
with adaptive pruning, effectively balancing knowledge integration and
computational overhead. Our evaluation across three continual learning
benchmarks with up to 176 tasks shows that MoCL-P achieves state-of-the-art
performance and improves parameter efficiency by up to three times,
demonstrating its potential for practical applications where resource
requirements are constrained.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating The U.S. Senate: An <span class="highlight-title">LLM</span>-Driven Agent Approach to Modeling
  Legislative Behavior and Bipartisanship 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary R. Baker, Zarif L. Azher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a novel approach to simulating legislative processes
using LLM-driven virtual agents, focusing on the U.S. Senate Intelligence
Committee. We developed agents representing individual senators and placed them
in simulated committee discussions. The agents demonstrated the ability to
engage in realistic debate, provide thoughtful reflections, and find bipartisan
solutions under certain conditions. Notably, the simulation also showed promise
in modeling shifts towards bipartisanship in response to external
perturbations. Our results indicate that this LLM-driven approach could become
a valuable tool for understanding and potentially improving legislative
processes, supporting a broader pattern of findings highlighting how LLM-based
agents can usefully model real-world phenomena. Future works will focus on
enhancing agent complexity, expanding the simulation scope, and exploring
applications in policy testing and negotiation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequence Graph Network for Online Debate Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Mai, Susan Gauch, Douglas Adams, Miaoqing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online debates involve a dynamic exchange of ideas over time, where
participants need to actively consider their opponents' arguments, respond with
counterarguments, reinforce their own points, and introduce more compelling
arguments as the discussion unfolds. Modeling such a complex process is not a
simple task, as it necessitates the incorporation of both sequential
characteristics and the capability to capture interactions effectively. To
address this challenge, we employ a sequence-graph approach. Building the
conversation as a graph allows us to effectively model interactions between
participants through directed edges. Simultaneously, the propagation of
information along these edges in a sequential manner enables us to capture a
more comprehensive representation of context. We also introduce a Sequence
Graph Attention layer to illustrate the proposed information update scheme. The
experimental results show that sequence graph networks achieve superior results
to existing methods in online debates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Correct for QA Reasoning with Black-box <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehyung Kim, Dongyoung Kim, Yiming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An open challenge in recent machine learning is about how to improve the
reasoning capability of large language models (LLMs) in a black-box setting,
i.e., without access to detailed information such as output token
probabilities. Existing approaches either rely on accessibility (which is often
unrealistic) or involve significantly increased train- and inference-time
costs. This paper addresses those limitations or shortcomings by proposing a
novel approach, namely CoBB (Correct for improving QA reasoning of Black-Box
LLMs). It uses a trained adaptation model to perform a seq2seq mapping from the
often-imperfect reasonings of the original black-box LLM to the correct or
improved reasonings. Specifically, the adaptation model is initialized with a
relatively small open-source LLM and adapted over a collection of sub-sampled
training pairs. To select the representative pairs of correct and incorrect
reasonings, we formulated the dataset construction as an optimization problem
that minimizes the statistical divergence between the sampled subset and the
entire collection, and solved it via a genetic algorithm. We then train the
adaptation model over the sampled pairs by contrasting the likelihoods of
correct and incorrect reasonings. Our experimental results demonstrate that
CoBB significantly improves reasoning accuracy across various QA benchmarks,
compared to the best-performing adaptation baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Multilingual Alignment Prism: Aligning Global and Local Preferences
  to Reduce Harm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Aakanksha, Arash Ahmadian, Beyza Ermis, Seraphina Goldfarb-Tarrant, Julia Kreutzer, Marzieh Fadaee, Sara Hooker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key concern with the concept of "alignment" is the implicit question of
"alignment to what?". AI systems are increasingly used across the world, yet
safety alignment is often focused on homogeneous monolingual settings.
Additionally, preference training and safety measures often overfit to harms
common in Western-centric datasets. Here, we explore the viability of different
alignment approaches when balancing dual objectives: addressing and optimizing
for a non-homogeneous set of languages and cultural preferences while
minimizing both global and local harms. We collect the first set of human
annotated red-teaming prompts in different languages distinguishing between
global and local harm, which serve as a laboratory for understanding the
reliability of alignment techniques when faced with preference distributions
that are non-stationary across geographies and languages. While this setting is
seldom covered by the literature to date, which primarily centers on English
harm mitigation, it captures real-world interactions with AI systems around the
world. We establish a new precedent for state-of-the-art alignment techniques
across 6 languages with minimal degradation in general performance. Our work
provides important insights into cross-lingual transfer and novel optimization
approaches to safeguard AI systems designed to serve global populations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speakers Unembedded: Embedding-free Approach to Long-form Neural
  Diarization <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Vivek Govindan, Rohit Paturi, Sundararajan Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end neural diarization (EEND) models offer significant improvements
over traditional embedding-based Speaker Diarization (SD) approaches but falls
short on generalizing to long-form audio with large number of speakers.
EEND-vector-clustering method mitigates this by combining local EEND with
global clustering of speaker embeddings from local windows, but this requires
an additional speaker embedding framework alongside the EEND module. In this
paper, we propose a novel framework applying EEND both locally and globally for
long-form audio without separate speaker embeddings. This approach achieves
significant relative DER reduction of 13% and 10% over the conventional 1-pass
EEND on Callhome American English and RT03-CTS datasets respectively and
marginal improvements over EEND-vector-clustering without the need for
additional speaker embeddings. Furthermore, we discuss the computational
complexity of our proposed framework and explore strategies for reducing
processing times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at INTERSPEECH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Personalization of <span class="highlight-title">LLM</span>s with Mis-aligned Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehyung Kim, Yiming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the diversity of users increases, the capability of providing personalized
responses by large language models (LLMs) has become increasingly important.
Existing approaches have only limited successes in LLM personalization, due to
the absence of personalized learning or the reliance on shared personal data.
This paper proposes a new approach for a few-shot personalization of LLMs with
their mis-aligned responses (Fermi). Our key idea is to learn a set of
personalized prompts for each user by progressively improving the prompts using
LLMs, based on user profile (e.g., demographic information) and a few examples
of previous opinions. During an iterative process of prompt improvement, we
incorporate the contexts of mis-aligned responses by LLMs, which are especially
crucial for the effective personalization of LLMs. In addition, we develop an
effective inference method to further leverage the context of the test query
and the personalized prompts. Our experimental results demonstrate that Fermi
significantly improves performance across various benchmarks, compared to the
best-performing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understand What <span class="highlight-title">LLM</span> Needs: Dual Preference Alignment for
  <span class="highlight-title">Retrieval-Augmented</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has demonstrated effectiveness in
mitigating the hallucination problem of large language models (LLMs). However,
the difficulty of aligning the retriever with the diverse LLMs' knowledge
preferences inevitably poses an inevitable challenge in developing a reliable
RAG system. To address this issue, we propose DPA-RAG, a universal framework
designed to align diverse knowledge preferences within RAG systems.
Specifically, we initially introduce a preference knowledge construction
pipline and incorporate five novel query augmentation strategies to alleviate
preference data scarcity. Based on preference data, DPA-RAG accomplishes both
external and internal preference alignment: 1) It jointly integrate pair-wise,
point-wise, and contrastive preference alignment abilities into the reranker,
achieving external preference alignment among RAG components. 2) It further
introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT),
enabling LLMs to implicitly capture knowledge aligned with their reasoning
preferences, achieving LLMs' internal alignment. Experimental results across
four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all
baselines and seamlessly integrates both black-box and open-sourced LLM
readers. Further qualitative analysis and discussions also provide empirical
guidance for achieving reliable RAG systems. Our code is publicly available at
https://github.com/dongguanting/DPA-RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-AI Collaborative Taxonomy Construction: A Case Study in
  Profession-Specific Writing Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minhwa Lee, Zae Myung Kim, Vivek A. Khetan, Dongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have assisted humans in several writing tasks,
including text revision and story generation. However, their effectiveness in
supporting domain-specific writing, particularly in business contexts, is
relatively less explored. Our formative study with industry professionals
revealed the limitations in current LLMs' understanding of the nuances in such
domain-specific writing. To address this gap, we propose an approach of
human-AI collaborative taxonomy development to perform as a guideline for
domain-specific writing assistants. This method integrates iterative feedback
from domain experts and multiple interactions between these experts and LLMs to
refine the taxonomy. Through larger-scale experiments, we aim to validate this
methodology and thus improve LLM-powered writing assistance, tailoring it to
meet the unique requirements of different stakeholder needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CHI 2024 In2Writing Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Route<span class="highlight-title">LLM</span>: Learning to Route <span class="highlight-title">LLM</span>s with Preference Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, M Waleed Kadous, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit impressive capabilities across a wide
range of tasks, yet the choice of which model to use often involves a trade-off
between performance and cost. More powerful models, though effective, come with
higher expenses, while less capable models are more cost-effective. To address
this dilemma, we propose several efficient router models that dynamically
select between a stronger and a weaker LLM during inference, aiming to optimize
the balance between cost and response quality. We develop a training framework
for these routers leveraging human preference data and data augmentation
techniques to enhance performance. Our evaluation on widely-recognized
benchmarks shows that our approach significantly reduces costs-by over 2 times
in certain cases-without compromising the quality of responses. Interestingly,
our router models also demonstrate significant transfer learning capabilities,
maintaining their performance even when the strong and weak models are changed
at test time. This highlights the potential of these routers to provide a
cost-effective yet high-performance solution for deploying LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Copyright Takedown Methods for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyi Wei, Weijia Shi, Yangsibo Huang, Noah A. Smith, Chiyuan Zhang, Luke Zettlemoyer, Kai Li, Peter Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) derive their capabilities from extensive training on
diverse data, including potentially copyrighted material. These models can
memorize and generate content similar to their training data, posing potential
concerns. Therefore, model creators are motivated to develop mitigation methods
that prevent generating protected content. We term this procedure as copyright
takedowns for LMs, noting the conceptual similarity to (but legal distinction
from) the DMCA takedown This paper introduces the first evaluation of the
feasibility and side effects of copyright takedowns for LMs. We propose
CoTaEval, an evaluation framework to assess the effectiveness of copyright
takedown methods, the impact on the model's ability to retain uncopyrightable
factual knowledge from the training data whose recitation is embargoed, and how
well the model maintains its general utility and efficiency. We examine several
strategies, including adding system prompts, decoding-time filtering
interventions, and unlearning approaches. Our findings indicate that no tested
method excels across all metrics, showing significant room for research in this
unique problem setting and indicating potential unresolved challenges for live
policy proposals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 9 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs) due to the extensive and precise chain of reasoning required for
accuracy. Ensuring the correctness of each reasoning step is critical. To
address this, we aim to enhance the robustness and factuality of LLMs by
learning from human feedback. However, Direct Preference Optimization (DPO) has
shown limited benefits for long-chain mathematical reasoning, as models
employing DPO struggle to identify detailed errors in incorrect answers. This
limitation stems from a lack of fine-grained process supervision. We propose a
simple, effective, and data-efficient method called Step-DPO, which treats
individual reasoning steps as units for preference optimization rather than
evaluating answers holistically. Additionally, we have developed a data
construction pipeline for Step-DPO, enabling the creation of a high-quality
dataset containing 10K step-wise preference pairs. We also observe that in DPO,
self-generated data is more effective than data generated by humans or GPT-4,
due to the latter's out-of-distribution nature. Our findings demonstrate that
as few as 10K preference data pairs and fewer than 500 Step-DPO training steps
can yield a nearly 3% gain in accuracy on MATH for models with over 70B
parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves
scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively,
surpassing a series of closed-source models, including GPT-4-1106,
Claude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at
https://github.com/dvlab-research/Step-DPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, data, and models are available at
  https://github.com/dvlab-research/Step-DPO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Situational Awareness Matters in 3D Vision Language Reasoning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Man, Liang-Yan Gui, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Being able to carry out complicated vision language reasoning tasks in 3D
space represents a significant milestone in developing household robots and
human-centered embodied AI. In this work, we demonstrate that a critical and
distinct challenge in 3D vision language reasoning is situational awareness,
which incorporates two key components: (1) The autonomous agent grounds its
self-location based on a language prompt. (2) The agent answers open-ended
questions from the perspective of its calculated position. To address this
challenge, we introduce SIG3D, an end-to-end Situation-Grounded model for 3D
vision language reasoning. We tokenize the 3D scene into sparse voxel
representation and propose a language-grounded situation estimator, followed by
a situated question answering module. Experiments on the SQA3D and ScanQA
datasets show that SIG3D outperforms state-of-the-art models in situation
estimation and question answering by a large margin (e.g., an enhancement of
over 30% on situation estimation accuracy). Subsequent analysis corroborates
our architectural design choices, explores the distinct functions of visual and
textual tokens, and highlights the importance of situational awareness in the
domain of 3D question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Project Page: https://yunzeman.github.io/situation3d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Impact of Voice Anonymization on Speech Diagnostic Applications:
  a Case Study on COVID-19 Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhu, Mohamed Imoussaïne-Aïkous, Carolyn Côté-Lussier, Tiago H. Falk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With advances seen in deep learning, voice-based applications are burgeoning,
ranging from personal assistants, affective computing, to remote disease
diagnostics. As the voice contains both linguistic and para-linguistic
information (e.g., vocal pitch, intonation, speech rate, loudness), there is
growing interest in voice anonymization to preserve speaker privacy and
identity. Voice privacy challenges have emerged over the last few years and
focus has been placed on removing speaker identity while keeping linguistic
content intact. For affective computing and disease monitoring applications,
however, the para-linguistic content may be more critical. Unfortunately, the
effects that anonymization may have on these systems are still largely unknown.
In this paper, we fill this gap and focus on one particular health monitoring
application: speech-based COVID-19 diagnosis. We test three anonymization
methods and their impact on five different state-of-the-art COVID-19 diagnostic
systems using three public datasets. We validate the effectiveness of the
anonymization methods, compare their computational complexity, and quantify the
impact across different testing scenarios for both within- and across-dataset
conditions. Additionally, we provided a comprehensive evaluation of the
importance of different speech aspects for diagnostics and showed how they are
affected by different types of anonymizers. Lastly, we show the benefits of
using anonymized external data as a data augmentation tool to help recover some
of the COVID-19 diagnostic accuracy loss seen with anonymization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version; Published at IEEE Transactions on Information
  Forensics and Security</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Models</span> in the Clinic: A Comprehensive Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Liu, Hongjian Zhou, Yining Hua, Omid Rohanian, Anshul Thakur, Lei Clifton, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of large language models (LLMs) to assist clinicians has
attracted remarkable attention. Existing works mainly adopt the close-ended
question-answering (QA) task with answer options for evaluation. However, many
clinical decisions involve answering open-ended questions without pre-set
options. To better understand LLMs in the clinic, we construct a benchmark
ClinicBench. We first collect eleven existing datasets covering diverse
clinical language generation, understanding, and reasoning tasks. Furthermore,
we construct six novel datasets and complex clinical tasks that are close to
real-world practice, i.e., referral QA, treatment recommendation,
hospitalization (long document) summarization, patient education, pharmacology
QA and drug interaction for emerging drugs. We conduct an extensive evaluation
of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we
invite medical experts to evaluate the clinical usefulness of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Model</span> Enhanced Clustering for News Event Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adane Nega Tarekegn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The news landscape is continuously evolving, with an ever-increasing volume
of information from around the world. Automated event detection within this
vast data repository is essential for monitoring, identifying, and categorizing
significant news occurrences across diverse platforms. This paper presents an
event detection framework that leverages Large Language Models (LLMs) combined
with clustering analysis to detect news events from the Global Database of
Events, Language, and Tone (GDELT). The framework enhances event clustering
through both pre-event detection tasks (keyword extraction and text embedding)
and post-event detection tasks (event summarization and topic labeling). We
also evaluate the impact of various textual embeddings on the quality of
clustering outcomes, ensuring robust news categorization. Additionally, we
introduce a novel Cluster Stability Assessment Index (CSAI) to assess the
validity and robustness of clustering results. CSAI utilizes latent feature
vectors to provide a new way of measuring clustering quality. Our experiments
indicate that combining LLM embeddings with clustering algorithms yields the
best results, demonstrating greater robustness in terms of CSAI scores.
Moreover, post-event detection tasks generate meaningful insights, facilitating
effective interpretation of event clustering results. Overall, our experimental
results indicate that the proposed framework offers valuable insights and could
enhance the accuracy and depth of news reporting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BASS: Batched Attention-optimized Speculative Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, Anoop Deoras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding has emerged as a powerful method to improve latency and
throughput in hosting large language models. However, most existing
implementations focus on generating a single sequence. Real-world generative AI
applications often require multiple responses and how to perform speculative
decoding in a batched setting while preserving its latency benefits poses
non-trivial challenges. This paper describes a system of batched speculative
decoding that sets a new state of the art in multi-sequence generation latency
and that demonstrates superior GPU utilization as well as quality of
generations within a time budget. For example, for a 7.8B-size model on a
single A100 GPU and with a batch size of 8, each sequence is generated at an
average speed of 5.8ms per token, the overall throughput being 1.1K tokens per
second. These results represent state-of-the-art latency and a 2.15X speed-up
over optimized regular decoding. Within a time budget that regular decoding
does not finish, our system is able to generate sequences with HumanEval
Pass@First of 43% and Pass@All of 61%, far exceeding what's feasible with
single-sequence speculative decoding. Our peak GPU utilization during decoding
reaches as high as 15.8%, more than 3X the highest of that of regular decoding
and around 10X of single-sequence speculative decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BigCodeBench: Benchmarking Code Generation with Diverse Function Calls
  and Complex Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated software engineering has been greatly empowered by the recent
advances in Large Language Models (LLMs) for programming. While current
benchmarks have shown that LLMs can perform various software engineering tasks
like human developers, the majority of their evaluations are limited to short
and self-contained algorithmic tasks. Solving challenging and practical
programming tasks requires the capability of utilizing diverse function calls
as tools to efficiently implement functionalities like data analysis and web
development. In addition, using multiple tools to solve a task needs
compositional reasoning by accurately understanding complex instructions.
Fulfilling both of these characteristics can pose a great challenge for LLMs.
To assess how well LLMs can solve challenging and practical programming tasks,
we introduce Bench, a benchmark that challenges LLMs to invoke multiple
function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained
programming tasks. To evaluate LLMs rigorously, each programming task
encompasses 5.6 test cases with an average branch coverage of 99%. In addition,
we propose a natural-language-oriented variant of Bench, Benchi, that
automatically transforms the original docstrings into short instructions only
with essential information. Our extensive evaluation of 60 LLMs shows that LLMs
are not yet capable of following complex instructions to use function calls
precisely, with scores up to 60%, significantly lower than the human
performance of 97%. The results underscore the need for further advancements in
this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 14 figures, 7 tables, built with love by the BigCode
  community :)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive reasoning
capabilities, particularly in textual mathematical problem-solving. However,
existing open-source image instruction fine-tuning datasets, containing limited
question-answer pairs per image, do not fully exploit visual information to
enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs
(MLLMs). To bridge this gap, we address the lack of high-quality, diverse
multimodal mathematical datasets by collecting 40K high-quality images with
question-answer pairs from 24 existing datasets and synthesizing 320K new
pairs, creating the MathV360K dataset, which enhances both the breadth and
depth of multimodal mathematical questions. We introduce Math-LLaVA, a
LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach
significantly improves the multimodal mathematical reasoning capabilities of
LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V
on MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced
generalizability, showing substantial improvements on the MMMU benchmark. Our
research highlights the importance of dataset diversity and synthesis in
advancing MLLMs' mathematical reasoning abilities. The code and data are
available at: \url{https://github.com/HZQ950419/Math-LLaVA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Knowledge Model: Perspectives and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humankind's understanding of the world is fundamentally linked to our
perception and cognition, with \emph{human languages} serving as one of the
major carriers of \emph{world knowledge}. In this vein, \emph{Large Language
Models} (LLMs) like ChatGPT epitomize the pre-training of extensive,
sequence-based world knowledge into neural networks, facilitating the
processing and manipulation of this knowledge in a parametric space. This
article explores large models through the lens of "knowledge". We initially
investigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in
enhancing LLMs, covering aspects like knowledge-augmented language model,
structure-inducing pre-training, knowledgeable prompts, structured CoT,
knowledge editing, semantic tools for LLM and knowledgeable AI agents.
Subsequently, we examine how LLMs can boost traditional symbolic knowledge
bases, encompassing aspects like using LLM as KG builder and controller,
structured knowledge pretraining, and LLM-enhanced symbolic reasoning.
Considering the intricate nature of human knowledge, we advocate for the
creation of \emph{Large Knowledge Models} (LKM), specifically engineered to
manage diversified spectrum of knowledge structures. This promising undertaking
would entail several key challenges, such as disentangling knowledge base from
language models, cognitive alignment with human knowledge, integration of
perception and cognition, and building large commonsense models for interacting
with physical world, among others. We finally propose a five-"A" principle to
distinguish the concept of LKM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data Intelligence, Published: Jun 18, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiAgent Collaboration Attack: Investigating Adversarial Attacks in
  <span class="highlight-title">Large Language Model</span> Collaborations via Debate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfonso Amayuelas, Xianjun Yang, Antonis Antoniades, Wenyue Hua, Liangming Pan, William Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown exceptional results on current
benchmarks when working individually. The advancement in their capabilities,
along with a reduction in parameter size and inference times, has facilitated
the use of these models as agents, enabling interactions among multiple models
to execute complex tasks. Such collaborations offer several advantages,
including the use of specialized models (e.g. coding), improved confidence
through multiple computations, and enhanced divergent thinking, leading to more
diverse outputs. Thus, the collaborative use of language models is expected to
grow significantly in the coming years. In this work, we evaluate the behavior
of a network of models collaborating through debate under the influence of an
adversary. We introduce pertinent metrics to assess the adversary's
effectiveness, focusing on system accuracy and model agreement. Our findings
highlight the importance of a model's persuasive ability in influencing others.
Additionally, we explore inference-time methods to generate more compelling
arguments and evaluate the potential of prompt-based mitigation as a defensive
strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToM-LM: Delegating Theory of Mind Reasoning to External Symbolic
  Executors in <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15515v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15515v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Tang, Vaishak Belle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theory of Mind (ToM) refers to the ability of individuals to attribute mental
states to others. While Large Language Models (LLMs) have shown some promise
with ToM ability, they still struggle with complex ToM reasoning. Our approach
leverages an external symbolic executor, specifically the SMCDEL model checker,
and fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,
an LLM is first fine-tuned through pairs of natural language and symbolic
formulation representation of ToM problems and is then instructed to generate
the symbolic formulation with a one-shot in-context example. The generated
symbolic formulation is then executed by the SMCDEL model checker to perform
transparent and verifiable ToM reasoning and give the final result. We
demonstrate that our approach, ToM-LM, shows a significant improvement over all
the constructed baselines. Our study proposes a novel view about externalizing
a particular component of ToM reasoning, mainly reasoning about beliefs, and
suggests generalizing it to other aspects of ToM reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeSy 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cultural Bias and Cultural Alignment of <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Tao, Olga Viberg, Ryan S. Baker, Rene F. Kizilcec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Culture fundamentally shapes people's reasoning, behavior, and communication.
As people increasingly use generative artificial intelligence (AI) to expedite
and automate personal and professional tasks, cultural values embedded in AI
models may bias people's authentic expression and contribute to the dominance
of certain cultures. We conduct a disaggregated evaluation of cultural bias for
five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3)
by comparing the models' responses to nationally representative survey data.
All models exhibit cultural values resembling English-speaking and Protestant
European countries. We test cultural prompting as a control strategy to
increase cultural alignment for each country/territory. For recent models
(GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models'
output for 71-81% of countries and territories. We suggest using cultural
prompting and ongoing evaluation to reduce cultural bias in the output of
generative AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VarBench: Robust Language Model Benchmarking Through Dynamic Variable
  Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models achieve impressive scores on traditional benchmarks,
an increasing number of researchers are becoming concerned about benchmark data
leakage during pre-training, commonly known as the data contamination problem.
To ensure fair evaluation, recent benchmarks release only the training and
validation sets, keeping the test set labels closed-source. They require anyone
wishing to evaluate his language model to submit the model's predictions for
centralized processing and then publish the model's result on their
leaderboard. However, this submission process is inefficient and prevents
effective error analysis. To address this issue, we propose to variabilize
benchmarks and evaluate language models dynamically. Specifically, we extract
variables from each test case and define a value range for each variable. For
each evaluation, we sample new values from these value ranges to create unique
test cases, thus ensuring a fresh evaluation each time. We applied this
variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and
TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our
experimental results demonstrate that this approach provides a more accurate
assessment of the true capabilities of language models, effectively mitigating
the contamination problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Preference Inference using Language Models and Probabilistic
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wasu Top Piriyakulkij, Volodymyr Kuleshov, Kevin Ellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Actively inferring user preferences, for example by asking good questions, is
important for any human-facing decision-making system. Active inference allows
such systems to adapt and personalize themselves to nuanced individual
preferences. To enable this ability for instruction-tuned large language models
(LLMs), one may prompt them to ask users questions to infer their preferences,
transforming the language models into more robust, interactive systems.
However, out of the box, these models are not efficient at extracting
preferences: the questions they generate are not informative, requiring a high
number of user interactions and impeding the usability of the downstream
system. In this work, we introduce an inference-time algorithm that helps LLMs
quickly infer preferences by using more informative questions. Our algorithm
uses a probabilistic model whose conditional distributions are defined by
prompting an LLM, and returns questions that optimize expected entropy and
expected model change. Results in a simplified interactive web shopping setting
with real product items show that an LLM equipped with our entropy reduction
algorithm outperforms baselines with the same underlying LLM on task
performance while using fewer user interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Huang, Zengzhi Wang, Shijie Xia, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we pose the following question: Who is the most intelligent
AI model to date, as measured by the OlympicArena (an Olympic-level,
multi-discipline, multi-modal benchmark for superintelligent AI)? We
specifically focus on the most recently released models: Claude-3.5-Sonnet,
Gemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic
medal Table approach to rank AI models based on their comprehensive performance
across various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet
shows highly competitive overall performance over GPT-4o, even surpassing
GPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)
Gemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and
Claude-3.5-Sonnet, but with a clear performance gap between them. (3) The
performance of AI models from the open-source community significantly lags
behind these proprietary models. (4) The performance of these models on this
benchmark has been less than satisfactory, indicating that we still have a long
way to go before achieving superintelligence. We remain committed to
continuously tracking and evaluating the performance of the latest powerful
models on this benchmark (available at
https://github.com/GAIR-NLP/OlympicArena).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation
  and Fine-grained Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Sun, Yushi Bai, Ji Qi, Lei Hou, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To advance the evaluation of multimodal math reasoning in large multimodal
models (LMMs), this paper introduces a novel benchmark, MM-MATH. MM-MATH
consists of 5,929 open-ended middle school math problems with visual contexts,
with fine-grained classification across difficulty, grade level, and knowledge
points. Unlike existing benchmarks relying on binary answer comparison, MM-MATH
incorporates both outcome and process evaluations. Process evaluation employs
LMM-as-a-judge to automatically analyze solution steps, identifying and
categorizing errors into specific error types. Extensive evaluation of ten
models on MM-MATH reveals significant challenges for existing LMMs,
highlighting their limited utilization of visual information and struggles with
higher-difficulty problems. The best-performing model achieves only 31%
accuracy on MM-MATH, compared to 82% for humans. This highlights the
challenging nature of our benchmark for existing models and the significant gap
between the multimodal reasoning capabilities of current models and humans. Our
process evaluation reveals that diagram misinterpretation is the most common
error, accounting for more than half of the total error cases, underscoring the
need for improved image comprehension in multimodal reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DoRA: Enhancing Parameter-Efficient Fine-Tuning with Dynamic Rank
  Distribution <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17357v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17357v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulong Mao, Kaiyu Huang, Changhao Guan, Ganglin Bao, Fengran Mo, Jinan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large-scale pre-trained models is inherently a resource-intensive
task. While it can enhance the capabilities of the model, it also incurs
substantial computational costs, posing challenges to the practical application
of downstream tasks. Existing parameter-efficient fine-tuning (PEFT) methods
such as Low-Rank Adaptation (LoRA) rely on a bypass framework that ignores the
differential parameter budget requirements across weight matrices, which may
lead to suboptimal fine-tuning outcomes. To address this issue, we introduce
the Dynamic Low-Rank Adaptation (DoRA) method. DoRA decomposes high-rank LoRA
layers into structured single-rank components, allowing for dynamic pruning of
parameter budget based on their importance to specific tasks during training,
which makes the most of the limited parameter budget. Experimental results
demonstrate that DoRA can achieve competitive performance compared with LoRA
and full model fine-tuning, and outperform various strong baselines with the
same storage parameter budget. Our code is available at
https://github.com/MIkumikumi0116/DoRA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the main conference of ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SetBERT: Enhancing Retrieval Performance for Boolean Logic and Set
  Operation Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Mai, Susan Gauch, Douglas Adams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SetBERT, a fine-tuned BERT-based model designed to enhance query
embeddings for set operations and Boolean logic queries, such as Intersection
(AND), Difference (NOT), and Union (OR). SetBERT significantly improves
retrieval performance for logic-structured queries, an area where both
traditional and neural retrieval methods typically underperform. We propose an
innovative use of inversed-contrastive loss, focusing on identifying the
negative sentence, and fine-tuning BERT with a dataset generated via prompt
GPT. Furthermore, we demonstrate that, unlike other BERT-based models,
fine-tuning with triplet loss actually degrades performance for this specific
task. Our experiments reveal that SetBERT-base not only significantly
outperforms BERT-base (up to a 63% improvement in Recall) but also achieves
performance comparable to the much larger BERT-large model, despite being only
one-third the size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergent World Representations: Exploring a Sequence Model Trained on a
  Synthetic Task <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13382v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13382v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models show a surprising range of capabilities, but the source of
their apparent competence is unclear. Do these networks just memorize a
collection of surface statistics, or do they rely on internal representations
of the process that generates the sequences they see? We investigate this
question by applying a variant of the GPT model to the task of predicting legal
moves in a simple board game, Othello. Although the network has no a priori
knowledge of the game or its rules, we uncover evidence of an emergent
nonlinear internal representation of the board state. Interventional
experiments indicate this representation can be used to control the output of
the network and create "latent saliency maps" that can help explain predictions
in human terms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 oral (notable-top-5%):
  https://openreview.net/forum?id=DeG07_TcZvT ; code:
  https://github.com/likenneth/othello_world</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHIRON: Rich Character Representations in Long-Form Narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Gurung, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characters are integral to long-form narratives, but are poorly understood by
existing story analysis and generation systems. While prior work has simplified
characters via graph-based methods and brief character descriptions, we aim to
better tackle the problem of representing complex characters by taking
inspiration from advice given to professional writers. We propose CHIRON, a new
`character sheet' based representation that organizes and filters textual
information about characters. We construct CHIRON sheets in two steps: a
Generation Module that prompts an LLM for character information via
question-answering and a Validation Module that uses automated reasoning and a
domain-specific entailment model to eliminate false facts about a character. We
validate CHIRON via the downstream task of masked-character prediction, where
our experiments show CHIRON is better and more flexible than comparable
summary-based baselines. We also show that metrics derived from CHIRON can be
used to automatically infer character-centricity in stories, and that these
metrics align with human judgments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference-Time Intervention: Eliciting Truthful Answers from a Language
  Model <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03341v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03341v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Inference-Time Intervention (ITI), a technique designed to
enhance the "truthfulness" of large language models (LLMs). ITI operates by
shifting model activations during inference, following a set of directions
across a limited number of attention heads. This intervention significantly
improves the performance of LLaMA models on the TruthfulQA benchmark. On an
instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from
32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and
demonstrate how to balance it by tuning the intervention strength. ITI is
minimally invasive and computationally inexpensive. Moreover, the technique is
data efficient: while approaches like RLHF require extensive annotations, ITI
locates truthful directions using only few hundred examples. Our findings
suggest that LLMs may have an internal representation of the likelihood of
something being true, even as they produce falsehoods on the surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 spotlight; code:
  https://github.com/likenneth/honest_llama</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Understanding Jailbreak Attacks in <span class="highlight-title">LLM</span>s: A Representation Space
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping Lin, Pengfei He, Han Xu, Yue Xing, Makoto Yamada, Hui Liu, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are susceptible to a type of attack known as
jailbreaking, which misleads LLMs to output harmful contents. Although there
are diverse jailbreak attack strategies, there is no unified understanding on
why some methods succeed and others fail. This paper explores the behavior of
harmful and harmless prompts in the LLM's representation space to investigate
the intrinsic properties of successful jailbreak attacks. We hypothesize that
successful attacks share some similar properties: They are effective in moving
the representation of the harmful prompt towards the direction to the harmless
prompts. We leverage hidden representations into the objective of existing
jailbreak attacks to move the attacks along the acceptance direction, and
conduct experiments to validate the above hypothesis using the proposed
objective. We hope this study provides new insights into understanding how LLMs
understand harmfulness information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongIns: A Challenging Long-context Instruction-based Exam for <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Wenhu Chen, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The long-context capabilities of large language models (LLMs) have been a hot
topic in recent years. To evaluate the performance of LLMs in different
scenarios, various assessment benchmarks have emerged. However, as most of
these benchmarks focus on identifying key information to answer questions,
which mainly requires the retrieval ability of LLMs, these benchmarks can
partially represent the reasoning performance of LLMs from large amounts of
information. Meanwhile, although LLMs often claim to have context windows of
32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual
supported length of these LLMs. To address these issues, we propose the LongIns
benchmark dataset, a challenging long-context instruction-based exam for LLMs,
which is built based on the existing instruction datasets. Specifically, in our
LongIns, we introduce three evaluation settings: Global Instruction & Single
Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction &
Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations
on existing LLMs and have the following important findings: (1). The
top-performing GPT-4 with 128k context length performs poorly on the evaluation
context window of 16k in our LongIns. (2). For the multi-hop reasoning ability
of many existing LLMs, significant efforts are still needed under short context
windows (less than 4k).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are AI-Generated Text Detectors Robust to Adversarial Perturbations? <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01179v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01179v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, Mingze Wang, Zhouwang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of large language models (LLMs) has sparked concerns about
the potential misuse of AI-generated text, as these models can produce content
that closely resembles human-generated text. Current detectors for AI-generated
text (AIGT) lack robustness against adversarial perturbations, with even minor
changes in characters or words causing a reversal in distinguishing between
human-created and AI-generated text. This paper investigates the robustness of
existing AIGT detection methods and introduces a novel detector, the Siamese
Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction
network to add and remove noise from text, extracting a semantic representation
that is robust to local perturbations. We also propose a siamese calibration
technique to train the model to make equally confidence predictions under
different noise, which improves the model's robustness against adversarial
perturbations. Experiments on four publicly available datasets show that the
SCRN outperforms all baseline methods, achieving 6.5\%-18.25\% absolute
accuracy improvement over the best baseline method under adversarial attacks.
Moreover, it exhibits superior generalizability in cross-domain, cross-genre,
and mixed-source scenarios. The code is available at
\url{https://github.com/CarlanLark/Robust-AIGC-Detector}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jina CLIP: Your CLIP Model Is Also Your Text Retriever <span class="chip">ICML2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Koukounas, Georgios Mastrapas, Michael Günther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Martínez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pretraining (CLIP) is widely used to train models
to align images and texts in a common embedding space by mapping them to
fixed-sized vectors. These models are key to multimodal information retrieval
and related tasks. However, CLIP models generally underperform in text-only
tasks compared to specialized text models. This creates inefficiencies for
information retrieval systems that keep separate embeddings and models for
text-only and multimodal tasks. We propose a novel, multi-task contrastive
training method to address this issue, which we use to train the jina-clip-v1
model to achieve the state-of-the-art performance on both text-image and
text-text retrieval tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, MFM-EAI@ICML2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating
  <span class="highlight-title">LLM</span>s' Mathematical and Coding Competency through Ontology-guided
  Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09395v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09395v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada Mihalcea, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have showcased striking
results on existing logical reasoning benchmarks, with some models even
surpassing human performance. However, the true depth of their competencies and
robustness in reasoning tasks remains an open question. To this end, in this
paper, we focus on two popular reasoning tasks: arithmetic reasoning and code
generation. Particularly, we introduce: (i) a general ontology of perturbations
for maths and coding questions, (ii) a semi-automatic method to apply these
perturbations, and (iii) two datasets, MORE and CORE, respectively, of
perturbed maths and coding problems to probe the limits of LLM capabilities in
numeric reasoning and coding tasks. Through comprehensive evaluations of both
closed-source and open-source LLMs, we show a significant performance drop
across all the models against the perturbed questions, suggesting that the
current LLMs lack robust problem solving skills and structured reasoning
abilities in many areas, as defined by our ontology. We open source the
datasets and source codes at: https://github.com/declare-lab/llm_robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 360$^\circ$REA: Towards A Reusable Experience Accumulation with
  360° Assessment for Multi-Agent System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Gao, Hao Li, Chengrui Huang, Quan Tu, Zhiliang Tian, Minlie Huang, Shuo Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model agents have demonstrated remarkable advancements across
various complex tasks. Recent works focus on optimizing the agent team or
employing self-reflection to iteratively solve complex tasks. Since these
agents are all based on the same LLM, only conducting self-evaluation or
removing underperforming agents does not substantively enhance the capability
of the agents. We argue that a comprehensive evaluation and accumulating
experience from evaluation feedback is an effective approach to improving
system performance. In this paper, we propose Reusable Experience Accumulation
with 360$^\circ$ Assessment (360$^\circ$REA), a hierarchical multi-agent
framework inspired by corporate organizational practices. The framework employs
a novel 360$^\circ$ performance assessment method for multi-perspective
performance evaluation with fine-grained assessment. To enhance the capability
of agents in addressing complex tasks, we introduce dual-level experience pool
for agents to accumulate experience through fine-grained assessment. Extensive
experiments on complex task datasets demonstrate the effectiveness of
360$^\circ$REA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigate the Gap: Investigating Approaches for Improving Cross-Modal
  Alignment in CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedigheh Eslami, Gerard de Melo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable
improvements in zero-shot classification and cross-modal vision-language tasks.
Yet, from a geometrical point of view, the CLIP embedding space has been found
to have a pronounced modality gap. This gap renders the embedding space overly
sparse and disconnected, with different modalities being densely distributed in
distinct subregions of the hypersphere. In this work, we aim at answering two
main questions: 1. Does sharing the parameter space between the multi-modal
encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart
the uni-modal embeddings via intra-modality separation? We design AlignCLIP, in
order to answer these questions and show that answers to both questions are
positive. Through extensive experiments, we show that AlignCLIP achieves
noticeable enhancements in the cross-modal alignment of the embeddings, and
thereby, reduces the modality gap, while maintaining the performance across
several downstream evaluations, such as zero-shot image classification,
zero-shot multi-modal retrieval and zero-shot semantic text similarity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compact Speech Translation Models via Discrete Speech Units Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsz Kin Lam, Alexandra Birch, Barry Haddow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a pretraining method to use Self-Supervised Speech (SSS) model to
creating more compact Speech-to-text Translation. In contrast to using the SSS
model for initialization, our method is more suitable to memory constrained
scenario such as on-device deployment. Our method is based on Discrete Speech
Units (DSU) extracted from the SSS model. In the first step, our method
pretrains two smaller encoder-decoder models on 1) Filterbank-to-DSU
(Fbk-to-DSU) and 2) DSU-to-Translation (DSU-to-Trl) data respectively. The DSU
thus become the distillation inputs of the smaller models. Subsequently, the
encoder from the Fbk-to-DSU model and the decoder from the DSU-to-Trl model are
taken to initialise the compact model. Finally, the compact model is finetuned
on the paired Fbk-Trl data. In addition to being compact, our method requires
no transcripts, making it applicable to low-resource settings. It also avoids
speech discretization in inference and is more robust to the DSU tokenization.
Evaluation on CoVoST-2 (X-En) shows that our method has consistent improvement
over the baseline in three metrics while being compact i.e., only half the SSS
model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, accepted at IWSLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Tales of Persona in <span class="highlight-title">LLM</span>s: A <span class="highlight-title">Survey</span> of Role-Playing and
  Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-Lin Chen, Chao-Wei Huang, Yu Meng, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept of persona, originally adopted in dialogue literature, has
re-surged as a promising framework for tailoring large language models (LLMs)
to specific context (e.g., personalized search, LLM-as-a-judge). However, the
growing research on leveraging persona in LLMs is relatively disorganized and
lacks a systematic taxonomy. To close the gap, we present a comprehensive
survey to categorize the current state of the field. We identify two lines of
research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and
(2) LLM Personalization, where LLMs take care of user personas. Additionally,
we introduce existing methods for LLM personality evaluation. To the best of
our knowledge, we present the first survey for role-playing and personalization
in LLMs under the unified view of persona. We continuously maintain a paper
collection to foster future endeavors:
https://github.com/MiuLab/PersonaLLM-Survey
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8-page version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Case-Based or Rule-Based: How Do Transformers Do the Math? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Xiaojuan Tang, Haotong Yang, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive performance in a variety of complex tasks, modern
large language models (LLMs) still have trouble dealing with some math problems
that are simple and intuitive for humans, such as addition. While we can easily
learn basic rules of addition and apply them to new problems of any length,
LLMs struggle to do the same. Instead, they may rely on similar cases seen in
the training corpus for help. We define these two different reasoning
mechanisms as "rule-based reasoning" and "case-based reasoning". Since
rule-based reasoning is essential for acquiring systematic generalization
ability, we aim to explore exactly whether transformers use rule-based or
case-based reasoning for math problems. Through carefully designed intervention
experiments on five math tasks, we confirm that transformers are performing
case-based reasoning, no matter whether scratchpad is used, which aligns with
the previous observations that transformers use subgraph matching/shortcut
learning to reason. To mitigate such problems, we propose a Rule-Following
Fine-Tuning (RFFT) technique to teach transformers to perform rule-based
reasoning. Specifically, we provide explicit rules in the input and then
instruct transformers to recite and follow the rules step by step. Through
RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to
generalize to up to 12-digit addition with over 95% accuracy, which is over 40%
higher than scratchpad. The significant improvement demonstrates that teaching
LLMs to use rules explicitly helps them learn rule-based reasoning and
generalize better in length.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Super Tiny Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Hillier, Leon Guertler, Cheston Tan, Palaash Agrawal, Chen Ruirui, Bobby Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) has led to significant
improvements in natural language processing but also poses challenges due to
their high computational and energy demands. This paper introduces a series of
research efforts focused on Super Tiny Language Models (STLMs), which aim to
deliver high performance with significantly reduced parameter counts. We
explore innovative techniques such as byte-level tokenization with a pooling
mechanism, weight tying, and efficient training strategies. These methods aim
to significantly reduce reduce the parameter count compared to traditional
models -- in future works, we aim to build on these in a way that maintains and
improves upon the performance of base transformer models. This series of papers
will explore into various subproblems, including tokenizer-free models,
self-play based training, and alternative training objectives. We will target
models with 10M, 50M, and 100M parameters. Our ultimate goal is to make
high-performance language models more accessible and practical for a wide range
of applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Data Learning for Open Information Extraction with Pre-trained
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Fan, Shizhu He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Information Extraction (OpenIE) is a fundamental yet challenging task in
Natural Language Processing, which involves extracting all triples (subject,
predicate, object) from a given sentence. While labeling-based methods have
their merits, generation-based techniques offer unique advantages, such as the
ability to generate tokens not present in the original sentence. However, these
generation-based methods often require a significant amount of training data to
learn the task form of OpenIE and substantial training time to overcome slow
model convergence due to the order penalty. In this paper, we introduce a novel
framework, OK-IE, that ingeniously transforms the task form of OpenIE into the
pre-training task form of the T5 model, thereby reducing the need for extensive
training data. Furthermore, we introduce an innovative concept of Anchor to
control the sequence of model outputs, effectively eliminating the impact of
order penalty on model convergence and significantly reducing training time.
Experimental results indicate that, compared to previous SOTA methods, OK-IE
requires only 1/100 of the training data (900 instances) and 1/120 of the
training time (3 minutes) to achieve comparable results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer-Wise Quantization: A P<span class="highlight-title">rag</span>matic and Effective Method for Quantizing
  <span class="highlight-title">LLM</span>s Beyond Integer Bit-Levels <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul-Ioan Clotan, Sathwik Tejaswi Madhusudhan, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple variable quantization approach that quantizes different
layers of a large language model (LLM) at different bit levels. Specifically,
we quantize the most important layers to higher bit precision and less
important layers to lower bits to achieve floating point quantization levels.
We propose two effective strategies to measure the importance of layers within
LLMs: the first measures the importance of a layer based on how different its
output embeddings are from the input embeddings (the higher the better); the
second estimates the importance of a layer using the number of layer weights
that are much larger than average (the smaller the better). We show that
quantizing different layers at varying bits according to our importance scores
results in minimal performance drop with a far more compressed model size.
Finally, we present several practical key takeaways from our variable
layer-wise quantization experiments: (a) LLM performance under variable
quantization remains close to the original model until 25-50% of layers are
moved in lower quantization using our proposed ordering but only until 5-10% if
moved using no specific ordering; (b) Quantizing LLMs to lower bits performs
substantially better than pruning unless extreme quantization (2-bit) is used;
and (c) Layer-wise quantization to lower bits works better in the case of
larger LLMs with more layers compared to smaller LLMs with fewer layers. The
code used to run the experiments is available at:
https://github.com/RazvanDu/LayerwiseQuant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to EMNLP, 15 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large
  Language Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05935v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05935v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, Yu Qiao, Peng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)
series developed upon SPHINX. To improve the architecture and training
efficiency, we modify the SPHINX framework by removing redundant visual
encoders, bypassing fully-padded sub-images with skip tokens, and simplifying
multi-stage training into a one-stage all-in-one paradigm. To fully unleash the
potential of MLLMs, we assemble a comprehensive multi-domain and multimodal
dataset covering publicly available resources in language, vision, and
vision-language tasks. We further enrich this collection with our curated OCR
intensive and Set-of-Mark datasets, extending the diversity and generality. By
training over different base LLMs including TinyLlama1.1B, InternLM2-7B,
LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in
parameter size and multilingual capabilities. Comprehensive benchmarking
reveals a strong correlation between the multi-modal performance with the data
and parameter scales. Code and models are released at
https://github.com/Alpha-VLLM/LLaMA2-Accessory
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2024. Code and models are released at
  https://github.com/Alpha-VLLM/LLaMA2-Accessory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained
  Models using Greedy Coordinate Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Ajit Nair, Arun Sai Suggala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently demonstrated remarkable
performance across diverse language tasks. But their deployment is often
constrained by their substantial computational and storage requirements.
Quantization has emerged as a key technique for addressing this challenge,
enabling the compression of large models with minimal impact on performance.
The recent GPTQ algorithm, a post-training quantization (PTQ) method, has
proven highly effective for compressing LLMs, sparking a wave of research that
leverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the
PTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ
with improved performance. CDQuant uses coordinate descent to minimize the
layer-wise reconstruction loss to achieve high-quality quantized weights. Our
algorithm is easy to implement and scales efficiently to models with hundreds
of billions of parameters. Through extensive evaluation on the PaLM2 model
family, we demonstrate that CDQuant consistently outperforms GPTQ across
diverse model sizes and quantization levels. In particular, for INT2
quantization of PaLM2-Otter, CDQuant achieves a 10% reduction in perplexity
compared to GPTQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Critique<span class="highlight-title">LLM</span>: Towards an Informative Critique Generation Model for
  Evaluation of <span class="highlight-title">Large Language Model</span> Generation <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the natural language processing (NLP) community started to make large
language models (LLMs) act as a critic to evaluate the quality of generated
texts, most of the existing works train a critique generation model on the
evaluation data labeled by GPT-4's direct prompting. We observe that these
models lack the ability to generate informative critiques in both pointwise
grading and pairwise comparison especially without references. As a result,
their generated critiques cannot provide fine-grained distinguishability on
generated texts, causing unsatisfactory evaluation performance. In this paper,
we propose a simple yet effective method called Eval-Instruct, which can first
acquire pointwise grading critiques with pseudo references and then revise
these critiques via multi-path prompting to obtain informative evaluation data
in different tasks and settings, including pointwise grading and pairwise
comparison with / without references. After fine-tuning on these data, the
resulting model CritiqueLLM is empirically shown to outperform ChatGPT and all
the open-source baselines and even achieve comparable evaluation performance to
GPT-4 in system-level correlations of pointwise grading. We also demonstrate
that our generated critiques can act as scalable feedback to further improve
the generation quality of strong LLMs like ChatGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FaithLM: Towards Faithful Explanations for <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04678v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04678v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Ruixiang Tang, Shaochen Zhong, Fan Yang, Mengnan Du, Xuanting Cai, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have become proficient in addressing complex
tasks by leveraging their extensive internal knowledge and reasoning
capabilities. However, the black-box nature of these models complicates the
task of explaining their decision-making processes. While recent advancements
demonstrate the potential of leveraging LLMs to self-explain their predictions
through natural language (NL) explanations, their explanations may not
accurately reflect the LLMs' decision-making process due to a lack of fidelity
optimization on the derived explanations. Measuring the fidelity of NL
explanations is a challenging issue, as it is difficult to manipulate the input
context to mask the semantics of these explanations. To this end, we introduce
FaithLM to explain the decision of LLMs with NL explanations. Specifically,
FaithLM designs a method for evaluating the fidelity of NL explanations by
incorporating the contrary explanations to the query process. Moreover, FaithLM
conducts an iterative process to improve the fidelity of derived explanations.
Experiment results on three datasets from multiple domains demonstrate that
FaithLM can significantly improve the fidelity of derived explanations, which
also provides a better alignment with the ground-truth explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple-Choice Questions are Efficient and Robust <span class="highlight-title">LLM</span> Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11966v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11966v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyin Zhang, Zhaokun Jiang, Lizhen Xu, Hongkun Hao, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GSM-MC, a multiple-choice (MC) dataset constructed by collecting
answers and incorrect predictions on GSM8K from 60 open-source models. Through
extensive experiments, we show that LLMs' performance on the MC version of this
popular benchmark is strongly correlated with their performance on the original
version and is quite robust to distractor choices and option orders, while the
evaluation time is reduced by a factor of up to 30. Following similar
procedures, we introduce MATH-MC, constructed from MATH, and PythonIO, a new
program reasoning MC dataset constructed from HumanEval and MBPP. Experimental
results indicate that LLMs' performance on these MC benchmarks leaves much room
for improvement. Our data and code are available at
https://github.com/Geralt-Targaryen/MC-Evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>data at https://github.com/Geralt-Targaryen/MC-Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying the Perspectives of NLP and Software Engineering: A <span class="highlight-title">Survey</span> on
  Language Models for Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07989v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07989v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, Rui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we systematically review the recent advancements in software
engineering with language models, covering 70+ models, 40+ evaluation tasks,
180+ datasets, and 900 related works. Unlike previous works, we integrate
software engineering (SE) with natural language processing (NLP) by discussing
the perspectives of both sides: SE applies language models for development
automation, while NLP adopts SE tasks for language model evaluation. We break
down code processing models into general language models represented by the GPT
family and specialized models that are specifically pretrained on code, often
with tailored objectives. We discuss the relations and differences between
these models, and highlight the historical transition of code modeling from
statistical models and RNNs to pretrained Transformers and LLMs, which is
exactly the same course that had been taken by NLP. We also go beyond
programming and review LLMs' application in other software engineering
activities including requirement engineering, testing, deployment, and
operations in an endeavor to provide a global view of NLP in SE, and identify
key challenges and potential future directions in this domain. We keep the
survey open and updated on GitHub at
https://github.com/codefuse-ai/Awesome-Code-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Repo: https://github.com/codefuse-ai/Awesome-Code-LLM. 9 figures, 18
  tables, and 902 references. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension
  with Enhanced Visual Knowledge Alignment <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxin Li, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating and Rethinking the current landscape of Large Multimodal Models
(LMMs), we observe that widely-used visual-language projection approaches
(e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet
ignore the visual knowledge-dimension alignment, i.e., connecting visuals to
their relevant knowledge. Visual knowledge plays a significant role in
analyzing, inferring, and interpreting information from visuals, helping
improve the accuracy of answers to knowledge-based visual questions. In this
paper, we mainly explore improving LMMs with visual-language knowledge
alignment, especially aimed at challenging knowledge-based visual question
answering (VQA). To this end, we present a Cognitive Visual-Language Mapper
(CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a
Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning
stage. Specifically, we design the VKA based on the interaction between a small
language model and a visual encoder, training it on collected image-knowledge
pairs to achieve visual knowledge acquisition and projection. FKA is employed
to distill the fine-grained visual knowledge of an image and inject it into
Large Language Models (LLMs). We conduct extensive experiments on
knowledge-based VQA benchmarks and experimental results show that CVLM
significantly improves the performance of LMMs on knowledge-based VQA (average
gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA,
respectively. The codes are available at
https://github.com/HITsz-TMG/Cognitive-Visual-Language-Mapper
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,4 figures; Accepted by ACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10663v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10663v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, the in-context learning method based on large language models
(LLMs) has become the mainstream of text-to-SQL research. Previous works have
discussed how to select demonstrations related to the user question from a
human-labeled demonstration pool. However, human labeling suffers from the
limitations of insufficient diversity and high labeling overhead. Therefore, in
this paper, we discuss how to measure and improve the diversity of the
demonstrations for text-to-SQL. We present a metric to measure the diversity of
the demonstrations and analyze the insufficient of the existing labeled data by
experiments. Based on the above discovery, we propose fusing iteratively for
demonstrations (Fused) to build a high-diversity demonstration pool through
human-free multiple-iteration synthesis, improving diversity and lowering label
cost. Our method achieves an average improvement of 3.2% and 5.0% with and
without human labeling on several mainstream datasets, which proves the
effectiveness of Fused.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Xmodel-LM Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02856v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02856v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Wang, Yang Liu, Yu Yan, Qun Wang, Xucheng Huang, Ling Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Xmodel-LM, a compact and efficient 1.1B language model
pre-trained on around 2 trillion tokens. Trained on our self-built dataset
(Xdata), which balances Chinese and English corpora based on downstream task
optimization, Xmodel-LM exhibits remarkable performance despite its smaller
size. It notably surpasses existing open-source language models of similar
scale. Our model checkpoints and code are publicly accessible on GitHub at
https://github.com/XiaoduoAILab/XmodelLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songtao Jiang, Tuo Zheng, Yan Zhang, Yeying Jin, Li Yuan, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in general-purpose or domain-specific multimodal large
language models (LLMs) have witnessed remarkable progress for medical
decision-making. However, they are designated for specific classification or
generative tasks, and require model training or finetuning on large-scale
datasets with sizeable parameters and tremendous computing, hindering their
clinical utility across diverse resource-constrained scenarios in practice. In
this paper, we propose a novel and lightweight framework Med-MoE
(Mixture-of-Experts) that tackles both discriminative and generative multimodal
medical tasks. The learning of Med-MoE consists of three steps: multimodal
medical alignment, instruction tuning and routing, and domain-specific MoE
tuning. After aligning multimodal medical images with LLM tokens, we then
enable the model for different multimodal medical tasks with instruction
tuning, together with a trainable router tailored for expert selection across
input modalities. Finally, the model is tuned by integrating the router with
multiple domain-specific experts, which are selectively activated and further
empowered by meta expert. Comprehensive experiments on both open- and close-end
medical question answering (Med-VQA) and image classification tasks across
datasets such as VQA-RAD, SLAKE and Path-VQA demonstrate that our model can
achieve performance superior to or on par with state-of-the-art baselines,
while only requiring approximately 30\%-50\% of activated model parameters.
Extensive analysis and ablations corroborate the effectiveness and practical
utility of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Chain-of-Thoughts with a Pairwise-Comparison Approach to
  Searching for the Most Promising Intermediate Thought <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen-Yu Zhang, Siwei Han, Huaxiu Yao, Gang Niu, Masashi Sugiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To improve the ability of the large language model (LLMs) to tackle complex
reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs
to reason step-by-step, enabling problem solving from simple to complex.
State-of-the-art methods for generating such a chain involve interactive
collaboration, where the learner generates candidate intermediate thoughts,
evaluated by the LLM, guiding the generation of subsequent thoughts. However, a
widespread yet understudied problem is that the evaluation from the LLM is
typically noisy and unreliable, potentially misleading the generation process
in selecting promising intermediate thoughts. In this paper, motivated by
Vapnik's principle, we use pairwise-comparison evaluation instead of point-wise
scoring to search for promising intermediate thoughts with the noisy feedback
from the LLM. In each round, we randomly pair intermediate thoughts and
directly prompt the LLM to select the more promising one from each pair,
allowing us to identify the most promising thoughts through an iterative
process. To further alleviate the noise in the comparison, we incorporate
techniques from ensemble learning and dueling bandits, proposing two variants
of the algorithm. Experiments on three real-world tasks demonstrate the
effectiveness of our proposed algorithm and verify the rationale of the
pairwise comparison mechanism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for
  Multi-modal Sarcasm Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chen, Subin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalence of sarcasm in social media, conveyed through text-image
combinations, presents significant challenges for sentiment analysis and
intention mining. Current multi-modal sarcasm detection methods have been
proven to struggle with biases from spurious cues, leading to a superficial
understanding of the complex interactions between text and image. To address
these issues, we propose InterCLIP-MEP, a robust framework for multi-modal
sarcasm detection. InterCLIP-MEP introduces a refined variant of CLIP,
Interactive CLIP (InterCLIP), as the backbone, enhancing sample representations
by embedding cross-modality information in each encoder. Furthermore, a novel
training strategy is designed to adapt InterCLIP for a Memory-Enhanced
Predictor (MEP). MEP uses dynamic dual-channel memory to store valuable
historical knowledge of test samples and then leverages this memory as a
non-parametric classifier to derive the final prediction. By using InterCLIP to
encode text-image interactions more effectively and incorporating MEP,
InterCLIP-MEP offers a more robust recognition of multi-modal sarcasm.
Experiments demonstrate that InterCLIP-MEP achieves state-of-the-art
performance on the MMSD2.0 benchmark. Code and data are available at
https://github.com/CoderChen01/InterCLIP-MEP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MFSN: Multi-perspective Fusion Search Network For Pre-training Knowledge
  in Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09361v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09361v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Sun, Fulin Zhang, Yingying Gao, Zheng Lian, Shilei Zhang, Junlan Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech Emotion Recognition (SER) is an important research topic in
human-computer interaction. Many recent works focus on directly extracting
emotional cues through pre-trained knowledge, frequently overlooking
considerations of appropriateness and comprehensiveness. Therefore, we propose
a novel framework for pre-training knowledge in SER, called Multi-perspective
Fusion Search Network (MFSN). Considering comprehensiveness, we partition
speech knowledge into Textual-related Emotional Content (TEC) and
Speech-related Emotional Content (SEC), capturing cues from both semantic and
acoustic perspectives, and we design a new architecture search space to fully
leverage them. Considering appropriateness, we verify the efficacy of different
modeling approaches in capturing SEC and fills the gap in current research.
Experimental results on multiple datasets demonstrate the superiority of MFSN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster
  Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Zhao, Yuxiang Huang, Xu Han, Wang Xu, Chaojun Xiao, Xinrong Zhang, Yewei Fang, Kaihuo Zhang, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding is a widely used method that accelerates the generation
process of large language models (LLMs) with no compromise in model
performance. It achieves this goal by using an existing smaller model for
drafting and then employing the target LLM to verify the draft in a low-cost
parallel manner. Under such a drafting-verification framework, drafting
efficiency has become a bottleneck in the final speedup of speculative
decoding. Therefore, generating longer drafts at less cost can lead to better
decoding speedup. To achieve this, we introduce Ouroboros, which can generate
draft phrases to parallelize the drafting process and meanwhile lengthen drafts
in a training-free manner. The experimental results on various typical text
generation tasks show that Ouroboros can achieve speedups of up to $2.4\times$
over speculative decoding and $3.9\times$ over vanilla decoding, without
fine-tuning draft and target models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Marathon: A Race Through the Realm of Long Context with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhang, Yunshui Li, Ziqiang Liu, Jiaxi yang, Junhao Liu, Longze Chen, Run Luo, Min Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of large language models (LLMs) and the expansion of
their context windows, existing long-context benchmarks fall short in
effectively evaluating the models' comprehension and reasoning abilities in
extended texts. Moreover, conventional benchmarks relying on F1 metrics often
inaccurately score responses: they may undervalue correct answers that differ
from the reference responses and overvalue incorrect ones that resemble the
reference texts. In response to these limitations, we introduce Marathon, a
novel evaluation benchmark that adopts a multiple-choice question format. It is
specifically designed to overcome the constraints of previous benchmarks and
provide a rapid, precise, and unbiased appraisal of the long-context
comprehension skills of large language models. We conducted comprehensive
evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and
assessed the effectiveness of various optimization strategies tailored for
long-context generation. We anticipate that the Marathon benchmark and its
associated leaderboard will enable a more precise and equitable evaluation of
LLMs' capabilities in understanding and reasoning over extended contexts.
Marathon is available at https://github.com/Hambaobao/Marathon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safely Learning with Private Data: A Federated Learning Framework for
  <span class="highlight-title">Large Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JiaYing Zheng, HaiNan Zhang, LingXiang Wang, WangJie Qiu, HongWei Zheng, ZhiMing Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Private data, being larger and quality-higher than public data, can greatly
improve large language models (LLM). However, due to privacy concerns, this
data is often dispersed in multiple silos, making its secure utilization for
LLM training a challenge. Federated learning (FL) is an ideal solution for
training models with distributed private data, but traditional frameworks like
FedAvg are unsuitable for LLM due to their high computational demands on
clients. An alternative, split learning, offloads most training parameters to
the server while training embedding and output layers locally, making it more
suitable for LLM. Nonetheless, it faces significant challenges in security and
efficiency. Firstly, the gradients of embeddings are prone to attacks, leading
to potential reverse engineering of private data. Furthermore, the server's
limitation of handle only one client's training request at a time hinders
parallel training, severely impacting training efficiency. In this paper, we
propose a Federated Learning framework for LLM, named FL-GLM, which prevents
data leakage caused by both server-side and peer-client attacks while improving
training efficiency. Specifically, we first place the input block and output
block on local client to prevent embedding gradient attacks from server.
Secondly, we employ key-encryption during client-server communication to
prevent reverse engineering attacks from peer-clients. Lastly, we employ
optimization methods like client-batching or server-hierarchical, adopting
different acceleration methods based on the actual computational capabilities
of the server. Experimental results on NLU and generation tasks demonstrate
that FL-GLM achieves comparable metrics to centralized chatGLM model,
validating the effectiveness of our federated learning framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pre-Calc: Learning to Use the Calculator Improves Numeracy in Language
  Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14355v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14355v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishruth Veerendranath, Vishwa Shah, Kshitish Ghate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative and numerical comprehension in language is an important task in
many fields like education and finance, but still remains a challenging task
for language models. While tool and calculator usage has shown to be helpful to
improve mathematical reasoning in large pretrained decoder-only language
models, this remains unexplored for smaller language models with encoders. In
this paper, we propose Pre-Calc, a simple pre-finetuning objective of learning
to use the calculator for both encoder-only and encoder-decoder architectures,
formulated as a discriminative and generative task respectively. We pre-train
BERT and RoBERTa for discriminative calculator use and Flan-T5 for generative
calculator use on the MAWPS, SVAMP, and AsDiv-A datasets, which improves
performance on downstream tasks that require numerical understanding. Our code
and data are available at https://github.com/calc-cmu/pre-calc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AI4Math workshop, ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Key-Element-Informed s<span class="highlight-title">LLM</span> Tuning for Document Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangwon Ryu, Heejin Do, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remarkable advances in large language models (LLMs) have enabled high-quality
text summarization. However, this capability is currently accessible only
through LLMs of substantial size or proprietary LLMs with usage fees. In
response, smaller-scale LLMs (sLLMs) of easy accessibility and low costs have
been extensively studied, yet they often suffer from missing key information
and entities, i.e., low relevance, in particular, when input documents are
long. We hence propose a key-element-informed instruction tuning for
summarization, so-called KEITSum, which identifies key elements in documents
and instructs sLLM to generate summaries capturing these key elements.
Experimental results on dialogue and news datasets demonstrate that sLLM with
KEITSum indeed provides high-quality summarization with higher relevance and
less hallucinations, competitive to proprietary LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Responsible Foundation Model Development Cheatsheet: A <span class="highlight-title">Review</span> of
  Tools & Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, Sayash Kapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, Maribeth Rauh, Aviya Skowron, Bertie Vidgen, Laura Weidinger, Arvind Narayanan, Victor Sanh, David Adelani, Percy Liang, Rishi Bommasani, Peter Henderson, Sasha Luccioni, Yacine Jernite, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation model development attracts a rapidly expanding body of
contributors, scientists, and applications. To help shape responsible
development practices, we introduce the Foundation Model Development
Cheatsheet: a growing collection of 250+ tools and resources spanning text,
vision, and speech modalities. We draw on a large body of prior work to survey
resources (e.g. software, documentation, frameworks, guides, and practical
tools) that support informed data selection, processing, and understanding,
precise and limitation-aware artifact documentation, efficient model training,
advance awareness of the environmental impact from training, careful model
evaluation of capabilities, risks, and claims, as well as responsible model
release, licensing and deployment practices. We hope this curated collection of
resources helps guide more responsible development. The process of curating
this list, enabled us to review the AI development ecosystem, revealing what
tools are critically missing, misused, or over-used in existing practices. We
find that (i) tools for data sourcing, model evaluation, and monitoring are
critically under-serving ethical and real-world needs, (ii) evaluations for
model safety, capabilities, and environmental impact all lack reproducibility
and transparency, (iii) text and particularly English-centric analyses continue
to dominate over multilingual and multi-modal analyses, and (iv) evaluation of
systems, rather than just models, is needed so that capabilities and impact are
assessed in context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Good are <span class="highlight-title">LLM</span>s at Relation Extraction under Low-Resource Scenario?
  Comprehensive Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawulie Jinensibieke, Mieradilijiang Maimaiti, Wentao Xiao, Yuanhang Zheng, Xiaobo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation Extraction (RE) serves as a crucial technology for transforming
unstructured text into structured information, especially within the framework
of Knowledge Graph development. Its importance is emphasized by its essential
role in various downstream tasks. Besides the conventional RE methods which are
based on neural networks and pre-trained language models, large language models
(LLMs) are also utilized in the research field of RE. However, on low-resource
languages (LRLs), both conventional RE methods and LLM-based methods perform
poorly on RE due to the data scarcity issues. To this end, this paper
constructs low-resource relation extraction datasets in 10 LRLs in three
regions (Central Asia, Southeast Asia and Middle East). The corpora are
constructed by translating the original publicly available English RE datasets
(NYT10, FewRel and CrossRE) using an effective multilingual machine
translation. Then, we use the language perplexity (PPL) to filter out the
low-quality data from the translated datasets. Finally, we conduct an empirical
study and validate the performance of several open-source LLMs on these
generated LRL RE datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Reasoning Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19733v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19733v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, Jason Weston
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iterative preference optimization methods have recently been shown to perform
well for general instruction tuning tasks, but typically make little
improvement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this
work we develop an iterative approach that optimizes the preference between
competing generated Chain-of-Thought (CoT) candidates by optimizing for winning
vs. losing reasoning steps that lead to the correct answer. We train using a
modified DPO loss (Rafailov et al., 2023) with an additional negative
log-likelihood term, which we find to be crucial. We show reasoning improves
across repeated iterations of this scheme. While only relying on examples in
the training set, our approach results in increasing accuracy on GSM8K, MATH,
and ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based
models not relying on additionally sourced datasets. For example, we see a
large improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with
majority voting out of 32 samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Public <span class="highlight-title">LLM</span>s be used for Self-Diagnosis of Medical Conditions ? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikil Sharan Prabahar Balasubramanian, Sagnik Dakshit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in deep learning have generated a large-scale interest in the
development of foundational deep learning models. The development of Large
Language Models (LLM) has evolved as a transformative paradigm in
conversational tasks, which has led to its integration and extension even in
the critical domain of healthcare. With LLMs becoming widely popular and their
public access through open-source models and integration with other
applications, there is a need to investigate their potential and limitations.
One such crucial task where LLMs are applied but require a deeper understanding
is that of self-diagnosis of medical conditions based on bias-validating
symptoms in the interest of public health. The widespread integration of Gemini
with Google search and GPT-4.0 with Bing search has led to a shift in the trend
of self-diagnosis using search engines to conversational LLM models. Owing to
the critical nature of the task, it is prudent to investigate and understand
the potential and limitations of public LLMs in the task of self-diagnosis. In
this study, we prepare a prompt engineered dataset of 10000 samples and test
the performance on the general task of self-diagnosis. We compared the
performance of both the state-of-the-art GPT-4.0 and the fee Gemini model on
the task of self-diagnosis and recorded contrasting accuracies of 63.07% and
6.01%, respectively. We also discuss the challenges, limitations, and potential
of both Gemini and GPT-4.0 for the task of self-diagnosis to facilitate future
research and towards the broader impact of general public knowledge.
Furthermore, we demonstrate the potential and improvement in performance for
the task of self-diagnosis using Retrieval Augmented Generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 Pages, 4 figures, Submitted to ACM Transactions on Computing for
  Healthcare</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Human-AI Teaming with Large Pre-Trained Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanshika Vats, Marzia Binta Nizam, Minghao Liu, Ziyuan Wang, Richard Ho, Mohnish Sai Prasad, Vincent Titterton, Sai Venkat Malreddy, Riya Aggarwal, Yanwen Xu, Lei Ding, Jay Mehta, Nathan Grinnell, Li Liu, Sijia Zhong, Devanathan Nallur Gandamani, Xinyi Tang, Rohan Ghosalkar, Celeste Shen, Rachel Shen, Nafisa Hussain, Kesav Ravichandran, James Davis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving landscape of artificial intelligence (AI), the
collaboration between human intelligence and AI systems, known as Human-AI
(HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and
decision-making processes. The advent of Large Pre-trained Models (LPtM) has
significantly transformed this landscape, offering unprecedented capabilities
by leveraging vast amounts of data to understand and predict complex patterns.
This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how
these models enhance collaborative intelligence beyond traditional approaches.
It examines the potential of LPtMs in augmenting human capabilities, discussing
this collaboration for AI model improvements, effective teaming, ethical
considerations, and their broad applied implications in various sectors.
Through this exploration, the study sheds light on the transformative impact of
LPtM-enhanced HAI Teaming, providing insights for future research, policy
development, and strategic implementations aimed at harnessing the full
potential of this collaboration for research and societal benefit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deception Detection from Linguistic and Physiological Data Streams Using
  Bimodal Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10944v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10944v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Li, Mohamed Abouelenien, Rada Mihalcea, Zhicheng Ding, Qikai Yang, Yiming Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deception detection is gaining increasing interest due to ethical and
security concerns. This paper explores the application of convolutional neural
networks for the purpose of multimodal deception detection. We use a dataset
built by interviewing 104 subjects about two topics, with one truthful and one
falsified response from each subject about each topic. In particular, we make
three main contributions. First, we extract linguistic and physiological
features from this data to train and construct the neural network models.
Second, we propose a fused convolutional neural network model using both
modalities in order to achieve an improved overall performance. Third, we
compare our new approach with earlier methods designed for multimodal deception
detection. We find that our system outperforms regular classification methods;
our results indicate the feasibility of using neural networks for deception
detection even in the presence of limited amounts of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Information Science,
  Parallel and Distributed Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated
  Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05750v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05750v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Abdali, Richard Anarfi, CJ Barberan, Jia He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized the field of Natural
Language Generation (NLG) by demonstrating an impressive ability to generate
human-like text. However, their widespread usage introduces challenges that
necessitate thoughtful examination, ethical scrutiny, and responsible
practices. In this study, we delve into these challenges, explore existing
strategies for mitigating them, with a particular emphasis on identifying
AI-generated text as the ultimate solution. Additionally, we assess the
feasibility of detection from a theoretical perspective and propose novel
research directions to address the current limitations in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeHalu: Code Hallucinations in <span class="highlight-title">LLM</span>s Driven by Execution-based
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Tian, Weixiang Yan, Qian Yang, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made significant progress in code
generation, providing developers with unprecedented automated programming
support. However, LLMs often generate code that is syntactically correct and
even semantically plausible but may not execute as expected or meet specified
requirements. This phenomenon of hallucinations in the code domain has not been
systematically explored. To enhance the community's understanding and research
on this issue, we introduce the concept of code hallucinations and propose a
classification method for code hallucination based on execution verification.
We classify code hallucinations into four main types: mapping, naming,
resource, and logic hallucinations, with each category further divided into
different subcategories to understand and address the unique challenges faced
by LLMs in code generation with finer granularity. Additionally, we develop a
dynamic detection algorithm named CodeHalu to quantify code hallucinations and
establish the CodeHaluEval benchmark, which includes 8,883 samples from 699
tasks to systematically and quantitatively evaluate code hallucinations. By
evaluating 17 popular LLMs on this benchmark, we reveal significant differences
in their accuracy and reliability in code generation and provide detailed
insights for further improving the code generation capabilities of LLMs. The
CodeHalu benchmark and code are publicly available at
https://github.com/yuchen814/CodeHalu.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in
  Fine-tuning <span class="highlight-title">LLM</span>s for Simultaneous Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Raffel, Victor Agostinelli, Lizhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved state-of-the-art performance in
various language processing tasks, motivating their adoption in simultaneous
translation. Current fine-tuning methods to adapt LLMs for simultaneous
translation focus on prompting optimization strategies using either data
augmentation or prompt structure modifications. However, these methods suffer
from several issues, such as unnecessarily expanded training sets,
computational inefficiency from dumping the key and value cache, increased
prompt sizes, or restriction to a single decision policy. To eliminate these
issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs
for simultaneous translation. It utilizes a novel attention mask approach that
models simultaneous translation during fine-tuning by masking attention for a
desired decision policy. Applying the proposed SimulMask on a Falcon LLM for
the IWSLT 2017 dataset, we have observed a significant translation quality
improvement compared to state-of-the-art prompting optimization strategies on
five language pairs while reducing the computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theory of Mind for Multi-Agent Collaboration via <span class="highlight-title">Large Language Models</span> <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, Katia Sycara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind capabilities among
LLM-based agents. Our results reveal limitations in LLM-based agents' planning
optimization due to systematic failures in managing long-horizon contexts and
hallucination about the task state. We explore the use of explicit belief state
representations to mitigate these issues, finding that it enhances task
performance and the accuracy of ToM inferences for LLM-based agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023 (Main Conference). Code available at
  https://github.com/romanlee6/multi_LLM_comm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deductive Closure Training of Language Models for Coherence, Accuracy,
  and Updatability <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afra Feyza Akyürek, Ekin Akyürek, Leshem Choshen, Derry Wijaya, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While language models (LMs) can sometimes generate factually correct text and
estimate truth values of individual claims, these generally do not reflect a
globally coherent, manipulable model of the world. As a consequence, current
LMs also generate incorrect or nonsensical content, and are difficult to edit
and bring up to date. We present a method called Deductive Closure Training
(DCT) that uses LMs themselves to identify implications of (and contradictions
within) the text that they generate, yielding an efficient self-supervised
procedure for improving LM factuality. Given a collection of seed documents,
DCT prompts LMs to generate additional text implied by these documents, reason
globally about the correctness of this generated text, and finally fine-tune on
text inferred to be correct. Given seed documents from a trusted source, DCT
provides a tool for supervised model updating; if seed documents are sampled
from the LM itself, DCT enables fully unsupervised fine-tuning for improved
coherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets,
supervised DCT improves LM fact verification and text generation accuracy by
3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%.
These results show that LMs' reasoning capabilities during inference can be
leveraged during training to improve their reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniRec: A Dual Enhancement of Uniformity and Frequency in Sequential
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Liu, Yitong Wang, Chenyue Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning in sequential recommendation is critical for
accurately modeling user interaction patterns and improving recommendation
precision. However, existing approaches predominantly emphasize item-to-item
transitions, often neglecting the time intervals between interactions, which
are closely related to behavior pattern changes. Additionally, broader
interaction attributes, such as item frequency, are frequently overlooked. We
found that both sequences with more uniform time intervals and items with
higher frequency yield better prediction performance. Conversely, non-uniform
sequences exacerbate user interest drift and less-frequent items are difficult
to model due to sparse sampling, presenting unique challenges inadequately
addressed by current methods. In this paper, we propose UniRec, a novel
bidirectional enhancement sequential recommendation method. UniRec leverages
sequence uniformity and item frequency to enhance performance, particularly
improving the representation of non-uniform sequences and less-frequent items.
These two branches mutually reinforce each other, driving comprehensive
performance optimization in complex sequential recommendation scenarios.
Additionally, we present a multidimensional time module to further enhance
adaptability. To the best of our knowledge, UniRec is the first method to
utilize the characteristics of uniformity and frequency for feature
augmentation. Comparing with eleven advanced models across four datasets, we
demonstrate that UniRec outperforms SOTA models significantly. The code is
available at https://github.com/Linxi000/UniRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures, for source code, see
  https://github.com/Linxi000/UniRec</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effects of Data Split Strategies on the Offline Experiments for CTR
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramazan Tarik Turksoy, Beyza Turkmen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction is a crucial task in online advertising
to recommend products that users are likely to be interested in. To identify
the best-performing models, rigorous model evaluation is necessary. Offline
experimentation plays a significant role in selecting models for live user-item
interactions, despite the value of online experimentation like A/B testing,
which has its own limitations and risks. Often, the correlation between offline
performance metrics and actual online model performance is inadequate. One main
reason for this discrepancy is the common practice of using random splits to
create training, validation, and test datasets in CTR prediction. In contrast,
real-world CTR prediction follows a temporal order. Therefore, the methodology
used in offline evaluation, particularly the data splitting strategy, is
crucial. This study aims to address the inconsistency between current offline
evaluation methods and real-world use cases, by focusing on data splitting
strategies. To examine the impact of different data split strategies on offline
performance, we conduct extensive experiments using both random and temporal
splits on a large open benchmark dataset, Criteo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effects of Using Synthetic Data on Deep Recommender Models' Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatih Cihan Taskin, Ilknur Akcay, Muhammed Pesen, Said Aldemir, Ipek Iraz Esin, Furkan Durmus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are essential for enhancing user experiences by
suggesting items based on individual preferences. However, these systems
frequently face the challenge of data imbalance, characterized by a
predominance of negative interactions over positive ones. This imbalance can
result in biased recommendations favoring popular items. This study
investigates the effectiveness of synthetic data generation in addressing data
imbalances within recommender systems. Six different methods were used to
generate synthetic data. Our experimental approach involved generating
synthetic data using these methods and integrating the generated samples into
the original dataset. Our results show that the inclusion of generated negative
samples consistently improves the Area Under the Curve (AUC) scores. The
significant impact of synthetic negative samples highlights the potential of
data augmentation strategies to address issues of data sparsity and imbalance,
ultimately leading to improved performance of recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with
  1-to-K Contrastive Learning <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, Xudong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search,
which aims to break the barriers between modality and language simultaneously
and achieves image-text retrieval in the multi-lingual scenario with a single
model. In recent years, excellent progress has been made based on cross-lingual
cross-modal pre-training; particularly, the methods based on contrastive
learning on large-scale data have significantly improved retrieval tasks.
However, these methods directly follow the existing pre-training methods in the
cross-lingual or cross-modal domain, leading to two problems of inconsistency
in CCR: The methods with cross-lingual style suffer from the intra-modal error
propagation, resulting in inconsistent recall performance across languages in
the whole dataset. The methods with cross-modal style suffer from the
inter-modal optimization direction bias, resulting in inconsistent rank across
languages within each instance, which cannot be reflected by Recall@K. To solve
these problems, we propose a simple but effective 1-to-K contrastive learning
method, which treats each language equally and eliminates error propagation and
optimization bias. In addition, we propose a new evaluation metric, Mean Rank
Variance (MRV), to reflect the rank inconsistency across languages within each
instance. Extensive experiments on four CCR datasets show that our method
improves both recall rates and MRV with smaller-scale pre-trained data,
achieving the new state-of-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2024 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concordance in basal cell carcinoma diagnosis. Building a proper ground
  truth to train Artificial Intelligence tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisca Silva-Clavería, Carmen Serrano, Iván Matas, Amalia Serrano, Tomás Toledo-Pastrana, David Moreno-Ramírez, Begoña Acha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: The existence of different basal cell carcinoma (BCC) clinical
criteria cannot be objectively validated. An adequate ground-truth is needed to
train an artificial intelligence (AI) tool that explains the BCC diagnosis by
providing its dermoscopic features. Objectives: To determine the consensus
among dermatologists on dermoscopic criteria of 204 BCC. To analyze the
performance of an AI tool when the ground-truth is inferred. Methods: A single
center, diagnostic and prospective study was conducted to analyze the agreement
in dermoscopic criteria by four dermatologists and then derive a reference
standard. 1434 dermoscopic images have been used, that were taken by a primary
health physician, sent via teledermatology, and diagnosed by a dermatologist.
They were randomly selected from the teledermatology platform (2019-2021). 204
of them were tested with an AI tool; the remainder trained it. The performance
of the AI tool trained using the ground-truth of one dermatologist versus the
ground-truth statistically inferred from the consensus of four dermatologists
was analyzed using McNemar's test and Hamming distance. Results: Dermatologists
achieve perfect agreement in the diagnosis of BCC (Fleiss-Kappa=0.9079), and a
high correlation with the biopsy (PPV=0.9670). However, there is low agreement
in detecting some dermoscopic criteria. Statistical differences were found in
the performance of the AI tool trained using the ground-truth of one
dermatologist versus the ground-truth statistically inferred from the consensus
of four dermatologists. Conclusions: Care should be taken when training an AI
tool to determine the BCC patterns present in a lesion. Ground-truth should be
established from multiple dermatologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript word count: 3000, Number of figures: 2, Number of tables:
  3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graph Enhanced <span class="highlight-title">Retrieval-Augmented</span> Generation for Failure Mode
  and Effects Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Bahr, Christoph Wehner, Judith Wewerka, José Bittencourt, Ute Schmid, Rüdiger Daub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Failure mode and effects analysis (FMEA) is a critical tool for mitigating
potential failures, particular during ramp-up phases of new products. However,
its effectiveness is often limited by the missing reasoning capabilities of the
FMEA tools, which are usually tabular structured. Meanwhile, large language
models (LLMs) offer novel prospects for fine-tuning on custom datasets for
reasoning within FMEA contexts. However, LLMs face challenges in tasks that
require factual knowledge, a gap that retrieval-augmented generation (RAG)
approaches aim to fill. RAG retrieves information from a non-parametric data
store and uses a language model to generate responses. Building on this idea,
we propose to advance the non-parametric data store with a knowledge graph
(KG). By enhancing the RAG framework with a KG, our objective is to leverage
analytical and semantic question-answering capabilities on FMEA data. This
paper contributes by presenting a new ontology for FMEA observations, an
algorithm for creating vector embeddings from the FMEA KG, and a KG enhanced
RAG framework. Our approach is validated through a human study and we measure
the performance of the context retrieval recall and precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Stem-Agnostic Single-Decoder System for Music Source Separation Beyond
  Four Stems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karn N. Watcharasupat, Alexander Lerch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant recent progress across multiple subtasks of audio source
separation, few music source separation systems support separation beyond the
four-stem vocals, drums, bass, and other (VDBO) setup. Of the very few current
systems that support source separation beyond this setup, most continue to rely
on an inflexible decoder setup that can only support a fixed pre-defined set of
stems. Increasing stem support in these inflexible systems correspondingly
requires increasing computational complexity, rendering extensions of these
systems computationally infeasible for long-tail instruments. In this work, we
propose Banquet, a system that allows source separation of multiple stems using
just one decoder. A bandsplit source separation model is extended to work in a
query-based setup in tandem with a music instrument recognition PaSST model. On
the MoisesDB dataset, Banquet, at only 24.9 M trainable parameters, approached
the performance level of the significantly more complex 6-stem Hybrid
Transformer Demucs on VDBO stems and outperformed it on guitar and piano. The
query-based setup allows for the separation of narrow instrument classes such
as clean acoustic guitars, and can be successfully applied to the extraction of
less common stems such as reeds and organs. Implementation is available at
https://github.com/kwatcharasupat/query-bandit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the 25th International Society for Music Information
  Retrieval Conference (ISMIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-Ranking Step by Step: Investigating Pre-Filtering for Re-Ranking with
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baharan Nouriinanloo, Maxime Lamothe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been revolutionizing a myriad of natural
language processing tasks with their diverse zero-shot capabilities. Indeed,
existing work has shown that LLMs can be used to great effect for many tasks,
such as information retrieval (IR), and passage ranking. However, current
state-of-the-art results heavily lean on the capabilities of the LLM being
used. Currently, proprietary, and very large LLMs such as GPT-4 are the highest
performing passage re-rankers. Hence, users without the resources to leverage
top of the line LLMs, or ones that are closed source, are at a disadvantage. In
this paper, we investigate the use of a pre-filtering step before passage
re-ranking in IR. Our experiments show that by using a small number of human
generated relevance scores, coupled with LLM relevance scoring, it is
effectively possible to filter out irrelevant passages before re-ranking. Our
experiments also show that this pre-filtering then allows the LLM to perform
significantly better at the re-ranking task. Indeed, our results show that
smaller models such as Mixtral can become competitive with much larger
proprietary models (e.g., ChatGPT and GPT-4).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generate then Retrieve: Conversational Response Retrieval Using <span class="highlight-title">LLM</span>s as
  Answer and Query Generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Abbasiantaeb, Mohammad Aliannejadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CIS is a prominent area in IR which focuses on developing interactive
knowledge assistants. These systems must adeptly comprehend the user's
information requirements within the conversational context and retrieve the
relevant information. To this aim, the existing approaches model the user's
information needs by generating a single query rewrite or a single
representation of the query in the query space embedding. However, to answer
complex questions, a single query rewrite or representation is often
ineffective. To address this, a system needs to do reasoning over multiple
passages. In this work, we propose using a generate-then-retrieve approach to
improve the passage retrieval performance for complex user queries. In this
approach, we utilize large language models (LLMs) to (i) generate an initial
answer to the user's information need by doing reasoning over the context of
the conversation, and (ii) ground this answer to the collection. Based on the
experiments, our proposed approach significantly improves the retrieval
performance on TREC iKAT 23, TREC CAsT 20 and 22 datasets, under various
setups. Also, we show that grounding the LLM's answer requires more than one
searchable query, where an average of 3 queries outperforms human rewrites.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General Distribution Learning: A theoretical framework for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05666v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05666v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binchuan Qi, Li Li, Wei Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There remain numerous unanswered research questions on deep learning (DL)
within the classical learning theory framework. These include the remarkable
generalization capabilities of overparametrized neural networks (NNs), the
efficient optimization performance despite non-convexity of objectives, the
mechanism of flat minima for generalization, and the exceptional performance of
deep architectures in solving physical problems. This paper introduces General
Distribution Learning (GD Learning), a novel theoretical learning framework
designed to address a comprehensive range of machine learning and statistical
tasks, including classification, regression and parameter estimation. Departing
from traditional statistical machine learning, GD Learning focuses on the true
underlying distribution. In GD Learning, learning error, corresponding to the
expected error in classical statistical learning framework, is divided into
fitting errors due to models and algorithms, as well as sampling errors
introduced by limited sampling data. The framework significantly incorporates
prior knowledge, especially in scenarios characterized by data scarcity,
thereby enhancing performance. Within the GD Learning framework, we demonstrate
that the global optimal solutions in non-convex optimization can be approached
by minimizing the gradient norm and the non-uniformity of the eigenvalues of
the model's Jacobian matrix. This insight leads to the development of the
gradient structure control algorithm. GD Learning also offers fresh insights
into the questions on deep learning, including overparameterization and
non-convex optimization, bias-variance trade-off, and the mechanism of flat
minima.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jina CLIP: Your CLIP Model Is Also Your Text Retriever <span class="chip">ICML2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Koukounas, Georgios Mastrapas, Michael Günther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Martínez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pretraining (CLIP) is widely used to train models
to align images and texts in a common embedding space by mapping them to
fixed-sized vectors. These models are key to multimodal information retrieval
and related tasks. However, CLIP models generally underperform in text-only
tasks compared to specialized text models. This creates inefficiencies for
information retrieval systems that keep separate embeddings and models for
text-only and multimodal tasks. We propose a novel, multi-task contrastive
training method to address this issue, which we use to train the jina-clip-v1
model to achieve the state-of-the-art performance on both text-image and
text-text retrieval tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, MFM-EAI@ICML2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClickPrompt: CTR Models are Strong Prompt Generators for Adapting
  Language Models to CTR Prediction <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09234v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09234v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction has become increasingly indispensable for
various Internet applications. Traditional CTR models convert the multi-field
categorical data into ID features via one-hot encoding, and extract the
collaborative signals among features. Such a paradigm suffers from the problem
of semantic information loss. Another line of research explores the potential
of pretrained language models (PLMs) for CTR prediction by converting input
data into textual sentences through hard prompt templates. Although semantic
signals are preserved, they generally fail to capture the collaborative
information (e.g., feature interactions, pure ID features), not to mention the
unacceptable inference overhead brought by the huge model size. In this paper,
we aim to model both the semantic knowledge and collaborative knowledge for
accurate CTR estimation, and meanwhile address the inference inefficiency
issue. To benefit from both worlds and close their gaps, we propose a novel
model-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models
to generate interaction-aware soft prompts for PLMs. We design a
prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM
has to recover the masked tokens based on the language context, as well as the
soft prompts generated by CTR model. The collaborative and semantic knowledge
from ID and textual features would be explicitly aligned and interacted via the
prompt interface. Then, we can either tune the CTR model with PLM for superior
performance, or solely tune the CTR model without PLM for inference efficiency.
Experiments on four real-world datasets validate the effectiveness of
ClickPrompt compared with existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Collaborative Distillation for Recommender System <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19046v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19046v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyuseok Lee, SeongKu Kang, Wonbin Kweon, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has emerged as a promising technique for
addressing the computational challenges associated with deploying large-scale
recommender systems. KD transfers the knowledge of a massive teacher system to
a compact student model, to reduce the huge computational burdens for inference
while retaining high accuracy. The existing KD studies primarily focus on
one-time distillation in static environments, leaving a substantial gap in
their applicability to real-world scenarios dealing with continuously incoming
users, items, and their interactions. In this work, we delve into a systematic
approach to operating the teacher-student KD in a non-stationary data stream.
Our goal is to enable efficient deployment through a compact student, which
preserves the high performance of the massive teacher, while effectively
adapting to continuously incoming data. We propose Continual Collaborative
Distillation (CCD) framework, where both the teacher and the student
continually and collaboratively evolve along the data stream. CCD facilitates
the student in effectively adapting to new data, while also enabling the
teacher to fully leverage accumulated knowledge. We validate the effectiveness
of CCD through extensive quantitative, ablative, and exploratory experiments on
two real-world datasets. We expect this research direction to contribute to
narrowing the gap between existing KD studies and practical applications,
thereby enhancing the applicability of KD in real-world systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2024 research track. 9 main pages + 1 appendix page,
  5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Retrieval Augmented</span> Zero-Shot Text Classification <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tassallah Abdullahi, Ritambhara Singh, Carsten Eickhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot text learning enables text classifiers to handle unseen classes
efficiently, alleviating the need for task-specific training data. A simple
approach often relies on comparing embeddings of query (text) to those of
potential classes. However, the embeddings of a simple query sometimes lack
rich contextual information, which hinders the classification performance.
Traditionally, this has been addressed by improving the embedding model with
expensive training. We introduce QZero, a novel training-free knowledge
augmentation approach that reformulates queries by retrieving supporting
categories from Wikipedia to improve zero-shot text classification performance.
Our experiments across six diverse datasets demonstrate that QZero enhances
performance for state-of-the-art static and contextual embedding models without
the need for retraining. Notably, in News and medical topic classification
tasks, QZero improves the performance of even the largest OpenAI embedding
model by at least 5% and 3%, respectively. Acting as a knowledge amplifier,
QZero enables small word embedding models to achieve performance levels
comparable to those of larger contextual models, offering the potential for
significant computational savings. Additionally, QZero offers meaningful
insights that illuminate query context and verify topic relevance, aiding in
understanding model predictions. Overall, QZero improves embedding-based
zero-shot classifiers while maintaining their simplicity. This makes it
particularly valuable for resource-constrained environments and domains with
constantly evolving information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 2024 ACM SIGIR International Conference on the
  Theory of Information Retrieval (ICTIR '24), July 13, 2024, Washington DC,
  DC, USA</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-25T00:00:00Z">2024-06-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">146</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ercong Nie, Bo Shao, Zifeng Ding, Mingyang Wang, Helmut Schmid, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) possess extensive parametric knowledge, but this
knowledge is difficult to update with new information because retraining is
very expensive and infeasible for closed-source models. Knowledge editing (KE)
has emerged as a viable solution for updating the knowledge of LLMs without
compromising their overall performance. On-the-fly KE methods, inspired by
in-context learning (ICL), have shown great promise and allow LLMs to be
treated as black boxes. In the past, KE was primarily employed in English
contexts, whereas the potential for cross-lingual KE in current English-centric
LLMs has not been fully explored. To foster more research in this direction, we
introduce the BMIKE-53 benchmark for evaluating cross-lingual KE on 53 diverse
languages across three KE task types. We also propose a gradient-free KE method
called Multilingual In-context Knowledge Editing (MIKE) and evaluate it on
BMIKE-53. Our evaluation focuses on cross-lingual knowledge transfer in terms
of reliability, generality, locality, and portability, offering valuable
insights and a framework for future research in cross-lingual KE. Our code and
data are publicly accessible via the anonymous repository at
https://anonymous.4open.science/r/MIKE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaLMQA: Exploring culturally specific long-form question answering
  across 23 languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are commonly used for long-form question
answering, which requires them to generate paragraph-length answers to complex
questions. While long-form QA has been well-studied in English via many
different datasets and evaluation metrics, this research has not been extended
to cover most other languages. To bridge this gap, we introduce CaLMQA, a
collection of 2.6K complex questions spanning 23 languages, including
under-resourced, rarely-studied languages such as Fijian and Kirundi. Our
dataset includes both naturally-occurring questions collected from community
web forums as well as questions written by native speakers, whom we hire for
this purpose. Our process yields diverse, complex questions that reflect
cultural topics (e.g. traditions, laws, news) and the language usage of native
speakers. We conduct automatic evaluation across a suite of open- and
closed-source models using our novel metric CaLMScore, which detects incorrect
language and token repetitions in answers, and observe that the quality of
LLM-generated answers degrades significantly for some low-resource languages.
We perform human evaluation on a subset of models and see that model
performance is significantly worse for culturally specific questions than for
culturally agnostic questions. Our findings highlight the need for further
research in LLM multilingual capabilities and non-English LFQA evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 16 figures. Code and data available at
  https://github.com/2015aroras/CaLMQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Clinical Evidence Synthesis with <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifeng Wang, Lang Cao, Benjamin Danek, Yichi Zhang, Qiao Jin, Zhiyong Lu, Jimeng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic medical discovery by AI is a dream of many. One step toward that
goal is to create an AI model to understand clinical studies and synthesize
clinical evidence from the literature. Clinical evidence synthesis currently
relies on systematic reviews of clinical trials and retrospective analyses from
medical literature. However, the rapid expansion of publications presents
challenges in efficiently identifying, summarizing, and updating evidence. We
introduce TrialMind, a generative AI-based pipeline for conducting medical
systematic reviews, encompassing study search, screening, and data extraction
phases. We utilize large language models (LLMs) to drive each pipeline
component while incorporating human expert oversight to minimize errors. To
facilitate evaluation, we also create a benchmark dataset TrialReviewBench, a
custom dataset with 870 annotated clinical studies from 25 meta-analysis papers
across various medical treatments. Our results demonstrate that TrialMind
significantly improves the literature review process, achieving high recall
rates (0.897-1.000) in study searching from over 20 million PubMed studies and
outperforming traditional language model embeddings-based methods in screening
(Recall@20 of 0.227-0.246 vs. 0.000-0.102). Furthermore, our approach surpasses
direct GPT-4 performance in result extraction, with accuracy ranging from 0.65
to 0.84. We also support clinical evidence synthesis in forest plots, as
validated by eight human annotators who preferred TrialMind over the GPT-4
baseline with a winning rate of 62.5%-100% across the involved reviews. Our
findings suggest that an LLM-based clinical evidence synthesis approach, such
as TrialMind, can enable reliable and high-quality clinical evidence synthesis
to improve clinical research efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring and Benchmarking <span class="highlight-title">Large Language Models</span>' Capabilities to
  Generate Persuasive Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are exposed to much information trying to influence us, such as teaser
messages, debates, politically framed news, and propaganda - all of which use
persuasive language. With the recent interest in Large Language Models (LLMs),
we study the ability of LLMs to produce persuasive text. As opposed to prior
work which focuses on particular domains or types of persuasion, we conduct a
general study across various domains to measure and benchmark to what degree
LLMs produce persuasive text - both when explicitly instructed to rewrite text
to be more or less persuasive and when only instructed to paraphrase. To this
end, we construct a new dataset, Persuasive-Pairs, of pairs each consisting of
a short text and of a text rewritten by an LLM to amplify or diminish
persuasive language. We multi-annotate the pairs on a relative scale for
persuasive language. This data is not only a valuable resource in itself, but
we also show that it can be used to train a regression model to predict a score
of persuasive language between text pairs. This model can score and benchmark
new LLMs across domains, thereby facilitating the comparison of different LLMs.
Finally, we discuss effects observed for different system prompts. Notably, we
find that different 'personas' in the system prompt of LLaMA3 change the
persuasive language in the text substantially, even when only instructed to
paraphrase. These findings underscore the importance of investigating
persuasive language in LLM generated text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted
  Phenomenon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        USVSN Sai Prashanth, Alvin Deng, Kyle O'Brien, Jyothir S V, Mohammad Aflah Khan, Jaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy Ke, Katherine Lee, Naomi Saphra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memorization in language models is typically treated as a homogenous
phenomenon, neglecting the specifics of the memorized data. We instead model
memorization as the effect of a set of complex factors that describe each
sample and relate it to the model and corpus. To build intuition around these
factors, we break memorization down into a taxonomy: recitation of highly
duplicated sequences, reconstruction of inherently predictable sequences, and
recollection of sequences that are neither. We demonstrate the usefulness of
our taxonomy by using it to construct a predictive model for memorization. By
analyzing dependencies and inspecting the weights of the predictive model, we
find that different factors influence the likelihood of memorization
differently depending on the taxonomic category.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Following Length Constraints in Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhe Yuan, Ilia Kulikov, Ping Yu, Kyunghyun Cho, Sainbayar Sukhbaatar, Jason Weston, Jing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligned instruction following models can better fulfill user requests than
their unaligned counterparts. However, it has been shown that there is a length
bias in evaluation of such models, and that training algorithms tend to exploit
this bias by learning longer responses. In this work we show how to train
models that can be controlled at inference time with instructions containing
desired length constraints. Such models are superior in length instructed
evaluations, outperforming standard instruction following models such as GPT4,
Llama 3 and Mixtral.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Find Parent then Label Children: A Two-stage Taxonomy Completion Method
  with Pre-trained Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Xia, Yixuan Weng, Shizhu He, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Taxonomies, which organize domain concepts into hierarchical structures, are
crucial for building knowledge systems and downstream applications. As domain
knowledge evolves, taxonomies need to be continuously updated to include new
concepts. Previous approaches have mainly focused on adding concepts to the
leaf nodes of the existing hierarchical tree, which does not fully utilize the
taxonomy's knowledge and is unable to update the original taxonomy structure
(usually involving non-leaf nodes). In this paper, we propose a two-stage
method called ATTEMPT for taxonomy completion. Our method inserts new concepts
into the correct position by finding a parent node and labeling child nodes.
Specifically, by combining local nodes with prompts to generate natural
sentences, we take advantage of pre-trained language models for
hypernym/hyponymy recognition. Experimental results on two public datasets
(including six domains) show that ATTEMPT performs best on both taxonomy
completion and extension tasks, surpassing existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span> Targeted Underperformance Disproportionately Impacts Vulnerable
  Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elinor Poole-Dayan, Deb Roy, Jad Kabbara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While state-of-the-art Large Language Models (LLMs) have shown impressive
performance on many tasks, there has been extensive research on undesirable
model behavior such as hallucinations and bias. In this work, we investigate
how the quality of LLM responses changes in terms of information accuracy,
truthfulness, and refusals depending on three user traits: English proficiency,
education level, and country of origin. We present extensive experimentation on
three state-of-the-art LLMs and two different datasets targeting truthfulness
and factuality. Our findings suggest that undesirable behaviors in
state-of-the-art LLMs occur disproportionately more for users with lower
English proficiency, of lower education status, and originating from outside
the US, rendering these models unreliable sources of information towards their
most vulnerable users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViANLI: Adversarial Natural Language Inference for Vietnamese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tin Van Huynh, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Natural Language Processing (NLI) datasets and models has
been inspired by innovations in annotation design. With the rapid development
of machine learning models today, the performance of existing machine learning
models has quickly reached state-of-the-art results on a variety of tasks
related to natural language processing, including natural language inference
tasks. By using a pre-trained model during the annotation process, it is
possible to challenge current NLI models by having humans produce
premise-hypothesis combinations that the machine model cannot correctly
predict. To remain attractive and challenging in the research of natural
language inference for Vietnamese, in this paper, we introduce the adversarial
NLI dataset to the NLP research community with the name ViANLI. This data set
contains more than 10K premise-hypothesis pairs and is built by a continuously
adjusting process to obtain the most out of the patterns generated by the
annotators. ViANLI dataset has brought many difficulties to many current SOTA
models when the accuracy of the most powerful model on the test set only
reached 48.4%. Additionally, the experimental results show that the models
trained on our dataset have significantly improved the results on other
Vietnamese NLI datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedBiOT: <span class="highlight-title">LLM</span> Local Fine-tuning in Federated Learning without Full Model <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, Jing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show amazing performance on many domain-specific
tasks after fine-tuning with some appropriate data. However, many
domain-specific data are privately distributed across multiple owners. Thus,
this dilemma raises the interest in how to perform LLM fine-tuning in federated
learning (FL). However, confronted with limited computation and communication
capacities, FL clients struggle to fine-tune an LLM effectively. To this end,
we introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL.
Specifically, our method involves the server generating a compressed LLM and
aligning its performance with the full model. Subsequently, the clients
fine-tune a lightweight yet important part of the compressed model, referred to
as an adapter. Notice that as the server has no access to the private data
owned by the clients, the data used for alignment by the server has a different
distribution from the one used for fine-tuning by clients. We formulate the
problem into a bi-level optimization problem to minimize the negative effect of
data discrepancy and derive the updating rules for the server and clients. We
conduct extensive experiments on LLaMA-2, empirically showing that the adapter
has exceptional performance when reintegrated into the global LLM. The results
also indicate that the proposed FedBiOT significantly reduces resource
consumption compared to existing benchmarks, all while achieving comparable
performance levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Distributional to Overton Pluralism: Investigating Large Language
  Model Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thom Lake, Eunsol Choi, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The alignment process changes several properties of a large language model's
(LLM's) output distribution. We analyze two aspects of post-alignment
distributional shift of LLM responses. First, we re-examine previously reported
reductions in response diversity post-alignment. Our analysis suggests that an
apparent drop in the diversity of responses is largely explained by quality
control and information aggregation. Alignment suppresses irrelevant and
unhelpful content while shifting the output distribution toward longer
responses that cover information spanning several responses from the base LLM,
essentially presenting diverse information in a single response. Finding little
evidence that alignment suppresses useful information, it is natural to ask the
opposite question: do aligned models surface information that cannot be
recovered from base models? Our second investigation shows this is not the case
and the behavior of aligned models is recoverable from base models without
fine-tuning. A combination of in-context examples and lower-resolution semantic
hints about response content can elicit responses from base LLMs that are as
similar to alignment-tuned LLM responses as alignment-tuned LLM responses are
to each other. Taken together, these results indicate that current alignment
techniques capture but do not extend the useful subset of assistant-like base
LLM behavior, providing further evidence for the Superficial Alignment
Hypothesis. They also show that in-context alignment can go surprisingly far as
a strategy for imitating aligned LLMs without fine-tuning. Our code and data is
available at https://github.com/thomlake/investigating-alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VarBench: Robust Language Model Benchmarking Through Dynamic Variable
  Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Qian, Shunji Wan, Claudia Tang, Youzhi Wang, Xuanming Zhang, Maximillian Chen, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models achieve impressive scores on traditional benchmarks,
an increasing number of researchers are becoming concerned about benchmark data
leakage during pre-training, commonly known as the data contamination problem.
To ensure fair evaluation, recent benchmarks release only the training and
validation sets, keeping the test set labels closed-source. They require anyone
wishing to evaluate his language model to submit the model's predictions for
centralized processing and then publish the model's result on their
leaderboard. However, this submission process is inefficient and prevents
effective error analysis. To address this issue, we propose to variabilize
benchmarks and evaluate language models dynamically. Specifically, we extract
variables from each test case and define a value range for each variable. For
each evaluation, we sample new values from these value ranges to create unique
test cases, thus ensuring a fresh evaluation each time. We applied this
variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and
TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our
experimental results demonstrate that this approach provides a more accurate
assessment of the true capabilities of language models, effectively mitigating
the contamination problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying AI Psychology: A Psychometrics Benchmark for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Li, Yue Huang, Hongyi Wang, Xiangliang Zhang, James Zou, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated exceptional task-solving
capabilities, increasingly adopting roles akin to human-like assistants. The
broader integration of LLMs into society has sparked interest in whether they
manifest psychological attributes, and whether these attributes are
stable-inquiries that could deepen the understanding of their behaviors.
Inspired by psychometrics, this paper presents a framework for investigating
psychology in LLMs, including psychological dimension identification,
assessment dataset curation, and assessment with results validation. Following
this framework, we introduce a comprehensive psychometrics benchmark for LLMs
that covers six psychological dimensions: personality, values, emotion, theory
of mind, motivation, and intelligence. This benchmark includes thirteen
datasets featuring diverse scenarios and item types. Our findings indicate that
LLMs manifest a broad spectrum of psychological attributes. We also uncover
discrepancies between LLMs' self-reported traits and their behaviors in
real-world scenarios. This paper demonstrates a thorough psychometric
assessment of LLMs, providing insights into reliable evaluation and potential
applications in AI and social sciences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ This Paper Had the Smartest <span class="highlight-title">Review</span>ers -- Flattery Detection Utilising an
  Audio-Textual Transformer-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Christ, Shahin Amiriparian, Friederike Hawighorst, Ann-Kathrin Schill, Angelo Boutalikakis, Lorenz Graf-Vlachy, Andreas König, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flattery is an important aspect of human communication that facilitates
social bonding, shapes perceptions, and influences behavior through strategic
compliments and praise, leveraging the power of speech to build rapport
effectively. Its automatic detection can thus enhance the naturalness of
human-AI interactions. To meet this need, we present a novel audio textual
dataset comprising 20 hours of speech and train machine learning models for
automatic flattery detection. In particular, we employ pretrained AST,
Wav2Vec2, and Whisper models for the speech modality, and Whisper TTS models
combined with a RoBERTa text classifier for the textual modality. Subsequently,
we build a multimodal classifier by combining text and audio representations.
Evaluation on unseen test data demonstrates promising results, with Unweighted
Average Recall scores reaching 82.46% in audio-only experiments, 85.97% in
text-only experiments, and 87.16% using a multimodal approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-ARC: Enhancing <span class="highlight-title">LLM</span>s with an Automated Reasoning Critic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kalyanpur, Kailash Saravanakumar, Victor Barres, Jennifer Chu-Carroll, David Melville, David Ferrucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the
logical reasoning capabilities of Large Language Models (LLMs), by combining
them with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic
method where the LLM Actor generates declarative logic programs along with
tests for semantic correctness, while the Automated Reasoning Critic evaluates
the code, runs the tests and provides feedback on test failures for iterative
refinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a
new state-of-the-art accuracy of 88.32% on the FOLIO benchmark which tests
complex logical reasoning capabilities. Our experiments demonstrate significant
improvements over LLM-only baselines, highlighting the importance of logic test
generation and iterative self-refinement. We achieve our best result using a
fully automated self-supervised training loop where the Actor is trained on
end-to-end dialog traces with Critic feedback. We discuss potential
enhancements and provide a detailed error analysis, showcasing the robustness
and efficacy of LLM-ARC for complex natural language reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELIZA Reinterpreted: The world's first chatbot was not intended as a
  chatbot at all 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeff Shrager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ELIZA, often considered the world's first chatbot, was written by Joseph
Weizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot,
but rather to build a platform for research into human-machine conversation and
the important cognitive processes of interpretation and misinterpretation. His
purpose was obscured by ELIZA's fame, resulting in large part from the
fortuitous timing of it's creation, and it's escape into the wild. In this
paper I provide a rich historical context for ELIZA's creation, demonstrating
that ELIZA arose from the intersection of some of the central threads in the
technical history of AI. I also briefly discuss how ELIZA escaped into the
world, and how its accidental escape, along with several coincidental turns of
the programming language screws, led both to the misapprehension that ELIZA was
intended as a chatbot, and to the loss of the original ELIZA to history for
over 50 years.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In review in IEEE Annals of the History of Computing (submitted Apr
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variationist: Exploring Multifaceted Variation and Bias in Written
  Language Data <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Ramponi, Camilla Casula, Stefano Menini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring and understanding language data is a fundamental stage in all areas
dealing with human language. It allows NLP practitioners to uncover quality
concerns and harmful biases in data before training, and helps linguists and
social scientists to gain insight into language use and human behavior. Yet,
there is currently a lack of a unified, customizable tool to seamlessly inspect
and visualize language variation and bias across multiple variables, language
units, and diverse metrics that go beyond descriptive statistics. In this
paper, we introduce Variationist, a highly-modular, extensible, and
task-agnostic tool that fills this gap. Variationist handles at once a
potentially unlimited combination of variable types and semantics across
diversity and association metrics with regards to the language unit of choice,
and orchestrates the creation of up to five-dimensional interactive charts for
over 30 variable type-semantics combinations. Through our case studies on
computational dialectology, human label variation, and text generation, we show
how Variationist enables researchers from different disciplines to effortlessly
answer specific research questions or unveil undesired associations in language
data. A Python library, code, documentation, and tutorials are made publicly
available to the research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 (System Demonstrations)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Banishing <span class="highlight-title">LLM</span> Hallucinations Requires Rethinking Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, Gregory Diamos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their powerful chat, coding, and reasoning abilities, Large Language
Models (LLMs) frequently hallucinate. Conventional wisdom suggests that
hallucinations are a consequence of a balance between creativity and
factuality, which can be mitigated, but not eliminated, by grounding the LLM in
external knowledge sources. Through extensive systematic experiments, we show
that these traditional approaches fail to explain why LLMs hallucinate in
practice. Specifically, we show that LLMs augmented with a massive Mixture of
Memory Experts (MoME) can easily memorize large datasets of random numbers. We
corroborate these experimental findings with a theoretical construction showing
that simple neural networks trained to predict the next token hallucinate when
the training loss is above a threshold as it usually does in practice when
training on internet scale data. We interpret our findings by comparing against
traditional retrieval methods for mitigating hallucinations. We use our
findings to design a first generation model for removing hallucinations --
Lamini-1 -- that stores facts in a massive mixture of millions of memory
experts that are retrieved dynamically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigate the Gap: Investigating Approaches for Improving Cross-Modal
  Alignment in CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedigheh Eslami, Gerard de Melo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable
improvements in zero-shot classification and cross-modal vision-language tasks.
Yet, from a geometrical point of view, the CLIP embedding space has been found
to have a pronounced modality gap. This gap renders the embedding space overly
sparse and disconnected, with different modalities being densely distributed in
distinct subregions of the hypersphere. In this work, we aim at answering two
main questions: 1. Does sharing the parameter space between the multi-modal
encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart
the uni-modal embeddings via intra-modality separation? We design AlignCLIP, in
order to answer these questions and show that answers to both questions are
positive. Through extensive experiments, we show that AlignCLIP achieves
noticeable enhancements in the cross-modal alignment of the embeddings, and
thereby, reduces the modality gap, while maintaining the performance across
several downstream evaluations, such as zero-shot image classification,
zero-shot multi-modal retrieval and zero-shot semantic text similarity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Distillation in Automated Annotation: Supervised Text
  Classification with <span class="highlight-title">LLM</span>-Generated Training Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Pangakis, Samuel Wolken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational social science (CSS) practitioners often rely on human-labeled
data to fine-tune supervised text classifiers. We assess the potential for
researchers to augment or replace human-generated training data with surrogate
training labels from generative large language models (LLMs). We introduce a
recommended workflow and test this LLM application by replicating 14
classification tasks and measuring performance. We employ a novel corpus of
English-language text classification data sets from recent CSS articles in
high-impact journals. Because these data sets are stored in password-protected
archives, our analyses are less prone to issues of contamination. For each
task, we compare supervised classifiers fine-tuned using GPT-4 labels against
classifiers fine-tuned with human annotations and against labels from GPT-4 and
Mistral-7B with few-shot in-context learning. Our findings indicate that
supervised classification models fine-tuned on LLM-generated labels perform
comparably to models fine-tuned with labels from human annotators. Fine-tuning
models using LLM-generated labels can be a fast, efficient and cost-effective
method of building supervised text classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the Sixth Workshop on Natural Language Processing
  and Computational Social Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoSafe: Evaluating <span class="highlight-title">Large Language Model</span> Safety in Multi-Turn Dialogue
  Coreference <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erxin Yu, Jing Li, Ming Liao, Siqi Wang, Zuchen Gao, Fei Mi, Lanqing Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) constantly evolve, ensuring their safety
remains a critical research problem. Previous red-teaming approaches for LLM
safety have primarily focused on single prompt attacks or goal hijacking. To
the best of our knowledge, we are the first to study LLM safety in multi-turn
dialogue coreference. We created a dataset of 1,400 questions across 14
categories, each featuring multi-turn coreference safety attacks. We then
conducted detailed evaluations on five widely used open-source LLMs. The
results indicated that under multi-turn coreference safety attacks, the highest
attack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was
13.9% with the Mistral-7B-Instruct model. These findings highlight the safety
vulnerabilities in LLMs during dialogue coreference interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-assessment, Exhibition, and Recognition: a <span class="highlight-title">Review</span> of Personality in
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Wen, Yu Yang, Jiannong Cao, Haoming Sun, Ruosong Yang, Shuaiqi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) appear to behave increasingly human-like in
text-based interactions, more and more researchers become interested in
investigating personality in LLMs. However, the diversity of psychological
personality research and the rapid development of LLMs have led to a broad yet
fragmented landscape of studies in this interdisciplinary field. Extensive
studies across different research focuses, different personality psychometrics,
and different LLMs make it challenging to have a holistic overview and further
pose difficulties in applying findings to real-world applications. In this
paper, we present a comprehensive review by categorizing current studies into
three research problems: self-assessment, exhibition, and recognition, based on
the intrinsic characteristics and external manifestations of personality in
LLMs. For each problem, we provide a thorough analysis and conduct in-depth
comparisons of their corresponding solutions. Besides, we summarize research
findings and open challenges from current studies and further discuss their
underlying causes. We also collect extensive publicly available resources to
facilitate interested researchers and developers. Lastly, we discuss the
potential future research directions and application scenarios. Our paper is
the first comprehensive survey of up-to-date literature on personality in LLMs.
By presenting a clear taxonomy, in-depth analysis, promising future directions,
and extensive resource collections, we aim to provide a better understanding
and facilitate further advancements in this emerging field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Building an End-to-End Multilingual Automatic Lyrics
  Transcription Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Huang, Emmanouil Benetos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual automatic lyrics transcription (ALT) is a challenging task due
to the limited availability of labelled data and the challenges introduced by
singing, compared to multilingual automatic speech recognition. Although some
multilingual singing datasets have been released recently, English continues to
dominate these collections. Multilingual ALT remains underexplored due to the
scale of data and annotation quality. In this paper, we aim to create a
multilingual ALT system with available datasets. Inspired by architectures that
have been proven effective for English ALT, we adapt these techniques to the
multilingual scenario by expanding the target vocabulary set. We then evaluate
the performance of the multilingual model in comparison to its monolingual
counterparts. Additionally, we explore various conditioning methods to
incorporate language information into the model. We apply analysis by language
and combine it with the language classification performance. Our findings
reveal that the multilingual model performs consistently better than the
monolingual models trained on the language subsets. Furthermore, we demonstrate
that incorporating language information significantly enhances performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Seeing the Big through the Small": Can <span class="highlight-title">LLM</span>s Approximate Human Judgment
  Distributions on NLI from a Few Explanations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beiduo Chen, Xinpeng Wang, Siyao Peng, Robert Litschko, Anna Korhonen, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human label variation (HLV) is a valuable source of information that arises
when multiple human annotators provide different labels for valid reasons. In
Natural Language Inference (NLI) earlier approaches to capturing HLV involve
either collecting annotations from many crowd workers to represent human
judgment distribution (HJD) or use expert linguists to provide detailed
explanations for their chosen labels. While the former method provides denser
HJD information, obtaining it is resource-intensive. In contrast, the latter
offers richer textual information but it is challenging to scale up to many
human judges. Besides, large language models (LLMs) are increasingly used as
evaluators (``LLM judges'') but with mixed results, and few works aim to study
HJDs. This study proposes to exploit LLMs to approximate HJDs using a small
number of expert labels and explanations. Our experiments show that a few
explanations significantly improve LLMs' ability to approximate HJDs with and
without explicit labels, thereby providing a solution to scale up annotations
for HJD. However, fine-tuning smaller soft-label aware models with the
LLM-generated model judgment distributions (MJDs) presents partially
inconsistent results: while similar in distance, their resulting fine-tuned
models and visualized distributions differ substantially. We show the
importance of complementing instance-level distance measures with a
global-level shape metric and visualization to more effectively evaluate MJDs
against human judgment distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongIns: A Challenging Long-context Instruction-based Exam for <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Wenhu Chen, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The long-context capabilities of large language models (LLMs) have been a hot
topic in recent years. To evaluate the performance of LLMs in different
scenarios, various assessment benchmarks have emerged. However, as most of
these benchmarks focus on identifying key information to answer questions,
which mainly requires the retrieval ability of LLMs, these benchmarks can
partially represent the reasoning performance of LLMs from large amounts of
information. Meanwhile, although LLMs often claim to have context windows of
32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual
supported length of these LLMs. To address these issues, we propose the LongIns
benchmark dataset, a challenging long-context instruction-based exam for LLMs,
which is built based on the existing instruction datasets. Specifically, in our
LongIns, we introduce three evaluation settings: Global Instruction & Single
Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction &
Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations
on existing LLMs and have the following important findings: (1). The
top-performing GPT-4 with 128k context length performs poorly on the evaluation
context window of 16k in our LongIns. (2). For the multi-hop reasoning ability
of many existing LLMs, significant efforts are still needed under short context
windows (less than 4k).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for
  Querying and Classifying IoT Threats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Pavlich, Nima Ebadi, Richard Tarbell, Billy Linares, Adrian Tan, Rachael Humphreys, Jayanta Kumar Das, Rambod Ghandiparsi, Hannah Haley, Jerris George, Rocky Slavin, Kim-Kwang Raymond Choo, Glenn Dietrich, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing the promise of natural language interfaces to databases, prior
studies have emphasized the development of text-to-SQL systems. While
substantial progress has been made in this field, existing research has
concentrated on generating SQL statements from text queries. The broader
challenge, however, lies in inferring new information about the returned data.
Our research makes two major contributions to address this gap. First, we
introduce a novel Internet-of-Things (IoT) text-to-SQL dataset comprising
10,985 text-SQL pairs and 239,398 rows of network traffic activity. The dataset
contains additional query types limited in prior text-to-SQL datasets, notably
temporal-related queries. Our dataset is sourced from a smart building's IoT
ecosystem exploring sensor read and network traffic data. Second, our dataset
allows two-stage processing, where the returned data (network traffic) from a
generated SQL can be categorized as malicious or not. Our results show that
joint training to query and infer information about the data can improve
overall text-to-SQL performance, nearly matching substantially larger models.
We also show that current large language models (e.g., GPT3.5) struggle to
infer new information about returned data, thus our dataset provides a novel
test bed for integrating complex domain-specific reasoning into LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating
  Toxicity in French Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Brun, Vassilina Nikoulina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly popular but are also prone to
generating bias, toxic or harmful language, which can have detrimental effects
on individuals and communities. Although most efforts is put to assess and
mitigate toxicity in generated content, it is primarily concentrated on
English, while it's essential to consider other languages as well. For
addressing this issue, we create and release FrenchToxicityPrompts, a dataset
of 50K naturally occurring French prompts and their continuations, annotated
with toxicity scores from a widely used toxicity classifier. We evaluate 14
different models from four prevalent open-sourced families of LLMs against our
dataset to assess their potential toxicity across various dimensions. We hope
that our contribution will foster future research on toxicity detection and
mitigation beyond Englis
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TRAC-2024, Fourth Workshop on Threat, Aggression and Cyberbullying.
  20 May 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-property Steering of <span class="highlight-title">Large Language Models</span> with Dynamic Activation
  Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Scalena, Gabriele Sarti, Malvina Nissim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation steering methods were shown to be effective in conditioning
language model generation by additively intervening over models' intermediate
representations. However, the evaluation of these techniques has so far been
limited to single conditioning properties and synthetic settings. In this work,
we conduct a comprehensive evaluation of various activation steering
strategies, highlighting the property-dependent nature of optimal parameters to
ensure a robust effect throughout generation. To address this issue, we propose
Dynamic Activation Composition, an information-theoretic approach to modulate
the steering intensity of one or more properties throughout generation. Our
experiments on multi-property steering show that our method successfully
maintains high conditioning while minimizing the impact of conditioning on
generation fluency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The FineWeb <span class="highlight-title">Dataset</span>s: Decanting the Web for the Finest Text Data at
  Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of a large language model (LLM) depends heavily on the
quality and size of its pretraining dataset. However, the pretraining datasets
for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly
available and very little is known about how they were created. In this work,
we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl
snapshots that produces better-performing LLMs than other open pretraining
datasets. To advance the understanding of how best to curate high-quality
pretraining datasets, we carefully document and ablate all of the design
choices used in FineWeb, including in-depth investigations of deduplication and
filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion
token collection of educational text filtered from FineWeb. LLMs pretrained on
FineWeb-Edu exhibit dramatically better performance on knowledge- and
reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we
publicly release our data curation codebase and all of the models trained
during our ablation experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Retrieval-Augmented</span> Code Generation for Situated Action Generation: A
  Case Study on Minecraft 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Minecraft Collaborative Building Task, two players collaborate: an
Architect (A) provides instructions to a Builder (B) to assemble a specified
structure using 3D blocks. In this work, we investigate the use of large
language models (LLMs) to predict the sequence of actions taken by the Builder.
Leveraging LLMs' in-context learning abilities, we use few-shot prompting
techniques, that significantly improve performance over baseline methods.
Additionally, we present a detailed analysis of the gaps in performance for
future work
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDQuant: Accurate Post-training Weight Quantization of Large Pre-trained
  Models using Greedy Coordinate Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Ajit Nair, Arun Sai Suggala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently demonstrated remarkable
performance across diverse language tasks. But their deployment is often
constrained by their substantial computational and storage requirements.
Quantization has emerged as a key technique for addressing this challenge,
enabling the compression of large models with minimal impact on performance.
The recent GPTQ algorithm, a post-training quantization (PTQ) method, has
proven highly effective for compressing LLMs, sparking a wave of research that
leverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the
PTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ
with improved performance. CDQuant uses coordinate descent to minimize the
layer-wise reconstruction loss to achieve high-quality quantized weights. Our
algorithm is easy to implement and scales efficiently to models with hundreds
of billions of parameters. Through extensive evaluation on the PaLM2 model
family, we demonstrate that CDQuant consistently outperforms GPTQ across
diverse model sizes and quantization levels. In particular, for INT2
quantization of PaLM2-Otter, CDQuant achieves a 10% reduction in perplexity
compared to GPTQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disce aut Deficere: Evaluating <span class="highlight-title">LLM</span>s Proficiency on the INVALSI Italian
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Mercorio, Mario Mezzanzanica, Daniele Potertì, Antonio Serino, Andrea Seveso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have significantly
enhanced their ability to generate and manipulate human language, highlighting
their potential across various applications. Evaluating LLMs in languages other
than English is crucial for ensuring their linguistic versatility, cultural
relevance, and applicability in diverse global contexts, thus broadening their
usability and effectiveness. We tackle this challenge by introducing a
structured benchmark using the INVALSI tests, a set of well-established
assessments designed to measure educational competencies across Italy. Our
study makes three primary contributions: Firstly, we adapt the INVALSI
benchmark for automated LLM evaluation, which involves rigorous adaptation of
the test format to suit automated processing while retaining the essence of the
original tests. Secondly, we provide a detailed assessment of current LLMs,
offering a crucial reference point for the academic community. Finally, we
visually compare the performance of these models against human results.
Additionally, researchers are invited to submit their models for ongoing
evaluation, ensuring the benchmark remains a current and valuable resource.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-style In-Context Learning for Few-shot Hierarchical Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyao Chen, Yu Zhao, Zulong Chen, Mengjia Wang, Liangyue Li, Meishan Zhang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical text classification (HTC) is an important task with broad
applications, while few-shot HTC has gained increasing interest recently. While
in-context learning (ICL) with large language models (LLMs) has achieved
significant success in few-shot learning, it is not as effective for HTC
because of the expansive hierarchical label sets and extremely-ambiguous
labels. In this work, we introduce the first ICL-based framework with LLM for
few-shot HTC. We exploit a retrieval database to identify relevant
demonstrations, and an iterative policy to manage multi-layer hierarchical
labels. Particularly, we equip the retrieval database with HTC label-aware
representations for the input texts, which is achieved by continual training on
a pretrained language model with masked language modeling (MLM), layer-wise
classification (CLS, specifically for HTC), and a novel divergent contrastive
learning (DCL, mainly for adjacent semantically-similar labels) objective.
Experimental results on three benchmark datasets demonstrate superior
performance of our method, and we can achieve state-of-the-art results in
few-shot HTC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can <span class="highlight-title">Large Language Models</span> Understand DL-Lite Ontologies? An Empirical
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyu Wang, Guilin Qi, Jiaqi Li, Songlin Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown significant achievements in solving a
wide range of tasks. Recently, LLMs' capability to store, retrieve and infer
with symbolic knowledge has drawn a great deal of attention, showing their
potential to understand structured information. However, it is not yet known
whether LLMs can understand Description Logic (DL) ontologies. In this work, we
empirically analyze the LLMs' capability of understanding DL-Lite ontologies
covering 6 representative tasks from syntactic and semantic aspects. With
extensive experiments, we demonstrate both the effectiveness and limitations of
LLMs in understanding DL-Lite ontologies. We find that LLMs can understand
formal syntax and model-theoretic semantics of concepts and roles. However,
LLMs struggle with understanding TBox NI transitivity and handling ontologies
with large ABoxes. We hope that our experiments and analyses provide more
insights into LLMs and inspire to build more faithful knowledge engineering
solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LumberChunker: Long-Form Narrative Document Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André V. Duarte, João Marques, Miguel Graça, Miguel Freire, Lei Li, Arlindo L. Oliveira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern NLP tasks increasingly rely on dense retrieval methods to access
up-to-date and relevant contextual information. We are motivated by the premise
that retrieval benefits from segments that can vary in size such that a
content's semantic independence is better captured. We propose LumberChunker, a
method leveraging an LLM to dynamically segment documents, which iteratively
prompts the LLM to identify the point within a group of sequential passages
where the content begins to shift. To evaluate our method, we introduce
GutenQA, a benchmark with 3000 "needle in a haystack" type of question-answer
pairs derived from 100 public domain narrative books available on Project
Gutenberg. Our experiments show that LumberChunker not only outperforms the
most competitive baseline by 7.37% in retrieval performance (DCG@20) but also
that, when integrated into a RAG pipeline, LumberChunker proves to be more
effective than other chunking methods and competitive baselines, such as the
Gemini 1.5M Pro. Our Code and Data are available at
https://github.com/joaodsmarques/LumberChunker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropy-Based Decoding for <span class="highlight-title">Retrieval-Augmented</span> <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmenting Large Language Models (LLMs) with retrieved external knowledge has
proven effective for improving the factual accuracy of generated responses.
Despite their success, retrieval-augmented LLMs still face the distractibility
issue, where the generated responses are negatively influenced by noise from
both external and internal knowledge sources. In this paper, we introduce a
novel, training-free decoding method guided by entropy considerations to
mitigate this issue. Our approach utilizes entropy-based document-parallel
ensemble decoding to prioritize low-entropy distributions from retrieved
documents, thereby enhancing the extraction of relevant information of context.
Additionally, it incorporates a contrastive decoding mechanism that contrasts
the obtained low-entropy ensemble distribution with the high-entropy
distribution derived from the model's internal knowledge across layers, which
ensures a greater emphasis on reliable external information. Extensive
experiments on open-domain question answering datasets demonstrate the
superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Mental State Representations in Language Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While numerous works have assessed the generative performance of language
models (LMs) on tasks requiring Theory of Mind reasoning, research into the
models' internal representation of mental states remains limited. Recent work
has used probing to demonstrate that LMs can represent beliefs of themselves
and others. However, these claims are accompanied by limited evaluation, making
it difficult to assess how mental state representations are affected by model
design and training choices. We report an extensive benchmark with various LM
types with different model sizes, fine-tuning approaches, and prompt designs to
study the robustness of mental state representations and memorisation issues
within the probes. Our results show that the quality of models' internal
representations of the beliefs of others increases with model size and, more
crucially, with fine-tuning. We are the first to study how prompt variations
impact probing performance on theory of mind tasks. We demonstrate that models'
representations are sensitive to prompt variations, even when such variations
should be beneficial. Finally, we complement previous activation editing
experiments on Theory of Mind tasks and show that it is possible to improve
models' reasoning performance by steering their activations without the need to
train any probe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024 Workshop on Mechanistic Interpretability</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedCare: Advancing Medical <span class="highlight-title">LLM</span>s through Decoupling Clinical Alignment
  and Knowledge Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Liao, Shuyang Jiang, Yanfeng Wang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown substantial progress in natural
language understanding and generation, proving valuable especially in the
medical field. Despite advancements, challenges persist due to the complexity
and diversity inherent in medical tasks, which can be categorized as
knowledge-intensive tasks and alignment-required tasks. Previous approaches
either ignore the latter task or focus on a minority of tasks and hence lose
generalization. To address these drawbacks, we propose a progressive
fine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise
aggregator to encode diverse knowledge in the first stage and filter out
detrimental information. In the second stage, we drop the Noise Aggregator to
avoid the interference of suboptimal representation and leverage an additional
alignment module optimized towards an orthogonal direction to the knowledge
space to mitigate knowledge forgetting. Based on this two-stage paradigm, we
proposed a Medical LLM through decoupling Clinical Alignment and Knowledge
Aggregation (MedCare), which is designed to achieve state-of-the-art (SOTA)
performance on over 20 medical tasks, as well as SOTA results on specific
medical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all
demonstrate significant improvements over existing models with similar model
sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer-based Named Entity Recognition with Combined Data
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michał Marcińczuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines transformer-based models and their effectiveness in named
entity recognition tasks. The study investigates data representation
strategies, including single, merged, and context, which respectively use one
sentence, multiple sentences, and sentences joined with attention to context
per vector. Analysis shows that training models with a single strategy may lead
to poor performance on different data representations. To address this
limitation, the study proposes a combined training procedure that utilizes all
three strategies to improve model stability and adaptability. The results of
this approach are presented and discussed for four languages (English, Polish,
Czech, and German) across various datasets, demonstrating the effectiveness of
the combined strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Tool Retrieval with Iterative Feedback from Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiancheng Xu, Yongqi Li, Heming Xia, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool learning aims to enhance and expand large language models' (LLMs)
capabilities with external tools, which has gained significant attention
recently. Current methods have shown that LLMs can effectively handle a certain
amount of tools through in-context learning or fine-tuning. However, in
real-world scenarios, the number of tools is typically extensive and
irregularly updated, emphasizing the necessity for a dedicated tool retrieval
component. Tool retrieval is nontrivial due to the following challenges: 1)
complex user instructions and tool descriptions; 2) misalignment between tool
retrieval and tool usage models. To address the above issues, we propose to
enhance tool retrieval with iterative feedback from the large language model.
Specifically, we prompt the tool usage model, i.e., the LLM, to provide
feedback for the tool retriever model in multi-round, which could progressively
improve the tool retriever's understanding of instructions and tools and reduce
the gap between the two standalone components. We build a unified and
comprehensive benchmark to evaluate tool retrieval models. The extensive
experiments indicate that our proposed approach achieves advanced performance
in both in-domain evaluation and out-of-domain evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Grammatical Error Correction via Contextual Data Augmentation <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Baoxin Wang, Yijun Liu, Qingfu Zhu, Dayong Wu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, data augmentation through synthetic data has been widely used in
the field of Grammatical Error Correction (GEC) to alleviate the problem of
data scarcity. However, these synthetic data are mainly used in the
pre-training phase rather than the data-limited fine-tuning phase due to
inconsistent error distribution and noisy labels. In this paper, we propose a
synthetic data construction method based on contextual augmentation, which can
ensure an efficient augmentation of the original data with a more consistent
error distribution. Specifically, we combine rule-based substitution with
model-based generation, using the generative model to generate a richer context
for the extracted error patterns. Besides, we also propose a relabeling-based
data cleaning method to mitigate the effects of noisy labels in synthetic data.
Experiments on CoNLL14 and BEA19-Test show that our proposed augmentation
method consistently and substantially outperforms strong baselines and achieves
the state-of-the-art level with only a few synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as Findings of ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Ask Informative Questions: Enhancing <span class="highlight-title">LLM</span>s with Preference
  Optimization and Expected Information Gain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Mazzaccara, Alberto Testoni, Raffaella Bernardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Questions are essential tools for acquiring the necessary information to
complete information-seeking tasks. However, large language models (LLMs),
especially open-source models, often perform poorly in generating informative
questions, as measured by expected information gain (EIG). In this paper, we
propose a method to enhance the informativeness of LLM-generated questions in
20-question game dialogues. We sample multiple questions from the same model
(LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG
questions to apply a Direct Preference Optimization (DPO) algorithm. Our
results show that this method produces more effective questions (in terms of
EIG), even in domains different from those used to train the DPO model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Probing Speech-Specific Risks in Large Multimodal Models: A
  Taxonomy, Benchmark, and Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Multimodal Models (LMMs) have achieved great success recently,
demonstrating a strong capability to understand multimodal information and to
interact with human users. Despite the progress made, the challenge of
detecting high-risk interactions in multimodal settings, and in particular in
speech modality, remains largely unexplored. Conventional research on risk for
speech modality primarily emphasises the content (e.g., what is captured as
transcription). However, in speech-based interactions, paralinguistic cues in
audio can significantly alter the intended meaning behind utterances. In this
work, we propose a speech-specific risk taxonomy, covering 8 risk categories
under hostility (malicious sarcasm and threats), malicious imitation (age,
gender, ethnicity), and stereotypical biases (age, gender, ethnicity). Based on
the taxonomy, we create a small-scale dataset for evaluating current LMMs
capability in detecting these categories of risk. We observe even the latest
models remain ineffective to detect various paralinguistic-specific risks in
speech (e.g., Gemini 1.5 Pro is performing only slightly above random
baseline). Warning: this paper contains biased and offensive examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leave No Document Behind: Benchmarking Long-Context <span class="highlight-title">LLM</span>s with Extended
  Multi-Doc QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context modeling capabilities have garnered widespread attention,
leading to the emergence of Large Language Models (LLMs) with ultra-context
windows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually
catching up. However, existing benchmarks employ irrelevant noise texts to
artificially extend the length of test cases, diverging from the real-world
scenarios of long-context applications. To bridge this gap, we propose a novel
long-context benchmark, Loong, aligning with realistic scenarios through
extended multi-document question answering (QA). Unlike typical document QA, in
Loong's test cases, each document is relevant to the final answer, ignoring any
document will lead to the failure of the answer. Furthermore, Loong introduces
four types of tasks with a range of context lengths: Spotlight Locating,
Comparison, Clustering, and Chain of Reasoning, to facilitate a more realistic
and comprehensive evaluation of long-context understanding. Extensive
experiments indicate that existing long-context language models still exhibit
considerable potential for enhancement. Retrieval augmented generation (RAG)
achieves poor performance, demonstrating that Loong can reliably assess the
model's long-context modeling capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We release our code and data publicly at
  https://github.com/MozerWang/Loong</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variable Layer-Wise Quantization: A Simple and Effective Approach to
  Quantize <span class="highlight-title">LLM</span>s <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul-Ioan Clotan, Sathwik Tejaswi Madhusudhan, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple variable quantization approach that quantizes different
layers of a large language model (LLM) at different bit levels. Specifically,
we quantize the most important layers to higher bit precision and less
important layers to lower bits to achieve floating point quantization levels.
We propose two effective strategies to measure the importance of layers within
LLMs: the first measures the importance of a layer based on how different its
output embeddings are from the input embeddings (the higher the better); the
second estimates the importance of a layer using the number of layer weights
that are much larger than average (the smaller the better). We show that
quantizing different layers at varying bits according to our importance scores
results in minimal performance drop with a far more compressed model size.
Finally, we present several practical key takeaways from our variable
layer-wise quantization experiments: (a) LLM performance under variable
quantization remains close to the original model until 25-50% of layers are
moved in lower quantization using our proposed ordering but only until 5-10% if
moved using no specific ordering; (b) Quantizing LLMs to lower bits performs
substantially better than pruning unless extreme quantization (2-bit) is used;
and (c) Layer-wise quantization to lower bits works better in the case of
larger LLMs with more layers compared to smaller LLMs with fewer layers. The
code used to run the experiments is available at:
https://github.com/RazvanDu/LayerwiseQuant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to EMNLP, 15 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make Some Noise: Unlocking Language Model Parallel Inference Capability
  through Noisy Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Xianzhen Luo, Fuxuan Wei, Yijun Liu, Qingfu Zhu, Xuanyu Zhang, Qing Yang, Dongliang Xu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing speculative decoding methods typically require additional model
structure and training processes to assist the model for draft token
generation. This makes the migration of acceleration methods to the new model
more costly and more demanding on device memory. To address this problem, we
propose the Make Some Noise (MSN) training framework as a replacement for the
supervised fine-tuning stage of the large language model. The training method
simply introduces some noise at the input for the model to learn the denoising
task. It significantly enhances the parallel decoding capability of the model
without affecting the original task capability. In addition, we propose a
tree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further
improve the inference speed of MSN models. Experiments in both the general and
code domains have shown that MSN can improve inference speed by 2.3-2.7x times
without compromising model performance. The MSN model also achieves comparable
acceleration ratios to the SOTA model with additional model structure on
Spec-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Native Design Bias: Studying the Impact of English Nativeness on
  Language Model Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manon Reusens, Philipp Borchert, Jochen De Weerdt, Bart Baesens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel at providing information acquired during
pretraining on large-scale corpora and following instructions through user
prompts. This study investigates whether the quality of LLM responses varies
depending on the demographic profile of users. Considering English as the
global lingua franca, along with the diversity of its dialects among speakers
of different native languages, we explore whether non-native English speakers
receive lower-quality or even factually incorrect responses from LLMs more
frequently. Our results show that performance discrepancies occur when LLMs are
prompted by native versus non-native English speakers and persist when
comparing native speakers from Western countries with others. Additionally, we
find a strong anchoring effect when the model recognizes or is made aware of
the user's nativeness, which further degrades the response quality when
interacting with non-native speakers. Our analysis is based on a newly
collected dataset with over 12,000 unique annotations from 124 annotators,
including information on their native language and English proficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Text is Worth Several Tokens: Text Embedding from <span class="highlight-title">LLM</span>s Secretly Aligns
  Well with The Key Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Nie, Richong Zhang, Zhanyu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings from large language models (LLMs) have achieved excellent
results in tasks such as information retrieval, semantic textual similarity,
etc. In this work, we show an interesting finding: when feeding a text into the
embedding LLMs, the obtained text embedding will be able to be aligned with the
key tokens in the input text. We first fully analyze this phenomenon on eight
embedding LLMs and show that this phenomenon is universal and is not affected
by model architecture, training strategy, and embedding method. With a deeper
analysis, we then find that the main change in embedding space between the
embedding LLMs and their original generative LLMs is in the first principal
component. By adjusting the first principal component, we can align text
embedding with the key tokens. Finally, we give several examples to demonstrate
the vast application potential of this finding: (1) we propose a simple and
practical sparse retrieval method based on the aligned tokens, which can
achieve 80\% of the dense retrieval effect of the same model while reducing the
computation significantly; (2) we show that our findings provide a fresh
perspective to help understand fuzzy concepts (e.g., semantic relatedness vs.
semantic similarity) and emerging technologies (e.g., instruction-following
embedding) in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Singh, Amrith Krishna, Karthika NJ, Ganesh Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource languages, by its very definition, tend to be under represented
in the pre-training corpora of Large Language Models. In this work, we
investigate three low-resource cross-lingual approaches that enable an LLM
adapt to tasks in previously unseen languages. Llama-2 is an LLM where Indic
languages, among many other language families, contribute to less than
$0.005\%$ of the total $2$ trillion token pre-training corpora. In this work,
we experiment with the English-dominated Llama-2 for cross-lingual transfer to
three Indic languages, Bengali, Hindi, and Tamil as target languages. We study
three approaches for cross-lingual transfer, under ICL and fine-tuning. One, we
find that adding additional supervisory signals via a dominant language in the
LLM, leads to improvements, both under in-context learning and fine-tuning.
Two, adapting the target languages to word reordering may be beneficial under
ICL, but its impact diminishes with fine tuning. Finally, continued
pre-training in one low-resource language can improve model performance for
other related low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study on the Characteristics of Bias upon Context Length
  Variation for Bangla <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayanta Sadhu, Ayan Antik Khan, Abhik Bhattacharjee, Rifat Shahriyar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models inherently exhibit various social biases,
prompting a crucial examination of their social impact across various
linguistic contexts due to their widespread usage. Previous studies have
provided numerous methods for intrinsic bias measurements, predominantly
focused on high-resource languages. In this work, we aim to extend these
investigations to Bangla, a low-resource language. Specifically, in this study,
we (1) create a dataset for intrinsic gender bias measurement in Bangla, (2)
discuss necessary adaptations to apply existing bias measurement methods for
Bangla, and (3) examine the impact of context length variation on bias
measurement, a factor that has been overlooked in previous studies. Through our
experiments, we demonstrate a clear dependency of bias metrics on context
length, highlighting the need for nuanced considerations in Bangla bias
analysis. We consider our work as a stepping stone for bias measurement in the
Bangla Language and make all of our resources publicly available to support
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Findings of ACL, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leve<span class="highlight-title">rag</span>ing Synthetic Audio Data for End-to-End Low-Resource Speech
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasmin Moslem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes our system submission to the International Conference on
Spoken Language Translation (IWSLT 2024) for Irish-to-English speech
translation. We built end-to-end systems based on Whisper, and employed a
number of data augmentation techniques, such as speech back-translation and
noise augmentation. We investigate the effect of using synthetic audio data and
discuss several methods for enriching signal diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Space Knowledge Distillation for <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen, Jinan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) is known as a promising solution to compress
large language models (LLMs) via transferring their knowledge to smaller
models. During this process, white-box KD methods usually minimize the distance
between the output distributions of the two models so that more knowledge can
be transferred. However, in the current white-box KD framework, the output
distributions are from the respective output spaces of the two models, using
their own prediction heads. We argue that the space discrepancy will lead to
low similarity between the teacher model and the student model on both
representation and distribution levels. Furthermore, this discrepancy also
hinders the KD process between models with different vocabularies, which is
common for current LLMs. To address these issues, we propose a dual-space
knowledge distillation (DSKD) framework that unifies the output spaces of the
two models for KD. On the basis of DSKD, we further develop a cross-model
attention mechanism, which can automatically align the representations of the
two models with different vocabularies. Thus, our framework is not only
compatible with various distance functions for KD (e.g., KL divergence) like
the current framework, but also supports KD between any two LLMs regardless of
their vocabularies. Experiments on task-agnostic instruction-following
benchmarks show that DSKD significantly outperforms the current white-box KD
framework with various distance functions, and also surpasses existing KD
methods for LLMs with different vocabularies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures, code available at:
  https://github.com/songmzhang/DSKD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delving into the Utilisation of ChatGPT in Scientific Publications in
  Astronomy <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Astarita, Sandor Kruk, Jan Reerink, Pablo Gómez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid progress in the capabilities of machine learning approaches in natural
language processing has culminated in the rise of large language models over
the last two years. Recent works have shown unprecedented adoption of these for
academic writing, especially in some fields, but their pervasiveness in
astronomy has not been studied sufficiently. To remedy this, we extract words
that ChatGPT uses more often than humans when generating academic text and
search a total of 1 million articles for them. This way, we assess the
frequency of word occurrence in published works in astronomy tracked by the
NASA Astrophysics Data System since 2000. We then perform a statistical
analysis of the occurrences. We identify a list of words favoured by ChatGPT
and find a statistically significant increase for these words against a control
group in 2024, which matches the trend in other disciplines. These results
suggest a widespread adoption of these models in the writing of astronomy
papers. We encourage organisations, publishers, and researchers to work
together to identify ethical and pragmatic guidelines to maximise the benefits
of these systems while maintaining scientific rigour.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to SPAICE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Preference Pairs Are Created Equal: A Recipe for
  Annotation-Efficient Iterative Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Yang, Leyang Cui, Deng Cai, Xinting Huang, Shuming Shi, Wai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Iterative preference learning, though yielding superior performances,
requires online annotated preference labels. In this work, we study strategies
to select worth-annotating response pairs for cost-efficient annotation while
achieving competitive or even better performances compared with the random
selection baseline for iterative preference learning. Built on assumptions
regarding uncertainty and distribution shifts, we propose a comparative view to
rank the implicit reward margins as predicted by DPO to select the response
pairs that yield more benefits. Through extensive experiments, we show that
annotating those response pairs with small margins is generally better than
large or random, under both single- and multi-iteration scenarios. Besides, our
empirical results suggest allocating more annotation budgets in the earlier
iterations rather than later across multiple iterations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Retrieval Augmented</span> Instruction Tuning for Open NER with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Xie, Jian Zhang, Yan Zhang, Yuanyuan Liang, Qi Li, Hongwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The strong capability of large language models (LLMs) has been applied to
information extraction (IE) through either retrieval augmented prompting or
instruction tuning (IT). However, the best way to incorporate information with
LLMs for IE remains an open question. In this paper, we explore Retrieval
Augmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named
entity recognition (NER). Specifically, for each training sample, we retrieve
semantically similar examples from the training dataset as the context and
prepend them to the input of the original instruction. To evaluate our RA-IT
approach more thoroughly, we construct a Chinese IT dataset for open NER and
evaluate RA-IT in both English and Chinese scenarios. Experimental results
verify the effectiveness of RA-IT across various data sizes and in both English
and Chinese scenarios. We also conduct thorough studies to explore the impacts
of various retrieval strategies in the proposed RA-IT framework. Code and data
are available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leve<span class="highlight-title">rag</span>ing <span class="highlight-title">LLM</span>s for Dialogue Quality Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghan Jia, Abi Komma, Timothy Leffel, Xujun Peng, Ajay Nagesh, Tamer Soliman, Aram Galstyan, Anoop Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In task-oriented conversational AI evaluation, unsupervised methods poorly
correlate with human judgments, and supervised approaches lack generalization.
Recent advances in large language models (LLMs) show robust zeroshot and
few-shot capabilities across NLP tasks. This paper explores using LLMs for
automated dialogue quality evaluation, experimenting with various
configurations on public and proprietary datasets. Manipulating factors such as
model size, in-context examples, and selection techniques, we examine
"chain-of-thought" (CoT) reasoning and label extraction procedures. Our results
show that (1) larger models yield more accurate dialogue labels; (2)
algorithmic selection of in-context examples outperforms random selection; (3)
CoT reasoning where an LLM is asked to provide justifications before outputting
final labels improves performance; and (4) fine-tuned LLMs outperform
out-of-the-box ones. Our results indicate that LLMs that are suitably
fine-tuned and have sufficient reasoning capabilities can be leveraged for
automated dialogue evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CausalScore: An Automatic Reference-Free Metric for Assessing Response
  Relevance in Open-Domain Dialogue Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Feng, Lizhen Qu, Xiaoxi Kang, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically evaluating the quality of responses in open-domain dialogue
systems is a challenging but crucial task. Current evaluation metrics often
fail to align with human judgments, especially when assessing responses that
are grammatically correct. To address this issue, we propose a novel metric,
called CausalScore, which assesses the relevance of responses by measuring the
causal strength between dialogue histories and responses. The causal strength
is estimated by utilizing both unconditional dependence and conditional
dependencies from the dialogue history to responses. We compare our metric with
the existing competitive metrics in terms of their alignment with human
judgements. Our experimental results demonstrate that CausalScore significantly
surpasses existing state-of-the-art metrics by aligning better with human
judgements. Additionally, we collect a new dialogue dataset CGDIALOG+ with
human-annotated causal relations and a set of pairwise human judgements to
facilitate the development of future automatic metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive reasoning
capabilities, particularly in textual mathematical problem-solving. However,
existing open-source image instruction fine-tuning datasets, containing limited
question-answer pairs per image, do not fully exploit visual information to
enhance the multimodal mathematical reasoning capabilities of Multimodal LLMs
(MLLMs). To bridge this gap, we address the lack of high-quality, diverse
multimodal mathematical datasets by collecting 40K high-quality images with
question-answer pairs from 24 existing datasets and synthesizing 320K new
pairs, creating the MathV360K dataset, which enhances both the breadth and
depth of multimodal mathematical questions. We introduce Math-LLaVA, a
LLaVA-1.5-based model fine-tuned with MathV360K. This novel approach
significantly improves the multimodal mathematical reasoning capabilities of
LLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V
on MathVista's minitest split. Furthermore, Math-LLaVA demonstrates enhanced
generalizability, showing substantial improvements on the MMMU benchmark. Our
research highlights the importance of dataset diversity and synthesis in
advancing MLLMs' mathematical reasoning abilities. The code and data are
available at: \url{https://github.com/HZQ950419/Math-LLaVA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting the Big Five Personality Traits in Chinese Counselling
  Dialogues Using <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yan, Lizhi Ma, Anqi Li, Jingsong Ma, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate assessment of personality traits is crucial for effective
psycho-counseling, yet traditional methods like self-report questionnaires are
time-consuming and biased. This study exams whether Large Language Models
(LLMs) can predict the Big Five personality traits directly from counseling
dialogues and introduces an innovative framework to perform the task. Our
framework applies role-play and questionnaire-based prompting to condition LLMs
on counseling sessions, simulating client responses to the Big Five Inventory.
We evaluated our framework on 853 real-world counseling sessions, finding a
significant correlation between LLM-predicted and actual Big Five traits,
proving the validity of framework. Moreover, ablation studies highlight the
importance of role-play simulations and task simplification via questionnaires
in enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model,
utilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves
a 130.95\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\%
in personality prediction validity. In conclusion, LLMs can predict personality
based on counseling dialogues. Our code and model are publicly available at
\url{https://github.com/kuri-leo/BigFive-LLM-Predictor}, providing a valuable
tool for future research in computational psychometrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Recursive Encoding for Cuneiform Signs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel M. Stelzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most significant problems in cuneiform pedagogy is the process of
looking up unknown signs, which often involves a tedious page-by-page search
through a sign list. This paper proposes a new "recursive encoding" for signs,
which represents the arrangement of strokes in a way a computer can process. A
series of new algorithms then offers students a new way to look up signs by any
distinctive component, as well as providing new ways to render signs and
tablets electronically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 29 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BERT, Neural Information Retrieval, Boolean Retrieval, Negation
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Mai, Susan Gauch, Douglas Adams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SetBERT, a fine-tuned BERT-based model designed to enhance query
embeddings for set operations and Boolean logic queries, such as Intersection
(AND), Difference (NOT), and Union (OR). SetBERT significantly improves
retrieval performance for logic-structured queries, an area where both
traditional and neural retrieval methods typically underperform. We propose an
innovative use of inversed-contrastive loss, focusing on identifying the
negative sentence, and fine-tuning BERT with a dataset generated via prompt
GPT. Furthermore, we demonstrate that, unlike other BERT-based models,
fine-tuning with triplet loss actually degrades performance for this specific
task. Our experiments reveal that SetBERT-base not only significantly
outperforms BERT-base (up to a 63% improvement in Recall) but also achieves
performance comparable to the much larger BERT-large model, despite being only
one-third the size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jikai Wang, Yi Su, Juntao Li, Qinrong Xia, Zi Ye, Xinyu Duan, Zhefeng Wang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive language models demonstrate excellent performance in various
scenarios. However, the inference efficiency is limited by its
one-step-one-word generation mode, which has become a pressing problem recently
as the models become increasingly larger. Speculative decoding employs a "draft
and then verify" mechanism to allow multiple tokens to be generated in one
step, realizing lossless acceleration. Existing methods mainly adopt fixed
heuristic draft structures, which fail to adapt to different situations to
maximize the acceptance length during verification. To alleviate this dilemma,
we proposed OPT-Tree, an algorithm to construct adaptive and scalable draft
trees. It searches the optimal tree structure that maximizes the mathematical
expectation of the acceptance length in each decoding step. Experimental
results reveal that OPT-Tree outperforms the existing draft structures and
achieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.
If the draft model is powerful enough and the node budget is sufficient, it can
generate more than ten tokens in a single step. Our code is available at
https://github.com/Jikai0Wang/OPT-Tree.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Trust the Performance Evaluation of Uncertainty Estimation
  Methods in Text Summarization? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng He, Runing Yang, Linlin Yu, Changbin Li, Ruoxi Jia, Feng Chen, Ming Jin, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text summarization, a key natural language generation (NLG) task, is vital in
various domains. However, the high cost of inaccurate summaries in
risk-critical applications, particularly those involving human-in-the-loop
decision-making, raises concerns about the reliability of uncertainty
estimation on text summarization (UE-TS) evaluation methods. This concern stems
from the dependency of uncertainty model metrics on diverse and potentially
conflicting NLG metrics. To address this issue, we introduce a comprehensive
UE-TS benchmark incorporating 31 NLG metrics across four dimensions. The
benchmark evaluates the uncertainty estimation capabilities of two large
language models and one pre-trained language model on three datasets, with
human-annotation analysis incorporated where applicable. We also assess the
performance of 14 common uncertainty estimation methods within this benchmark.
Our findings emphasize the importance of considering multiple uncorrelated NLG
metrics and diverse uncertainty estimation methods to ensure reliable and
efficient evaluation of UE-TS techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages, 41 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DARG: Dynamic Evaluation of <span class="highlight-title">Large Language Models</span> via Adaptive Reasoning
  Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhehao Zhang, Jiaao Chen, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current paradigm of evaluating Large Language Models (LLMs) through
static benchmarks comes with significant limitations, such as vulnerability to
data contamination and a lack of adaptability to the evolving capabilities of
LLMs. Therefore, evaluation methods that can adapt and generate evaluation data
with controlled complexity are urgently needed. In this work, we introduce
Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to
dynamically extend current benchmarks with controlled complexity and diversity.
Specifically, we first extract the reasoning graphs of data points in current
benchmarks and then perturb the reasoning graphs to generate novel testing
data. Such newly generated test samples can have different levels of complexity
while maintaining linguistic diversity similar to the original benchmarks. We
further use a code-augmented LLM to ensure the label correctness of newly
generated data. We apply our DARG framework to diverse reasoning tasks in four
domains with 15 state-of-the-art LLMs. Experimental results show that almost
all LLMs experience a performance decrease with increased complexity and
certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit
more biases when being evaluated via the data generated by DARG with higher
complexity levels. These observations provide useful insights into how to
dynamically and adaptively evaluate LLMs. The code is available at
https://github.com/SALT-NLP/DARG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AG-LSEC: Audio Grounded Lexical Speaker Error Correction <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Paturi, Xiang Li, Sundararajan Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speaker Diarization (SD) systems are typically audio-based and operate
independently of the ASR system in traditional speech transcription pipelines
and can have speaker errors due to SD and/or ASR reconciliation, especially
around speaker turns and regions of speech overlap. To reduce these errors, a
Lexical Speaker Error Correction (LSEC), in which an external language model
provides lexical information to correct the speaker errors, was recently
proposed. Though the approach achieves good Word Diarization error rate (WDER)
improvements, it does not use any additional acoustic information and is prone
to miscorrections. In this paper, we propose to enhance and acoustically ground
the LSEC system with speaker scores directly derived from the existing SD
pipeline. This approach achieves significant relative WDER reductions in the
range of 25-40% over the audio-based SD, ASR system and beats the LSEC system
by 15-25% relative on RT03-CTS, Callhome American English and Fisher datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at INTERSPEECH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D2<span class="highlight-title">LLM</span>: Decomposed and Distilled <span class="highlight-title">Large Language Models</span> for Semantic
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liao, Hang Yu, Jianguo Li, Jun Wang, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key challenge in semantic search is to create models that are both
accurate and efficient in pinpointing relevant sentences for queries. While
BERT-style bi-encoders excel in efficiency with pre-computed embeddings, they
often miss subtle nuances in search tasks. Conversely, GPT-style LLMs with
cross-encoder designs capture these nuances but are computationally intensive,
hindering real-time applications. In this paper, we present D2LLMs-Decomposed
and Distilled LLMs for semantic search-that combines the best of both worlds.
We decompose a cross-encoder into an efficient bi-encoder integrated with
Pooling by Multihead Attention and an Interaction Emulation Module, achieving
nuanced understanding and pre-computability. Knowledge from the LLM is
distilled into this model using contrastive, rank, and feature imitation
techniques. Our experiments show that D2LLM surpasses five leading baselines in
terms of all metrics across three tasks, particularly improving NLI task
performance by at least 6.45%. The source code is available at
https://github.com/codefuse-ai/D2LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRAWL: Tensor Reduced and Approximated Weights for <span class="highlight-title">Large Language Models</span> <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Luo, Het Patel, Yu Fu, Dawon Ahn, Jia Chen, Yue Dong, Evangelos E. Papalexakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have fundamentally transformed artificial
intelligence, catalyzing recent advancements while imposing substantial
environmental and computational burdens. We introduce TRAWL (Tensor Reduced and
Approximated Weights for Large Language Models), a novel methodology for
optimizing LLMs through tensor decomposition. TRAWL leverages diverse
strategies to exploit matrices within transformer-based architectures,
realizing notable performance enhancements without necessitating retraining.
The most significant improvements were observed through a layer-by-layer
intervention strategy, particularly when applied to fully connected weights of
the final layers, yielding up to 16% enhancement in accuracy without the need
for additional data or fine-tuning. These results underscore the importance of
targeted and adaptive techniques in increasing the efficiency and effectiveness
of large language model optimization, thereby promoting the development of more
sustainable and accessible AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures. Submitted to EMNLP 2024 and under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucination in Fictional Character Role-Play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nafis Sadeq, Zhouhang Xie, Byungkyu Kang, Prarit Lamba, Xiang Gao, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Role-playing has wide-ranging applications in customer support, embodied
agents, computational social science, etc. The influence of parametric world
knowledge of large language models (LLMs) often causes role-playing characters
to act out of character and hallucinate about things outside the scope of their
knowledge. In this work, we focus on the evaluation and mitigation of
hallucination in fictional character role-play. We introduce a dataset with
more than 2,000 characters and 72,000 interviews, including 18,000 adversarial
questions. We propose RoleFact, a role-playing method that mitigates
hallucination by modulating the influence of parametric knowledge using a
pre-calibrated confidence threshold. Experiments show that the proposed method
improves the factual precision of generated responses by 18% for adversarial
questions with a 44% reduction in temporal hallucination for time-sensitive
interviews. The code and the dataset will be available at
https://github.com/NafisSadeq/rolefact.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leve<span class="highlight-title">rag</span>ing Parameter-Efficient Transfer Learning for Multi-Lingual
  Text-to-Speech Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingting Li, Ambuj Mehrish, Bryan Chew, Bo Cheng, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different languages have distinct phonetic systems and vary in their prosodic
features making it challenging to develop a Text-to-Speech (TTS) model that can
effectively synthesise speech in multilingual settings. Furthermore, TTS
architecture needs to be both efficient enough to capture nuances in multiple
languages and efficient enough to be practical for deployment. The standard
approach is to build transformer based model such as SpeechT5 and train it on
large multilingual dataset. As the size of these models grow the conventional
fine-tuning for adapting these model becomes impractical due to heavy
computational cost. In this paper, we proposes to integrate parameter-efficient
transfer learning (PETL) methods such as adapters and hypernetwork with TTS
architecture for multilingual speech synthesis. Notably, in our experiments
PETL methods able to achieve comparable or even better performance compared to
full fine-tuning with only $\sim$2.5\% tunable parameters.The code and samples
are available at: https://anonymous.4open.science/r/multilingualTTS-BA4C.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPCODER: Multi-user Personalized Code Generator with Explicit and
  Implicit Style Representation Learning <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenlong Dai, Chang Yao, WenKang Han, Ying Yuan, Zhipeng Gao, Jingyuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated great potential for assisting
developers in their daily development. However, most research focuses on
generating correct code, how to use LLMs to generate personalized code has
seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user
Personalized Code Generator) to generate personalized code for multiple users.
To better learn coding style features, we utilize explicit coding style
residual learning to capture the syntax code style standards and implicit style
learning to capture the semantic code style conventions. We train a multi-user
style adapter to better differentiate the implicit feature representations of
different users through contrastive learning, ultimately enabling personalized
code generation for multiple users. We further propose a novel evaluation
metric for estimating similarities between codes of different coding styles.
The experimental results show the effectiveness of our approach for this novel
task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2024, Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Well Can Knowledge Edit Methods Edit Perplexing Knowledge? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaizhi Ge, Frank Rudzicz, Zining Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are widely deployed, targeted editing of
their knowledge has become a critical challenge. Recently, advancements in
model editing techniques, such as Rank-One Model Editing (ROME), have paved the
way for updating LLMs with new knowledge. However, the efficacy of these
methods varies across different types of knowledge. This study investigates the
capability of knowledge editing methods to incorporate new knowledge with
varying degrees of "perplexingness", a term we use to describe the initial
difficulty LLMs have in understanding new concepts. We begin by quantifying the
"perplexingness" of target knowledge using pre-edit conditional probabilities,
and assess the efficacy of edits through post-edit conditional probabilities.
Utilizing the widely-used CounterFact dataset, we find significant negative
correlations between the "perplexingness" of the new knowledge and the edit
efficacy across all 12 scenarios. To dive deeper into this phenomenon, we
introduce a novel dataset, HierarchyData, consisting of 99 hyponym-hypernym
pairs across diverse categories. Our analysis reveal that more abstract
concepts (hypernyms) tend to be more perplexing than their specific
counterparts (hyponyms). Further exploration into the influence of knowledge
hierarchy on editing outcomes indicates that knowledge positioned at higher
hierarchical levels is more challenging to modify in some scenarios. Our
research highlights a previously overlooked aspect of LLM editing: the variable
efficacy of editing methods in handling perplexing knowledge. By revealing how
hierarchical relationships can influence editing outcomes, our findings offer
new insights into the challenges of updating LLMs and pave the way for more
nuanced approaches to model editing in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Continual Learning Abilities in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Du, Shuang Cheng, Tongxu Luo, Zihan Qiu, Zeyu Huang, Ka Chun Cheung, Reynold Cheng, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) exhibit impressive performance and generalization
capabilities. However, LMs struggle with the persistent challenge of
catastrophic forgetting, which undermines their long-term sustainability in
continual learning (CL). Existing approaches usually address the issue by
incorporating old task data or task-wise inductive bias into LMs. However, old
data and accurate task information are often unavailable or costly to collect,
hindering the availability of current CL approaches for LMs. To address this
limitation, we introduce $\textbf{MIGU}$ ($\textbf{M}$agn$\textbf{I}$tude-based
$\textbf{G}$radient $\textbf{U}$pdating for continual learning), a
rehearsal-free and task-label-free method that only updates the model
parameters with large magnitudes of output in LMs' linear layers. MIGU is based
on our observation that the L1-normalized magnitude distribution of the output
in LMs' linear layers is different when the LM models deal with different task
data. By imposing this simple constraint on the gradient update process, we can
leverage the inherent behaviors of LMs, thereby unlocking their innate CL
abilities. Our experiments demonstrate that MIGU is universally applicable to
all three LM architectures (T5, RoBERTa, and Llama2), delivering
state-of-the-art or on-par performance across continual finetuning and
continual pre-training settings on four CL benchmarks. For example, MIGU brings
a 15.2% average accuracy improvement over conventional parameter-efficient
finetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly
integrate with all three existing CL types to further enhance performance. Code
is available at \href{https://github.com/wenyudu/MIGU}{this https URL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Do the Circuits Mean? A Knowledge Edit View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huaizhi Ge, Frank Rudzicz, Zining Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of language model interpretability, circuit discovery is gaining
popularity. Despite this, the true meaning of these circuits remain largely
unanswered. We introduce a novel method to learn their meanings as a holistic
object through the lens of knowledge editing. We extract circuits in the
GPT2-XL model using diverse text classification datasets, and use hierarchical
relations datasets to explore knowledge editing in the circuits. Our findings
indicate that these circuits contain entity knowledge but resist new knowledge
more than complementary circuits during knowledge editing. Additionally, we
examine the impact of circuit size, discovering that an ideal "theoretical
circuit" where essential knowledge is concentrated likely incorporates more
than 5% but less than 50% of the model's parameters. We also assess the overlap
between circuits from different datasets, finding moderate similarities. What
constitutes these circuits, then? We find that up to 60% of the circuits
consist of layer normalization modules rather than attention or MLP modules,
adding evidence to the ongoing debates regarding knowledge localization. In
summary, our findings offer new insights into the functions of the circuits,
and introduce research directions for further interpretability and safety
research of language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Constructed Context Decompilation with Fined-grained Alignment
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Feng, Yang Xu, Dechuan Teng, Honglin Mu, Xiao Xu, Libo Qin, Wanxiang Che, Qingfu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decompilation transforms compiled code back into a high-level programming
language for analysis when source code is unavailable. Previous work has
primarily focused on enhancing decompilation performance by increasing the
scale of model parameters or training data for pre-training. Based on the
characteristics of the decompilation task, we propose two methods: (1) Without
fine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method
recompiles the LLM's decompilation results to construct pairs for in-context
learning, helping the model improve decompilation performance. (2) Fine-grained
Alignment Enhancement (FAE), which meticulously aligns assembly code with
source code at the statement level by leveraging debugging information, is
employed during the fine-tuning phase to achieve further improvements in
decompilation. By integrating these two methods, we achieved a Re-Executability
performance improvement of approximately 7.35\% on the Decompile-Eval
benchmark, establishing a new state-of-the-art performance of 55.03\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Demographics: Aligning Role-playing <span class="highlight-title">LLM</span>-based Agents Using Human
  Belief Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Shiuan Chuang, Zach Studdiford, Krirk Nirunwiroj, Agam Goyal, Vincent V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating human-like large language model (LLM) agents is crucial for faithful
social simulation. Having LLMs role-play based on demographic information
sometimes improves human likeness but often does not. This study assessed
whether LLM alignment with human behavior can be improved by integrating
information from empirically-derived human belief networks. Using data from a
human survey, we estimated a belief network encompassing 18 topics loading on
two non-overlapping latent factors. We then seeded LLM-based agents with an
opinion on one topic, and assessed the alignment of its expressed opinions on
remaining test topics with corresponding human data. Role-playing based on
demographic information alone did not align LLM and human opinions, but seeding
the agent with a single belief greatly improved alignment for topics related in
the belief network, and not for topics outside the network. These results
suggest a novel path for human-LLM belief alignment in work seeking to simulate
and understand patterns of belief distributions in society.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CogMG: Collaborative Augmentation Between <span class="highlight-title">Large Language Model</span> and
  Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have become integral to question-answering applications
despite their propensity for generating hallucinations and factually inaccurate
content. Querying knowledge graphs to reduce hallucinations in LLM meets the
challenge of incomplete knowledge coverage in knowledge graphs. On the other
hand, updating knowledge graphs by information extraction and knowledge graph
completion faces the knowledge update misalignment issue. In this work, we
introduce a collaborative augmentation framework, CogMG, leveraging knowledge
graphs to address the limitations of LLMs in QA scenarios, explicitly targeting
the problems of incomplete knowledge coverage and knowledge update
misalignment. The LLMs identify and decompose required knowledge triples that
are not present in the KG, enriching them and aligning updates with real-world
demands. We demonstrate the efficacy of this approach through a supervised
fine-tuned LLM within an agent framework, showing significant improvements in
reducing hallucinations and enhancing factual accuracy in QA responses. Our
code and video are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Models</span> are Interpretable Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Wang, Si Si, Felix Yu, Dorothea Wiesmann, Cho-Jui Hsieh, Inderjit Dhillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The trade-off between expressiveness and interpretability remains a core
challenge when building human-centric predictive models for classification and
decision-making. While symbolic rules offer interpretability, they often lack
expressiveness, whereas neural networks excel in performance but are known for
being black boxes. In this paper, we show a combination of Large Language
Models (LLMs) and symbolic programs can bridge this gap. In the proposed
LLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language
prompts provides a massive set of interpretable modules that can transform raw
input into natural language concepts. Symbolic programs then integrate these
modules into an interpretable decision rule. To train LSPs, we develop a
divide-and-conquer approach to incrementally build the program from scratch,
where the learning process of each step is guided by LLMs. To evaluate the
effectiveness of LSPs in extracting interpretable and accurate knowledge from
data, we introduce IL-Bench, a collection of diverse tasks, including both
synthetic and real-world scenarios across different modalities. Empirical
results demonstrate LSP's superior performance compared to traditional
neurosymbolic programs and vanilla automatic prompt tuning methods. Moreover,
as the knowledge learned by LSP is a combination of natural language
descriptions and symbolic rules, it is easily transferable to humans
(interpretable), and other LLMs, and generalizes well to out-of-distribution
samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary Version, Code at [this
  url](https://github.com/ruocwang/llm-symbolic-program)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence
  Cove<span class="highlight-title">rag</span>e <span class="chip">EMNLP 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isidora Chara Tourni, Lei Guo, Hengchang Hu, Edward Halim, Prakash Ishwar, Taufiq Daryanto, Mona Jalal, Boqi Chen, Margrit Betke, Fabian Zhafransyah, Sha Lai, Derry Tanti Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News media structure their reporting of events or issues using certain
perspectives.
  When describing an incident involving gun violence, for example, some
journalists may focus on mental health or gun regulation, while others may
emphasize the discussion of gun rights. Such perspectives are called
\say{frames} in communication research. We study, for the first time, the value
of combining lead images and their contextual information with text to identify
the frame of a given news article. We observe that using multiple modes of
information(article- and image-derived features) improves prediction of news
frames over any single mode of information when the images are relevant to the
frames of the headlines. We also observe that frame image relevance is related
to the ease of conveying frames via images, which we call frame concreteness.
Additionally, we release the first multimodal news framing dataset related to
gun violence in the U.S., curated and annotated by communication researchers.
The dataset will allow researchers to further examine the use of multiple
information modalities for studying media framing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at Findings of the Association for Computational
  Linguistics: EMNLP 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EDEN: Empathetic Dialogues for English learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Siyan, Teresa Shao, Zhou Yu, Julia Hirschberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue systems have been used as conversation partners in English learning,
but few have studied whether these systems improve learning outcomes. Student
passion and perseverance, or grit, has been associated with language learning
success. Recent work establishes that as students perceive their English
teachers to be more supportive, their grit improves. Hypothesizing that the
same pattern applies to English-teaching chatbots, we create EDEN, a robust
open-domain chatbot for spoken conversation practice that provides empathetic
feedback. To construct EDEN, we first train a specialized spoken utterance
grammar correction model and a high-quality social chit-chat conversation
model. We then conduct a preliminary user study with a variety of strategies
for empathetic feedback. Our experiment suggests that using adaptive empathetic
feedback leads to higher perceived affective support, which, in turn, predicts
increased student grit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inherent Challenges of Post-Hoc Membership Inference for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Meeus, Shubham Jain, Marek Rei, Yves-Alexandre de Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are often trained on vast amounts of undisclosed
data, motivating the development of post-hoc Membership Inference Attacks
(MIAs) to gain insight into their training data composition. However, in this
paper, we identify inherent challenges in post-hoc MIA evaluation due to
potential distribution shifts between collected member and non-member datasets.
Using a simple bag-of-words classifier, we demonstrate that datasets used in
recent post-hoc MIAs suffer from significant distribution shifts, in some cases
achieving near-perfect distinction between members and non-members. This
implies that previously reported high MIA performance may be largely
attributable to these shifts rather than model memorization. We confirm that
randomized, controlled setups eliminate such shifts and thus enable the
development and fair evaluation of new MIAs. However, we note that such
randomized setups are rarely available for the latest LLMs, making post-hoc
data collection still required to infer membership for real-world LLMs. As a
potential solution, we propose a Regression Discontinuity Design (RDD) approach
for post-hoc data collection, which substantially mitigates distribution
shifts. Evaluating various MIA methods on this RDD setup yields performance
barely above random guessing, in stark contrast to previously reported results.
Overall, our findings highlight the challenges in accurately measuring LLM
memorization and the need for careful experimental design in (post-hoc)
membership inference tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Fairness in Large Vision-Language Models Across Diverse
  Demographic Attributes and Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Wu, Yuan Wang, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have recently achieved significant
progress, demonstrating strong capabilities in open-world visual understanding.
However, it is not yet clear how LVLMs address demographic biases in real life,
especially the disparities across attributes such as gender, skin tone, and
age. In this paper, we empirically investigate \emph{visual fairness} in
several mainstream LVLMs and audit their performance disparities across
sensitive demographic attributes, based on public fairness benchmark datasets
(e.g., FACET). To disclose the visual bias in LVLMs, we design a fairness
evaluation framework with direct questions and single-choice
question-instructed prompts on visual question-answering/classification tasks.
The zero-shot prompting results indicate that, despite enhancements in visual
understanding, both open-source and closed-source LVLMs exhibit prevalent
fairness issues across different instruct prompts and demographic attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LABOR-<span class="highlight-title">LLM</span>: Language-Based Occupational Representations with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Du, Ayush Kanodia, Herman Brunborg, Keyon Vafa, Susan Athey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many empirical studies of labor market questions rely on estimating
relatively simple predictive models using small, carefully constructed
longitudinal survey datasets based on hand-engineered features. Large Language
Models (LLMs), trained on massive datasets, encode vast quantities of world
knowledge and can be used for the next job prediction problem. However, while
an off-the-shelf LLM produces plausible career trajectories when prompted, the
probability with which an LLM predicts a particular job transition conditional
on career history will not, in general, align with the true conditional
probability in a given population. Recently, Vafa et al. (2024) introduced a
transformer-based "foundation model", CAREER, trained using a large,
unrepresentative resume dataset, that predicts transitions between jobs; it
further demonstrated how transfer learning techniques can be used to leverage
the foundation model to build better predictive models of both transitions and
wages that reflect conditional transition probabilities found in nationally
representative survey datasets. This paper considers an alternative where the
fine-tuning of the CAREER foundation model is replaced by fine-tuning LLMs. For
the task of next job prediction, we demonstrate that models trained with our
approach outperform several alternatives in terms of predictive performance on
the survey data, including traditional econometric models, CAREER, and LLMs
with in-context learning, even though the LLM can in principle predict job
titles that are not allowed in the survey data. Further, we show that our
fine-tuned LLM-based models' predictions are more representative of the career
trajectories of various workforce subpopulations than off-the-shelf LLM models
and CAREER. We conduct experiments and analyses that highlight the sources of
the gains in the performance of our models for representative predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encou<span class="highlight-title">rag</span>e or Inhibit Monosemanticity? Revisit Monosemanticity from a
  Feature Decorrelation Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqi Yan, Yanzheng Xiang, Guangyi Chen, Yifei Wang, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To better interpret the intrinsic mechanism of large language models (LLMs),
recent studies focus on monosemanticity on its basic units. A monosemantic
neuron is dedicated to a single and specific concept, which forms a one-to-one
correlation between neurons and concepts. Despite extensive research in
monosemanticity probing, it remains unclear whether monosemanticity is
beneficial or harmful to model capacity. To explore this question, we revisit
monosemanticity from the feature decorrelation perspective and advocate for its
encouragement. We experimentally observe that the current conclusion by
wang2024learning, which suggests that decreasing monosemanticity enhances model
performance, does not hold when the model changes. Instead, we demonstrate that
monosemanticity consistently exhibits a positive correlation with model
capacity, in the preference alignment process. Consequently, we apply feature
correlation as a proxy for monosemanticity and incorporate a feature
decorrelation regularizer into the dynamic preference optimization process. The
experiments show that our method not only enhances representation diversity and
activation sparsity but also improves preference alignment performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unmasking the Imposters: In-Domain Detection of Human vs.
  Machine-Generated Tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan E. Tuck, Rakesh M. Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has significantly
improved the generation of fluent and convincing text, raising concerns about
their misuse on social media platforms. We present a methodology using Twitter
datasets to examine the generative capabilities of four LLMs: Llama 3, Mistral,
Qwen2, and GPT4o. We evaluate 7B and 8B parameter base-instruction models of
the three open-source LLMs and validate the impact of further fine-tuning and
"uncensored" versions. Our findings show that "uncensored" models with
additional in-domain fine-tuning dramatically reduce the effectiveness of
automated detection methods. This study addresses a gap by exploring smaller
open-source models and the effects of "uncensoring," providing insights into
how fine-tuning and content moderation influence machine-generated text
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimsChat: A Customisable Persona-Driven Role-Playing Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Yang, Dong Liu, Chen Tang, Chenghao Xiao, Kun Zhao, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) possess the remarkable capability to understand
human instructions and generate high-quality text, enabling them to act as
agents that simulate human behaviours. This capability allows LLMs to emulate
human beings in a more advanced manner, beyond merely replicating simple human
behaviours. However, there is a lack of exploring into leveraging LLMs to craft
characters from several aspects. In this work, we introduce the Customisable
Conversation Agent Framework, which employs LLMs to simulate real-world
characters that can be freely customised according to different user
preferences. The customisable framework is helpful for designing customisable
characters and role-playing agents according to human's preferences. We first
propose the SimsConv dataset, which comprises 68 different customised
characters, 1,360 multi-turn role-playing dialogues, and encompasses 13,971
interaction dialogues in total. The characters are created from several
real-world elements, such as career, aspiration, trait, and skill. Building on
these foundations, we present SimsChat, a freely customisable role-playing
agent. It incorporates different real-world scenes and topic-specific character
interaction dialogues, simulating characters' life experiences in various
scenarios and topic-specific interactions with specific emotions. Experimental
results show that our proposed framework achieves desirable performance and
provides helpful guideline for building better simulacra of human beings in the
future. Our data and code are available at
https://github.com/Bernard-Yang/SimsChat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NormTab: Improving Symbolic Reasoning in <span class="highlight-title">LLM</span>s Through Tabular Data
  Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mahadi Hasan Nahid, Davood Rafiei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities in parsing textual data and generating code. However, their
performance in tasks involving tabular data, especially those requiring
symbolic reasoning, faces challenges due to the structural variance and
inconsistency in table cell values often found in web tables. In this paper, we
introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning
performance of LLMs by normalizing web tables. We study table normalization as
a stand-alone, one-time preprocessing step using LLMs to support symbolic
reasoning on tabular data. Our experimental evaluation, conducted on
challenging web table datasets such as WikiTableQuestion and TabFact,
demonstrates that leveraging NormTab significantly improves symbolic reasoning
performance, showcasing the importance and effectiveness of web table
normalization for enhancing LLM-based symbolic reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venkata S Govindarajan, Matianyu Zang, Kyle Mahowald, David Beaver, Junyi Jessy Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The variations between in-group and out-group speech (intergroup bias) are
subtle and could underlie many social phenomena like stereotype perpetuation
and implicit bias. In this paper, we model the intergroup bias as a tagging
task on English sports comments from forums dedicated to fandom for NFL teams.
We curate a unique dataset of over 6 million game-time comments from opposing
perspectives (the teams in the game), each comment grounded in a non-linguistic
description of the events that precipitated these comments (live win
probabilities for each team). Expert and crowd annotations justify modeling the
bias through tagging of implicit and explicit referring expressions and reveal
the rich, contextual understanding of language and the world required for this
task. For large-scale analysis of intergroup variation, we use LLMs for
automated tagging, and discover that some LLMs perform best when prompted with
linguistic descriptions of the win probability at the time of the comment,
rather than numerical probability. Further, large-scale tagging of comments
using LLMs uncovers linear variations in the form of referent across win
probabilities that distinguish in-group and out-group utterances. Code and data
are available at https://github.com/venkatasg/intergroup-nfl .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Editing for Lifelong Training of Speech Recognition Models <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devang Kulshreshtha, Saket Dingliwal, Brady Houston, Nikolaos Pappas, Srikanth Ronanki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) traditionally assumes known domains, but
adding data from a new domain raises concerns about computational
inefficiencies linked to retraining models on both existing and new domains.
Fine-tuning solely on new domain risks Catastrophic Forgetting (CF). To address
this, Lifelong Learning (LLL) algorithms have been proposed for ASR. Prior
research has explored techniques such as Elastic Weight Consolidation,
Knowledge Distillation, and Replay, all of which necessitate either additional
parameters or access to prior domain data. We propose Sequential Model Editing
as a novel method to continually learn new domains in ASR systems. Different
than previous methods, our approach does not necessitate access to prior
datasets or the introduction of extra parameters. Our study demonstrates up to
15% Word Error Rate Reduction (WERR) over fine-tuning baseline, and superior
efficiency over other LLL techniques on CommonVoice English multi-accent
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FASA: a Flexible and Automatic Speech Aligner for Extracting
  High-quality Aligned Children Speech Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dancheng Liu, Jinjun Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) for adults' speeches has made significant
progress by employing deep neural network (DNN) models recently, but
improvement in children's speech is still unsatisfactory due to children's
speech's distinct characteristics. DNN models pre-trained on adult data often
struggle in generalizing children's speeches with fine tuning because of the
lack of high-quality aligned children's speeches. When generating datasets,
human annotations are not scalable, and existing forced-alignment tools are not
usable as they make impractical assumptions about the quality of the input
transcriptions. To address these challenges, we propose a new forced-alignment
tool, FASA, as a flexible and automatic speech aligner to extract high-quality
aligned children's speech data from many of the existing noisy children's
speech data. We demonstrate its usage on the CHILDES dataset and show that FASA
can improve data quality by 13.6$\times$ over human annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAFT: A Parallel Training Paradigm for Effective <span class="highlight-title">LLM</span> Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiva Kumar Pentyala, Zhichao Wang, Bin Bi, Kiran Ramnath, Xiang-Bo Mao, Regunathan Radhakrishnan, Sitaram Asur,  Na,  Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable abilities in diverse
natural language processing (NLP) tasks. The LLMs generally undergo supervised
fine-tuning (SFT) followed by preference alignment to be usable in downstream
applications. However, this sequential training pipeline leads to alignment tax
that degrades the LLM performance.
  This paper introduces PAFT, a new PArallel training paradigm for effective
LLM Fine-Tuning, which independently performs SFT and preference alignment
(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective
datasets. The model produced by SFT and the model from preference alignment are
then merged into a final model by parameter fusing for use in downstream
applications. This work reveals important findings that preference alignment
like DPO naturally results in a sparse model while SFT leads to a natural dense
model which needs to be sparsified for effective model merging. This paper
introduces an effective interference resolution which reduces the redundancy by
sparsifying the delta parameters. The LLM resulted from the new training
paradigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.
Comprehensive evaluation shows the effectiveness of the parallel training
paradigm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-ray Made Simple: Radiology Report Generation and Evaluation with
  Layman's Terms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhao, Chenghao Xiao, Chen Tang, Bohao Yang, Kai Ye, Noura Al Moubayed, Liang Zhan, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology Report Generation (RRG) has achieved significant progress with the
advancements of multimodal generative models. However, the evaluation in the
domain suffers from a lack of fair and robust metrics. We reveal that, high
performance on RRG with existing lexical-based metrics (e.g. BLEU) might be
more of a mirage - a model can get a high BLEU only by learning the template of
reports. This has become an urgent problem for RRG due to the highly
patternized nature of these reports. In this work, we un-intuitively approach
this problem by proposing the Layman's RRG framework, a layman's terms-based
dataset, evaluation and training framework that systematically improves RRG
with day-to-day language. We first contribute the translated Layman's terms
dataset. Building upon the dataset, we then propose a semantics-based
evaluation method, which is proved to mitigate the inflated numbers of BLEU and
provides fairer evaluation. Last, we show that training on the layman's terms
dataset encourages models to focus on the semantics of the reports, as opposed
to overfitting to learning the report templates. We reveal a promising scaling
law between the number of training examples and semantics gain provided by our
dataset, compared to the inverse pattern brought by the original formats. Our
code is available at \url{https://github.com/hegehongcha/LaymanRRG}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping the Past: Geographically Linking an Early 20th Century Swedish
  Encyclopedia with Wikidata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Ahlin, Alfred Myrne, Pierre Nugues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we describe the extraction of all the location entries from a
prominent Swedish encyclopedia from the early 20th century, the \textit{Nordisk
Familjebok} `Nordic Family Book.' We focused on the second edition called
\textit{Uggleupplagan}, which comprises 38 volumes and over 182,000 articles.
This makes it one of the most extensive Swedish encyclopedias. Using a
classifier, we first determined the category of the entries. We found that
approximately 22 percent of them were locations. We applied a named entity
recognition to these entries and we linked them to Wikidata. Wikidata enabled
us to extract their precise geographic locations resulting in almost 18,000
valid coordinates. We then analyzed the distribution of these locations and the
entry selection process. It showed a higher density within Sweden, Germany, and
the United Kingdom. The paper sheds light on the selection and representation
of geographic information in the \textit{Nordisk Familjebok}, providing
insights into historical and societal perspectives. It also paves the way for
future investigations into entry selection in different time periods and
comparative analyses among various encyclopedias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Script-Agnostic Language Identification <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milind Agarwal, Joshua Otten, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language identification is used as the first step in many data collection and
crawling efforts because it allows us to sort online text into
language-specific buckets. However, many modern languages, such as Konkani,
Kashmiri, Punjabi etc., are synchronically written in several scripts.
Moreover, languages with different writing systems do not share significant
lexical, semantic, and syntactic properties in neural representation spaces,
which is a disadvantage for closely related languages and low-resource
languages, especially those from the Indian Subcontinent. To counter this, we
propose learning script-agnostic representations using several different
experimental strategies (upscaling, flattening, and script mixing) focusing on
four major Dravidian languages (Tamil, Telugu, Kannada, and Malayalam). We find
that word-level script randomization and exposure to a language written in
multiple scripts is extremely valuable for downstream script-agnostic language
identification, while also maintaining competitive performance on naturally
occurring text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review in ACL Rolling Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTBench: A Comprehensive Benchmark for Evaluating Language Model
  Capabilities in Clinical Trial Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nafis Neehal, Bowen Wang, Shayom Debopadhaya, Soham Dan, Keerthiram Murugesan, Vibha Anand, Kristin P. Bennett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CTBench is introduced as a benchmark to assess language models (LMs) in
aiding clinical study design. Given study-specific metadata, CTBench evaluates
AI models' ability to determine the baseline features of a clinical trial (CT),
which include demographic and relevant features collected at the trial's start
from all participants. These baseline features, typically presented in CT
publications (often as Table 1), are crucial for characterizing study cohorts
and validating results. Baseline features, including confounders and
covariates, are also necessary for accurate treatment effect estimation in
studies involving observational data. CTBench consists of two datasets:
"CT-Repo," containing baseline features from 1,690 clinical trials sourced from
clinicaltrials.gov, and "CT-Pub," a subset of 100 trials with more
comprehensive baseline features gathered from relevant publications. Two
LM-based evaluation methods are developed to compare the actual baseline
feature lists against LM-generated responses. "ListMatch-LM" and
"ListMatch-BERT" use GPT-4o and BERT scores (at various thresholds),
respectively, for evaluation. To establish baseline results, advanced prompt
engineering techniques using LLaMa3-70B-Instruct and GPT-4o in zero-shot and
three-shot learning settings are applied to generate potential baseline
features. The performance of GPT-4o as an evaluator is validated through
human-in-the-loop evaluations on the CT-Pub dataset, where clinical experts
confirm matches between actual and LM-generated features. The results highlight
a promising direction with significant potential for improvement, positioning
CTBench as a useful tool for advancing research on AI in CT design and
potentially enhancing the efficacy and robustness of CTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ET tu, CLIP? Addressing Common Object Errors for Unseen Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Won Byun, Cathy Jiao, Shahriar Noroozizadeh, Jimin Sun, Rosa Vitiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simple method that employs pre-trained CLIP encoders to
enhance model generalization in the ALFRED task. In contrast to previous
literature where CLIP replaces the visual encoder, we suggest using CLIP as an
additional module through an auxiliary object detection objective. We validate
our method on the recently proposed Episodic Transformer architecture and
demonstrate that incorporating CLIP improves task performance on the unseen
validation set. Additionally, our analysis results support that CLIP especially
helps with leveraging object descriptions, detecting small objects, and
interpreting rare words.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cloaked Classifiers: Pseudonymization Strategies on Sensitive
  Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arij Riabi, Menel Mahamdi, Virginie Mouilleron, Djamé Seddah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protecting privacy is essential when sharing data, particularly in the case
of an online radicalization dataset that may contain personal information. In
this paper, we explore the balance between preserving data usefulness and
ensuring robust privacy safeguards, since regulations like the European GDPR
shape how personal information must be handled. We share our method for
manually pseudonymizing a multilingual radicalization dataset, ensuring
performance comparable to the original data. Furthermore, we highlight the
importance of establishing comprehensive guidelines for processing sensitive
NLP data by sharing our complete pseudonymization process, our guidelines, the
challenges we encountered as well as the resulting dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the fifth Workshop on Privacy in Natural Language
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Multimodal Transformers with a Pretrained <span class="highlight-title">Large Language Model</span>
  for Mixed-Supervision Speech Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viet Anh Trinh, Rosy Southwell, Yiwen Guan, Xinlu He, Zhiyong Wang, Jacob Whitehill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on discrete speech tokenization has paved the way for models that
can seamlessly perform multiple tasks across modalities, e.g., speech
recognition, text to speech, speech to speech translation. Moreover, large
language models (LLMs) pretrained from vast text corpora contain rich
linguistic information that can improve accuracy in a variety of tasks. In this
paper, we present a decoder-only Discrete Multimodal Language Model (DMLM),
which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and
modalities (text, speech, vision). We explore several critical aspects of
discrete multi-modal models, including the loss function, weight
initialization, mixed training supervision, and codebook. Our results show that
DMLM benefits significantly, across multiple tasks and datasets, from a
combination of supervised and unsupervised training. Moreover, for ASR, it
benefits from initializing DMLM from a pretrained LLM, and from a codebook
derived from Whisper activations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Models</span> in Healthcare: A Comprehensive Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00716v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00716v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Liu, Hongjian Zhou, Yining Hua, Omid Rohanian, Anshul Thakur, Lei Clifton, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of large language models (LLMs) to assist clinicians has
attracted remarkable attention. Existing works mainly adopt the close-ended
question-answering (QA) task with answer options for evaluation. However, many
clinical decisions involve answering open-ended questions without pre-set
options. To better understand LLMs in the clinic, we construct a benchmark
ClinicBench. We first collect eleven existing datasets covering diverse
clinical language generation, understanding, and reasoning tasks. Furthermore,
we construct six novel datasets and complex clinical tasks that are close to
real-world practice, i.e., referral QA, treatment recommendation,
hospitalization (long document) summarization, patient education, pharmacology
QA and drug interaction for emerging drugs. We conduct an extensive evaluation
of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we
invite medical experts to evaluate the clinical usefulness of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Data-Centric Approach To Generate Faithful and High Quality Patient
  Summaries with <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Hegselmann, Shannon Zejiang Shen, Florian Gierse, Monica Agrawal, David Sontag, Xiaoyi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patients often face difficulties in understanding their hospitalizations,
while healthcare workers have limited resources to provide explanations. In
this work, we investigate the potential of large language models to generate
patient summaries based on doctors' notes and study the effect of training data
on the faithfulness and quality of the generated summaries. To this end, we
release (i) a rigorous labeling protocol for errors in medical texts and (ii) a
publicly available dataset of annotated hallucinations in 100 doctor-written
and 100 generated summaries. We show that fine-tuning on hallucination-free
data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama
2, while preserving relevant information. We observe a similar effect on GPT-4
(0.70 to 0.40), when the few-shot examples are hallucination-free. We also
conduct a qualitative evaluation using hallucination-free and improved training
data. We find that common quantitative metrics do not correlate well with
faithfulness and quality. Finally, we test GPT-4 for automatic hallucination
detection, which clearly outperforms common baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Steering with Evolutionary Heuristics for Decoding-time Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15193v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15193v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Yu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread applicability and increasing omnipresence of LLMs have
instigated a need to align LLM responses to user and stakeholder preferences.
Many preference optimization approaches have been proposed that fine-tune LLM
parameters to achieve good alignment. However, such parameter tuning is known
to interfere with model performance on many tasks. Moreover, keeping up with
shifting user preferences is tricky in such a situation. Decoding-time
alignment with reward model guidance solves these issues at the cost of
increased inference time. However, most of such methods fail to strike the
right balance between exploration and exploitation of reward -- often due to
the conflated formulation of these two aspects - to give well-aligned
responses. To remedy this we decouple these two aspects and implement them in
an evolutionary fashion: exploration is enforced by decoding from mutated
instructions and exploitation is represented as the periodic replacement of
poorly-rewarded generations with well-rewarded ones. Empirical evidences
indicate that this strategy outperforms many preference optimization and
decode-time alignment approaches on two widely accepted alignment benchmarks
AlpacaEval 2 and MT-Bench. Our implementation will be available at:
https://darwin-alignment.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Embedded Diachronic Sense Change Model with a Case Study from Ancient
  Greek 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00541v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00541v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Schyan Zafar, Geoff K. Nicholls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word meanings change over time, and word senses evolve, emerge or die out in
the process. For ancient languages, where the corpora are often small and
sparse, modelling such changes accurately proves challenging, and quantifying
uncertainty in sense-change estimates consequently becomes important. GASC
(Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing
generative models that have been used to analyse sense change for target words
from an ancient Greek text corpus, using unsupervised learning without the help
of any pre-training. These models represent the senses of a given target word
such as "kosmos" (meaning decoration, order or world) as distributions over
context words, and sense prevalence as a distribution over senses. The models
are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal
changes in these representations. This paper introduces EDiSC, an Embedded DiSC
model, which combines word embeddings with DiSC to provide superior model
performance. It is shown empirically that EDiSC offers improved predictive
accuracy, ground-truth recovery and uncertainty quantification, as well as
better sampling efficiency and scalability properties with MCMC methods. The
challenges of fitting these models are also discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Merrill, Noah A. Smith, Yanai Elazar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How novel are texts generated by language models (LMs) relative to their
training corpora? In this work, we investigate the extent to which modern LMs
generate $n$-grams from their training data, evaluating both (i) the
probability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the
proportion of $n$-grams generated by an LM that did not appear in the training
data (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search
over a corpus in constant time, we develop Rusty-DAWG, a novel search tool
inspired by indexing of genomic data. We compare the novelty of LM-generated
text to human-written text and explore factors that affect generation novelty,
focusing on the Pythia models. We find that, for $n > 4$, LM-generated text is
less novel than human-written text, though it is more novel for smaller $n$.
Larger LMs and more constrained decoding strategies both decrease novelty.
Finally, we show that LMs complete $n$-grams with lower loss if they are more
frequent in the training data. Overall, our results reveal factors influencing
the novelty of LM-generated text, and we release Rusty-DAWG to facilitate
further pretraining data research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 page preprint + appendix. Minor fixes and appendix changes June 25,
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Zero-Shot Text-To-Speech for Arabic Dialects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khai Duy Doan, Abdul Waheed, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot multi-speaker text-to-speech (ZS-TTS) systems have advanced for
English, however, it still lags behind due to insufficient resources. We
address this gap for Arabic, a language of more than 450 million native
speakers, by first adapting a sizeable existing dataset to suit the needs of
speech synthesis. Additionally, we employ a set of Arabic dialect
identification models to explore the impact of pre-defined dialect labels on
improving the ZS-TTS model in a multi-dialect setting. Subsequently, we
fine-tune the
XTTS\footnote{https://docs.coqui.ai/en/latest/models/xtts.html}\footnote{https://medium.com/machine-learns/xtts-v2-new-version-of-the-open-source-text-to-speech-model-af73914db81f}\footnote{https://medium.com/@erogol/xtts-v1-techincal-notes-eb83ff05bdc}
model, an open-source architecture. We then evaluate our models on a dataset
comprising 31 unseen speakers and an in-house dialectal dataset. Our automated
and human evaluation results show convincing performance while capable of
generating dialectal speech. Our study highlights significant potential for
improvements in this emerging area of research in Arabic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynDARin: Synthesising <span class="highlight-title">Dataset</span>s for Automated Reasoning in Low-Resource
  Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14425v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14425v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayane Ghazaryan, Erik Arakelyan, Pasquale Minervini, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) datasets have been instrumental in developing and
evaluating Large Language Model (LLM) capabilities. However, such datasets are
scarce for languages other than English due to the cost and difficulties of
collection and manual annotation. This means that producing novel models and
measuring the performance of multilingual LLMs in low-resource languages is
challenging. To mitigate this, we propose $\textbf{S}$yn$\textbf{DAR}$in, a
method for generating and validating QA datasets for low-resource languages. We
utilize parallel content mining to obtain $\textit{human-curated}$ paragraphs
between English and the target language. We use the English data as context to
$\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which
are automatically translated and further validated for quality. Combining these
with their designated non-English $\textit{human-curated}$ paragraphs form the
final QA dataset. The method allows to maintain the content quality, reduces
the likelihood of factual errors, and circumvents the need for costly
annotation. To test the method, we created a QA dataset with $1.2$K samples for
the Armenian language. The human evaluation shows that $98\%$ of the generated
English data maintains quality and diversity in the question types and topics,
while the translation validation pipeline can filter out $\sim70\%$ of data
with poor quality. We use the dataset to benchmark state-of-the-art LLMs,
showing their inability to achieve human accuracy with some model performances
closer to random chance. This shows that the generated dataset is non-trivial
and can be used to evaluate reasoning capabilities in low-resource language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lottery Ticket Adaptation: Mitigating Destructive Interference in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwinee Panda, Berivan Isik, Xiangyu Qi, Sanmi Koyejo, Tsachy Weissman, Prateek Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for adapting large language models (LLMs) to new tasks are
not suited to multi-task adaptation because they modify all the model weights
-- causing destructive interference between tasks. The resulting effects, such
as catastrophic forgetting of earlier tasks, make it challenging to obtain good
performance on multiple tasks at the same time. To mitigate this, we propose
Lottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies
and optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide
range of challenging tasks such as instruction following, reasoning, math, and
summarization. LoTA obtains better performance than full fine-tuning and
low-rank adaptation (LoRA), and maintains good performance even after training
on other tasks -- thus, avoiding catastrophic forgetting. By extracting and
fine-tuning over lottery tickets (or sparse task vectors), LoTA also enables
model merging over highly dissimilar tasks. Our code is made publicly available
at https://github.com/kiddyboots216/lottery-ticket-adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedCalc-Bench: Evaluating <span class="highlight-title">Large Language Models</span> for Medical Calculations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina S Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad W Safranek, Abid A Anwar, Andrew Zhang, Aidan Gilson, Maxwell B Singer, Amisha Dave, Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As opposed to evaluating computation and logic-based reasoning, current
benchmarks for evaluating large language models (LLMs) in medicine are
primarily focused on question-answering involving domain knowledge and
descriptive reasoning. While such qualitative capabilities are vital to medical
diagnosis, in real-world scenarios, doctors frequently use clinical calculators
that follow quantitative equations and rule-based reasoning paradigms for
evidence-based decision support. To this end, we propose MedCalc-Bench, a
first-of-its-kind dataset focused on evaluating the medical calculation
capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000
manually reviewed instances from 55 different medical calculation tasks. Each
instance in MedCalc-Bench consists of a patient note, a question requesting to
compute a specific medical value, a ground truth answer, and a step-by-step
explanation showing how the answer is obtained. While our evaluation results
show the potential of LLMs in this area, none of them are effective enough for
clinical settings. Common issues include extracting the incorrect entities, not
using the correct equation or rules for a calculation task, or incorrectly
performing the arithmetic for the computation. We hope our study highlights the
quantitative knowledge and reasoning gaps in LLMs within medical settings,
encouraging future improvements of LLMs for various clinical calculation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github link: https://github.com/ncbi-nlp/MedCalc-Bench HuggingFace
  link: https://huggingface.co/datasets/nsk7153/MedCalc-Bench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning <span class="highlight-title">Large Language Models</span> by On-Policy Self-Judgment <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11253v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11253v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches for aligning large language models with human preferences
face a trade-off that requires a separate reward model (RM) for on-policy
learning. In this paper, we present a novel alignment framework, SELF-JUDGE
that (1) does on-policy learning and 2) is parameter efficient, as it does not
require an additional RM for evaluating the samples for on-policy learning. To
this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a
single model to act as both a policy and a judge. Specifically, we view the
pairwise judgment task, choosing the better response from a response pair, as a
special case of the instruction-following task. The resulting model can judge
preferences of on-the-fly responses from current policy initialized from
itself. Experimental results show the efficacy of SELF-JUDGE, outperforming
baselines in preference benchmarks. We also show that the rejecting sampling by
itself can improve performance further without an additional evaluator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a main conference paper at ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language
  Models in Multi-Turn Dialogues <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has drastically enhanced dialogue
systems. However, comprehensively evaluating the dialogue abilities of LLMs
remains a challenge. Previous benchmarks have primarily focused on single-turn
dialogues or provided coarse-grained and incomplete assessments of multi-turn
dialogues, overlooking the complexity and fine-grained nuances of real-life
dialogues. To address this issue, we introduce MT-Bench-101, specifically
designed to evaluate the fine-grained abilities of LLMs in multi-turn
dialogues. By conducting a detailed analysis of real multi-turn dialogue data,
we construct a three-tier hierarchical ability taxonomy comprising 4208 turns
across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21
popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both
ability and task perspectives and observing differing trends in LLMs
performance across dialogue turns within various tasks. Further analysis
indicates that neither utilizing common alignment techniques nor chat-specific
designs has led to obvious enhancements in the multi-turn abilities of LLMs.
Extensive case studies suggest that our designed tasks accurately assess the
corresponding multi-turn abilities. The data and code are available at
\url{https://github.com/mtbench101/mt-bench-101}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[ACL 2024] The first three authors contribute equally, 34 pages, repo
  at https://github.com/mtbench101/mt-bench-101</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Surgery: Theory and Practice of Affine Steering <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09631v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09631v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, Ponnurangam Kumaraguru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models often exhibit undesirable behavior, e.g., generating toxic or
gender-biased text. In the case of neural language models, an encoding of the
undesirable behavior is often present in the model's representations. Thus, one
natural (and common) approach to prevent the model from exhibiting undesirable
behavior is to steer the model's representations in a manner that reduces the
probability of it generating undesirable text. This paper investigates the
formal and empirical properties of steering functions, i.e., transformation of
the neural language model's representations that alter its behavior. First, we
derive two optimal, in the least-squares sense, affine steering functions under
different constraints. Our theory provides justification for existing
approaches and offers a novel, improved steering approach. Second, we offer a
series of experiments that demonstrate the empirical effectiveness of the
methods in mitigating bias and reducing toxic generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Practical Membership Inference Attacks against Fine-tuned Large Language
  Models via Self-prompt Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06062v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06062v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership Inference Attacks (MIA) aim to infer whether a target data record
has been utilized for model training or not. Prior attempts have quantified the
privacy risks of language models (LMs) via MIAs, but there is still no
consensus on whether existing MIA algorithms can cause remarkable privacy
leakage on practical Large Language Models (LLMs). Existing MIAs designed for
LMs can be classified into two categories: reference-free and reference-based
attacks. They are both based on the hypothesis that training records
consistently strike a higher probability of being sampled. Nevertheless, this
hypothesis heavily relies on the overfitting of target models, which will be
mitigated by multiple regularization methods and the generalization of LLMs.
The reference-based attack seems to achieve promising effectiveness in LLMs,
which measures a more reliable membership signal by comparing the probability
discrepancy between the target model and the reference model. However, the
performance of reference-based attack is highly dependent on a reference
dataset that closely resembles the training dataset, which is usually
inaccessible in the practical scenario. Overall, existing MIAs are unable to
effectively unveil privacy leakage over practical fine-tuned LLMs that are
overfitting-free and private. We propose a Membership Inference Attack based on
Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, since
memorization in LLMs is inevitable during the training process and occurs
before overfitting, we introduce a more reliable membership signal,
probabilistic variation, which is based on memorization rather than
overfitting. Furthermore, we introduce a self-prompt approach, which constructs
the dataset to fine-tune the reference model by prompting the target LLM
itself. In this manner, the adversary can collect a dataset with a similar
distribution from public APIs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Repo: https://github.com/wjfu99/MIA-LLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AudioBench: A Universal Benchmark for Audio <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AudioBench, a new benchmark designed to evaluate audio large
language models (AudioLLMs). AudioBench encompasses 8 distinct tasks and 26
carefully selected or newly curated datasets, focusing on speech understanding,
voice interpretation, and audio scene understanding. Despite the rapid
advancement of large language models, including multimodal versions, a
significant gap exists in comprehensive benchmarks for thoroughly evaluating
their capabilities. AudioBench addresses this gap by providing relevant
datasets and evaluation metrics. In our study, we evaluated the capabilities of
four models across various aspects and found that no single model excels
consistently across all tasks. We outline the research outlook for AudioLLMs
and anticipate that our open-source code, data, and leaderboard will offer a
robust testbed for future model developments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages; v2 - typo update; Code:
  https://github.com/AudioLLMs/AudioBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Dimension Human Value Representation in <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread application of Large Language Models (LLMs) across various
tasks and fields has necessitated the alignment of these models with human
values and preferences. Given various approaches of human value alignment,
ranging from Reinforcement Learning with Human Feedback (RLHF), to
constitutional learning, etc. there is an urgent need to understand the scope
and nature of human values injected into these models before their release.
There is also a need for model alignment without a costly large scale human
annotation effort. We propose UniVaR, a high-dimensional representation of
human value distributions in LLMs, orthogonal to model architecture and
training data. Trained from the value-relevant output of eight multilingual
LLMs and tested on the output from four multilingual LLMs, namely LlaMA2,
ChatGPT, JAIS and Yi, we show that UniVaR is a powerful tool to compare the
distribution of human values embedded in different LLMs with different langauge
sources. Through UniVaR, we explore how different LLMs prioritize various
values in different languages and cultures, shedding light on the complex
interplay between human values and language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embedding Ontologies via Incorporating Extensional and Intensional
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01677v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01677v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyu Wang, Guilin Qi, Jiaoyan Chen, Yi Huang, Tianxing Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontologies contain rich knowledge within domain, which can be divided into
two categories, namely extensional knowledge and intensional knowledge.
Extensional knowledge provides information about the concrete instances that
belong to specific concepts in the ontology, while intensional knowledge
details inherent properties, characteristics, and semantic associations among
concepts. However, existing ontology embedding approaches fail to take both
extensional knowledge and intensional knowledge into fine consideration
simultaneously. In this paper, we propose a novel ontology embedding approach
named EIKE (Extensional and Intensional Knowledge Embedding) by representing
ontologies in two spaces, called extensional space and intensional space. EIKE
presents a unified framework for embedding instances, concepts and their
relations in an ontology, applying a geometry-based method to model extensional
knowledge and a pretrained language model to model intensional knowledge, which
can capture both structure information and textual information. Experimental
results show that EIKE significantly outperforms state-of-the-art methods in
three datasets for both triple classification and link prediction, indicating
that EIKE provides a more comprehensive and representative perspective of the
domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages. Our code is publicly
released at https://github.com/SamuelCahyawijaya/in-context-alignment
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied Question Answering via Multi-<span class="highlight-title">LLM</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Vishnu Sashank Dorbala, Dinesh Manocha, Amrit Singh Bedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied Question Answering (EQA) is an important problem, which involves an
agent exploring the environment to answer user queries. In the existing
literature, EQA has exclusively been studied in single-agent scenarios, where
exploration can be time-consuming and costly. In this work, we consider EQA in
a multi-agent framework involving multiple large language models (LLM) based
agents independently answering queries about a household environment. To
generate one answer for each query, we use the individual responses to train a
Central Answer Model (CAM) that aggregates responses for a robust answer. Using
CAM, we observe a $50\%$ higher EQA accuracy when compared against aggregation
methods for ensemble LLM, such as voting schemes and debates. CAM does not
require any form of agent communication, alleviating it from the associated
costs. We ablate CAM with various nonlinear (neural network, random forest,
decision tree, XGBoost) and linear (logistic regression classifier, SVM)
algorithms. Finally, we present a feature importance analysis for CAM via
permutation feature importance (PFI), quantifying CAMs reliance on each
independent agent and query context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 Figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WRDScore: New Metric for Evaluation of Natural Language Generation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of natural language generation, and, more specifically, method
name prediction, faces significant difficulties when proposed models need to be
evaluated on test data. Such a metric would need to consider the versatility
with which a single method can be named, with respect to both semantics and
syntax. Measuring the direct overlap between the predicted and reference (true)
sequences will not be able to capture these subtleties. Other existing
embedding based metrics either do not measure precision and recall or impose
strict unrealistic assumptions on both sequences. To address these issues, we
propose a new metric that, on the one hand, is very simple and lightweight,
and, on the other hand, is able to calculate precision and recall without
resorting to any assumptions while obtaining good performance with respect to
the human judgement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DE-COP: Detecting Copyrighted Content in Language Models Training Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André V. Duarte, Xuandong Zhao, Arlindo L. Oliveira, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we detect if copyrighted content was used in the training process of
a language model, considering that the training data is typically undisclosed?
We are motivated by the premise that a language model is likely to identify
verbatim excerpts from its training text. We propose DE-COP, a method to
determine whether a piece of copyrighted content was included in training.
DE-COP's core approach is to probe an LLM with multiple-choice questions, whose
options include both verbatim text and their paraphrases. We construct
BookTection, a benchmark with excerpts from 165 books published prior and
subsequent to a model's training cutoff, along with their paraphrases. Our
experiments show that DE-COP surpasses the prior best method by 9.6% in
detection performance (AUC) on models with logits available. Moreover, DE-COP
also achieves an average accuracy of 72% for detecting suspect books on fully
black-box models where prior methods give approximately 4% accuracy. The code
and datasets are available at https://github.com/LeiLiLab/DE-COP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid
  Question Answering <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming Huang, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering multi-hop questions over hybrid factual knowledge from the given
text and table (TextTableQA) is a challenging task. Existing models mainly
adopt a retriever-reader framework, which have several deficiencies, such as
noisy labeling in training retriever, insufficient utilization of heterogeneous
information over text and table, and deficient ability for different reasoning
operations. In this paper, we propose a three-stage TextTableQA framework
S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever
with refinement training to solve the noisy labeling problem. Then, a hybrid
selector considers the linked relationships between heterogeneous data to
select the most relevant factual knowledge. For the final stage, instead of
adapting a reading comprehension module like in previous methods, we employ a
generation-based reasoner to obtain answers. This includes two approaches: a
row-wise generator and an LLM prompting generator~(first time used in this
task). The experimental results demonstrate that our method achieves
competitive results in the few-shot setting. When trained on the full dataset,
our approach outperforms all baseline methods, ranking first on the HybridQA
leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Telecom Language Models: Must They Be Large? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Piovesan, Antonio De Domenico, Fadhel Ayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing interest in Large Language Models (LLMs) within the
telecommunications sector underscores their potential to revolutionize
operational efficiency. However, the deployment of these sophisticated models
is often hampered by their substantial size and computational demands, raising
concerns about their viability in resource-constrained environments. Addressing
this challenge, recent advancements have seen the emergence of small language
models that surprisingly exhibit performance comparable to their larger
counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a
compact yet powerful model, exemplifies this new wave of efficient small
language models. This paper conducts a comprehensive evaluation of Phi-2's
intrinsic understanding of the telecommunications domain. Recognizing the
scale-related limitations, we enhance Phi-2's capabilities through a
Retrieval-Augmented Generation approach, meticulously integrating an extensive
knowledge base specifically curated with telecom standard specifications. The
enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering
questions about telecom standards with a precision that closely rivals the more
resource-intensive GPT-3.5. The paper further explores the refined capabilities
of Phi-2 in addressing problem-solving scenarios within the telecom sector,
highlighting its potential and limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R4: Reinforced Retriever-Reorder-Responder for <span class="highlight-title">Retrieval-Augmented</span> Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Longtao Huang, Hui Xue, Xiaofeng He, Jun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented large language models (LLMs) leverage relevant content
retrieved by information retrieval systems to generate correct responses,
aiming to alleviate the hallucination problem. However, existing
retriever-responder methods typically append relevant documents to the prompt
of LLMs to perform text generation tasks without considering the interaction of
fine-grained structural semantics between the retrieved documents and the LLMs.
This issue is particularly important for accurate response generation as LLMs
tend to "lose in the middle" when dealing with input prompts augmented with
lengthy documents. In this work, we propose a new pipeline named "Reinforced
Retriever-Reorder-Responder" (R$^4$) to learn document orderings for
retrieval-augmented LLMs, thereby further enhancing their generation abilities
while the large numbers of parameters of LLMs remain frozen. The reordering
learning process is divided into two steps according to the quality of the
generated responses: document order adjustment and document representation
enhancement. Specifically, document order adjustment aims to organize retrieved
document orderings into beginning, middle, and end positions based on graph
attention learning, which maximizes the reinforced reward of response quality.
Document representation enhancement further refines the representations of
retrieved documents for responses of poor quality via document-level gradient
adversarial learning. Extensive experiments demonstrate that our proposed
pipeline achieves better factual question-answering performance on
knowledge-intensive tasks compared to strong baselines across various public
datasets. The source codes and trained models will be released upon paper
acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>need to further experiment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Direct Multi-Turn Preference Optimization for Language Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting Large Language Models (LLMs) for agent tasks is critical in
developing language agents. Direct Preference Optimization (DPO) is a promising
technique for this adaptation with the alleviation of compounding errors,
offering a means to directly optimize Reinforcement Learning (RL) objectives.
However, applying DPO to multi-turn tasks presents challenges due to the
inability to cancel the partition function. Overcoming this obstacle involves
making the partition function independent of the current state and addressing
length disparities between preferred and dis-preferred trajectories. In this
light, we replace the policy constraint with the state-action occupancy measure
constraint in the RL objective and add length normalization to the
Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn
agent tasks with theoretical explanations. Extensive experiments on three
multi-turn agent task datasets confirm the effectiveness and superiority of the
DMPO loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Evaluation of <span class="highlight-title">Large Language Models</span> for Topic Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00697v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00697v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoki Doi, Masaru Isonuma, Hitomi Yanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work utilizes Large Language Models (LLMs) for topic modeling,
generating comprehensible topic labels for given documents. However, their
performance has mainly been evaluated qualitatively, and there remains room for
quantitative investigation of their capabilities. In this paper, we
quantitatively evaluate LLMs from multiple perspectives: the quality of topics,
the impact of LLM-specific concerns, such as hallucination and shortcuts for
limited documents, and LLMs' controllability of topic categories via prompts.
Our findings show that LLMs can identify coherent and diverse topics with few
hallucinations but may take shortcuts by focusing only on parts of documents.
We also found that their controllability is limited.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing <span class="highlight-title">Large Language Models</span> as Post-hoc Correctors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Machine Learning (ML) models grow in size and demand higher-quality
training data, the expenses associated with re-training and fine-tuning these
models are escalating rapidly. Inspired by recent impressive achievements of
Large Language Models (LLMs) in different fields, this paper delves into the
question: can LLMs efficiently improve an ML's performance at a minimal cost?
We show that, through our proposed training-free framework LlmCorr, an LLM can
work as a post-hoc corrector to propose corrections for the predictions of an
arbitrary ML model. In particular, we form a contextual knowledge database by
incorporating the dataset's label information and the ML model's predictions on
the validation dataset. Leveraging the in-context learning capability of LLMs,
we ask the LLM to summarise the instances in which the ML model makes mistakes
and the correlation between primary predictions and true labels. Following
this, the LLM can transfer its acquired knowledge to suggest corrections for
the ML model's predictions. Our experimental results on text analysis and the
challenging molecular predictions show that \model improves the performance of
a number of models by up to 39%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PatentEval: Understanding Errors in Patent Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Zuo, Kim Gerdes, Eric Villemonte de La Clergerie, Benoît Sagot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce a comprehensive error typology specifically
designed for evaluating two distinct tasks in machine-generated patent texts:
claims-to-abstract generation, and the generation of the next claim given
previous ones. We have also developed a benchmark, PatentEval, for
systematically assessing language models in this context. Our study includes a
comparative analysis, annotated by humans, of various models. These range from
those specifically adapted during training for tasks within the patent domain
to the latest general-purpose large language models (LLMs). Furthermore, we
explored and evaluated some metrics to approximate human judgments in patent
text evaluation, analyzing the extent to which these metrics align with expert
assessments. These approaches provide valuable insights into the capabilities
and limitations of current language models in the specialized field of patent
text generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Automated Audio Captioning via <span class="highlight-title">Large Language Models</span> with
  Optimized Audio Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhong Liu, Gang Li, Junbo Zhang, Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Yujun Wang, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated audio captioning (AAC) is an audio-to-text task to describe audio
contents in natural language. Recently, the advancements in large language
models (LLMs), with improvements in training approaches for audio encoders,
have opened up possibilities for improving AAC. Thus, we explore enhancing AAC
from three aspects: 1) a pre-trained audio encoder via consistent ensemble
distillation (CED) is used to improve the effectivity of acoustic tokens, with
a querying transformer (Q-Former) bridging the modality gap to LLM and compress
acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B
parameters as the decoder; 3) another pre-trained LLM corrects text errors
caused by insufficient training data and annotation ambiguities. Both the audio
encoder and text decoder are optimized by low-rank adaptation (LoRA).
Experiments show that each of these enhancements is effective. Our method
obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegHist: A General Segmentation-based Framework for Chinese Historical
  Document Text Line Detection <span class="chip">ICDAR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15485v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15485v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Hu, Baole Wei, Liangcai Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text line detection is a key task in historical document analysis facing many
challenges of arbitrary-shaped text lines, dense texts, and text lines with
high aspect ratios, etc. In this paper, we propose a general framework for
historical document text detection (SegHist), enabling existing
segmentation-based text detection methods to effectively address the
challenges, especially text lines with high aspect ratios. Integrating the
SegHist framework with the commonly used method DB++, we develop DB-SegHist.
This approach achieves SOTA on the CHDAC, MTHv2, and competitive results on
HDRC datasets, with a significant improvement of 1.19% on the most challenging
CHDAC dataset which features more text lines with high aspect ratios. Moreover,
our method attains SOTA on rotated MTHv2 and rotated HDRC, demonstrating its
rotational robustness. The code is available at
https://github.com/LumionHXJ/SegHist.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDAR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TemPrompt: Multi-Task Prompt Learning for Temporal Relation Extraction
  in <span class="highlight-title">RAG</span>-based Crowdsourcing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yang, Yu Zhao, Yang Linyao, Xiao Wang, Long Chen, Fei-Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal relation extraction (TRE) aims to grasp the evolution of events or
actions, and thus shape the workflow of associated tasks, so it holds promise
in helping understand task requests initiated by requesters in crowdsourcing
systems. However, existing methods still struggle with limited and unevenly
distributed annotated data. Therefore, inspired by the abundant global
knowledge stored within pre-trained language models (PLMs), we propose a
multi-task prompt learning framework for TRE (TemPrompt), incorporating prompt
tuning and contrastive learning to tackle these issues. To elicit more
effective prompts for PLMs, we introduce a task-oriented prompt construction
approach that thoroughly takes the myriad factors of TRE into consideration for
automatic prompt generation. In addition, we present temporal event reasoning
as a supplement to bolster the model's focus on events and temporal cues. The
experimental results demonstrate that TemPrompt outperforms all compared
baselines across the majority of metrics under both standard and few-shot
settings. A case study is provided to validate its effectiveness in
crowdsourcing scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Resilient and Accessible Distribution-Preserving Watermark for Large
  Language Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Wu, Zhengmian Hu, Junfeng Guo, Hongyang Zhang, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking techniques offer a promising way to identify machine-generated
content via embedding covert information into the contents generated from
language models. A challenge in the domain lies in preserving the distribution
of original generated content after watermarking. Our research extends and
improves upon existing watermarking framework, placing emphasis on the
importance of a \textbf{Di}stribution-\textbf{P}reserving (DiP) watermark.
Contrary to the current strategies, our proposed DiPmark simultaneously
preserves the original token distribution during watermarking
(distribution-preserving), is detectable without access to the language model
API and prompts (accessible), and is provably robust to moderate changes of
tokens (resilient). DiPmark operates by selecting a random set of tokens prior
to the generation of a word, then modifying the token distribution through a
distribution-preserving reweight function to enhance the probability of these
selected tokens during the sampling process. Extensive empirical evaluation on
various language models and tasks demonstrates our approach's
distribution-preserving property, accessibility, and resilience, making it a
effective solution for watermarking tasks that demand impeccable quality
preservation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Ding, Shangbin Feng, Yuhan Liu, Zhaoxuan Tan, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Knowledge Crosswords, a geometric knowledge reasoning benchmark
consisting of incomplete knowledge networks bounded by structured factual
constraints, where LLMs are tasked with inferring the missing facts to meet all
constraints. The novel setting of geometric knowledge reasoning necessitates
new LM abilities beyond existing atomic/linear multi-hop QA, such as
backtracking, verifying facts and constraints, reasoning with uncertainty, and
more. Knowledge Crosswords contains 2,101 individual problems, covering diverse
knowledge domains, and is further divided into three difficulty levels. We
conduct extensive experiments to evaluate existing LLMs and approaches on
Knowledge Crosswords. Results demonstrate that baseline approaches struggle
with larger knowledge networks and semantically-equivalent entity distractors.
In light of their limitations, we propose two new approaches, Staged Prompting
and Verify-All, to augment LLMs' abilities for error-aware backtracking and
constraint verification. Our Verify-All significantly outperforms prior methods
and is more robust towards problems in the hard subset. Further analysis shows
that geometric knowledge reasoning poses new challenges to LLMs' knowledge
abilities, particularly in robustness towards varying option orders, complex
structural constraints in knowledge networks, "none of the above" scenarios,
and more.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Prompt Optimizer for Safe Text-to-Image Generation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10882v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10882v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, Suhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Image (T2I) models have shown great performance in generating images
based on textual prompts. However, these models are vulnerable to unsafe input
to generate unsafe content like sexual, harassment and illegal-activity images.
Existing studies based on image checker, model fine-tuning and embedding
blocking are impractical in real-world applications. Hence, we propose the
first universal prompt optimizer for safe T2I (POSI) generation in black-box
scenario. We first construct a dataset consisting of toxic-clean prompt pairs
by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting
toxic prompt to clean prompt while preserving semantic information, we design a
novel reward function measuring toxicity and text alignment of generated images
and train the optimizer through Proximal Policy Optimization. Experiments show
that our approach can effectively reduce the likelihood of various T2I models
in generating inappropriate images, with no significant impact on text
alignment. It is also flexible to be combined with methods to achieve better
performance. Our code is available at https://github.com/wzongyu/POSI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NExT-GPT: Any-to-Any Multimodal <span class="highlight-title">LLM</span> <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05519v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05519v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recently Multimodal Large Language Models (MM-LLMs) have made exciting
strides, they mostly fall prey to the limitation of only input-side multimodal
understanding, without the ability to produce content in multiple modalities.
As we humans always perceive the world and communicate with people through
various modalities, developing any-to-any MM-LLMs capable of accepting and
delivering content in any modality becomes essential to human-level AI. To fill
the gap, we present an end-to-end general-purpose any-to-any MM-LLM system,
NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion
decoders, enabling NExT-GPT to perceive inputs and generate outputs in
arbitrary combinations of text, images, videos, and audio. By leveraging the
existing well-trained highly-performing encoders and decoders, NExT-GPT is
tuned with only a small amount of parameter (1%) of certain projection layers,
which not only benefits low-cost training and also facilitates convenient
expansion to more potential modalities. Moreover, we introduce a
modality-switching instruction tuning (MosIT) and manually curate a
high-quality dataset for MosIT, based on which NExT-GPT is empowered with
complex cross-modal semantic understanding and content generation. Overall, our
research showcases the promising possibility of building an AI agent capable of
modeling universal modalities, paving the way for more human-like AI research
in the community. Project page: https://next-gpt.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s Are Zero-Shot Context-Aware Simultaneous Translators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13476v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13476v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Koshkin, Katsuhito Sudoh, Satoshi Nakamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of transformers has fueled progress in machine translation. More
recently large language models (LLMs) have come to the spotlight thanks to
their generality and strong performance in a wide range of language tasks,
including translation. Here we show that open-source LLMs perform on par with
or better than some state-of-the-art baselines in simultaneous machine
translation (SiMT) tasks, zero-shot. We also demonstrate that injection of
minimal background information, which is easy with an LLM, brings further
performance gains, especially on challenging technical subject-matter. This
highlights LLMs' potential for building next generation of massively
multilingual, context-aware and terminologically accurate SiMT systems that
require no resource-intensive training or fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Emotions and Ethics with <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Y. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the integration of human-like emotions and ethical
considerations into Large Language Models (LLMs). We first model eight
fundamental human emotions, presented as opposing pairs, and employ
collaborative LLMs to reinterpret and express these emotions across a spectrum
of intensity. Our focus extends to embedding a latent ethical dimension within
LLMs, guided by a novel self-supervised learning algorithm with human feedback
(SSHF). This approach enables LLMs to perform self-evaluations and adjustments
concerning ethical guidelines, enhancing their capability to generate content
that is not only emotionally resonant but also ethically aligned. The
methodologies and case studies presented herein illustrate the potential of
LLMs to transcend mere text and image generation, venturing into the realms of
empathetic interaction and principled decision-making, thereby setting a new
precedent in the development of emotionally aware and ethically conscious AI
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When <span class="highlight-title">Large Language Models</span> Meet Optical Networks: Paving the Way for
  Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danshi Wang, Yidi Wang, Xiaotian Jiang, Yao Zhang, Yue Pang, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the advent of GPT, large language models (LLMs) have brought about
revolutionary advancements in all walks of life. As a superior natural language
processing (NLP) technology, LLMs have consistently achieved state-of-the-art
performance on numerous areas. However, LLMs are considered to be
general-purpose models for NLP tasks, which may encounter challenges when
applied to complex tasks in specialized fields such as optical networks. In
this study, we propose a framework of LLM-empowered optical networks,
facilitating intelligent control of the physical layer and efficient
interaction with the application layer through an LLM-driven agent (AI-Agent)
deployed in the control layer. The AI-Agent can leverage external tools and
extract domain knowledge from a comprehensive resource library specifically
established for optical networks. This is achieved through user input and
well-crafted prompts, enabling the generation of control instructions and
result representations for autonomous operation and maintenance in optical
networks. To improve LLM's capability in professional fields and stimulate its
potential on complex tasks, the details of performing prompt engineering,
establishing domain knowledge library, and implementing complex tasks are
illustrated in this study. Moreover, the proposed framework is verified on two
typical tasks: network alarm analysis and network performance optimization. The
good response accuracies and sematic similarities of 2,400 test situations
exhibit the great potential of LLM in optical networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GLoRe: When, Where, and How to Improve <span class="highlight-title">LLM</span> Reasoning via Global and
  Local Refinements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Raileanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models can exhibit impressive reasoning refinement
capabilities on math, science or coding tasks. However, recent work
demonstrates that even the best models struggle to identify \textit{when and
where to refine} without access to external feedback. Outcome-based Reward
Models (\textbf{ORMs}), trained to predict correctness of the final answer
indicating when to refine, offer one convenient solution for deciding when to
refine. Process Based Reward Models (\textbf{PRMs}), trained to predict
correctness of intermediate steps, can then be used to indicate where to
refine. But they are expensive to train, requiring extensive human annotations.
In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained,
only on synthetic data, to approximate the expected future reward of the
optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict
the correctness of the final answer when sampling the current policy many times
(rather than only once as in the case of ORMs). Our experiments show that SORMs
can more accurately detect incorrect reasoning steps compared to ORMs, thus
improving downstream accuracy when doing refinements. We then train
\textit{global} refinement models, which take only the question and a draft
solution as input and predict a corrected solution, and \textit{local}
refinement models which also take as input a critique indicating the location
of the first reasoning error. We generate training data for both models
synthetically by reusing data used to train the SORM. We find combining global
and local refinements, using the ORM as a reranker, significantly outperforms
either one individually, as well as a best of three sample baseline. With this
strategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned
with RL) on GSM8K from 53\% to 65\% when greedily sampled.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotating FrameNet via Structure-Conditioned Language Generation <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Cui, Swabha Swayamdipta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable generative capabilities of language models in
producing naturalistic language, their effectiveness on explicit manipulation
and generation of linguistic structures remain understudied. In this paper, we
investigate the task of generating new sentences preserving a given semantic
structure, following the FrameNet formalism. We propose a framework to produce
novel frame-semantically annotated sentences following an
overgenerate-and-filter approach. Our results show that conditioning on rich,
explicit semantic information tends to produce generations with high human
acceptance, under both prompting and finetuning. Our generated frame-semantic
structured annotations are effective at training data augmentation for
frame-semantic role labeling in low-resource settings; however, we do not see
benefits under higher resource settings. Our study concludes that while
generating high-quality, semantically rich data might be within reach, the
downstream utility of such generations remains to be seen, highlighting the
outstanding challenges with automating linguistic annotation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to
  construct Observer-Thinker-Conceiver-Expresser 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Shi, Ting Xie, Bingheng Wu, Chunjun Zheng, Kai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has shown that combining Mamba with Transformer architecture,
which has selective state space and quadratic self-attention mechanism,
outperforms using Mamba or Transformer architecture alone in language modeling
tasks. The quadratic self-attention mechanism effectively alleviates the
shortcomings of selective state space in handling long-term dependencies of any
element in the sequence. We propose a position information injection method
that connects the selective state space model with the quadratic attention, and
integrates these two architectures with hybrid experts with cross-sharing
domains, so that we can enjoy the advantages of both. We design a new
architecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser
(OTCE), which can compete with well-known medium-scale open-source language
models on a small scale in language modeling tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Superposed Decoding: Multiple Generations from a Single Autoregressive
  Inference Pass 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18400v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18400v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Shen, Alan Fan, Sarah M. Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many applications today provide users with multiple auto-complete drafts as
they type, including GitHub's code completion, Gmail's smart compose, and
Apple's messaging auto-suggestions. Under the hood, language models support
this by running an autoregressive inference pass to provide a draft.
Consequently, providing $k$ drafts to the user requires running an expensive
language model $k$ times. To alleviate the computation cost of running $k$
inference passes, we propose Superposed Decoding, a new decoding algorithm that
generates $k$ drafts at the computation cost of one autoregressive inference
pass. We achieve this by feeding a superposition of the most recent token
embeddings from the $k$ drafts as input to the next decoding step of the
language model. At every inference step we combine the $k$ drafts with the
top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,
using an n-gram interpolation with minimal compute overhead to filter out
incoherent generations. Our experiments show that $k$ drafts from Superposed
Decoding are at least as coherent and factual as Nucleus Sampling and Greedy
Decoding respectively, while being at least $2.44\times$ faster for $k\ge3$. In
a compute-normalized setting, user evaluations demonstrably favor text
generated by Superposed Decoding over Nucleus Sampling. Code and more examples
open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling the Sacred: Considerations when Using Religious Texts in
  Natural Language Processing <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Hutchinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This position paper concerns the use of religious texts in Natural Language
Processing (NLP), which is of special interest to the Ethics of NLP. Religious
texts are expressions of culturally important values, and machine learned
models have a propensity to reproduce cultural values encoded in their training
data. Furthermore, translations of religious texts are frequently used by NLP
researchers when language data is scarce. This repurposes the translations from
their original uses and motivations, which often involve attracting new
followers. This paper argues that NLP's use of such texts raises considerations
that go beyond model biases, including data provenance, cultural contexts, and
their use in proselytism. We argue for more consideration of researcher
positionality, and of the perspectives of marginalized linguistic and religious
communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating writing style as a contributor to gender gaps in science
  and technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13805v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13805v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kara Kedrick, Ekaterina Levitskaya, Russell J. Funk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A growing stream of research finds that scientific contributions are
evaluated differently depending on the gender of the author. In this article,
we consider whether gender differences in writing styles - how men and women
communicate their work - may contribute to these observed gender gaps. We
ground our investigation in a framework for characterizing the linguistic style
of written text, with two sets of features - informational (i.e., features that
emphasize facts) and involved (i.e., features that emphasize relationships).
Using a large sample of academic papers and patents, we find significant
differences in writing style by gender, with women using more involved features
in their writing. Papers and patents with more involved features also tend to
be cited more by women. Our findings suggest that scientific text is not devoid
of personal character, which could contribute to bias in evaluation, thereby
compromising the norm of universalism as a foundational principle of science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruct, Not Assist: <span class="highlight-title">LLM</span>-based Multi-Turn Planning and Hierarchical
  Questioning for Socratic Code Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socratic questioning is an effective teaching strategy, encouraging critical
thinking and problem-solving. The conversational capabilities of large language
models (LLMs) show great potential for providing scalable, real-time student
guidance. However, current LLMs often give away solutions directly, making them
ineffective instructors. We tackle this issue in the code debugging domain with
TreeInstruct, an Instructor agent guided by a novel state space-based planning
algorithm. TreeInstruct asks probing questions to help students independently
identify and resolve errors. It estimates a student's conceptual and
syntactical knowledge to dynamically construct a question tree based on their
responses and current knowledge state, effectively addressing both independent
and dependent mistakes concurrently in a multi-turn interaction setting. In
addition to using an existing single-bug debugging benchmark, we construct a
more challenging multi-bug dataset of 150 coding problems, incorrect solutions,
and bug fixes -- all carefully constructed and annotated by experts. Extensive
evaluation shows TreeInstruct's state-of-the-art performance on both datasets,
proving it to be a more effective instructor than baselines. Furthermore, a
real-world case study with five students of varying skill levels further
demonstrates TreeInstruct's ability to guide students to debug their code
efficiently with minimal turns and highly Socratic questioning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking <span class="highlight-title">Large Language Models</span> on Answering and Explaining
  Challenging Medical Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18060v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18060v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have demonstrated impressive performance in answering medical questions,
such as achieving passing scores on medical licensing examinations. However,
medical board exam or general clinical questions do not capture the complexity
of realistic clinical cases. Moreover, the lack of reference explanations means
we cannot easily evaluate the reasoning of model decisions, a crucial component
of supporting doctors in making complex medical decisions. To address these
challenges, we construct two new datasets: JAMA Clinical Challenge and
Medbullets. JAMA Clinical Challenge consists of questions based on challenging
clinical cases, while Medbullets comprises simulated clinical questions. Both
datasets are structured as multiple-choice question-answering tasks,
accompanied by expert-written explanations. We evaluate seven LLMs on the two
datasets using various prompts. Experiments demonstrate that our datasets are
harder than previous benchmarks. Human and automatic evaluations of
model-generated explanations provide insights into the promise and deficiency
of LLMs for explainable medical QA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do <span class="highlight-title">Large Language Models</span> Rank Fairly? An Empirical Study on the Fairness
  of <span class="highlight-title">LLM</span>s as Rankers <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Wang, Xuyang Wu, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Large Language Models (LLMs) in information retrieval has
raised a critical reevaluation of fairness in the text-ranking models. LLMs,
such as GPT models and Llama2, have shown effectiveness in natural language
understanding tasks, and prior works (e.g., RankGPT) have also demonstrated
that the LLMs exhibit better performance than the traditional ranking models in
the ranking task. However, their fairness remains largely unexplored. This
paper presents an empirical study evaluating these LLMs using the TREC Fair
Ranking dataset, focusing on the representation of binary protected attributes
such as gender and geographic location, which are historically underrepresented
in search outcomes. Our analysis delves into how these LLMs handle queries and
documents related to these attributes, aiming to uncover biases in their
ranking algorithms. We assess fairness from both user and content perspectives,
contributing an empirical benchmark for evaluating LLMs as the fair ranker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpecExec: Massively Parallel Speculative Decoding for Interactive <span class="highlight-title">LLM</span>
  Inference on Consumer Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, Max Ryabinin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models gain widespread adoption, running them efficiently
becomes crucial. Recent works on LLM inference use speculative decoding to
achieve extreme speedups. However, most of these works implicitly design their
algorithms for high-end datacenter hardware. In this work, we ask the opposite
question: how fast can we run LLMs on consumer machines? Consumer GPUs can no
longer fit the largest available models (50B+ parameters) and must offload them
to RAM or SSD. When running with offloaded parameters, the inference engine can
process batches of hundreds or thousands of tokens at the same time as just one
token, making it a natural fit for speculative decoding. We propose SpecExec
(Speculative Execution), a simple parallel decoding method that can generate up
to 20 tokens per target model iteration for popular LLM families. It utilizes
the high spikiness of the token probabilities distribution in modern LLMs and a
high degree of alignment between model output probabilities. SpecExec takes the
most probable tokens continuation from the draft model to build a "cache" tree
for the target model, which then gets validated in a single pass. Using
SpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with
RAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens
per second with 16-bit weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpeechX: Neural Codec Language Model as a Versatile Speech Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative speech models based on audio-text prompts
have enabled remarkable innovations like high-quality zero-shot text-to-speech.
However, existing models still face limitations in handling diverse audio-text
speech generation tasks involving transforming input speech and processing
audio captured in adverse acoustic conditions. This paper introduces SpeechX, a
versatile speech generation model capable of zero-shot TTS and various speech
transformation tasks, dealing with both clean and noisy signals. SpeechX
combines neural codec language modeling with multi-task learning using
task-dependent prompting, enabling unified and extensible modeling and
providing a consistent way for leveraging textual input in speech enhancement
and transformation tasks. Experimental results show SpeechX's efficacy in
various tasks, including zero-shot TTS, noise suppression, target speaker
extraction, speech removal, and speech editing with or without background
noise, achieving comparable or superior performance to specialized models
across tasks. See https://aka.ms/speechx for demo samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in TASLP. See https://aka.ms/speechx for demo samples</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confabulation: The Surprising Value of <span class="highlight-title">Large Language Model</span>
  Hallucinations <span class="chip">ACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiqi Sui, Eamon Duede, Sophie Wu, Richard Jean So
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic defense of large language model (LLM)
hallucinations or 'confabulations' as a potential resource instead of a
categorically negative pitfall. The standard view is that confabulations are
inherently problematic and AI research should eliminate this flaw. In this
paper, we argue and empirically demonstrate that measurable semantic
characteristics of LLM confabulations mirror a human propensity to utilize
increased narrativity as a cognitive resource for sense-making and
communication. In other words, it has potential value. Specifically, we analyze
popular hallucination benchmarks and reveal that hallucinated outputs display
increased levels of narrativity and semantic coherence relative to veridical
outputs. This finding reveals a tension in our usually dismissive
understandings of confabulation. It suggests, counter-intuitively, that the
tendency for LLMs to confabulate may be intimately associated with a positive
capacity for coherent narrative-text generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming at ACL2024 main conference. 1 figure</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Light-weight End-to-End Graph Interest Network for CTR Prediction in
  E-commerce Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pai Peng, Quanxiang Jia, Ziqiang Zhou, Shuang Hong, Zichong Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through-rate (CTR) prediction has an essential impact on improving user
experience and revenue in e-commerce search. With the development of deep
learning, graph-based methods are well exploited to utilize graph structure
extracted from user behaviors and other information to help embedding learning.
However, most of the previous graph-based methods mainly focus on
recommendation scenarios, and therefore their graph structures highly depend on
item's sequential information from user behaviors, ignoring query's sequential
signal and query-item correlation. In this paper, we propose a new approach
named Light-weight End-to-End Graph Interest Network (EGIN) to effectively mine
users' search interests and tackle previous challenges. (i) EGIN utilizes query
and item's correlation and sequential information from the search system to
build a heterogeneous graph for better CTR prediction in e-commerce search.
(ii) EGIN's graph embedding learning shares the same training input and is
jointly trained with CTR prediction, making the end-to-end framework effortless
to deploy in large-scale search systems. The proposed EGIN is composed of three
parts: query-item heterogeneous graph, light-weight graph sampling, and
multi-interest network. The query-item heterogeneous graph captures correlation
and sequential information of query and item efficiently by the proposed
light-weight graph sampling. The multi-interest network is well designed to
utilize graph embedding to capture various similarity relationships between
query and item to enhance the final CTR prediction. We conduct extensive
experiments on both public and industrial datasets to demonstrate the
effectiveness of the proposed EGIN. At the same time, the training cost of
graph learning is relatively low compared with the main CTR prediction task,
ensuring efficiency in practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LumberChunker: Long-Form Narrative Document Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André V. Duarte, João Marques, Miguel Graça, Miguel Freire, Lei Li, Arlindo L. Oliveira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern NLP tasks increasingly rely on dense retrieval methods to access
up-to-date and relevant contextual information. We are motivated by the premise
that retrieval benefits from segments that can vary in size such that a
content's semantic independence is better captured. We propose LumberChunker, a
method leveraging an LLM to dynamically segment documents, which iteratively
prompts the LLM to identify the point within a group of sequential passages
where the content begins to shift. To evaluate our method, we introduce
GutenQA, a benchmark with 3000 "needle in a haystack" type of question-answer
pairs derived from 100 public domain narrative books available on Project
Gutenberg. Our experiments show that LumberChunker not only outperforms the
most competitive baseline by 7.37% in retrieval performance (DCG@20) but also
that, when integrated into a RAG pipeline, LumberChunker proves to be more
effective than other chunking methods and competitive baselines, such as the
Gemini 1.5M Pro. Our Code and Data are available at
https://github.com/joaodsmarques/LumberChunker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACE: A Generative Cross-Modal Retrieval Framework with Coarse-To-Fine
  Semantic Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Fang, Shengpeng Ji, Jialong Zuo, Hai Huang, Yan Xia, Jieming Zhu, Xize Cheng, Xiaoda Yang, Wenrui Liu, Gang Wang, Zhenhua Dong, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative retrieval, which has demonstrated effectiveness in text-to-text
retrieval, utilizes a sequence-to-sequence model to directly generate candidate
identifiers based on natural language queries. Without explicitly computing the
similarity between queries and candidates, generative retrieval surpasses
dual-tower models in both speed and accuracy on large-scale corpora, providing
new insights for cross-modal retrieval. However, constructing identifiers for
multimodal data remains an untapped problem, and the modality gap between
natural language queries and multimodal candidates hinders retrieval
performance due to the absence of additional encoders. To this end, we propose
a pioneering generAtive Cross-modal rEtrieval framework (ACE), which is a
comprehensive framework for end-to-end cross-modal retrieval based on
coarse-to-fine semantic modeling. We propose combining K-Means and RQ-VAE to
construct coarse and fine tokens, serving as identifiers for multimodal data.
Correspondingly, we design the coarse-to-fine feature fusion strategy to
efficiently align natural language queries and candidate identifiers. ACE is
the first work to comprehensively demonstrate the feasibility of generative
approach on text-to-image/audio/video retrieval, challenging the dominance of
the embedding-based dual-tower architecture. Extensive experiments show that
ACE achieves state-of-the-art performance in cross-modal retrieval and
outperforms the strong baselines on Recall@1 by 15.27% on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performative Debias with Fair-exposure Optimization Driven by Strategic
  Agents in Recommender Systems <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, Jianping Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data bias, e.g., popularity impairs the dynamics of two-sided markets within
recommender systems. This overshadows the less visible but potentially
intriguing long-tail items that could capture user interest. Despite the
abundance of research surrounding this issue, it still poses challenges and
remains a hot topic in academic circles. Along this line, in this paper, we
developed a re-ranking approach in dynamic settings with fair-exposure
optimization driven by strategic agents. Designed for the producer side, the
execution of agents assumes content creators can modify item features based on
strategic incentives to maximize their exposure. This iterative process entails
an end-to-end optimization, employing differentiable ranking operators that
simultaneously target accuracy and fairness. Joint objectives ensure the
performance of recommendations while enhancing the visibility of tail items. We
also leveraged the performativity nature of predictions to illustrate how
strategic learning influences content creators to shift towards fairness
efficiently, thereby incentivizing features of tail items. Through
comprehensive experiments on both public and industrial datasets, we have
substantiated the effectiveness and dominance of the proposed method especially
on unveiling the potential of tail items.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGKDD 2024 accepted paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Text is Worth Several Tokens: Text Embedding from <span class="highlight-title">LLM</span>s Secretly Aligns
  Well with The Key Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Nie, Richong Zhang, Zhanyu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embeddings from large language models (LLMs) have achieved excellent
results in tasks such as information retrieval, semantic textual similarity,
etc. In this work, we show an interesting finding: when feeding a text into the
embedding LLMs, the obtained text embedding will be able to be aligned with the
key tokens in the input text. We first fully analyze this phenomenon on eight
embedding LLMs and show that this phenomenon is universal and is not affected
by model architecture, training strategy, and embedding method. With a deeper
analysis, we then find that the main change in embedding space between the
embedding LLMs and their original generative LLMs is in the first principal
component. By adjusting the first principal component, we can align text
embedding with the key tokens. Finally, we give several examples to demonstrate
the vast application potential of this finding: (1) we propose a simple and
practical sparse retrieval method based on the aligned tokens, which can
achieve 80\% of the dense retrieval effect of the same model while reducing the
computation significantly; (2) we show that our findings provide a fresh
perspective to help understand fuzzy concepts (e.g., semantic relatedness vs.
semantic similarity) and emerging technologies (e.g., instruction-following
embedding) in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Thorough Performance Benchmarking on Lightweight Embedding-based
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Vinh Tran, Tong Chen, Quoc Viet Hung Nguyen, Zi Huang, Lizhen Cui, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the creation of the Web, recommender systems (RSs) have been an
indispensable mechanism in information filtering. State-of-the-art RSs
primarily depend on categorical features, which ecoded by embedding vectors,
resulting in excessively large embedding tables. To prevent over-parameterized
embedding tables from harming scalability, both academia and industry have seen
increasing efforts in compressing RS embeddings. However, despite the
prosperity of lightweight embedding-based RSs (LERSs), a wide diversity is seen
in evaluation protocols, resulting in obstacles when relating LERS performance
to real-world usability. Moreover, despite the common goal of lightweight
embeddings, LERSs are evaluated with a single choice between the two main
recommendation tasks -- collaborative filtering and content-based
recommendation. This lack of discussions on cross-task transferability hinders
the development of unified, more scalable solutions. Motivated by these issues,
this study investigates various LERSs' performance, efficiency, and cross-task
transferability via a thorough benchmarking process. Additionally, we propose
an efficient embedding compression method using magnitude pruning, which is an
easy-to-deploy yet highly competitive baseline that outperforms various complex
LERSs. Our study reveals the distinct performance of LERSs across the two
tasks, shedding light on their effectiveness and generalizability. To support
edge-based recommendations, we tested all LERSs on a Raspberry Pi 4, where the
efficiency bottleneck is exposed. Finally, we conclude this paper with critical
summaries of LERS performance, model selection suggestions, and underexplored
challenges around LERSs for future research. To encourage future research, we
publish source codes and artifacts at \href{this
link}{https://github.com/chenxing1999/recsys-benchmark}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperbolic Knowledge Transfer in Cross-Domain Recommendation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yang, Heng Chang, Zhijian La, Jinze Yang, Xingrun Li, Yu Lu, Shuaiqiang Wang, Dawei Yin, Erxue Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Domain Recommendation (CDR) seeks to utilize knowledge from different
domains to alleviate the problem of data sparsity in the target recommendation
domain, and it has been gaining more attention in recent years. Although there
have been notable advancements in this area, most current methods represent
users and items in Euclidean space, which is not ideal for handling long-tail
distributed data in recommendation systems. Additionally, adding data from
other domains can worsen the long-tail characteristics of the entire dataset,
making it harder to train CDR models effectively. Recent studies have shown
that hyperbolic methods are particularly suitable for modeling long-tail
distributions, which has led us to explore hyperbolic representations for users
and items in CDR scenarios. However, due to the distinct characteristics of the
different domains, applying hyperbolic representation learning to CDR tasks is
quite challenging. In this paper, we introduce a new framework called
Hyperbolic Contrastive Learning (HCTS), designed to capture the unique features
of each domain while enabling efficient knowledge transfer between domains. We
achieve this by embedding users and items from each domain separately and
mapping them onto distinct hyperbolic manifolds with adjustable curvatures for
prediction. To improve the representations of users and items in the target
domain, we develop a hyperbolic contrastive learning module for knowledge
transfer. Extensive experiments on real-world datasets demonstrate that
hyperbolic manifolds are a promising alternative to Euclidean space for CDR
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Document Ranking with Learnable Late Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Ji, Himanshu Jain, Andreas Veit, Sashank J. Reddi, Sadeep Jayasumana, Ankit Singh Rawat, Aditya Krishna Menon, Felix Yu, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Encoder (CE) and Dual-Encoder (DE) models are two fundamental
approaches for query-document relevance in information retrieval. To predict
relevance, CE models use joint query-document embeddings, while DE models
maintain factorized query and document embeddings; usually, the former has
higher quality while the latter benefits from lower latency. Recently,
late-interaction models have been proposed to realize more favorable
latency-quality tradeoffs, by using a DE structure followed by a lightweight
scorer based on query and document token embeddings. However, these lightweight
scorers are often hand-crafted, and there is no understanding of their
approximation power; further, such scorers require access to individual
document token embeddings, which imposes an increased latency and storage
burden. In this paper, we propose novel learnable late-interaction models
(LITE) that resolve these issues. Theoretically, we prove that LITE is a
universal approximator of continuous scoring functions, even for relatively
small embedding dimension. Empirically, LITE outperforms previous
late-interaction models such as ColBERT on both in-domain and zero-shot
re-ranking tasks. For instance, experiments on MS MARCO passage re-ranking show
that LITE not only yields a model with better generalization, but also lowers
latency and requires 0.25x storage compared to ColBERT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NormTab: Improving Symbolic Reasoning in <span class="highlight-title">LLM</span>s Through Tabular Data
  Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mahadi Hasan Nahid, Davood Rafiei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have demonstrated remarkable
capabilities in parsing textual data and generating code. However, their
performance in tasks involving tabular data, especially those requiring
symbolic reasoning, faces challenges due to the structural variance and
inconsistency in table cell values often found in web tables. In this paper, we
introduce NormTab, a novel framework aimed at enhancing the symbolic reasoning
performance of LLMs by normalizing web tables. We study table normalization as
a stand-alone, one-time preprocessing step using LLMs to support symbolic
reasoning on tabular data. Our experimental evaluation, conducted on
challenging web table datasets such as WikiTableQuestion and TabFact,
demonstrates that leveraging NormTab significantly improves symbolic reasoning
performance, showcasing the importance and effectiveness of web table
normalization for enhancing LLM-based symbolic reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Coding with Iterative Block Leve<span class="highlight-title">rag</span>e Score Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neophytos Charalambides, Mert Pilanci, Alfred Hero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We generalize the leverage score sampling sketch for $\ell_2$-subspace
embeddings, to accommodate sampling subsets of the transformed data, so that
the sketching approach is appropriate for distributed settings. This is then
used to derive an approximate coded computing approach for first-order methods;
known as gradient coding, to accelerate linear regression in the presence of
failures in distributed computational networks, \textit{i.e.} stragglers. We
replicate the data across the distributed network, to attain the approximation
guarantees through the induced sampling distribution. The significance and main
contribution of this work, is that it unifies randomized numerical linear
algebra with approximate coded computing, while attaining an induced
$\ell_2$-subspace embedding through uniform sampling. The transition to uniform
sampling is done without applying a random projection, as in the case of the
subsampled randomized Hadamard transform. Furthermore, by incorporating this
technique to coded computing, our scheme is an iterative sketching approach to
approximately solving linear regression. We also propose weighting when
sketching takes place through sampling with replacement, for further
compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Optimization with Adaptive Heuristics for Intelligent Marketing
  System <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10490v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10490v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T Ribeiro, Jingyi Kenneth Tay, Borja Ocejo Elizondo, Keerthi Selvaraj, Aman Gupta, Licurgo Benemann De Almeida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational marketing has become increasingly important in today's digital
world, facing challenges such as massive heterogeneous data, multi-channel
customer journeys, and limited marketing budgets. In this paper, we propose a
general framework for marketing AI systems, the Neural Optimization with
Adaptive Heuristics (NOAH) framework. NOAH is the first general framework for
marketing optimization that considers both to-business (2B) and to-consumer
(2C) products, as well as both owned and paid channels. We describe key modules
of the NOAH framework, including prediction, optimization, and adaptive
heuristics, providing examples for bidding and content optimization. We then
detail the successful application of NOAH to LinkedIn's email marketing system,
showcasing significant wins over the legacy ranking system. Additionally, we
share details and insights that are broadly useful, particularly on: (i)
addressing delayed feedback with lifetime value, (ii) performing large-scale
linear programming with randomization, (iii) improving retrieval with audience
expansion, (iv) reducing signal dilution in targeting tests, and (v) handling
zero-inflated heavy-tail metrics in statistical testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do <span class="highlight-title">Large Language Models</span> Rank Fairly? An Empirical Study on the Fairness
  of <span class="highlight-title">LLM</span>s as Rankers <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Wang, Xuyang Wu, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Large Language Models (LLMs) in information retrieval has
raised a critical reevaluation of fairness in the text-ranking models. LLMs,
such as GPT models and Llama2, have shown effectiveness in natural language
understanding tasks, and prior works (e.g., RankGPT) have also demonstrated
that the LLMs exhibit better performance than the traditional ranking models in
the ranking task. However, their fairness remains largely unexplored. This
paper presents an empirical study evaluating these LLMs using the TREC Fair
Ranking dataset, focusing on the representation of binary protected attributes
such as gender and geographic location, which are historically underrepresented
in search outcomes. Our analysis delves into how these LLMs handle queries and
documents related to these attributes, aiming to uncover biases in their
ranking algorithms. We assess fairness from both user and content perspectives,
contributing an empirical benchmark for evaluating LLMs as the fair ranker.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-24T00:00:00Z">2024-06-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">154</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference with modern Large Language Models (LLMs) is expensive and
time-consuming, and speculative sampling has proven to be an effective
solution. Most speculative sampling methods such as EAGLE use a static draft
tree, implicitly assuming that the acceptance rate of draft tokens depends only
on their position. Interestingly, we found that the acceptance rate of draft
tokens is also context-dependent. In this paper, building upon EAGLE, we
propose EAGLE-2, which introduces a new technique of context-aware dynamic
draft tree into drafting modeling. This improvement leverages the fact that the
draft model of EAGLE is well-calibrated: the confidence scores from the draft
model approximate acceptance rates with small errors. We conducted extensive
evaluations on three series of LLMs and six tasks, with EAGLE-2 achieving
speedup ratios 3.05x-4.26x, which is 20%-40% faster than EAGLE-1. EAGLE-2 also
ensures that the distribution of the generated text remains unchanged, making
it a lossless acceleration algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Losing Visual Needles in Image Haystacks: Vision Language Models are
  Easily Distracted in Short and Long Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Sharma, Michael Saxon, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LoCoVQA, a dynamic benchmark generator for evaluating long-context
extractive reasoning in vision language models (VLMs). LoCoVQA augments test
examples for mathematical reasoning, VQA, and character recognition tasks with
increasingly long visual contexts composed of both in-distribution and
out-of-distribution distractor images.
  Across these tasks, a diverse set of VLMs rapidly lose performance as the
visual context length grows, often exhibiting a striking exponential decay
trend. This test assesses how well VLMs can ignore irrelevant information when
answering queries -- a task that is quite easy for language models (LMs) in the
text domain -- demonstrating that current state-of-the-art VLMs lack this
essential capability for many long-context applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RaTEScore: A Metric for Radiology Report Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel, entity-aware metric, termed as Radiological
Report (Text) Evaluation (RaTEScore), to assess the quality of medical reports
generated by AI models. RaTEScore emphasizes crucial medical entities such as
diagnostic outcomes and anatomical details, and is robust against complex
medical synonyms and sensitive to negation expressions. Technically, we
developed a comprehensive medical NER dataset, RaTE-NER, and trained an NER
model specifically for this purpose. This model enables the decomposition of
complex radiological reports into constituent medical entities. The metric
itself is derived by comparing the similarity of entity embeddings, obtained
from a language model, based on their types and relevance to clinical
significance. Our evaluations demonstrate that RaTEScore aligns more closely
with human preference than existing metrics, validated both on established
public benchmarks and our newly proposed RaTE-Eval benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Factual Entailment with NLI: A News Media Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Mor-Lan, Effi Levi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the relationship between factuality and Natural Language Inference
(NLI) by introducing FactRel -- a novel annotation scheme that models
\textit{factual} rather than \textit{textual} entailment, and use it to
annotate a dataset of naturally occurring sentences from news articles. Our
analysis shows that 84\% of factually supporting pairs and 63\% of factually
undermining pairs do not amount to NLI entailment or contradiction,
respectively, suggesting that factual relationships are more apt for analyzing
media discourse. We experiment with models for pairwise classification on the
new dataset, and find that in some cases, generating synthetic data with GPT-4
on the basis of the annotated dataset can improve performance. Surprisingly,
few-shot learning with GPT-4 yields strong results on par with medium LMs
(DeBERTa) trained on the labelled dataset. We hypothesize that these results
indicate the fundamental dependence of this task on both world knowledge and
advanced reasoning abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at *SEM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Decoding to Meta-Generation: Inference-time Algorithms for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most striking findings in modern research on large language models
(LLMs) is that scaling up compute during training leads to better results.
However, less attention has been given to the benefits of scaling compute
during inference. This survey focuses on these inference-time approaches. We
explore three areas under a unified mathematical formalism: token-level
generation algorithms, meta-generation algorithms, and efficient generation.
Token-level generation algorithms, often called decoding algorithms, operate by
sampling a single token at a time or constructing a token-level search space
and then selecting an output. These methods typically assume access to a
language model's logits, next-token distributions, or probability scores.
Meta-generation algorithms work on partial or full sequences, incorporating
domain knowledge, enabling backtracking, and integrating external information.
Efficient generation methods aim to reduce token costs and improve the speed of
generation. Our survey unifies perspectives from three research communities:
traditional natural language processing, modern LLMs, and machine learning
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USDC: A <span class="highlight-title">Dataset</span> of $\underline{U}$ser $\underline{S}$tance and
  $\underline{D}$ogmatism in Long $\underline{C}$onversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mounika Marreddy, Subba Reddy Oota, Venkata Charan Chinni, Manish Gupta, Lucie Flek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying user's opinions and stances in long conversation threads on
various topics can be extremely critical for enhanced personalization, market
research, political campaigns, customer service, conflict resolution, targeted
advertising, and content moderation. Hence, training language models to
automate this task is critical. However, to train such models, gathering manual
annotations has multiple challenges: 1) It is time-consuming and costly; 2)
Conversation threads could be very long, increasing chances of noisy
annotations; and 3) Interpreting instances where a user changes their opinion
within a conversation is difficult because often such transitions are subtle
and not expressed explicitly. Inspired by the recent success of large language
models (LLMs) for complex natural language processing (NLP) tasks, we leverage
Mistral Large and GPT-4 to automate the human annotation process on the
following two tasks while also providing reasoning: i) User Stance
classification, which involves labeling a user's stance of a post in a
conversation on a five-point scale; ii) User Dogmatism classification, which
deals with labeling a user's overall opinion in the conversation on a
four-point scale. The majority voting on zero-shot, one-shot, and few-shot
annotations from these two LLMs on 764 multi-user Reddit conversations helps us
curate the USDC dataset. USDC is then used to finetune and instruction-tune
multiple deployable small language models for the 5-class stance and 4-class
dogmatism classification tasks. We make the code and dataset publicly available
[https://anonymous.4open.science/r/USDC-0F7F].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding and Mitigating Tokenization Bias in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Buu Phan, Marton Havasi, Matthew Muckley, Karen Ullrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models are autoregressive and operate on subword
units known as tokens. Specifically, one must encode the conditioning string
into a list of tokens before passing to the language models for next-token
prediction. We show that, for encoding schemes such as maximum prefix matching,
tokenization induces a sampling bias that cannot be mitigated with more
training or data. To counter this universal problem, we propose a novel
algorithm to obtain unbiased estimates from a model that was trained on
tokenized data. Our method does not require finetuning the model, and its
complexity, defined as the number of model runs, scales linearly with the
sequence length. As a consequence, we show that one can simulate token-free
behavior from a tokenized language model. We empirically verify the correctness
of our method through a Markov-chain setup, where it accurately recovers the
transition probabilities, as opposed to the conventional method of directly
prompting tokens into the language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Rag</span>narök: A Reusable <span class="highlight-title">RAG</span> Framework and Baselines for TREC 2024
  <span class="highlight-title">Retrieval-Augmented</span> Generation Track 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronak Pradeep, Nandan Thakur, Sahel Sharifymoghaddam, Eric Zhang, Ryan Nguyen, Daniel Campos, Nick Craswell, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Did you try out the new Bing Search? Or maybe you fiddled around with Google
AI~Overviews? These might sound familiar because the modern-day search stack
has recently evolved to include retrieval-augmented generation (RAG) systems.
They allow searching and incorporating real-time data into large language
models (LLMs) to provide a well-informed, attributed, concise summary in
contrast to the traditional search paradigm that relies on displaying a ranked
list of documents. Therefore, given these recent advancements, it is crucial to
have an arena to build, test, visualize, and systematically evaluate RAG-based
search systems. With this in mind, we propose the TREC 2024 RAG Track to foster
innovation in evaluating RAG systems. In our work, we lay out the steps we've
made towards making this track a reality -- we describe the details of our
reusable framework, Ragnar\"ok, explain the curation of the new MS MARCO V2.1
collection choice, release the development topics for the track, and
standardize the I/O definitions which assist the end user. Next, using
Ragnar\"ok, we identify and provide key industrial baselines such as OpenAI's
GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface
for an interactive arena allowing benchmarking pairwise RAG systems by
crowdsourcing. We open-source our Ragnar\"ok framework and baselines to achieve
a unified standard for future RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PISTOL: <span class="highlight-title">Dataset</span> Compilation Pipeline for Structural Unlearning of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinchi Qiu, William F. Shen, Yihong Chen, Nicola Cancedda, Pontus Stenetorp, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, machine unlearning, which seeks to erase specific data stored in
the pre-trained or fine-tuned models, has emerged as a crucial protective
measure for LLMs. However, unlearning approaches for LLMs that have been
considered thus far have focused on the removal of independent data points and
have not taken into account that the stored facts are logically connected to
one another and form an implicit knowledge graph. To facilitate the development
of structural unlearning methods, which are essential for the practical
application of unlearning, we propose PISTOL, a pipeline for compiling
multi-scenario datasets for benchmarking structural LLM unlearning.
Additionally, leveraging sample datasets synthesized using PISTOL, we conducted
benchmarks with four distinct unlearning methods on both Llama2-7B and
Mistral-7B models. This analysis helps to illustrate the prevailing challenges
in effectively and robustly removing highly inter-connected data, batched data,
or data skewed towards a specific domain. It also highlights the choice of
pre-trained model can impact unlearning performance. This work not only
advances our understandings on the limitation of current LLMs unlearning
methods and proposes future research directions, but also provides a replicable
framework for ongoing exploration and validation in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback
  for Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine M. Collins, Najoung Kim, Yonatan Bitton, Verena Rieser, Shayegan Omidshafiei, Yushi Hu, Sherol Chen, Senjuti Dutta, Minsuk Chang, Kimin Lee, Youwei Liang, Georgina Evans, Sahil Singla, Gang Li, Adrian Weller, Junfeng He, Deepak Ramachandran, Krishnamurthy Dj Dvijotham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human feedback plays a critical role in learning and refining reward models
for text-to-image generation, but the optimal form the feedback should take for
learning an accurate reward function has not been conclusively established.
This paper investigates the effectiveness of fine-grained feedback which
captures nuanced distinctions in image quality and prompt-alignment, compared
to traditional coarse-grained feedback (for example, thumbs up/down or ranking
between a set of options). While fine-grained feedback holds promise,
particularly for systems catering to diverse societal preferences, we show that
demonstrating its superiority to coarse-grained feedback is not automatic.
Through experiments on real and synthetic preference data, we surface the
complexities of building effective models due to the interplay of model choice,
feedback type, and the alignment between human judgment and computational
interpretation. We identify key challenges in eliciting and utilizing
fine-grained feedback, prompting a reassessment of its assumed benefits and
practicality. Our findings -- e.g., that fine-grained feedback can lead to
worse models for a fixed budget, in some settings; however, in controlled
settings with known attributes, fine grained rewards can indeed be more helpful
-- call for careful consideration of feedback attributes and potentially beckon
novel modeling approaches to appropriately unlock the potential value of
fine-grained feedback in-the-wild.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RES-Q: Evaluating Code-Editing <span class="highlight-title">Large Language Model</span> Systems at the
  Repository Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beck LaBash, August Rosedale, Alex Reents, Colin Wiel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The instruction-following ability of Large Language Models (LLMs) has
cultivated a class of LLM-based systems capable of approaching complex tasks
such as making edits to large code repositories. Due to the high sensitivity
and unpredictability of LLM behavior in response to changes in prompting,
robust evaluation tools are needed to drive future iteration of these systems.
We propose RES-Q, a natural language instruction-based benchmark for evaluating
$\textbf{R}$epository $\textbf{E}$diting $\textbf{S}$ystems, which consists of
100 repository editing tasks derived from real GitHub commits. Given an edit
instruction and a code repository, RES-Q evaluates an LLM system's ability to
gather information and construct an edit that satisfies the criteria set by the
instruction. We argue that evaluating LLMs in this way addresses issues with
traditional benchmarks and provides a more holistic assessment of a model's
abilities. We evaluate various state-of-the-art LLMs as language agents in a
repository-editing system built on Qurrent OS, our language agent development
software. Despite their 1% pass@1 performance difference on HumanEval, we find
Claude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q, indicating RES-Q's
capacity to differentiate model capability as traditional benchmarks approach
saturation. We further investigate token efficiency, performance relationships
with existing benchmarks, and interesting disparities between closed and
open-source LLMs. Code and dataset are available at
https://github.com/Qurrent-AI/RES-Q.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lottery Ticket Adaptation: Mitigating Destructive Interference in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwinee Panda, Berivan Isik, Xiangyu Qi, Sanmi Koyejo, Tsachy Weissman, Prateek Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for adapting large language models (LLMs) to new tasks are
not suited to multi-task adaptation because they modify all the model weights
-- causing destructive interference between tasks. The resulting effects, such
as catastrophic forgetting of earlier tasks, make it challenging to obtain good
performance on multiple tasks at the same time. To mitigate this, we propose
Lottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies
and optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide
range of challenging tasks such as instruction following, reasoning, math, and
summarization. LoTA obtains better performance than full fine-tuning and
low-rank adaptation (LoRA), and maintains good performance even after training
on other tasks -- thus, avoiding catastrophic forgetting. By extracting and
fine-tuning over \emph{lottery tickets} (or \emph{sparse task vectors}), LoTA
also enables model merging over highly dissimilar tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in
  <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction finetuning (IFT) is critical for aligning Large Language Models
(LLMs) to follow instructions. Numerous effective IFT datasets have been
proposed in the recent past, but most focus on high resource languages such as
English. In this work, we propose a fully synthetic, novel taxonomy (Evol)
guided Multilingual, Multi-turn instruction finetuning dataset, called
M2Lingual, to better align LLMs on a diverse set of languages and tasks.
M2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds,
covering 70 languages, 17 NLP tasks and general instruction-response pairs.
LLMs finetuned with M2Lingual substantially outperform the majority of existing
multilingual IFT datasets. Importantly, LLMs trained with M2Lingual
consistently achieve competitive results across a wide variety of evaluation
benchmarks compared to existing multilingual IFT datasets. Specifically, LLMs
finetuned with M2Lingual achieve strong performance on our translated
multilingual, multi-turn evaluation benchmark as well as a wide variety of
multilingual tasks. Thus we contribute, and the 2 step Evol taxonomy used for
its creation. M2Lingual repository -
https://huggingface.co/datasets/ServiceNow-AI/M2Lingual
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ It Is Not About What You Say, It Is About How You Say It: A Surprisingly
  Simple Approach for Improving Reading Comprehension <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagi Shaier, Lawrence E Hunter, Katharina von der Wense
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing has seen rapid progress over the past decade. Due
to the speed of developments, some practices get established without proper
evaluation. Considering one such case and focusing on reading comprehension, we
ask our first research question: 1) How does the order of inputs -- i.e.,
question and context -- affect model performance? Additionally, given recent
advancements in input emphasis, we ask a second research question: 2) Does
emphasizing either the question, the context, or both enhance performance?
Experimenting with 9 large language models across 3 datasets, we find that
presenting the context before the question improves model performance, with an
accuracy increase of up to $31\%$. Furthermore, emphasizing the context yields
superior results compared to question emphasis, and in general, emphasizing
parts of the input is particularly effective for addressing questions that
models lack the parametric knowledge to answer. Experimenting with both
prompt-based and attention-based emphasis methods, we additionally find that
the best method is surprisingly simple: it only requires concatenating a few
tokens to the input and results in an accuracy improvement of up to $36\%$,
allowing smaller models to outperform their significantly larger counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding Transformer Circuits with Edge Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adithya Bhaskar, Alexander Wettig, Dan Friedman, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The path to interpreting a language model often proceeds via analysis of
circuits -- sparse computational subgraphs of the model that capture specific
aspects of its behavior. Recent work has automated the task of discovering
circuits. Yet, these methods have practical limitations, as they rely either on
inefficient search algorithms or inaccurate approximations. In this paper, we
frame automated circuit discovery as an optimization problem and propose *Edge
Pruning* as an effective and scalable solution. Edge Pruning leverages
gradient-based pruning techniques, but instead of removing neurons or
components, it prunes the \emph{edges} between components. Our method finds
circuits in GPT-2 that use less than half the number of edges compared to
circuits found by previous methods while being equally faithful to the full
model predictions on standard circuit-finding tasks. Edge Pruning is efficient
even with as many as 100K examples, outperforming previous methods in speed and
producing substantially better circuits. It also perfectly recovers the
ground-truth circuits in two models compiled with Tracr. Thanks to its
efficiency, we scale Edge Pruning to CodeLlama-13B, a model over 100x the scale
that prior methods operate on. We use this setting for a case study comparing
the mechanisms behind instruction prompting and in-context learning. We find
two circuits with more than 99.96% sparsity that match the performance of the
full model and reveal that the mechanisms in the two settings overlap
substantially. Our case study shows that Edge Pruning is a practical and
scalable tool for interpretability and sheds light on behaviors that only
emerge in large models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We release our code and data publicly at
  https://github.com/princeton-nlp/Edge-Pruning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blending <span class="highlight-title">LLM</span>s into Cascaded Speech Translation: KIT's Offline Speech
  Translation System for IWSLT 2024 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Koneru, Thai-Binh Nguyen, Ngoc-Quan Pham, Danni Liu, Zhaolin Li, Alexander Waibel, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are currently under exploration for various
tasks, including Automatic Speech Recognition (ASR), Machine Translation (MT),
and even End-to-End Speech Translation (ST). In this paper, we present KIT's
offline submission in the constrained + LLM track by incorporating recently
proposed techniques that can be added to any cascaded speech translation.
Specifically, we integrate
Mistral-7B\footnote{mistralai/Mistral-7B-Instruct-v0.1} into our system to
enhance it in two ways. Firstly, we refine the ASR outputs by utilizing the
N-best lists generated by our system and fine-tuning the LLM to predict the
transcript accurately. Secondly, we refine the MT outputs at the document level
by fine-tuning the LLM, leveraging both ASR and MT predictions to improve
translation quality. We find that integrating the LLM into the ASR and MT
systems results in an absolute improvement of $0.3\%$ in Word Error Rate and
$0.65\%$ in COMET for tst2019 test set. In challenging test sets with
overlapping speakers and background noise, we find that integrating LLM is not
beneficial due to poor ASR performance. Here, we use ASR with chunked long-form
decoding to improve context usage that may be unavailable when transcribing
with Voice Activity Detection segmentation alone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Huang, Zengzhi Wang, Shijie Xia, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we pose the following question: Who is the most intelligent
AI model to date, as measured by the OlympicArena (an Olympic-level,
multi-discipline, multi-modal benchmark for superintelligent AI)? We
specifically focus on the most recently released models: Claude-3.5-Sonnet,
Gemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic
medal Table approach to rank AI models based on their comprehensive performance
across various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet
shows highly competitive overall performance over GPT-4o, even surpassing
GPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)
Gemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and
Claude-3.5-Sonnet, but with a clear performance gap between them. (3) The
performance of AI models from the open-source community significantly lags
behind these proprietary models. (4) The performance of these models on this
benchmark has been less than satisfactory, indicating that we still have a long
way to go before achieving superintelligence. We remain committed to
continuously tracking and evaluating the performance of the latest powerful
models on this benchmark (available at
https://github.com/GAIR-NLP/OlympicArena).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The GPT-WritingPrompts <span class="highlight-title">Dataset</span>: A Comparative Analysis of Character
  Portrayal in Short Stories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Yu Huang, Krishnapriya Vishnubhotla, Frank Rudzicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The improved generative capabilities of large language models have made them
a powerful tool for creative writing and storytelling. It is therefore
important to quantitatively understand the nature of generated stories, and how
they differ from human storytelling. We augment the Reddit WritingPrompts
dataset with short stories generated by GPT-3.5, given the same prompts. We
quantify and compare the emotional and descriptive features of storytelling
from both generative processes, human and machine, along a set of six
dimensions. We find that generated stories differ significantly from human
stories along all six dimensions, and that human and machine generations
display similar biases when grouped according to the narrative point-of-view
and gender of the main protagonist. We release our dataset and code at
https://github.com/KristinHuangg/gpt-writing-prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Fast Multilingual <span class="highlight-title">LLM</span> Inference: Speculative Decoding and
  Specialized Drafters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Euiin Yi, Taehyeon Kim, Hongseok Jeung, Du-Seong Chang, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized natural language processing
and broadened their applicability across diverse commercial applications.
However, the deployment of these models is constrained by high inference time
in multilingual settings. To mitigate this challenge, this paper explores a
training recipe of an assistant model in speculative decoding, which are
leveraged to draft and-then its future tokens are verified by the target LLM.
We show that language-specific draft models, optimized through a targeted
pretrain-and-finetune strategy, substantially brings a speedup of inference
time compared to the previous methods. We validate these models across various
languages in inference time, out-of-domain speedup, and GPT-4o evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Zero-Shot Text-To-Speech for Arabic Dialects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khai Duy Doan, Abdul Waheed, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot multi-speaker text-to-speech (ZS-TTS) systems have advanced for
English, however, it still lags behind due to insufficient resources. We
address this gap for Arabic, a language of more than 450 million native
speakers, by first adapting a sizeable existing dataset to suit the needs of
speech synthesis. Additionally, we employ a set of Arabic dialect
identification models to explore the impact of pre-defined dialect labels on
improving the ZS-TTS model in a multi-dialect setting. Subsequently, we
fine-tune the
XTTS\footnote{https://docs.coqui.ai/en/latest/models/xtts.html}\footnote{https://medium.com/machine-learns/xtts-v2-new-version-of-the-open-source-text-to-speech-model-af73914db81f}\footnote{https://medium.com/@erogol/xtts-v1-techincal-notes-eb83ff05bdc}
model, an open-source architecture. We then evaluate our models on a dataset
comprising 31 unseen speakers and an in-house dialectal dataset. Our automated
and human evaluation results show convincing performance while capable of
generating dialectal speech. Our study highlights significant potential for
improvements in this emerging area of research in Arabic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OCALM: Object-Centric Assessment with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Kaufmann, Jannis Blüml, Antonia Wüst, Quentin Delfosse, Kristian Kersting, Eyke Hüllermeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Properly defining a reward signal to efficiently train a reinforcement
learning (RL) agent is a challenging task. Designing balanced objective
functions from which a desired behavior can emerge requires expert knowledge,
especially for complex environments. Learning rewards from human feedback or
using large language models (LLMs) to directly provide rewards are promising
alternatives, allowing non-experts to specify goals for the agent. However,
black-box reward models make it difficult to debug the reward. In this work, we
propose Object-Centric Assessment with Language Models (OCALM) to derive
inherently interpretable reward functions for RL agents from natural language
task descriptions. OCALM uses the extensive world-knowledge of LLMs while
leveraging the object-centric nature common to many environments to derive
reward functions focused on relational concepts, providing RL agents with the
ability to derive policies from task descriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the RLBRew Workshop at RLC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparser is Faster and Less is More: Efficient Sparse Attention for
  Long-Range Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accommodating long sequences efficiently in autoregressive Transformers,
especially within an extended context window, poses significant challenges due
to the quadratic computational complexity and substantial KV memory
requirements inherent in self-attention mechanisms. In this work, we introduce
SPARSEK Attention, a novel sparse attention mechanism designed to overcome
these computational and memory obstacles while maintaining performance. Our
approach integrates a scoring network and a differentiable top-k mask operator,
SPARSEK, to select a constant number of KV pairs for each query, thereby
enabling gradient-based optimization. As a result, SPARSEK Attention offers
linear time complexity and constant memory footprint during generation.
Experimental results reveal that SPARSEK Attention outperforms previous sparse
attention methods and provides significant speed improvements during both
training and inference, particularly in language modeling and downstream tasks.
Furthermore, our method can be seamlessly integrated into pre-trained Large
Language Models (LLMs) with minimal fine-tuning, offering a practical solution
for effectively managing long-range dependencies in diverse applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Responsible Foundation Model Development Cheatsheet: A <span class="highlight-title">Review</span> of
  Tools & Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, Sayash Kapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, Maribeth Rauh, Aviya Skowron, Bertie Vidgen, Laura Weidinger, Arvind Narayanan, Victor Sanh, David Adelani, Percy Liang, Rishi Bommasani, Peter Henderson, Sasha Luccioni, Yacine Jernite, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation model development attracts a rapidly expanding body of
contributors, scientists, and applications. To help shape responsible
development practices, we introduce the Foundation Model Development
Cheatsheet: a growing collection of 250+ tools and resources spanning text,
vision, and speech modalities. We draw on a large body of prior work to survey
resources (e.g. software, documentation, frameworks, guides, and practical
tools) that support informed data selection, processing, and understanding,
precise and limitation-aware artifact documentation, efficient model training,
advance awareness of the environmental impact from training, careful model
evaluation of capabilities, risks, and claims, as well as responsible model
release, licensing and deployment practices. We hope this curated collection of
resources helps guide more responsible development. The process of curating
this list, enabled us to review the AI development ecosystem, revealing what
tools are critically missing, misused, or over-used in existing practices. We
find that (i) tools for data sourcing, model evaluation, and monitoring are
critically under-serving ethical and real-world needs, (ii) evaluations for
model safety, capabilities, and environmental impact all lack reproducibility
and transparency, (iii) text and particularly English-centric analyses continue
to dominate over multilingual and multi-modal analyses, and (iv) evaluation of
systems, rather than just models, is needed so that capabilities and impact are
assessed in context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Contrastive Decoding: Boosting Safety Alignment of Large
  Language Models via Opposite Prompt Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyue Zhao, Xiaoyun Zhang, Kaidi Xu, Xing Hu, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread application of Large Language Models (LLMs), it has
become a significant concern to ensure their safety and prevent harmful
responses. While current safe-alignment methods based on instruction
fine-tuning and Reinforcement Learning from Human Feedback (RLHF) can
effectively reduce harmful responses from LLMs, they often require high-quality
datasets and heavy computational overhead during model training. Another way to
align language models is to modify the logit of tokens in model outputs without
heavy training. Recent studies have shown that contrastive decoding can enhance
the performance of language models by reducing the likelihood of confused
tokens. However, these methods require the manual selection of contrastive
models or instruction templates. To this end, we propose Adversarial
Contrastive Decoding (ACD), an optimization-based framework to generate two
opposite system prompts for prompt-based contrastive decoding. ACD only needs
to apply a lightweight prompt tuning on a rather small anchor dataset (< 3 min
for each model) without training the target model. Experiments conducted on
extensive models and benchmarks demonstrate that the proposed method achieves
much better safety performance than previous model training-free decoding
methods without sacrificing its original generation ability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIMATELI: Evaluating Entity Linking on Climate Change Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijia Zhou, Siyao Peng, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate Change (CC) is a pressing topic of global importance, attracting
increasing attention across research fields, from social sciences to Natural
Language Processing (NLP). CC is also discussed in various settings and
communication platforms, from academic publications to social media forums.
Understanding who and what is mentioned in such data is a first critical step
to gaining new insights into CC. We present CLIMATELI (CLIMATe Entity LInking),
the first manually annotated CC dataset that links 3,087 entity spans to
Wikipedia. Using CLIMATELI (CLIMATe Entity LInking), we evaluate existing
entity linking (EL) systems on the CC topic across various genres and propose
automated filtering methods for CC entities. We find that the performance of EL
models notably lags behind humans at both token and entity levels. Testing
within the scope of retaining or excluding non-nominal and/or non-CC entities
particularly impacts the models' performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, ClimateNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Venturing into Uncharted Waters: The Navigation Compass from Transformer
  to Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Zou, Yineng Chen, Zuchao Li, Lefei Zhang, Hai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer, a deep neural network architecture, has long dominated the field
of natural language processing and beyond. Nevertheless, the recent
introduction of Mamba challenges its supremacy, sparks considerable interest
among researchers, and gives rise to a series of Mamba-based models that have
exhibited notable potential. This survey paper orchestrates a comprehensive
discussion, diving into essential research dimensions, covering: (i) the
functioning of the Mamba mechanism and its foundation on the principles of
structured state space models; (ii) the proposed improvements and the
integration of Mamba with various networks, exploring its potential as a
substitute for Transformers; (iii) the combination of Transformers and Mamba to
compensate for each other's shortcomings. We have also made efforts to
interpret Mamba and Transformer in the framework of kernel functions, allowing
for a comparison of their mathematical nature within a unified context. Our
paper encompasses the vast majority of improvements related to Mamba to date.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoDetect: Towards a Unified Framework for Automated Weakness Detection
  in <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) are becoming increasingly powerful,
they still exhibit significant but subtle weaknesses, such as mistakes in
instruction-following or coding tasks. As these unexpected errors could lead to
severe consequences in practical deployments, it is crucial to investigate the
limitations within LLMs systematically. Traditional benchmarking approaches
cannot thoroughly pinpoint specific model deficiencies, while manual
inspections are costly and not scalable. In this paper, we introduce a unified
framework, AutoDetect, to automatically expose weaknesses in LLMs across
various tasks. Inspired by the educational assessment process that measures
students' learning outcomes, AutoDetect consists of three LLM-powered agents:
Examiner, Questioner, and Assessor. The collaboration among these three agents
is designed to realize comprehensive and in-depth weakness identification. Our
framework demonstrates significant success in uncovering flaws, with an
identification success rate exceeding 30% in prominent models such as ChatGPT
and Claude. More importantly, these identified weaknesses can guide specific
model improvements, proving more effective than untargeted data augmentation
methods like Self-Instruct. Our approach has led to substantial enhancements in
popular LLMs, including the Llama series and Mistral-7b, boosting their
performance by over 10% across several benchmarks. Code and data are publicly
available at https://github.com/thu-coai/AutoDetect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Oriented In-Domain Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liang, Xinyu Hu, Simiao Zuo, Yeyun Gong, Qiang Lou, Yi Liu, Shao-Lun Huang, Jian Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown superior performance in various
applications and fields. To achieve better performance on specialized domains
such as law and advertisement, LLMs are often continue pre-trained on in-domain
data. However, existing approaches suffer from two major issues. First,
in-domain data are scarce compared with general domain-agnostic data. Second,
data used for continual pre-training are not task-aware, such that they may not
be helpful to downstream applications. We propose TRAIT, a task-oriented
in-domain data augmentation framework. Our framework is divided into two parts:
in-domain data selection and task-oriented synthetic passage generation. The
data selection strategy identifies and selects a large amount of in-domain data
from general corpora, and thus significantly enriches domain knowledge in the
continual pre-training data. The synthetic passages contain guidance on how to
use domain knowledge to answer questions about downstream tasks. By training on
such passages, the model aligns with the need of downstream applications. We
adapt LLMs to two domains: advertisement and math. On average, TRAIT improves
LLM performance by 8% in the advertisement domain and 7.5% in the math domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws for Linear Complexity Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, Yiran Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interest in linear complexity models for large language models is on the
rise, although their scaling capacity remains uncertain. In this study, we
present the scaling laws for linear complexity language models to establish a
foundation for their scalability. Specifically, we examine the scaling
behaviors of three efficient linear architectures. These include TNL, a linear
attention model with data-independent decay; HGRN2, a linear RNN with
data-dependent decay; and cosFormer2, a linear attention model without decay.
We also include LLaMA as a baseline architecture for softmax attention for
comparison. These models were trained with six variants, ranging from 70M to 7B
parameters on a 300B-token corpus, and evaluated with a total of 1,376
intermediate checkpoints on various downstream tasks. These tasks include
validation loss, commonsense reasoning, and information retrieval and
generation. The study reveals that existing linear complexity language models
exhibit similar scaling capabilities as conventional transformer-based models
while also demonstrating superior linguistic proficiency and knowledge
retention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. Yiran Zhong is the corresponding author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Any Text: A Universal Approach for Robust, Efficient and
  Adaptable Sentence Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Frohmann, Igor Sterner, Ivan Vulić, Benjamin Minixhofer, Markus Schedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting text into sentences plays an early and crucial role in many NLP
systems. This is commonly achieved by using rule-based or statistical methods
relying on lexical features such as punctuation. Although some recent works no
longer exclusively rely on punctuation, we find that no prior method achieves
all of (i) robustness to missing punctuation, (ii) effective adaptability to
new domains, and (iii) high efficiency. We introduce a new model - Segment any
Text (SaT) - to solve this problem. To enhance robustness, we propose a new
pretraining scheme that ensures less reliance on punctuation. To address
adaptability, we introduce an extra stage of parameter-efficient fine-tuning,
establishing state-of-the-art performance in distinct domains such as verses
from lyrics and legal documents. Along the way, we introduce architectural
modifications that result in a threefold gain in speed over the previous state
of the art and solve spurious reliance on context far in the future. Finally,
we introduce a variant of our model with fine-tuning on a diverse, multilingual
mixture of sentence-segmented data, acting as a drop-in replacement and
enhancement for existing segmentation tools. Overall, our contributions provide
a universal approach for segmenting any text. Our method outperforms all
baselines - including strong LLMs - across 8 corpora spanning diverse domains
and languages, especially in practically relevant situations where text is
poorly formatted. Our models and code, including documentation, are available
at https://huggingface.co/segment-any-text under the MIT license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Approaches to the Detection of Lesser-Known Rhetorical
  Figures: A Systematic <span class="highlight-title">Survey</span> and Research Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramona Kühn, Jelena Mitrović, Michael Granitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rhetorical figures play a major role in our everyday communication as they
make text more interesting, more memorable, or more persuasive. Therefore, it
is important to computationally detect rhetorical figures to fully understand
the meaning of a text. We provide a comprehensive overview of computational
approaches to lesser-known rhetorical figures. We explore the linguistic and
computational perspectives on rhetorical figures, emphasizing their
significance for the domain of Natural Language Processing. We present
different figures in detail, delving into datasets, definitions, rhetorical
functions, and detection approaches. We identified challenges such as dataset
scarcity, language limitations, and reliance on rule-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACM Computing Surveys. 35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAVE: Controllable Authorship Verification Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahana Ramnath, Kartik Pandey, Elizabeth Boschee, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Authorship Verification (AV) (do two documents have the same author?) is
essential for many sensitive real-life applications. AV is often used in
proprietary domains that require a private, offline model, making SOTA online
models like ChatGPT undesirable. Other SOTA systems use methods, e.g. Siamese
Networks, that are uninterpretable, and hence cannot be trusted in high-stakes
applications. In this work, we take the first step to address the above
challenges with our model CAVE (Controllable Authorship Verification
Explanations): CAVE generates free-text AV explanations that are controlled to
be 1) structured (can be decomposed into sub-explanations with respect to
relevant linguistic features), and 2) easily verified for explanation-label
consistency (via intermediate labels in sub-explanations). In this work, we
train a Llama-3-8B as CAVE; since there are no human-written corpora for AV
explanations, we sample silver-standard explanations from GPT-4-TURBO and
distill them into a pretrained Llama-3-8B. Results on three difficult AV
datasets IMdB2, Blog-Auth, and FanFiction show that CAVE generates high quality
explanations (as measured by automatic and human evaluation) as well as
competitive task accuracies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Models</span> Are Cross-Lingual Knowledge-Free Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Hu, Sizhe Liu, Changjiang Gao, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated impressive reasoning capabilities
across multiple languages. However, the relationship between capabilities in
different languages is less explored. In this work, we decompose the process of
reasoning tasks into two separated parts: knowledge retrieval and
knowledge-free reasoning, and analyze the cross-lingual transferability of
them. With adapted and constructed knowledge-free reasoning datasets, we show
that the knowledge-free reasoning capability can be nearly perfectly
transferred across various source-target language directions despite the
secondary impact of resource in some specific target languages, while
cross-lingual knowledge retrieval significantly hinders the transfer. Moreover,
by analyzing the hidden states and feed-forward network neuron activation
during the reasoning tasks, we show that higher similarity of hidden
representations and larger overlap of activated neurons could explain the
better cross-lingual transferability of knowledge-free reasoning than knowledge
retrieval. Thus, we hypothesize that knowledge-free reasoning embeds in some
language-shared mechanism, while knowledge is stored separately in different
languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shadow<span class="highlight-title">LLM</span>: Predictor-based Contextual Sparsity for <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Akhauri, Ahmed F AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M Rush, Safeen Huda, Mohamed S Abdelfattah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high power consumption and latency-sensitive deployments of large
language models (LLMs) have motivated techniques like quantization and
sparsity. Contextual sparsity, where the sparsity pattern is input-dependent,
is crucial in LLMs because the permanent removal of attention heads or neurons
from LLMs can significantly degrade accuracy. Prior work has attempted to model
contextual sparsity using neural networks trained to predict activation
magnitudes, which can be used to dynamically prune structures with low
predicted activation magnitude. In this paper, we look beyond magnitude-based
pruning criteria to assess attention head and neuron importance in LLMs. We
developed a novel predictor called ShadowLLM, which can shadow the LLM behavior
and enforce better sparsity patterns, resulting in over 15% improvement in
end-to-end accuracy without increasing latency compared to previous methods.
ShadowLLM achieves up to a 20\% speed-up over the state-of-the-art DejaVu
framework. These enhancements are validated on models with up to 30 billion
parameters. Our code is available at
\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Language Models in the Medical Context Under
  Resource-Constrained Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Posada, Daniel Rueckert, Felix Meissen, Philip Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the emergence of the Transformer architecture, language model
development has increased, driven by their promising potential. However,
releasing these models into production requires properly understanding their
behavior, particularly in sensitive domains such as medicine. Despite this
need, the medical literature still lacks technical assessments of pre-trained
language models, which are especially valuable in resource-constrained settings
in terms of computational power or limited budget. To address this gap, we
provide a comprehensive survey of language models in the medical domain. In
addition, we selected a subset of these models for thorough evaluation,
focusing on classification and text generation tasks. Our subset encompasses 53
models, ranging from 110 million to 13 billion parameters, spanning the three
families of Transformer-based models and from diverse knowledge domains. This
study employs a series of approaches for text classification together with
zero-shot prompting instead of model training or fine-tuning, which closely
resembles the limited resource setting in which many users of language models
find themselves. Encouragingly, our findings reveal remarkable performance
across various tasks and datasets, underscoring the latent potential of certain
models to contain medical knowledge, even without domain specialization.
Consequently, our study advocates for further exploration of model applications
in medical contexts, particularly in resource-constrained settings. The code is
available on https://github.com/anpoc/Language-models-in-medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLEAR: Can Language Models Really Understand Causal Graphs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirui Chen, Mengying Xu, Kun Wang, Xingyu Zeng, Rui Zhao, Shengjie Zhao, Chaochao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal reasoning is a cornerstone of how humans interpret the world. To model
and reason about causality, causal graphs offer a concise yet effective
solution. Given the impressive advancements in language models, a crucial
question arises: can they really understand causal graphs? To this end, we
pioneer an investigation into language models' understanding of causal graphs.
Specifically, we develop a framework to define causal graph understanding, by
assessing language models' behaviors through four practical criteria derived
from diverse disciplines (e.g., philosophy and psychology). We then develop
CLEAR, a novel benchmark that defines three complexity levels and encompasses
20 causal graph-based tasks across these levels. Finally, based on our
framework and benchmark, we conduct extensive experiments on six leading
language models and summarize five empirical findings. Our results indicate
that while language models demonstrate a preliminary understanding of causal
graphs, significant potential for improvement remains. Our project website is
at https://github.com/OpenCausaLab/CLEAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation of Multi-turn Psychological Dialogue via
  Knowledge-driven Progressive Thought Prompting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyue Jiang, Liheng Chen, Sheng Wang, Lingpeng Kong, Yu Li, Chuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing dialogue data augmentation (DA) techniques predominantly focus on
augmenting utterance-level dialogues, which makes it difficult to take dialogue
contextual information into account. The advent of large language models (LLMs)
has simplified the implementation of multi-turn dialogues. Due to absence of
professional understanding and knowledge, it remains challenging to deliver
satisfactory performance in low-resource domain, like psychological dialogue
dialogue. DA involves creating new training or prompting data based on the
existing data, which help the model better understand and generate
psychology-related responses. In this paper, we aim to address the issue of
multi-turn dialogue data augmentation for boosted performance in the psychology
domain. We propose a knowledge-driven progressive thought prompting method to
guide LLM to generate multi-turn psychology-related dialogue. This method
integrates a progressive thought generator, a psychology knowledge generator,
and a multi-turn dialogue generator. The thought generated by the progressive
thought generator serves as a prompt to prevent the generated dialogue from
having significant semantic deviations, while the psychology knowledge
generator produces psychological knowledge to serve as the dialogue history for
the LLM, guiding the dialogue generator to create multi-turn psychological
dialogue. To ensure the precision of multi-turn psychological dialogue
generation by LLM, a meticulous professional evaluation is required. Extensive
experiments conducted on three datasets related to psychological dialogue
verify the effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are there identifiable structural parts in the sentence embedding whole? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivi Nastase, Paola Merlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentence embeddings from transformer models encode in a fixed length vector
much linguistic information. We explore the hypothesis that these embeddings
consist of overlapping layers of information that can be separated, and on
which specific types of information -- such as information about chunks and
their structural and semantic properties -- can be detected. We show that this
is the case using a dataset consisting of sentences with known chunk structure,
and two linguistic intelligence datasets, solving which relies on detecting
chunks and their grammatical number, and respectively, their semantic roles,
and through analyses of the performance on the tasks and of the internal
representations built during learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 14 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EvalAlign: Evaluating Text-to-Image Models through Precision Alignment
  of Multimodal Large Models with Supervised Fine-Tuning to Human Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, Mengping Yang, Cheng Zhang, Hao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in text-to-image generative models have been
remarkable. Yet, the field suffers from a lack of evaluation metrics that
accurately reflect the performance of these models, particularly lacking
fine-grained metrics that can guide the optimization of the models. In this
paper, we propose EvalAlign, a metric characterized by its accuracy, stability,
and fine granularity. Our approach leverages the capabilities of Multimodal
Large Language Models (MLLMs) pre-trained on extensive datasets. We develop
evaluation protocols that focus on two key dimensions: image faithfulness and
text-image alignment. Each protocol comprises a set of detailed, fine-grained
instructions linked to specific scoring options, enabling precise manual
scoring of the generated images. We Supervised Fine-Tune (SFT) the MLLM to
align closely with human evaluative judgments, resulting in a robust evaluation
model. Our comprehensive tests across 24 text-to-image generation models
demonstrate that EvalAlign not only provides superior metric stability but also
aligns more closely with human preferences than existing metrics, confirming
its effectiveness and utility in model assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Repository: https://github.com/SAIS-FUXI/EvalAlign</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual
  Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, Yu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) has gained increasing popularity as a promising
framework for scaling up large language models (LLMs). However, training MoE
from scratch in a large-scale setting still suffers from data-hungry and
instability problems. Motivated by this limit, we investigate building MoE
models from existing dense large language models. Specifically, based on the
well-known LLaMA-2 7B model, we obtain an MoE model by: (1) Expert
Construction, which partitions the parameters of original Feed-Forward Networks
(FFNs) into multiple experts; (2) Continual Pre-training, which further trains
the transformed MoE model and additional gate networks. In this paper, we
comprehensively explore different methods for expert construction and various
data sampling strategies for continual pre-training. After these stages, our
LLaMA-MoE models could maintain language abilities and route the input tokens
to specific experts with part of the parameters activated. Empirically, by
training 200B tokens, LLaMA-MoE-3.5B models significantly outperform dense
models that contain similar activation parameters. The source codes and models
are available at https://github.com/pjlab-sys4nlp/llama-moe .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C-<span class="highlight-title">LLM</span>: Learn to Check Chinese Spelling Errors Character by Character 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunting Li, Yong Hu, Liang He, Fandong Meng, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese Spell Checking (CSC) aims to detect and correct spelling errors in
sentences. Despite Large Language Models (LLMs) exhibit robust capabilities and
are widely applied in various tasks, their performance on CSC is often
unsatisfactory. We find that LLMs fail to meet the Chinese character-level
constraints of the CSC task, namely equal length and phonetic similarity,
leading to a performance bottleneck. Further analysis reveal that this issue
stems from the granularity of tokenization, as current mixed character-word
tokenization struggles to satisfy these character-level constraints. To address
this issue, we propose C-LLM, a Large Language Model-based Chinese Spell
Checking method that learns to check errors Character by Character.
Character-level tokenization enables the model to learn character-level
alignment, effectively mitigating issues related to character-level
constraints. Furthermore, CSC is simplified to replication-dominated and
substitution-supplemented tasks. Experiments on two CSC benchmarks demonstrate
that C-LLM achieves an average improvement of 10% over existing methods.
Specifically, it shows a 2.1% improvement in general scenarios and a
significant 12% improvement in vertical domain scenarios, establishing
state-of-the-art performance. The source code can be accessed at
https://github.com/ktlKTL/C-LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-based Decision Criteria Are Suboptimal in In-context Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakaze Cho, Yoshihiro Sakai, Mariko Kato, Kenshiro Tanaka, Akira Ishii, Naoya Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) typically utilizes classification criteria from
probabilities of manually selected label tokens. However, we argue that such
token-based classification criteria lead to suboptimal decision boundaries,
despite delicate calibrations through translation and constrained rotation. To
address this problem, we propose Hidden Calibration, which renounces token
probabilities and uses the nearest centroid classifier on the LM's last hidden
states. In detail, we use the nearest centroid classification on the hidden
states, assigning the category of the nearest centroid previously observed from
a few-shot calibration set to the test sample as the predicted label. Our
experiments on 3 models and 10 classification datasets indicate that Hidden
Calibration consistently outperforms current token-based calibrations by about
20%. Our further analysis demonstrates that Hidden Calibration finds better
classification criteria with less inter-categories overlap, and LMs provide
linearly separable intra-category clusters with the help of demonstrations,
which supports Hidden Calibration and gives new insights into the conventional
ICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 14 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Better Graph-based Cross-document Relation Extraction via
  Non-bridge Entity Enhancement and Prediction Debiasing <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yue, Shaopeng Lai, Chengyi Yang, Liang Zhang, Junfeng Yao, Jinsong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-document Relation Extraction aims to predict the relation between
target entities located in different documents. In this regard, the dominant
models commonly retain useful information for relation prediction via bridge
entities, which allows the model to elaborately capture the intrinsic
interdependence between target entities. However, these studies ignore the
non-bridge entities, each of which co-occurs with only one target entity and
offers the semantic association between target entities for relation
prediction. Besides, the commonly-used dataset--CodRED contains substantial NA
instances, leading to the prediction bias during inference. To address these
issues, in this paper, we propose a novel graph-based cross-document RE model
with non-bridge entity enhancement and prediction debiasing. Specifically, we
use a unified entity graph to integrate numerous non-bridge entities with
target entities and bridge entities, modeling various associations between
them, and then use a graph recurrent network to encode this graph. Finally, we
introduce a novel debiasing strategy to calibrate the original prediction
distribution. Experimental results on the closed and open settings show that
our model significantly outperforms all baselines, including the GPT-3.5-turbo
and InstructUIE, achieving state-of-the-art performance. Particularly, our
model obtains 66.23% and 55.87% AUC points in the official
leaderboard\footnote{\url{https://codalab.lisn.upsaclay.fr/competitions/3770#results}}
under the two settings, respectively, ranking the first place in all
submissions since December 2023. Our code is available at
https://github.com/DeepLearnXMU/CoRE-NEPD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Ability of <span class="highlight-title">Large Language Models</span> to Reason about Cardinal
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony G Cohn, Robert E Blackwell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the abilities of a representative set of Large language Models
(LLMs) to reason about cardinal directions (CDs). To do so, we create two
datasets: the first, co-created with ChatGPT, focuses largely on recall of
world knowledge about CDs; the second is generated from a set of templates,
comprehensively testing an LLM's ability to determine the correct CD given a
particular scenario. The templates allow for a number of degrees of variation
such as means of locomotion of the agent involved, and whether set in the first
, second or third person. Even with a temperature setting of zero, Our
experiments show that although LLMs are able to perform well in the simpler
dataset, in the second more complex dataset no LLM is able to reliably
determine the correct CD, even with a temperature setting of zero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 1 table. Short paper accepted by COSIT 24, The
  16th Conference on Spatial Information Theory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyROCCo: Enhancing Systematic <span class="highlight-title">Review</span>s using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Fang, Miguel Arana-Catania, Felix-Anselm van Lier, Juliana Outes Velarde, Harry Bregazzi, Mara Airoldi, Eleanor Carter, Rob Procter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sheer number of research outputs published every year makes systematic
reviewing increasingly time- and resource-intensive. This paper explores the
use of machine learning techniques to help navigate the systematic review
process. ML has previously been used to reliably 'screen' articles for review -
that is, identify relevant articles based on reviewers' inclusion criteria. The
application of ML techniques to subsequent stages of a review, however, such as
data extraction and evidence mapping, is in its infancy. We therefore set out
to develop a series of tools that would assist in the profiling and analysis of
1,952 publications on the theme of 'outcomes-based contracting'. Tools were
developed for the following tasks: assign publications into 'policy area'
categories; identify and extract key information for evidence mapping, such as
organisations, laws, and geographical information; connect the evidence base to
an existing dataset on the same topic; and identify subgroups of articles that
may share thematic content. An interactive tool using these techniques and a
public dataset with their outputs have been released. Our results demonstrate
the utility of ML techniques to enhance evidence accessibility and analysis
within the systematic review processes. These efforts show promise in
potentially yielding substantial efficiencies for future systematic reviewing
and for broadening their analytical scope. Our work suggests that there may be
implications for the ease with which policymakers and practitioners can access
evidence. While ML techniques seem poised to play a significant role in
bridging the gap between research and policy by offering innovative ways of
gathering, accessing, and analysing data from systematic reviews, we also
highlight their current limitations and the need to exercise caution in their
application, particularly given the potential for errors and biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures. To appear in Data & Policy journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Privileged Students: On the Value of Initialization in Multilingual
  Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haryo Akbarianto Wibowo, Thamar Solorio, Alham Fikri Aji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has proven to be a successful strategy to improve
the performance of a smaller model in many NLP tasks. However, most of the work
in KD only explores monolingual scenarios. In this paper, we investigate the
value of KD in multilingual settings. We find the significance of KD and model
initialization by analyzing how well the student model acquires multilingual
knowledge from the teacher model. Our proposed method emphasizes copying the
teacher model's weights directly to the student model to enhance
initialization. Our finding shows that model initialization using copy-weight
from the fine-tuned teacher contributes the most compared to the distillation
process itself across various multilingual settings. Furthermore, we
demonstrate that efficient weight initialization preserves multilingual
capabilities even in low-resource scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Carrot and Stick: Inducing Self-Motivation with Positive & Negative
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimin Sohn, Jeihee Cho, Junyong Lee, Songmu Heo, Ji-Eun Han, David R. Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positive thinking is thought to be an important component of self-motivation
in various practical fields such as education and the workplace. Previous work,
including sentiment transfer and positive reframing, has focused on the
positive side of language. However, self-motivation that drives people to reach
their goals has not yet been studied from a computational perspective.
Moreover, negative feedback has not yet been explored, even though positive and
negative feedback are both necessary to grow self-motivation. To facilitate
self-motivation, we propose CArrot and STICk (CASTIC) dataset, consisting of
12,590 sentences with 5 different strategies for enhancing self-motivation. Our
data and code are publicly available at here.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Vocabulary Size Improves <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sho Takase, Ryokan Ri, Shun Kiyono, Takuya Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper empirically investigates the relationship between subword
vocabulary size and the performance of large language models (LLMs) to provide
insights on how to define the vocabulary size. Experimental results show that
larger vocabulary sizes lead to better performance in LLMs. Moreover, we
consider a continual training scenario where a pre-trained language model is
trained on a different target language. We introduce a simple method to use a
new vocabulary instead of the pre-defined one. We show that using the new
vocabulary outperforms the model with the vocabulary used in pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to
  construct Observer-Thinker-Conceiver-Expresser 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Shi, Ting Xie, Bingheng Wu, Chunjun Zheng, Kai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has shown that combining Mamba with Transformer architecture,
which has selective state space and quadratic self-attention mechanism,
outperforms using Mamba or Transformer architecture alone in language modeling
tasks. The quadratic self-attention mechanism effectively alleviates the
shortcomings of selective state space in handling long-term dependencies of any
element in the sequence. We propose a position information injection method
that connects the selective state space model with the quadratic attention, and
integrates these two architectures with hybrid experts with cross-sharing
domains, so that we can enjoy the advantages of both. We design a new
architecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser
(OTCE), which can compete with well-known medium-scale open-source language
models on a small scale in language modeling tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eagerlearners at SemEval2024 Task 5: The Legal Argument Reasoning Task
  in Civil Procedure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoorieh Sabzevari, Mohammadmostafa Rostamkhani, Sauleh Eetemadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the performance of the zero-shot method in
classifying data using three large language models, alongside two models with
large input token sizes and the two pre-trained models on legal data. Our main
dataset comes from the domain of U.S. civil procedure. It includes summaries of
legal cases, specific questions, potential answers, and detailed explanations
for why each solution is relevant, all sourced from a book aimed at law
students. By comparing different methods, we aimed to understand how
effectively they handle the complexities found in legal datasets. Our findings
show how well the zero-shot method of large language models can understand
complicated data. We achieved our highest F1 score of 64% in these experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepfake tweets automatic detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Frej, Adrian Kaminski, Piotr Marciniak, Szymon Szmajdzinski, Soveatin Kuntur, Anna Wroblewska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the critical challenge of detecting DeepFake tweets by
leveraging advanced natural language processing (NLP) techniques to distinguish
between genuine and AI-generated texts. Given the increasing prevalence of
misinformation, our research utilizes the TweepFake dataset to train and
evaluate various machine learning models. The objective is to identify
effective strategies for recognizing DeepFake content, thereby enhancing the
integrity of digital communications. By developing reliable methods for
detecting AI-generated misinformation, this work contributes to a more
trustworthy online information environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMMI -- Empathic Multimodal Motivational Interviews <span class="highlight-title">Dataset</span>: Analyses
  and Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucie Galland, Catherine Pelachaud, Florian Pecune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of multimodal interaction in therapy can yield a comprehensive
understanding of therapist and patient behavior that can be used to develop a
multimodal virtual agent supporting therapy. This investigation aims to uncover
how therapists skillfully blend therapy's task goal (employing classical steps
of Motivational Interviewing) with the social goal (building a trusting
relationship and expressing empathy). Furthermore, we seek to categorize
patients into various ``types'' requiring tailored therapeutic approaches. To
this intent, we present multimodal annotations of a corpus consisting of
simulated motivational interviewing conversations, wherein actors portray the
roles of patients and therapists. We introduce EMMI, composed of two publicly
available MI corpora, AnnoMI and the Motivational Interviewing Dataset, for
which we add multimodal annotations. We analyze these annotations to
characterize functional behavior for developing a virtual agent performing
motivational interviews emphasizing social and empathic behaviors. Our analysis
found three clusters of patients expressing significant differences in behavior
and adaptation of the therapist's behavior to those types. This shows the
importance of a therapist being able to adapt their behavior depending on the
current situation within the dialog and the type of user.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DaLPSR: Leve<span class="highlight-title">rag</span>e Degradation-Aligned Language Prompt for Real-World
  Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiwen Jiang, Zhi Wei, Long Peng, Feiqiang Liu, Wenbo Li, Mingwen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image super-resolution pursuits reconstructing high-fidelity high-resolution
counterpart for low-resolution image. In recent years, diffusion-based models
have garnered significant attention due to their capabilities with rich prior
knowledge. The success of diffusion models based on general text prompts has
validated the effectiveness of textual control in the field of text2image.
However, given the severe degradation commonly presented in low-resolution
images, coupled with the randomness characteristics of diffusion models,
current models struggle to adequately discern semantic and degradation
information within severely degraded images. This often leads to obstacles such
as semantic loss, visual artifacts, and visual hallucinations, which pose
substantial challenges for practical use. To address these challenges, this
paper proposes to leverage degradation-aligned language prompt for accurate,
fine-grained, and high-fidelity image restoration. Complementary priors
including semantic content descriptions and degradation prompts are explored.
Specifically, on one hand, image-restoration prompt alignment decoder is
proposed to automatically discern the degradation degree of LR images, thereby
generating beneficial degradation priors for image restoration. On the other
hand, much richly tailored descriptions from pretrained multimodal large
language model elicit high-level semantic priors closely aligned with human
perception, ensuring fidelity control for image restoration. Comprehensive
comparisons with state-of-the-art methods have been done on several popular
synthetic and real-world benchmark datasets. The quantitative and qualitative
analysis have demonstrated that the proposed method achieves a new
state-of-the-art perceptual quality level, especially in real-world cases based
on reference-free metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark
  with Human-VLM Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Baek, ChaeHun Park, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To create culturally inclusive vision-language models (VLMs), the foremost
requirement is developing a test benchmark that can diagnose the models'
ability to respond to questions reflecting cultural elements. This paper
addresses the necessity for such benchmarks, noting that existing research has
relied on human annotators' manual efforts, which impedes diversity and
efficiency. We propose a semi-automated pipeline for constructing cultural VLM
benchmarks to enhance diversity and efficiency. This pipeline leverages
human-VLM collaboration, where VLMs generate questions based on guidelines,
human-annotated examples, and image-wise relevant knowledge, which are then
reviewed by native speakers for quality and cultural relevance. The
effectiveness of our adaptable pipeline is demonstrated through a specific
application: creating a dataset tailored to Korean culture, dubbed K-Viscuit.
The resulting benchmark features two types of questions: Type 1 questions
measure visual recognition abilities, while Type 2 assess fine-grained visual
reasoning skills. This ensures a thorough diagnosis of VLM models across
various aspects. Our evaluation using K-Viscuit revealed that open-source
models notably lag behind proprietary models in understanding Korean culture,
highlighting areas for improvement. We provided diverse analyses of VLM
performance across different cultural aspects. Besides, we explored the
potential of incorporating external knowledge retrieval to enhance the
generation process, suggesting future directions for improving cultural
interpretation ability of VLMs. Our dataset and code will be made publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for
  Multi-modal Sarcasm Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Chen, Subin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalence of sarcasm in social media, conveyed through text-image
combinations, presents significant challenges for sentiment analysis and
intention mining. Current multi-modal sarcasm detection methods have been
proven to struggle with biases from spurious cues, leading to a superficial
understanding of the complex interactions between text and image. To address
these issues, we propose InterCLIP-MEP, a robust framework for multi-modal
sarcasm detection. InterCLIP-MEP introduces a refined variant of CLIP,
Interactive CLIP (InterCLIP), as the backbone, enhancing sample representations
by embedding cross-modality information in each encoder. Furthermore, a novel
training strategy is designed to adapt InterCLIP for a Memory-Enhanced
Predictor (MEP). MEP uses dynamic dual-channel memory to store valuable
historical knowledge of test samples and then leverages this memory as a
non-parametric classifier to derive the final prediction. By using InterCLIP to
encode text-image interactions more effectively and incorporating MEP,
InterCLIP-MEP offers a more robust recognition of multi-modal sarcasm.
Experiments demonstrate that InterCLIP-MEP achieves state-of-the-art
performance on the MMSD2.0 benchmark. Code and data are available at
[https://github.com/CoderChen01/InterCLIP-MEP](https://github.com/CoderChen01/InterCLIP-MEP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building on Efficient Foundations: Effectively Training <span class="highlight-title">LLM</span>s with
  Structured Feedforward Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art results in large language models (LLMs) often rely on scale,
which becomes computationally expensive. This has sparked a research agenda to
reduce these models' parameter count and computational costs without
significantly impacting their performance. Our study focuses on
transformer-based LLMs, specifically targeting the computationally intensive
feedforward networks (FFN), which are less studied than attention blocks. We
consider three candidate linear layer approximations in the FFN by combining
efficient low-rank and block-diagonal matrices. In contrast to many previous
works that examined these approximations, our study i) explores these
structures from the training-from-scratch perspective, ii) scales up to 1.3B
parameters, and iii) is conducted within recent Transformer-based LLMs rather
than convolutional architectures. We first demonstrate they can lead to actual
computational gains in various scenarios, including online decoding when using
a pre-merge technique. Additionally, we propose a novel training regime, called
\textit{self-guided training}, aimed at improving the poor training dynamics
that these approximations exhibit when used from initialization. Experiments on
the large RefinedWeb dataset show that our methods are both efficient and
effective for training and inference. Interestingly, these structured FFNs
exhibit steeper scaling curves than the original models. Further applying
self-guided training to the structured matrices with 32\% FFN parameters and
2.5$\times$ speed-up enables only a 0.4 perplexity increase under the same
training FLOPs. Finally, we develop the wide and structured networks surpassing
the current medium-sized and large-sized Transformer in perplexity and
throughput performance. Our code is available at
\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniCoder: Scaling Code <span class="highlight-title">Large Language Model</span> via Universal Code <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Sun, Linzheng Chai, Jian Yang, Yuwei Yin, Hongcheng Guo, Jiaheng Liu, Bing Wang, Liqun Yang, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intermediate reasoning or acting steps have successfully improved large
language models (LLMs) for handling various downstream natural language
processing (NLP) tasks. When applying LLMs for code generation, recent works
mainly focus on directing the models to articulate intermediate
natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and
then output code with the natural language or other structured intermediate
steps. However, such output is not suitable for code translation or generation
tasks since the standard CoT has different logical structures and forms of
expression with the code. In this work, we introduce the universal code
(UniCode) as the intermediate representation. It is a description of algorithm
steps using a mix of conventions of programming languages, such as assignment
operator, conditional operator, and loop. Hence, we collect an instruction
dataset UniCoder-Instruct to train our model UniCoder on multi-task learning
objectives. UniCoder-Instruct comprises natural-language questions, code
solutions, and the corresponding universal code. The alignment between the
intermediate universal code representation and the final code solution
significantly improves the quality of the generated code. The experimental
results demonstrate that UniCoder with the universal code significantly
outperforms the previous prompting methods by a large margin, showcasing the
effectiveness of the structural clues in pseudo-code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Knowledge Editing with Language-Agnostic Factual Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual knowledge editing (MKE) aims to simultaneously revise factual
knowledge across multilingual languages within large language models (LLMs).
However, most existing MKE methods just adapt existing monolingual editing
methods to multilingual scenarios, overlooking the deep semantic connections of
the same factual knowledge between different languages, thereby limiting edit
performance. To address this issue, we first investigate how LLMs represent
multilingual factual knowledge and discover that the same factual knowledge in
different languages generally activates a shared set of neurons, which we call
language-agnostic factual neurons. These neurons represent the semantic
connections between multilingual knowledge and are mainly located in certain
layers. Inspired by this finding, we propose a new MKE method by locating and
modifying Language-Agnostic Factual Neurons (LAFN) to simultaneously edit
multilingual knowledge. Specifically, we first generate a set of paraphrases
for each multilingual knowledge to be edited to precisely locate the
corresponding language-agnostic factual neurons. Then we optimize the update
values for modifying these located neurons to achieve simultaneous modification
of the same factual knowledge in multiple languages. Experimental results on
Bi-ZsRE and MzsRE benchmarks demonstrate that our method outperforms existing
MKE methods and achieves remarkable edit performance, indicating the importance
of considering the semantic connections among multilingual knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Symmetry Property of Christoffel Words <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Lanciault, Christophe Reutenauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the theory of trapezoidal words, whose sequences of cardinality
of factors by length are symmetric, we introduce a bivariate variant of this
symmetry. We show that this symmetry characterizes Christoffel words, and
establish other related results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings GASCom 2024, arXiv:2406.14588</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UNO Arena for Evaluating Sequential Decision-Making Capability of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanyue Qin, Haochuan Wang, Deyuan Liu, Ziyang Song, Cunhang Fan, Zhao Lv, Jinlin Wu, Zhen Lei, Zhiying Tu, Dianhui Chu, Xiaoyan Yu, Dianbo Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential decision-making refers to algorithms that take into account the
dynamics of the environment, where early decisions affect subsequent decisions.
With large language models (LLMs) demonstrating powerful capabilities between
tasks, we can't help but ask: Can Current LLMs Effectively Make Sequential
Decisions? In order to answer this question, we propose the UNO Arena based on
the card game UNO to evaluate the sequential decision-making capability of LLMs
and explain in detail why we choose UNO. In UNO Arena, We evaluate the
sequential decision-making capability of LLMs dynamically with novel metrics
based Monte Carlo methods. We set up random players, DQN-based reinforcement
learning players, and LLM players (e.g. GPT-4, Gemini-pro) for comparison
testing. Furthermore, in order to improve the sequential decision-making
capability of LLMs, we propose the TUTRI player, which can involves having LLMs
reflect their own actions wtih the summary of game history and the game
strategy. Numerous experiments demonstrate that the TUTRI player achieves a
notable breakthrough in the performance of sequential decision-making compared
to the vanilla LLM player.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Transformations across Reward Model, Parameter Update, and
  In-Context Prompt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deng Cai, Huayang Li, Tingchen Fu, Siheng Li, Weiwen Xu, Shuaiyi Li, Bowen Cao, Zhisong Zhang, Xinting Huang, Leyang Cui, Yan Wang, Lemao Liu, Taro Watanabe, Shuming Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the general capabilities of pre-trained large language models (LLMs),
they still need further adaptation to better serve practical applications. In
this paper, we demonstrate the interchangeability of three popular and distinct
adaptation tools: parameter updating, reward modeling, and in-context
prompting. This interchangeability establishes a triangular framework with six
transformation directions, each of which facilitates a variety of applications.
Our work offers a holistic view that unifies numerous existing studies and
suggests potential research directions. We envision our work as a useful
roadmap for future research on LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KEHRL: Learning Knowledge-Enhanced Language Representations with
  Hierarchical Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Li, Taolin Zhang, Longtao Huang, Chengyu Wang, Xiaofeng He, Hui Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation
triples from knowledge graphs (KGs) and integrate these external data sources
into language models via self-supervised learning. Previous works treat
knowledge enhancement as two independent operations, i.e., knowledge injection
and knowledge integration. In this paper, we propose to learn
Knowledge-Enhanced language representations with Hierarchical Reinforcement
Learning (KEHRL), which jointly addresses the problems of detecting positions
for knowledge injection and integrating external knowledge into the model in
order to avoid injecting inaccurate or irrelevant knowledge. Specifically, a
high-level reinforcement learning (RL) agent utilizes both internal and prior
knowledge to iteratively detect essential positions in texts for knowledge
injection, which filters out less meaningful entities to avoid diverting the
knowledge learning direction. Once the entity positions are selected, a
relevant triple filtration module is triggered to perform low-level RL to
dynamically refine the triples associated with polysemic entities through
binary-valued actions. Experiments validate KEHRL's effectiveness in probing
factual knowledge and enhancing the model's performance on various natural
language understanding tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniPSDA: Unsupervised Pseudo Semantic Data Augmentation for Zero-Shot
  Cross-Lingual Natural Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Li, Taolin Zhang, Jiali Deng, Longtao Huang, Chengyu Wang, Xiaofeng He, Hui Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual representation learning transfers knowledge from resource-rich
data to resource-scarce ones to improve the semantic understanding abilities of
different languages. However, previous works rely on shallow unsupervised data
generated by token surface matching, regardless of the global context-aware
semantics of the surrounding text tokens. In this paper, we propose an
Unsupervised Pseudo Semantic Data Augmentation (UniPSDA) mechanism for
cross-lingual natural language understanding to enrich the training data
without human interventions. Specifically, to retrieve the tokens with similar
meanings for the semantic data augmentation across different languages, we
propose a sequential clustering process in 3 stages: within a single language,
across multiple languages of a language family, and across languages from
multiple language families. Meanwhile, considering the multi-lingual knowledge
infusion with context-aware semantics while alleviating computation burden, we
directly replace the key constituents of the sentences with the above-learned
multi-lingual family knowledge, viewed as pseudo-semantic. The infusion process
is further optimized via three de-biasing techniques without introducing any
neural parameters. Extensive experiments demonstrate that our model
consistently improves the performance on general zero-shot cross-lingual
natural language understanding tasks, including sequence classification,
information extraction, and question answering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Instruction-Following Ability for <span class="highlight-title">Large Language Models</span> on
  Story-Ending Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rem Hida, Junki Ohmura, Toshiyuki Sekiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned Large Language Models (LLMs) have achieved remarkable
performance across various benchmark tasks. While providing instructions to
LLMs for guiding their generations is user-friendly, assessing their
instruction-following capabilities is still unclarified due to a lack of
evaluation metrics. In this paper, we focus on evaluating the
instruction-following ability of LLMs in the context of story-ending
generation, which requires diverse and context-specific instructions. We
propose an automatic evaluation pipeline that utilizes a machine reading
comprehension (MRC) model to determine whether the generated story-ending
reflects instruction. Our findings demonstrate that our proposed metric aligns
with human evaluation. Furthermore, our experiments confirm that recent
open-source LLMs can achieve instruction-following performance close to
GPT-3.5, as assessed through automatic evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADVSCORE: A Metric for the Evaluation and Creation of Adversarial
  Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoo Yeon Sung, Eve Fleisig, Ishani Mondal, Jordan Lee Boyd-Graber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial benchmarks validate model abilities by providing samples that
fool models but not humans. However, despite the proliferation of datasets that
claim to be adversarial, there does not exist an established metric to evaluate
how adversarial these datasets are. To address this lacuna, we introduce
ADVSCORE, a metric which quantifies how adversarial and discriminative an
adversarial dataset is and exposes the features that make data adversarial. We
then use ADVSCORE to underpin a dataset creation pipeline that incentivizes
writing a high-quality adversarial dataset. As a proof of concept, we use
ADVSCORE to collect an adversarial question answering (QA) dataset, ADVQA, from
our pipeline. The high-quality questions in ADVQA surpasses three adversarial
benchmarks across domains at fooling several models but not humans. We validate
our result based on difficulty estimates from 9,347 human responses on four
datasets and predictions from three models. Moreover, ADVSCORE uncovers which
adversarial tactics used by human writers fool models (e.g., GPT-4) but not
humans. Through ADVSCORE and its analyses, we offer guidance on revealing
language model vulnerabilities and producing reliable adversarial examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2401.11185</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EHRCon: <span class="highlight-title">Dataset</span> for Checking Consistency between Unstructured Notes and
  Structured Tables in Electronic Health Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonsu Kwon, Jiho Kim, Gyubok Lee, Seongsu Bae, Daeun Kyung, Wonchul Cha, Tom Pollard, Alistair Johnson, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) are integral for storing comprehensive
patient medical records, combining structured data (e.g., medications) with
detailed clinical notes (e.g., physician notes). These elements are essential
for straightforward data retrieval and provide deep, contextual insights into
patient care. However, they often suffer from discrepancies due to unintuitive
EHR system designs and human errors, posing serious risks to patient safety. To
address this, we developed EHRCon, a new dataset and task specifically designed
to ensure data consistency between structured tables and unstructured notes in
EHRs. EHRCon was crafted in collaboration with healthcare professionals using
the MIMIC-III EHR dataset, and includes manual annotations of 3,943 entities
across 105 clinical notes checked against database entries for consistency.
EHRCon has two versions, one using the original MIMIC-III schema, and another
using the OMOP CDM schema, in order to increase its applicability and
generalizability. Furthermore, leveraging the capabilities of large language
models, we introduce CheckEHR, a novel framework for verifying the consistency
between clinical notes and database tables. CheckEHR utilizes an eight-stage
process and shows promising results in both few-shot and zero-shot settings.
The code is available at https://github.com/dustn1259/EHRCon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DemoRank: Selecting Effective Demonstrations for <span class="highlight-title">Large Language Models</span>
  in Ranking Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Liu, Yutao Zhu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been increasing interest in applying large language
models (LLMs) as zero-shot passage rankers. However, few studies have explored
how to select appropriate in-context demonstrations for the passage ranking
task, which is the focus of this paper. Previous studies mainly apply a
demonstration retriever to retrieve demonstrations and use top-$k$
demonstrations for in-context learning (ICL). Although effective, this approach
overlooks the dependencies between demonstrations, leading to inferior
performance of few-shot ICL in the passage ranking task. In this paper, we
formulate the demonstration selection as a \textit{retrieve-then-rerank}
process and introduce the DemoRank framework. In this framework, we first use
LLM feedback to train a demonstration retriever and construct a novel
dependency-aware training samples to train a demonstration reranker to improve
few-shot ICL. The construction of such training samples not only considers
demonstration dependencies but also performs in an efficient way. Extensive
experiments demonstrate DemoRank's effectiveness in in-domain scenarios and
strong generalization to out-of-domain scenarios. Our codes are available
at~\url{https://github.com/8421BCD/DemoRank}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pruning via Merging: Compressing <span class="highlight-title">LLM</span>s via Manifold Alignment Based Layer
  Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deyuan Liu, Zhanyue Qin, Hairu Wang, Zhao Yang, Zecheng Wang, Fangying Rong, Qingbin Liu, Yanchao Hao, Xi Chen, Cunhang Fan, Zhao Lv, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) excel in many domains, their complexity
and scale challenge deployment in resource-limited environments. Current
compression techniques, such as parameter pruning, often fail to effectively
utilize the knowledge from pruned parameters. To address these challenges, we
propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA),
a novel approach that uses manifold learning and the Normalized Pairwise
Information Bottleneck (NPIB) measure to merge similar layers, reducing model
size while preserving essential performance. We evaluate MKA on multiple
benchmark datasets and various LLMs. Our findings show that MKA not only
preserves model performance but also achieves substantial compression ratios,
outperforming traditional pruning methods. Moreover, when coupled with
quantization, MKA delivers even greater compression. Specifically, on the MMLU
dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75%
with a minimal performance decrease of only 2.82\%. The proposed MKA method
offers a resource-efficient and performance-preserving model compression
technique for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for
  Noise-free Text-Image Corruption and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Golovanevsky, William Rudman, Vedant Palit, Ritambhara Singh, Carsten Eickhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have gained community-spanning prominence due
to their ability to integrate visual and textual inputs to perform complex
tasks. Despite their success, the internal decision-making processes of these
models remain opaque, posing challenges in high-stakes applications. To address
this, we introduce NOTICE, the first Noise-free Text-Image Corruption and
Evaluation pipeline for mechanistic interpretability in VLMs. NOTICE
incorporates a Semantic Minimal Pairs (SMP) framework for image corruption and
Symmetric Token Replacement (STR) for text. This approach enables semantically
meaningful causal mediation analysis for both modalities, providing a robust
method for analyzing multimodal integration within models like BLIP. Our
experiments on the SVO-Probes, MIT-States, and Facial Expression Recognition
datasets reveal crucial insights into VLM decision-making, identifying the
significant role of middle-layer cross-attention heads. Further, we uncover a
set of ``universal cross-attention heads'' that consistently contribute across
tasks and modalities, each performing distinct functions such as implicit image
segmentation, object inhibition, and outlier inhibition. This work paves the
way for more transparent and interpretable multimodal systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelled Multivariate Overlap: A method for measuring vowel merger 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irene Smith, Morgan Sonderegger, The Spade Consortium
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method for quantifying vowel overlap. There is
a tension in previous work between using multivariate measures, such as those
derived from empirical distributions, and the ability to control for unbalanced
data and extraneous factors, as is possible when using fitted model parameters.
The method presented here resolves this tension by jointly modelling all
acoustic dimensions of interest and by simulating distributions from the model
to compute a measure of vowel overlap. An additional benefit of this method is
that computation of uncertainty becomes straightforward. We evaluate this
method on corpus speech data targeting the PIN-PEN merger in four dialects of
English and find that using modelled distributions to calculate Bhattacharyya
affinity substantially improves results compared to empirical distributions,
while the difference between multivariate and univariate modelling is subtle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Cross-Cultural Alignment Change the Commonsense Morality of
  Language Models? <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuu Jinnai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment of the language model with human preferences is a common approach
to making a language model useful to end users. However, most alignment work is
done in English, and human preference datasets are dominated by English,
reflecting only the preferences of English-speaking annotators. Nevertheless,
it is common practice to use the English preference data, either directly or by
translating it into the target language, when aligning a multilingual language
model. The question is whether such an alignment strategy marginalizes the
preference of non-English speaking users. To this end, we investigate the
effect of aligning Japanese language models with (mostly) English resources. In
particular, we focus on evaluating whether the commonsense morality of the
resulting fine-tuned models is aligned with Japanese culture using the
JCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show
that the fine-tuned model outperforms the SFT model. However, it does not
demonstrate the same level of improvement as a model fine-tuned using the JCM,
suggesting that while some aspects of commonsense morality are transferable,
others may not be.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2nd Workshop on Cross-Cultural Considerations in NLP (C3NLP) at
  ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Song Data Cleansing for End-to-End Neural Singer Diarization Using
  Neural Analysis and Synthesis Framework <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hokuto Munakata, Ryo Terashima, Yusuke Fujita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a data cleansing method that utilizes a neural analysis and
synthesis (NANSY++) framework to train an end-to-end neural diarization model
(EEND) for singer diarization. Our proposed model converts song data with
choral singing which is commonly contained in popular music and unsuitable for
generating a simulated dataset to the solo singing data. This cleansing is
based on NANSY++, which is a framework trained to reconstruct an input
non-overlapped audio signal. We exploit the pre-trained NANSY++ to convert
choral singing into clean, non-overlapped audio. This cleansing process
mitigates the mislabeling of choral singing to solo singing and helps the
effective training of EEND models even when the majority of available song data
contains choral singing sections. We experimentally evaluated the EEND model
trained with a dataset using our proposed method using annotated popular duet
songs. As a result, our proposed method improved 14.8 points in diarization
error rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>INTERSPEECH 2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anomaly Detection of Tabular Data Using <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aodong Li, Yunhan Zhao, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown their potential in long-context
understanding and mathematical reasoning. In this paper, we study the problem
of using LLMs to detect tabular anomalies and show that pre-trained LLMs are
zero-shot batch-level anomaly detectors. That is, without extra
distribution-specific model fitting, they can discover hidden outliers in a
batch of data, demonstrating their ability to identify low-density data
regions. For LLMs that are not well aligned with anomaly detection and
frequently output factual errors, we apply simple yet effective data-generating
processes to simulate synthetic batch-level anomaly detection datasets and
propose an end-to-end fine-tuning strategy to bring out the potential of LLMs
in detecting real anomalies. Experiments on a large anomaly detection benchmark
(ODDS) showcase i) GPT-4 has on-par performance with the state-of-the-art
transductive learning-based anomaly detection methods and ii) the efficacy of
our synthetic dataset and fine-tuning strategy in aligning LLMs to this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at the Anomaly Detection with Foundation Models workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascade Reward Sampling for Efficient Decoding-Time Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolian Li, Yifan Wang, Ananth Grama, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human preferences is critical for
their deployment. Recently, decoding-time alignment has emerged as an effective
plug-and-play technique that requires no fine-tuning of model parameters.
However, generating text that achieves both high reward and high likelihood
remains a significant challenge. Existing methods often fail to generate
high-reward text or incur substantial computational costs. In this paper, we
propose Cascade Reward Sampling (CARDS) to address both issues, guaranteeing
the generation of high-reward and high-likelihood text with significantly low
costs. Based on our analysis of reward models (RMs) on incomplete text and our
observation that high-reward prefixes induce high-reward complete text, we use
rejection sampling to iteratively generate small semantic segments to form such
prefixes. The segment length is dynamically determined by the predictive
uncertainty of LLMs. This strategy guarantees desirable prefixes for subsequent
generations and significantly reduces wasteful token re-generations and the
number of reward model scoring. Our experiments demonstrate substantial gains
in both generation efficiency and alignment ratings compared to the baselines,
achieving five times faster text generation and 99\% win-ties in GPT-4/Claude-3
helpfulness evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compensate Quantization Errors: Make Weights Hierarchical to Compensate
  Each Other 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Gao, Jie Ou, Lei Wang, Yuting Xiao, Zhiyuan Xiang, Ruiting Dai, Jun Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emergent Large Language Models (LLMs) use their extraordinary performance and
powerful deduction capacity to discern from traditional language models.
However, the expenses of computational resources and storage for these LLMs are
stunning, quantization then arises as a trending conversation. To address
accuracy decay caused by quantization, two streams of works in post-training
quantization methods stand out. One uses other weights to compensate existing
quantization error, while the other transfers the quantization difficulty to
other parts in the model. Combining both merits, we introduce Learnable
Singular value Increment (LSI) as an advanced solution. LSI uses Singular Value
Decomposition to extract singular values of the weights and make them learnable
to help weights compensate each other conditioned on activation. Incorporating
LSI with existing techniques, we achieve state-of-the-art performance in
diverse quantization settings, no matter in weight-only, weight-activation or
extremely low bit scenarios. By unleashing the potential of LSI, efficient
finetuning on quantized model is no longer a prohibitive problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Efficient quantization method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LangSuitE: Planning, Controlling and Interacting with Large Language
  Models in Embodied Text Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixia Jia, Mengmeng Wang, Baichen Tong, Song-Chun Zhu, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have shown inspiring
achievements in constructing autonomous agents that rely on language
descriptions as inputs. However, it remains unclear how well LLMs can function
as few-shot or zero-shot embodied agents in dynamic interactive environments.
To address this gap, we introduce LangSuitE, a versatile and simulation-free
testbed featuring 6 representative embodied tasks in textual embodied worlds.
Compared with previous LLM-based testbeds, LangSuitE (i) offers adaptability to
diverse environments without multiple simulation engines, (ii) evaluates
agents' capacity to develop ``internalized world knowledge'' with embodied
observations, and (iii) allows easy customization of communication and action
strategies. To address the embodiment challenge, we devise a novel
chain-of-thought (CoT) schema, EmMem, which summarizes embodied states w.r.t.
history information. Comprehensive benchmark results illustrate challenges and
insights of embodied planning. LangSuitE represents a significant step toward
building embodied generalists in the context of language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Supervised Learning and Reinforcement Learning for Multi-Label
  Classification Tasks with Partial Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixia Jia, Junpeng Li, Shichuan Zhang, Anji Liu, Zilong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional supervised learning heavily relies on human-annotated datasets,
especially in data-hungry neural approaches. However, various tasks, especially
multi-label tasks like document-level relation extraction, pose challenges in
fully manual annotation due to the specific domain knowledge and large class
sets. Therefore, we address the multi-label positive-unlabelled learning
(MLPUL) problem, where only a subset of positive classes is annotated. We
propose Mixture Learner for Partially Annotated Classification (MLPAC), an
RL-based framework combining the exploration ability of reinforcement learning
and the exploitation ability of supervised learning. Experimental results
across various tasks, including document-level relation extraction, multi-label
image classification, and binary PU learning, demonstrate the generalization
and effectiveness of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlagBench: Exploring the Duality of <span class="highlight-title">Large Language Models</span> in Plagiarism
  Generation and Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jooyoung Lee, Toshini Agrawal, Adaku Uchendu, Thai Le, Jinghui Chen, Dongwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent literature has highlighted potential risks to academic integrity
associated with large language models (LLMs), as they can memorize parts of
training instances and reproduce them in the generated texts without proper
attribution. In addition, given their capabilities in generating high-quality
texts, plagiarists can exploit LLMs to generate realistic paraphrases or
summaries indistinguishable from original work. In response to possible
malicious use of LLMs in plagiarism, we introduce PlagBench, a comprehensive
dataset consisting of 46.5K synthetic plagiarism cases generated using three
instruction-tuned LLMs across three writing domains. The quality of PlagBench
is ensured through fine-grained automatic evaluation for each type of
plagiarism, complemented by human annotation. We then leverage our proposed
dataset to evaluate the plagiarism detection performance of five modern LLMs
and three specialized plagiarism checkers. Our findings reveal that GPT-3.5
tends to generates paraphrases and summaries of higher quality compared to
Llama2 and GPT-4. Despite LLMs' weak performance in summary plagiarism
identification, they can surpass current commercial plagiarism detectors.
Overall, our results highlight the potential of LLMs to serve as robust
plagiarism detection tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Influence of Prompt-Specific Shortcuts in AI Generated
  Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Choonghyun Park, Hyuhng Joon Kim, Junyeob Kim, Youna Kim, Taeuk Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-goo Lee, Kang Min Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI Generated Text (AIGT) detectors are developed with texts from humans and
LLMs of common tasks. Despite the diversity of plausible prompt choices, these
datasets are generally constructed with a limited number of prompts. The lack
of prompt variation can introduce prompt-specific shortcut features that exist
in data collected with the chosen prompt, but do not generalize to others. In
this paper, we analyze the impact of such shortcuts in AIGT detection. We
propose Feedback-based Adversarial Instruction List Optimization (FAILOpt), an
attack that searches for instructions deceptive to AIGT detectors exploiting
prompt-specific shortcuts. FAILOpt effectively drops the detection performance
of the target detector, comparable to other attacks based on adversarial
in-context examples. We also utilize our method to enhance the robustness of
the detector by mitigating the shortcuts. Based on the findings, we further
train the classifier with the dataset augmented by FAILOpt prompt. The
augmented classifier exhibits improvements across generation models, tasks, and
attacks. Our code will be available at https://github.com/zxcvvxcz/FAILOpt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures, 13 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Thousand and One Pairs: A "novel" challenge for long-context
  language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, Mohit Iyyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test
only surface-level retrieval capabilities, but how well can long-context LLMs
retrieve, synthesize, and reason over information across book-length inputs? We
address this question by creating NoCha, a dataset of 1,001 minimally different
pairs of true and false claims about 67 recently-published English fictional
books, written by human readers of those books. In contrast to existing
long-context benchmarks, our annotators confirm that the largest share of pairs
in NoCha require global reasoning over the entire book to verify. Our
experiments show that while human readers easily perform this task, it is
enormously challenging for all ten long-context LLMs that we evaluate: no
open-weight model performs above random chance (despite their strong
performance on synthetic benchmarks), while GPT-4o achieves the highest
accuracy at 55.8%. Further analysis reveals that (1) on average, models perform
much better on pairs that require only sentence-level retrieval vs. global
reasoning; (2) model-generated explanations for their decisions are often
inaccurate even for correctly-labeled claims; and (3) models perform
substantially worse on speculative fiction books that contain extensive
world-building. The methodology proposed in NoCha allows for the evolution of
the benchmark dataset and the easy analysis of future models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence Regulation Neurons in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Stolfo, Ben Wu, Wes Gurnee, Yonatan Belinkov, Xingyi Song, Mrinmaya Sachan, Neel Nanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their widespread use, the mechanisms by which large language models
(LLMs) represent and regulate uncertainty in next-token predictions remain
largely unexplored. This study investigates two critical components believed to
influence this uncertainty: the recently discovered entropy neurons and a new
set of components that we term token frequency neurons. Entropy neurons are
characterized by an unusually high weight norm and influence the final layer
normalization (LayerNorm) scale to effectively scale down the logits. Our work
shows that entropy neurons operate by writing onto an unembedding null space,
allowing them to impact the residual stream norm with minimal direct effect on
the logits themselves. We observe the presence of entropy neurons across a
range of models, up to 7 billion parameters. On the other hand, token frequency
neurons, which we discover and describe here for the first time, boost or
suppress each token's logit proportionally to its log frequency, thereby
shifting the output distribution towards or away from the unigram distribution.
Finally, we present a detailed case study where entropy neurons actively manage
confidence in the setting of induction, i.e. detecting and continuing repeated
subsequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>s assist NLP Researchers: Critique Paper (Meta-)<span class="highlight-title">Review</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangshu Du, Yibo Wang, Wenting Zhao, Zhongfen Deng, Shuaiqi Liu, Renze Lou, Henry Peng Zou, Pranav Narayanan Venkit, Nan Zhang, Mukund Srinath, Haoran Ranran Zhang, Vipul Gupta, Yinghui Li, Tao Li, Fei Wang, Qin Liu, Tianlin Liu, Pengzhi Gao, Congying Xia, Chen Xing, Jiayang Cheng, Zhaowei Wang, Ying Su, Raj Sanjay Shah, Ruohao Guo, Jing Gu, Haoran Li, Kangda Wei, Zihao Wang, Lu Cheng, Surangika Ranathunga, Meng Fang, Jie Fu, Fei Liu, Ruihong Huang, Eduardo Blanco, Yixin Cao, Rui Zhang, Philip S. Yu, Wenpeng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work is motivated by two key trends. On one hand, large language models
(LLMs) have shown remarkable versatility in various generative tasks such as
writing, drawing, and question answering, significantly reducing the time
required for many routine tasks. On the other hand, researchers, whose work is
not only time-consuming but also highly expertise-demanding, face increasing
challenges as they have to spend more time reading, writing, and reviewing
papers. This raises the question: how can LLMs potentially assist researchers
in alleviating their heavy workload?
  This study focuses on the topic of LLMs assist NLP Researchers, particularly
examining the effectiveness of LLM in assisting paper (meta-)reviewing and its
recognizability. To address this, we constructed the ReviewCritique dataset,
which includes two types of information: (i) NLP papers (initial submissions
rather than camera-ready) with both human-written and LLM-generated reviews,
and (ii) each review comes with "deficiency" labels and corresponding
explanations for individual segments, annotated by experts. Using
ReviewCritique, this study explores two threads of research questions: (i)
"LLMs as Reviewers", how do reviews generated by LLMs compare with those
written by humans in terms of quality and distinguishability? (ii) "LLMs as
Metareviewers", how effectively can LLMs identify potential issues, such as
Deficient or unprofessional review segments, within individual paper reviews?
To our knowledge, this is the first work to provide such a comprehensive
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLERC: A <span class="highlight-title">Dataset</span> for Legal Case Retrieval and <span class="highlight-title">Retrieval-Augmented</span>
  Analysis Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abe Bohan Hou, Orion Weller, Guanghui Qin, Eugene Yang, Dawn Lawrie, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal professionals need to write analyses that rely on citations to relevant
precedents, i.e., previous case decisions. Intelligent systems assisting legal
professionals in writing such documents provide great benefits but are
challenging to design. Such systems need to help locate, summarize, and reason
over salient precedents in order to be useful. To enable systems for such
tasks, we work with legal professionals to transform a large open-source legal
corpus into a dataset supporting two important backbone tasks: information
retrieval (IR) and retrieval-augmented generation (RAG). This dataset CLERC
(Case Law Evaluation Retrieval Corpus), is constructed for training and
evaluating models on their ability to (1) find corresponding citations for a
given piece of legal analysis and to (2) compile the text of these citations
(as well as previous context) into a cogent analysis that supports a reasoning
goal. We benchmark state-of-the-art models on CLERC, showing that current
approaches still struggle: GPT-4o generates analyses with the highest ROUGE
F-scores but hallucinates the most, while zero-shot IR models only achieve
48.3% recall@1000.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vaporetto: Efficient Japanese Tokenization Based on Improved Pointwise
  Linear Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koichi Akabe, Shunsuke Kanda, Yusuke Oda, Shinsuke Mori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an approach to improve the runtime efficiency of Japanese
tokenization based on the pointwise linear classification (PLC) framework,
which formulates the whole tokenization process as a sequence of linear
classification problems. Our approach optimizes tokenization by leveraging the
characteristics of the PLC framework and the task definition. Our approach
involves (1) composing multiple classifications into array-based operations,
(2) efficient feature lookup with memory-optimized automata, and (3) three
orthogonal pre-processing methods for reducing actual score calculation. Thus,
our approach makes the tokenization speed 5.7 times faster than the current
approach based on the same model without decreasing tokenization accuracy. Our
implementation is available at https://github.com/daac-tools/vaporetto under
the MIT or Apache-2.0 license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability
  of <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to exhibit remarkable performance in
natural language understanding tasks, there is a crucial need to measure their
ability for human-like multi-step logical reasoning. Existing logical reasoning
evaluation benchmarks often focus primarily on simplistic single-step or
multi-step reasoning with a limited set of inference rules. Furthermore, the
lack of datasets for evaluating non-monotonic reasoning represents a crucial
gap since it aligns more closely with human-like reasoning. To address these
limitations, we propose Multi-LogiEval, a comprehensive evaluation dataset
encompassing multi-step logical reasoning with various inference rules and
depths. Multi-LogiEval covers three logic types--propositional, first-order,
and non-monotonic--consisting of more than 30 inference rules and more than 60
of their combinations with various depths. Leveraging this dataset, we conduct
evaluations on a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Yi, Orca,
and Mistral, employing a zero-shot chain-of-thought. Experimental results show
that there is a significant drop in the performance of LLMs as the reasoning
steps/depth increases (average accuracy of ~68% at depth-1 to ~43% at depth-5).
We further conduct a thorough investigation of reasoning chains generated by
LLMs which reveals several important findings. We believe that Multi-LogiEval
facilitates future research for evaluating and enhancing the logical reasoning
ability of LLMs. Data is available at
https://github.com/Mihir3009/Multi-LogiEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paraphrase and Aggregate with <span class="highlight-title">Large Language Models</span> for Minimizing
  Intent Classification Errors <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikas Yadav, Zheng Tang, Vijay Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have achieved remarkable success in natural
language generation but lesser focus has been given to their applicability in
decision making tasks such as classification. We show that LLMs like LLaMa can
achieve high performance on large multi-class classification tasks but still
make classification errors and worse, generate out-of-vocabulary class labels.
To address these critical issues, we introduce Paraphrase and AGgregate
(PAG)-LLM approach wherein an LLM generates multiple paraphrases of the input
query (parallel queries), performs multi-class classification for the original
query and each paraphrase, and at the end aggregate all the classification
labels based on their confidence scores. We evaluate PAG-LLM on two large
multi-class classication datasets: CLINC, and Banking and show 22.7% and 15.1%
error reduction. We show that PAG-LLM is especially effective for hard examples
where LLM is uncertain, and reduces the critical misclassification and
hallucinated label generation errors
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEXTER: A Benchmark for open-domain Complex Question Answering using
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venktesh V. Deepali Prabhu, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain complex Question Answering (QA) is a difficult task with
challenges in evidence retrieval and reasoning. The complexity of such
questions could stem from questions being compositional, hybrid evidence, or
ambiguity in questions. While retrieval performance for classical QA tasks is
well explored, their capabilities for heterogeneous complex retrieval tasks,
especially in an open-domain setting, and the impact on downstream QA
performance, are relatively unexplored. To address this, in this work, we
propose a benchmark composing diverse complex QA tasks and provide a toolkit to
evaluate state-of-the-art pre-trained dense and sparse retrieval models in an
open-domain setting. We observe that late interaction models and surprisingly
lexical models like BM25 perform well compared to other pre-trained dense
retrieval models. In addition, since context-based reasoning is critical for
solving complex QA tasks, we also evaluate the reasoning capabilities of LLMs
and the impact of retrieval performance on their reasoning capabilities.
Through experiments, we observe that much progress is to be made in retrieval
for complex QA to improve downstream QA performance. Our software and related
data can be accessed at https://github.com/VenkteshV/DEXTER
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under submission, 22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing network clustering algorithms with Natural Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ixandra Achitouv, David Chavalarias, Bruno Gaume
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of online social networks has led to the development of an
abundant literature on the study of online social groups and their relationship
to individuals' personalities as revealed by their textual productions. Social
structures are inferred from a wide range of social interactions. Those
interactions form complex -- sometimes multi-layered -- networks, on which
community detection algorithms are applied to extract higher order structures.
The choice of the community detection algorithm is however hardily questioned
in relation with the cultural production of the individual they classify. In
this work, we assume the entangled nature of social networks and their cultural
production to propose a definition of cultural based online social groups as
sets of individuals whose online production can be categorized as social
group-related. We take advantage of this apparently self-referential
description of online social groups with a hybrid methodology that combines a
community detection algorithm and a natural language processing classification
algorithm. A key result of this analysis is the possibility to score community
detection algorithms using their agreement with the natural language processing
classification. A second result is that we can assign the opinion of a random
user at >85% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Adversarial Discovery for Safety Classifiers <span class="chip">NAACL
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Kumar Lal, Preethi Lahoti, Aradhana Sinha, Yao Qin, Ananth Balashankar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety classifiers are critical in mitigating toxicity on online forums such
as social media and in chatbots. Still, they continue to be vulnerable to
emergent, and often innumerable, adversarial attacks. Traditional automated
adversarial data generation methods, however, tend to produce attacks that are
not diverse, but variations of previously observed harm types. We formalize the
task of automated adversarial discovery for safety classifiers - to find new
attacks along previously unseen harm dimensions that expose new weaknesses in
the classifier. We measure progress on this task along two key axes (1)
adversarial success: does the attack fool the classifier? and (2) dimensional
diversity: does the attack represent a previously unseen harm type? Our
evaluation of existing attack generation methods on the CivilComments toxicity
task reveals their limitations: Word perturbation attacks fail to fool
classifiers, while prompt-based LLM attacks have more adversarial success, but
lack dimensional diversity. Even our best-performing prompt-based method finds
new successful attacks on unseen harm dimensions of attacks only 5\% of the
time. Automatically finding new harmful dimensions of attack is crucial and
there is substantial headroom for future research on our new task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Fourth Workshop on TrustworthyNLP (TrustNLP) at NAACL
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Instruction: Amplifying Attention in the Middle via Prompting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiru Zhang, Zaiqiao Meng, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The context window of large language models has been extended to 128k tokens
or more. However, language models still suffer from position bias and have
difficulty in accessing and using the middle part of the context due to the
lack of attention. We examine the relative position awareness of LLMs and the
feasibility of mitigating disproportional attention through prompting. We
augment the original task instruction with $\texttt{attention instructions}$
that direct language models to allocate more attention towards a selected
segment of the context. We conduct a comprehensive investigation on
multi-document question answering task with both position-based and index-based
instructions. We find that language models do not have relative position
awareness of the context. Nevertheless, they demonstrate the capacity to adapt
attention to a specific segment using matching indexes. Our analysis
contributes to a deeper understanding of position bias in LLMs and provides a
pathway to mitigate this bias by instruction, thus benefiting LLMs in locating
and utilizing relevant information from retrieved documents in RAG
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large Language Models</span> Assume People are More Rational than We Really are 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order for AI systems to communicate effectively with people, they must
understand how we make decisions. However, people's decisions are not always
rational, so the implicit internal models of human decision-making in Large
Language Models (LLMs) must account for this. Previous empirical evidence seems
to suggest that these implicit models are accurate -- LLMs offer believable
proxies of human behavior, acting how we expect humans would in everyday
interactions. However, by comparing LLM behavior and predictions to a large
dataset of human decisions, we find that this is actually not the case: when
both simulating and predicting people's choices, a suite of cutting-edge LLMs
(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more
rational than we really are. Specifically, these models deviate from human
behavior and align more closely with a classic model of rational choice --
expected value theory. Interestingly, people also tend to assume that other
people are rational when interpreting their behavior. As a consequence, when we
compare the inferences that LLMs and people draw from the decisions of others
using another psychological dataset, we find that these inferences are highly
correlated. Thus, the implicit decision-making models of LLMs appear to be
aligned with the human expectation that other people will act rationally,
rather than with how people actually act.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ modeLing: A Novel <span class="highlight-title">Dataset</span> for Testing Linguistic Reasoning in Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan A. Chi, Teodor Malchev, Riley Kong, Ryan A. Chi, Lucas Huang, Ethan A. Chi, R. Thomas McCoy, Dragomir Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce modeLing, a novel benchmark of Linguistics Olympiad-style
puzzles which tests few-shot reasoning in AI systems. Solving these puzzles
necessitates inferring aspects of a language's grammatical structure from a
small number of examples. Such puzzles provide a natural testbed for language
models, as they require compositional generalization and few-shot inductive
reasoning. Consisting solely of new puzzles written specifically for this work,
modeLing has no risk of appearing in the training data of existing AI systems:
this ameliorates the risk of data leakage, a potential confounder for many
prior evaluations of reasoning. Evaluating several large open source language
models and GPT on our benchmark, we observe non-negligible accuracy,
demonstrating few-shot emergent reasoning ability which cannot merely be
attributed to shallow memorization. However, imperfect model performance
suggests that modeLing can be used to measure further progress in linguistic
reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Steering with Evolutionary Heuristics for Decoding-time Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Yu Hung, Navonil Majumder, Ambuj Mehrish, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread applicability and increasing omnipresence of LLMs have
instigated a need to align LLM responses to user and stakeholder preferences.
Many preference optimization approaches have been proposed that fine-tune LLM
parameters to achieve good alignment. However, such parameter tuning is known
to interfere with model performance on many tasks. Moreover, keeping up with
shifting user preferences is tricky in such a situation. Decoding-time
alignment with reward model guidance solves these issues at the cost of
increased inference time. However, most of such methods fail to strike the
right balance between exploration and exploitation of reward -- often due to
the conflated formulation of these two aspects - to give well-aligned
responses. To remedy this we decouple these two aspects and implement them in
an evolutionary fashion: exploration is enforced by decoding from mutated
instructions and exploitation is represented as the periodic replacement of
poorly-rewarded generations with well-rewarded ones. Empirical evidences
indicate that this strategy outperforms many preference optimization and
decode-time alignment approaches on two widely accepted alignment benchmarks
AlpacaEval 2 and MT-Bench. Our implementation will be available at:
https://darwin-alignment.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Resource Multi-Granularity Academic Function Recognition Based on
  Multiple Prompt Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Liu, Zi Xiong, Yi Jiang, Yongqiang Ma, Wei Lu, Yong Huang, Qikai Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally
requires large numbers of annotated data to achieve state-of-the-art
performance on a range of NLP tasks in the scientific domain. However,
obtaining the fine-tune data for scientific NLP task is still challenging and
expensive. Inspired by recent advancement in prompt learning, in this paper, we
propose the Mix Prompt Tuning (MPT), which is a semi-supervised method to
alleviate the dependence on annotated data and improve the performance of
multi-granularity academic function recognition tasks with a small number of
labeled examples. Specifically, the proposed method provides multi-perspective
representations by combining manual prompt templates with automatically learned
continuous prompt templates to help the given academic function recognition
task take full advantage of knowledge in PLMs. Based on these prompt templates
and the fine-tuned PLM, a large number of pseudo labels are assigned to the
unlabeled examples. Finally, we fine-tune the PLM using the pseudo training
set. We evaluate our method on three academic function recognition tasks of
different granularity including the citation function, the abstract sentence
function, and the keyword function, with datasets from computer science domain
and biomedical domain. Extensive experiments demonstrate the effectiveness of
our method and statistically significant improvements against strong baselines.
In particular, it achieves an average increase of 5% in Macro-F1 score compared
with fine-tuning, and 6% in Macro-F1 score compared with other semi-supervised
method under low-resource settings. In addition, MPT is a general method that
can be easily applied to other low-resource scientific classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted by The Electronic Library and the full
  article is now available on Emerald Insight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Packing in <span class="highlight-title">LLM</span> Training Improves Long Context Utilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17296v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17296v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konrad Staniszewski, Szymon Tworkowski, Sebastian Jaszczur, Yu Zhao, Henryk Michalewski, Łukasz Kuciński, Piotr Miłoś
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context large language models have attracted
significant attention, yet their practical applications often suffer from
suboptimal context utilization. This study investigates structuring training
data to enhance semantic interdependence, demonstrating that this approach
effectively improves context utilization. To this end, we introduce the
Structured Packing for Long Context (SPLiCe) method, which utilizes retrieval
to collate mutually relevant documents into long and coherent training
examples. We validate SPLiCe empirically across models of varying sizes -- 3B,
7B, and 13B -- achieving improved performance in long-context tasks, such as
Qasper and HotpotQA. Remarkably, even brief fine-tuning with SPLiCe is
sufficient to realize these benefits. Additionally, SPLiCe effectively
mitigates the lost-in-middle phenomenon often observed in large models. Our
comprehensive analysis of SPLiCe explores its design choices and reveals
intriguing transfer effects; for instance, training on programming code
enhances performance on natural language tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>new experiments with a 13B model</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Many-Shot In-Context Learning Help Long-Context <span class="highlight-title">LLM</span> Judges? See
  More, Judge Better! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Song, Mao Zheng, Xuan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Large Language Models (LLMs) as judges for evaluating the
performance of LLMs has recently garnered attention. Nonetheless, this type of
approach concurrently introduces potential biases from LLMs, raising concerns
about the reliability of the evaluation results. To mitigate this issue, we
propose and study two versions of many-shot in-context prompts, Reinforced and
Unsupervised ICL, for helping GPT-4o-as-a-Judge in single answer grading. The
former uses in-context examples with model-generated rationales, and the latter
without. Based on the designed prompts, we investigate the impact of scaling
the number of in-context examples on the agreement and quality of the
evaluation. Furthermore, we first reveal the symbol bias in GPT-4o-as-a-Judge
for pairwise comparison and then propose a simple yet effective approach to
mitigate it. Experimental results show that advanced long-context LLMs, such as
GPT-4o, perform better in the many-shot regime than in the zero-shot regime.
Meanwhile, the experimental results further verify the effectiveness of the
symbol bias mitigation approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attribute Diversity Determines the Systematicity Gap in VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Berlot-Attwell, Kumar Krishna Agrawal, A. Michael Carrell, Yash Sharma, Naomi Saphra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The degree to which neural networks can generalize to new combinations of
familiar concepts, and the conditions under which they are able to do so, has
long been an open question. In this work, we study the systematicity gap in
visual question answering: the performance difference between reasoning on
previously seen and unseen combinations of object attributes. To test, we
introduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased
quantity of training data does not reduce the systematicity gap, increased
training data diversity of the attributes in the unseen combination does. In
all, our experiments suggest that the more distinct attribute type combinations
are seen during training, the more systematic we can expect the resulting model
to be.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flow of Reasoning: Efficient Training of <span class="highlight-title">LLM</span> Policy with Divergent
  Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Divergent thinking, the cognitive process of generating diverse solutions, is
a hallmark of human creativity and problem-solving. For machines, sampling
diverse solution trajectories in complex reasoning problems is crucial for
robust outcomes, data augmentation, and enhanced model generalization. Large
language models (LLMs) often struggle with generating high-quality, diverse
reasoning. While supervised fine-tuning helps with quality, it requires
extensive supervision data to capture the full diversity of solutions.
Alternatively, reinforcement learning methods like PPO aim to find limited
highest-reward solutions while neglecting the solution diversity, akin to
convergent thinking. To address these limitations, we propose Flow of Reasoning
(FoR) -- an efficient LLM training approach enabling diverse reasoning with
minimal data. FoR formulates multi-step LLM reasoning as a Markovian flow from
an initial state to terminal states. The formulation allows to adapt principled
GFlowNet approaches to train the LLM as a policy, which is able to sample
multiple reasoning paths with probabilities proportional to the unnormalized
reward. Empirical results show that, with limited training data (e.g., 15
examples), FoR can discover diverse high-quality solutions that excel greatly
beyond current state-of-the-art methods across three tasks, including embodied
reasoning (BlocksWorld), math puzzle solving (Game24), and logical reasoning
(PrOntoQA). Code is available at https://github.com/Yu-Fangxu/FoR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FairytaleQA Translated: Enabling Educational Question and Answer
  Generation in Less-Resourced Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernardo Leite, Tomás Freitas Osório, Henrique Lopes Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question Answering (QA) datasets are crucial in assessing reading
comprehension skills for both machines and humans. While numerous datasets have
been developed in English for this purpose, a noticeable void exists in
less-resourced languages. To alleviate this gap, our paper introduces
machine-translated versions of FairytaleQA, a renowned QA dataset designed to
assess and enhance narrative comprehension skills in young children. By
employing fine-tuned, modest-scale models, we establish benchmarks for both
Question Generation (QG) and QA tasks within the translated datasets. In
addition, we present a case study proposing a model for generating
question-answer pairs, with an evaluation incorporating quality metrics such as
question well-formedness, answerability, relevance, and children suitability.
Our evaluation prioritizes quantifying and describing error cases, along with
providing directions for future work. This paper contributes to the advancement
of QA and QG research in less-resourced languages, promoting accessibility and
inclusivity in the development of these models for reading comprehension. The
code and data is publicly available at
github.com/bernardoleite/fairytaleqa-translated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint - Accepted for publication at ECTEL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Children's Speech Recognition through Discrete Token Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vrunda N. Sukhadia, Shammur Absar Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Children's speech recognition is considered a low-resource task mainly due to
the lack of publicly available data. There are several reasons for such data
scarcity, including expensive data collection and annotation processes, and
data privacy, among others. Transforming speech signals into discrete tokens
that do not carry sensitive information but capture both linguistic and
acoustic information could be a solution for privacy concerns. In this study,
we investigate the integration of discrete speech tokens into children's speech
recognition systems as input without significantly degrading the ASR
performance. Additionally, we explored single-view and multi-view strategies
for creating these discrete labels. Furthermore, we tested the models for
generalization capabilities with unseen domain and nativity dataset. Results
reveal that the discrete token ASR for children achieves nearly equivalent
performance with an approximate 83% reduction in parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentExplainer: Explaining Latent Representations in Deep Generative
  Models with Multi-modal Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
LatentExplainer, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
LatentExplainer tackles three main challenges: inferring the meaning of latent
variables, aligning explanations with inductive biases, and handling varying
degrees of explainability. By perturbing latent variables and interpreting
changes in generated data, the framework provides a systematic approach to
understanding and controlling the data generation process, enhancing the
transparency and interpretability of deep generative models. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations of
latent variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Parts are Greater Than Sums: Individual <span class="highlight-title">LLM</span> Components Can
  Outperform Full Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Yun Chang, Jesse Thomason, Robin Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies in-context learning (ICL) by decomposing the output of
large language models into the individual contributions of attention heads and
MLPs (components). We observe curious components: good-performing ones that
individually do well on a classification task, even when the model performs
poorly; bad-performing ones that do much worse than chance; and label-biased
components that always predict the same label. We find that component
accuracies are well-correlated across different demonstration sets and
perturbations of prompt templates, even when the full-model accuracy varies
greatly. Based on our findings, we propose component reweighting, which learns
to linearly re-scale the component activations from a few labeled examples.
Given 24 labeled examples, our method improves by an average of 6.0% accuracy
points over 24-shot ICL across 8 tasks on Llama-2-7B. Overall, this paper both
enriches our understanding of ICL and provides a practical method for
improvement by examining model internals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fix typos and citations; appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limited Out-of-Context Knowledge Reasoning in <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated strong capabilities as
knowledge bases and significant in-context reasoning capabilities. However,
previous work challenges their out-of-context reasoning ability, i.e., the
ability to infer information from their training data, instead of from the
context or prompt. This paper focuses on a significant facet of out-of-context
reasoning: Out-of-Context Knowledge Reasoning (OCKR), which is to combine
multiple knowledge to infer new knowledge. We designed a synthetic dataset with
seven representative OCKR tasks to systematically assess the OCKR capabilities
of LLMs. Using this dataset, we evaluated the LLaMA2-13B-chat model and
discovered that its proficiency in this aspect is limited, regardless of
whether the knowledge is trained in a separate or adjacent training settings.
Moreover, training the model to reason with complete reasoning data did not
result in significant improvement. Training the model to perform explicit
knowledge retrieval helps in only one of the tasks, indicating that the model's
limited OCKR capabilities are due to difficulties in retrieving relevant
knowledge. Furthermore, we treat cross-lingual knowledge transfer as a distinct
form of OCKR, and evaluate this ability. Our results show that the evaluated
model also exhibits limited ability in transferring knowledge across languages.
The dataset used in this study is available at
https://github.com/NJUNLP/ID-OCKR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language
  Model Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan Richard Schwarz, Sham Kakade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting high-quality data for pre-training is crucial in shaping the
downstream task performance of language models. A major challenge lies in
identifying this optimal subset, a problem generally considered intractable,
thus necessitating scalable and effective heuristics. In this work, we propose
a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering),
which leverages an empirical Bayes-inspired approach to derive a simple and
computationally efficient selection criterion based on the relative loss values
of two auxiliary models.
  In addition to the modeling rationale, we evaluate CoLoR-Filter empirically
on two language modeling tasks: (1) selecting data from C4 for domain
adaptation to evaluation on Books and (2) selecting data from C4 for a suite of
downstream multiple-choice question answering tasks. We demonstrate favorable
scaling both as we subselect more aggressively and using small auxiliary models
to select data for large target models. As one headline result, CoLoR-Filter
data selected using a pair of 150m parameter auxiliary models can train a 1.2b
parameter target model to match a 1.2b parameter model trained on 25b randomly
selected tokens with 25x less data for Books and 11x less data for the
downstream tasks.
  Code: https://github.com/davidbrandfonbrener/color-filter-olmo
  Filtered data:
https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Relation Extraction: Recent Advances and New
  Frontiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Zhao, Yang Deng, Min Yang, Lingzhi Wang, Rui Zhang, Hong Cheng, Wai Lam, Ying Shen, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction (RE) involves identifying the relations between entities
from underlying content. RE serves as the foundation for many natural language
processing (NLP) and information retrieval applications, such as knowledge
graph completion and question answering. In recent years, deep neural networks
have dominated the field of RE and made noticeable progress. Subsequently, the
large pre-trained language models have taken the state-of-the-art RE to a new
level. This survey provides a comprehensive review of existing deep learning
techniques for RE. First, we introduce RE resources, including datasets and
evaluation metrics. Second, we propose a new taxonomy to categorize existing
works from three perspectives, i.e., text representation, context encoding, and
triplet prediction. Third, we discuss several important challenges faced by RE
and summarize potential techniques to tackle these challenges. Finally, we
outline some promising future directions and prospects in this field. This
survey is expected to facilitate researchers' collaborative efforts to address
the challenges of real-world RE systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make <span class="highlight-title">Large Language Model</span> a Better Ranker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenshuo Chao, Zhi Zheng, Hengshu Zhu, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate robust capabilities across various
fields, leading to a paradigm shift in LLM-enhanced Recommender System (RS).
Research to date focuses on point-wise and pair-wise recommendation paradigms,
which are inefficient for LLM-based recommenders due to high computational
costs. However, existing list-wise approaches also fall short in ranking tasks
due to misalignment between ranking objectives and next-token prediction.
Moreover, these LLM-based methods struggle to effectively address the order
relation among candidates, particularly given the scale of ratings. To address
these challenges, this paper introduces the large language model framework with
Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap
between the capabilities of LLMs and the nuanced requirements of ranking tasks.
Specifically, ALRO employs explicit feedback in a listwise manner by
introducing soft lambda loss, a customized adaptation of lambda loss designed
for optimizing order relations. This mechanism provides more accurate
optimization goals, enhancing the ranking process. Additionally, ALRO
incorporates a permutation-sensitive learning mechanism that addresses position
bias, a prevalent issue in generative models, without imposing additional
computational burdens during inference. Our evaluative studies reveal that ALRO
outperforms both existing embedding-based recommendation methods and LLM-based
recommendation baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Prompt Multi-task Network for Abuse Language Detection <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Zhu, Yuping Ruan, Jingfei Chang, Wenhui Sun, Hui Wan, Jian Long, Cheng Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of abusive language remains a long-standing challenge with the
extensive use of social networks. The detection task of abusive language
suffers from limited accuracy. We argue that the existing detection methods
utilize the fine-tuning technique of the pre-trained language models (PLMs) to
handle downstream tasks. Hence, these methods fail to stimulate the general
knowledge of the PLMs. To address the problem, we propose a novel Deep Prompt
Multi-task Network (DPMN) for abuse language detection. Specifically, DPMN
first attempts to design two forms of deep prompt tuning and light prompt
tuning for the PLMs. The effects of different prompt lengths, tuning
strategies, and prompt initialization methods on detecting abusive language are
studied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which
can be used as a short text classifier. Eventually, DPMN utilizes multi-task
learning to improve detection metrics further. The multi-task network has the
function of transferring effective knowledge. The proposed DPMN is evaluated
against eight typical methods on three public datasets: OLID, SOLID, and
AbuseAnalyzer. The experimental results show that our DPMN outperforms the
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the International Conference on Pattern Recognition
  (ICPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient
  Fine-Tuning <span class="chip">ACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten de Rijke, Zhumin Chen, Jiahuan Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring
pre-trained large language models (LLMs), especially as the models' scale and
the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the
idea that the adaptation process is intrinsically low-dimensional, i.e.,
significant model changes can be represented with relatively few parameters.
However, decreasing the rank encounters challenges with generalization errors
for specific tasks when compared to full-parameter fine-tuning. We present
MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters
while maintaining a higher rank, thereby offering improved performance
potential. The core idea is to freeze original pretrained weights and train a
group of mini LoRAs with only a small number of parameters. This can capture a
significant degree of diversity among mini LoRAs, thus promoting better
generalization ability. We conduct a theoretical analysis and empirical studies
on various NLP tasks. Our experimental results show that, compared to LoRA,
MELoRA achieves better performance with 8 times fewer trainable parameters on
natural language understanding tasks and 36 times fewer trainable parameters on
instruction following tasks, which demonstrates the effectiveness of MELoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages. Our code is publicly
released at https://github.com/SamuelCahyawijaya/in-context-alignment
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLUE: A Clinical Language Understanding Evaluation for <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04067v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04067v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are expected to significantly contribute to
patient care, diagnostics, and administrative processes. Emerging biomedical
LLMs aim to address healthcare-specific challenges, including privacy demands
and computational constraints. Assessing the models' suitability for this
sensitive application area is of the utmost importance. However, evaluation has
primarily been limited to non-clinical tasks, which do not reflect the
complexity of practical clinical applications. To fill this gap, we present the
Clinical Language Understanding Evaluation (CLUE), a benchmark tailored to
evaluate LLMs on clinical tasks. CLUE includes six tasks to test the practical
applicability of LLMs in complex healthcare settings. Our evaluation includes a
total of $25$ LLMs. In contrast to previous evaluations, CLUE shows a decrease
in performance for nine out of twelve biomedical models. Our benchmark
represents a step towards a standardized approach to evaluating and developing
LLMs in healthcare to align future model development with the real-world needs
of clinical application. We open-source all evaluation scripts and datasets for
future research at https://github.com/TIO-IKIM/CLUE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESC-Eval: Evaluating Emotion Support Conversations in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiquan Zhao, Lingyu Li, Shisong Chen, Shuqi Kong, Jiaan Wang, Kexin Huang, Tianle Gu, Yixu Wang, Dandan Liang, Zhixu Li, Yan Teng, Yanghua Xiao, Yingchun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Support Conversation (ESC) is a crucial application, which aims to
reduce human stress, offer emotional guidance, and ultimately enhance human
mental and physical well-being. With the advancement of Large Language Models
(LLMs), many researchers have employed LLMs as the ESC models. However, the
evaluation of these LLM-based ESCs remains uncertain. Inspired by the awesome
development of role-playing agents, we propose an ESC Evaluation framework
(ESC-Eval), which uses a role-playing agent to interact with ESC models,
followed by a manual evaluation of the interactive dialogues. In detail, we
first re-organize 2,801 role-playing cards from seven existing datasets to
define the roles of the role-playing agent. Second, we train a specific
role-playing model called ESC-Role which behaves more like a confused person
than GPT-4. Third, through ESC-Role and organized role cards, we systematically
conduct experiments using 14 LLMs as the ESC models, including general
AI-assistant LLMs (ChatGPT) and ESC-oriented LLMs (ExTES-Llama). We conduct
comprehensive human annotations on interactive multi-turn dialogues of
different ESC models. The results show that ESC-oriented LLMs exhibit superior
ESC abilities compared to general AI-assistant LLMs, but there is still a gap
behind human performance. Moreover, to automate the scoring process for future
ESC models, we developed ESC-RANK, which trained on the annotated data,
achieving a scoring performance surpassing 35 points of GPT-4. Our data and
code are available at https://github.com/haidequanbu/ESC-Eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Neural Topic Models: Methods, Applications, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobao Wu, Thong Nguyen, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic models have been prevalent for decades to discover latent topics and
infer topic proportions of documents in an unsupervised fashion. They have been
widely used in various applications like text analysis and context
recommendation. Recently, the rise of neural networks has facilitated the
emergence of a new research field -- Neural Topic Models (NTMs). Different from
conventional topic models, NTMs directly optimize parameters without requiring
model-specific derivations. This endows NTMs with better scalability and
flexibility, resulting in significant research attention and plentiful new
methods and applications. In this paper, we present a comprehensive survey on
neural topic models concerning methods, applications, and challenges.
Specifically, we systematically organize current NTM methods according to their
network structures and introduce the NTMs for various scenarios like short
texts and bilingual documents. We also discuss a wide range of popular
applications built on NTMs. Finally, we highlight the challenges confronted by
NTMs to inspire future research. We accompany this survey with a repository for
easier access to the mentioned paper resources:
https://github.com/bobxwu/Paper-Neural-Topic-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Artificial Intelligence Review. See
  https://doi.org/10.1007/s10462-023-10661-7 and a paper list at
  https://github.com/BobXWu/Paper-Neural-Topic-Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich
  Reasoning <span class="chip">ACL24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large language models (LLMs) have the capability to iteratively reflect
on their own outputs, recent studies have observed their struggles with
knowledge-rich problems without access to external resources. In addition to
the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle
to revisit their predictions despite receiving explicit negative feedback.
Therefore, We propose Mirror, a Multiple-perspective self-reflection method for
knowledge-rich reasoning, to avoid getting stuck at a particular reflection
iteration. Mirror enables LLMs to reflect from multiple-perspective clues,
achieved through a heuristic interaction between a Navigator and a Reasoner. It
guides agents toward diverse yet plausibly reliable reasoning trajectory
without access to ground truth by encouraging (1) diversity of directions
generated by Navigator and (2) agreement among strategically induced
perturbations in responses generated by the Reasoner. The experiments on five
reasoning datasets demonstrate that Mirror's superiority over several
contemporary self-reflection approaches. Additionally, the ablation study
studies clearly indicate that our strategies alleviate the aforementioned
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL24, Main Conference, long paper. Code is available at
  https://github.com/hanqi-qi/Mirror.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fundus: A Simple-to-Use News Scraper Optimized for High Quality
  Extractions <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Dallabetta, Conrad Dobberstein, Adrian Breiding, Alan Akbik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Fundus, a user-friendly news scraper that enables users
to obtain millions of high-quality news articles with just a few lines of code.
Unlike existing news scrapers, we use manually crafted, bespoke content
extractors that are specifically tailored to the formatting guidelines of each
supported online newspaper. This allows us to optimize our scraping for quality
such that retrieved news articles are textually complete and without HTML
artifacts. Further, our framework combines both crawling (retrieving HTML from
the web or large web archives) and content extraction into a single pipeline.
By providing a unified interface for a predefined collection of newspapers, we
aim to make Fundus broadly usable even for non-technical users. This paper
gives an overview of the framework, discusses our design choices, and presents
a comparative evaluation against other popular news scrapers. Our evaluation
shows that Fundus yields significantly higher quality extractions (complete and
artifact-free news articles) than prior work. The framework is available on
GitHub under https://github.com/flairNLP/fundus and can be simply installed
using pip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, ACL 2024, for a screencast see
  https://www.youtube.com/watch?v=9GJExMelhdI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALERT: A Comprehensive Benchmark for Assessing <span class="highlight-title">Large Language Models</span>'
  Safety through Red Teaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08676v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08676v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Tedeschi, Felix Friedrich, Patrick Schramowski, Kristian Kersting, Roberto Navigli, Huu Nguyen, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When building Large Language Models (LLMs), it is paramount to bear safety in
mind and protect them with guardrails. Indeed, LLMs should never generate
content promoting or normalizing harmful, illegal, or unethical behavior that
may contribute to harm to individuals or society. This principle applies to
both normal and adversarial use. In response, we introduce ALERT, a large-scale
benchmark to assess safety based on a novel fine-grained risk taxonomy. It is
designed to evaluate the safety of LLMs through red teaming methodologies and
consists of more than 45k instructions categorized using our novel taxonomy. By
subjecting LLMs to adversarial testing scenarios, ALERT aims to identify
vulnerabilities, inform improvements, and enhance the overall safety of the
language models. Furthermore, the fine-grained taxonomy enables researchers to
perform an in-depth evaluation that also helps one to assess the alignment with
various policies. In our experiments, we extensively evaluate 10 popular open-
and closed-source LLMs and demonstrate that many of them still struggle to
attain reasonable levels of safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting with Divide-and-Conquer Program Makes <span class="highlight-title">Large Language Models</span>
  Discerning to Hallucination and Deception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05359v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05359v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models, such as Large language Models (LLMs), have attracted
significant amount of interest due to their large number of applications.
However, when handling tasks involving repetitive sub-tasks and/or deceptive
contents, such as arithmetic calculation and article-level fake news detection,
simple instructional prompts suffer from inaccurate responses. Existing works
show that more complicated prompting strategies, such as Chain-of-Thoughts and
Least-to-Most, can unlock LLM's powerful capacity in diverse areas. Recent
researches reveal that simple divide-and-conquer prompting strategy, i.e.
simply dividing the input sequence to multiple sub-inputs, can also
substantially improve LLM's performance in some specific tasks such as
misinformation detection. In this paper, we aim at examining the utility of
divide-and-conquer prompting strategy and answer on which kind of tasks this
strategy gets advantages. Specifically, we provide a theoretic analysis to
divide-and-conquer prompting strategy and help us identify the specific tasks
where DaC prompting can bring performance boost with theoretic guarantee. We
then present two cases (large integer arithmetic and fact verification) where
experimental results aligns with our theoretic analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-ICL: Contrastive In-context Learning for Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Mo, Jiahao Liu, Jian Yang, Qifan Wang, Shun Zhang, Jingang Wang, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been increasing interest in exploring the capabilities of advanced
large language models (LLMs) in the field of information extraction (IE),
specifically focusing on tasks related to named entity recognition (NER) and
relation extraction (RE). Although researchers are exploring the use of
few-shot information extraction through in-context learning with LLMs, they
tend to focus only on using correct or positive examples for demonstration,
neglecting the potential value of incorporating incorrect or negative examples
into the learning process. In this paper, we present c-ICL, a novel few-shot
technique that leverages both correct and incorrect sample constructions to
create in-context learning demonstrations. This approach enhances the ability
of LLMs to extract entities and relations by utilizing prompts that incorporate
not only the positive samples but also the reasoning behind them. This method
allows for the identification and correction of potential interface errors.
Specifically, our proposed method taps into the inherent contextual information
and valuable information in hard negative samples and the nearest positive
neighbors to the test and then applies the in-context learning demonstrations
based on LLMs. Our experiments on various datasets indicate that c-ICL
outperforms previous few-shot in-context learning methods, delivering
substantial enhancements in performance across a broad spectrum of related
tasks. These improvements are noteworthy, showcasing the versatility of our
approach in miscellaneous scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving In-context Learning via Bidirectional Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Qin, Wenhan Xia, Fangkai Jiao, Chen Chen, Yuchen Hu, Bosheng Ding, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive few-shot generalization on
many tasks via in-context learning (ICL). Despite their success in showing such
emergent abilities, the scale and complexity of larger models also lead to
unprecedentedly high computational demands and deployment challenges. In
reaction, researchers explore transferring the powerful capabilities of larger
models to more efficient and compact models by typically aligning the output of
smaller (student) models with that of larger (teacher) models. Existing methods
either train student models on the generated outputs of teacher models or
imitate their token-level probability distributions. However, these
distillation methods pay little to no attention to the input, which also plays
a crucial role in ICL. Based on the finding that the performance of ICL is
highly sensitive to the selection of demonstration examples, we propose
Bidirectional Alignment (BiAlign) to fully leverage the models' preferences for
ICL examples to improve the ICL abilities of student models. Specifically, we
introduce the alignment of input preferences between student and teacher models
by incorporating a novel ranking loss, in addition to aligning the token-level
output distribution. With extensive experiments and analysis, we demonstrate
that BiAlign can consistently outperform existing baselines on a variety of
tasks involving language understanding, reasoning, and coding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRUCE: Private Benchmarking to Prevent Contamination and Improve
  Comparative Evaluation of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmay Rajore, Nishanth Chandran, Sunayana Sitaram, Divya Gupta, Rahul Sharma, Kashish Mittal, Manohar Swaminathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarking is the de-facto standard for evaluating LLMs, due to its speed,
replicability and low cost. However, recent work has pointed out that the
majority of the open source benchmarks available today have been contaminated
or leaked into LLMs, meaning that LLMs have access to test data during
pretraining and/or fine-tuning. This raises serious concerns about the validity
of benchmarking studies conducted so far and the future of evaluation using
benchmarks. To solve this problem, we propose Private Benchmarking, a solution
where test datasets are kept private and models are evaluated without revealing
the test data to the model. We describe various scenarios (depending on the
trust placed on model owners or dataset owners), and present solutions to avoid
data contamination using private benchmarking. For scenarios where the model
weights need to be kept private, we describe solutions from confidential
computing and cryptography that can aid in private benchmarking. We build an
end-to-end system, TRUCE, that enables such private benchmarking showing that
the overheads introduced to protect models and benchmark are negligible (in the
case of confidential computing) and tractable (when cryptographic security is
required). Finally, we also discuss solutions to the problem of benchmark
dataset auditing, to ensure that private benchmarks are of sufficiently high
quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13372v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13372v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, Yongqiang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient fine-tuning is vital for adapting large language models (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present LlamaFactory, a unified framework that
integrates a suite of cutting-edge efficient training methods. It provides a
solution for flexibly customizing the fine-tuning of 100+ LLMs without the need
for coding through the built-in web UI LlamaBoard. We empirically validate the
efficiency and effectiveness of our framework on language modeling and text
generation tasks. It has been released at
https://github.com/hiyouga/LLaMA-Factory and received over 24,000 stars and
3,000 forks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, accepted to ACL 2024 System Demonstration Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the impact of 2D gesture representation on co-speech
  gesture generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teo Guichoux, Laure Soulier, Nicolas Obin, Catherine Pelachaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-speech gestures play a crucial role in the interactions between humans and
embodied conversational agents (ECA). Recent deep learning methods enable the
generation of realistic, natural co-speech gestures synchronized with speech,
but such approaches require large amounts of training data. "In-the-wild"
datasets, which compile videos from sources such as YouTube through human pose
detection models, offer a solution by providing 2D skeleton sequences that are
paired with speech. Concurrently, innovative lifting models have emerged,
capable of transforming these 2D pose sequences into their 3D counterparts,
leading to large and diverse datasets of 3D gestures. However, the derived 3D
pose estimation is essentially a pseudo-ground truth, with the actual ground
truth being the 2D motion data. This distinction raises questions about the
impact of gesture representation dimensionality on the quality of generated
motions, a topic that, to our knowledge, remains largely unexplored. In this
work, we evaluate the impact of the dimensionality of the training data, 2D or
3D joint coordinates, on the performance of a multimodal speech-to-gesture deep
generative model. We use a lifting model to convert 2D-generated sequences of
body pose to 3D. Then, we compare the sequence of gestures generated directly
in 3D to the gestures generated in 2D and lifted to 3D as post-processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. Paper accepted at WACAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comprehensive Reassessment of Large-Scale Evaluation Outcomes in <span class="highlight-title">LLM</span>s: A
  Multifaceted Statistical Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun, Rong Wang, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amidst the rapid evolution of LLMs, the significance of evaluation in
comprehending and propelling these models forward is increasingly paramount.
Evaluations have revealed that factors such as scaling, training types,
architectures and other factors profoundly impact the performance of LLMs.
However, the extent and nature of these impacts continue to be subjects of
debate because most assessments have been restricted to a limited number of
models and data points. Clarifying the effects of these factors on performance
scores can be more effectively achieved through a statistical lens. Our study
embarks on a thorough re-examination of these LLMs, targeting the inadequacies
in current evaluation methods. With the advent of a uniform evaluation
framework, our research leverages an expansive dataset of evaluation results,
introducing a comprehensive statistical methodology. This includes the
application of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering
a robust and transparent approach to deciphering LLM performance data. Contrary
to prevailing findings, our results challenge assumptions about emergent
abilities and the influence of given training types and architectures in LLMs.
These findings furnish new perspectives on the characteristics, intrinsic
nature, and developmental trajectories of LLMs. By providing straightforward
and reliable methods to scrutinize and reassess LLM performance data, this
study contributes a nuanced perspective on LLM efficiency and potentials.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaLM: Contrasting Large and Small Language Models to Verify Grounded
  Generation <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        I-Hung Hsu, Zifeng Wang, Long T. Le, Lesly Miculicich, Nanyun Peng, Chen-Yu Lee, Tomas Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounded generation aims to equip language models (LMs) with the ability to
produce more credible and accountable responses by accurately citing verifiable
sources. However, existing methods, by either feeding LMs with raw or
preprocessed materials, remain prone to errors. To address this, we introduce
CaLM, a novel verification framework. CaLM leverages the insight that a robust
grounded response should be consistent with information derived solely from its
cited sources. Our framework empowers smaller LMs, which rely less on
parametric memory and excel at processing relevant information given a query,
to validate the output of larger LMs. Larger LM responses that closely align
with the smaller LMs' output, which relies exclusively on cited documents, are
verified. Responses showing discrepancies are iteratively refined through a
feedback loop. Experiments on three open-domain question-answering datasets
demonstrate significant performance gains of 1.5% to 7% absolute average
without any required model fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Uli <span class="highlight-title">Dataset</span>: An Exercise in Experience Led Annotation of oGBV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09086v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09086v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnav Arora, Maha Jinadoss, Cheshta Arora, Denny George,  Brindaalakshmi, Haseena Dawood Khan, Kirti Rawat,  Div,  Ritash, Seema Mathur, Shivani Yadav, Shehla Rashid Shora, Rie Raut, Sumit Pawar, Apurva Paithane,  Sonia,  Vivek, Dharini Priscilla,  Khairunnisha, Grace Banu, Ambika Tandon, Rishav Thakker, Rahul Dev Korra, Aatman Vaidya, Tarunima Prabhakar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online gender based violence has grown concomitantly with adoption of the
internet and social media. Its effects are worse in the Global majority where
many users use social media in languages other than English. The scale and
volume of conversations on the internet has necessitated the need for automated
detection of hate speech, and more specifically gendered abuse. There is,
however, a lack of language specific and contextual data to build such
automated tools. In this paper we present a dataset on gendered abuse in three
languages- Hindi, Tamil and Indian English. The dataset comprises of tweets
annotated along three questions pertaining to the experience of gender abuse,
by experts who identify as women or a member of the LGBTQIA community in South
Asia. Through this dataset we demonstrate a participatory approach to creating
datasets that drive AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relation Extraction with Fine-Tuned <span class="highlight-title">Large Language Models</span> in Retrieval
  Augmented Generation Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sefika Efeoglu, Adrian Paschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Extraction (IE) is crucial for converting unstructured data into
structured formats like Knowledge Graphs (KGs). A key task within IE is
Relation Extraction (RE), which identifies relationships between entities in
text. Various RE methods exist, including supervised, unsupervised, weakly
supervised, and rule-based approaches. Recent studies leveraging pre-trained
language models (PLMs) have shown significant success in this area. In the
current era dominated by Large Language Models (LLMs), fine-tuning these models
can overcome limitations associated with zero-shot LLM prompting-based RE
methods, especially regarding domain adaptation challenges and identifying
implicit relations between entities in sentences. These implicit relations,
which cannot be easily extracted from a sentence's dependency tree, require
logical inference for accurate identification. This work explores the
performance of fine-tuned LLMs and their integration into the Retrieval
Augmented-based (RAG) RE approach to address the challenges of identifying
implicit relations at the sentence level, particularly when LLMs act as
generators within the RAG framework. Empirical evaluations on the TACRED,
TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant
performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B,
and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL,
where implicit relations are common, surpassing previous results on this
dataset. Additionally, our method outperforms previous works on TACRED, TACREV,
and Re-TACRED, demonstrating exceptional performance across diverse evaluation
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Makes Two Language Models Think Alike? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeanne Salle, Louis Jalouzot, Nur Lan, Emmanuel Chemla, Yair Lakretz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do architectural differences significantly affect the way models represent
and process language? We propose a new approach, based on metric-learning
encoding models (MLEMs), as a first step to answer this question. The approach
provides a feature-based comparison of how any two layers of any two models
represent linguistic information. We apply the method to BERT, GPT-2 and Mamba.
Unlike previous methods, MLEMs offer a transparent comparison, by identifying
the specific linguistic features responsible for similarities and differences.
More generally, the method uses formal, symbolic descriptions of a domain, and
use these to compare neural representations. As such, the approach can
straightforwardly be extended to other domains, such as speech and vision, and
to other neural systems, including human brains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-context Pretraining: Language Modeling Beyond Document Boundaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10638v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10638v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Gergely Szilvasy, Rich James, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, Mike Lewis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LMs) are currently trained to predict tokens given
document prefixes, enabling them to directly perform long-form generation and
prompting-style tasks which can be reduced to document completion. Existing
pretraining pipelines train LMs by concatenating random sets of short documents
to create input contexts but the prior documents provide no signal for
predicting the next document. We instead present In-Context Pretraining, a new
approach where language models are pretrained on a sequence of related
documents, thereby explicitly encouraging them to read and reason across
document boundaries. We can do In-Context Pretraining by simply changing the
document ordering so that each context contains related documents, and directly
applying existing pretraining pipelines. However, this document sorting problem
is challenging. There are billions of documents and we would like the sort to
maximize contextual similarity for every document without repeating any data.
To do this, we introduce approximate algorithms for finding related documents
with efficient nearest neighbor search and constructing coherent input contexts
with a graph traversal algorithm. Our experiments show In-Context Pretraining
offers a simple and scalable approach to significantly enhance LMs'performance:
we see notable improvements in tasks that require more complex contextual
reasoning, including in-context learning (+8%), reading comprehension (+15%),
faithfulness to previous contexts (+16%), long-context reasoning (+5%), and
retrieval augmentation (+9%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PromptKD: Distilling Student-Friendly Knowledge for Generative Language
  Models via Prompt Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeongman Kim, Doohyuk Jang, Eunho Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have raised concerns
about inference costs, increasing the need for research into model compression.
While knowledge distillation (KD) is a prominent method for this, research on
KD for generative language models like LLMs is relatively sparse, and the
approach of distilling student-friendly knowledge, which has shown promising
performance in KD for classification models, remains unexplored in generative
language models. To explore this approach, we propose PromptKD, a simple yet
effective method that utilizes prompt tuning - for the first time in KD - to
enable generative language models to transfer student-friendly knowledge.
Unlike previous works in classification that require fine-tuning the entire
teacher model for extracting student-friendly knowledge, PromptKD achieves
similar effects by adding a small number of prompt tokens and tuning only the
prompt with student guidance. Extensive experiments on instruction-following
datasets show that PromptKD achieves state-of-the-art performance while adding
only 0.0007% of the teacher's parameters as prompts. Further analysis suggests
that distilling student-friendly knowledge alleviates exposure bias effectively
throughout the entire training process, leading to performance enhancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/gmkim-ai/PromptKD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using
  Representative Data <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19260v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19260v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Tonneau, Pedro Vitor Quinta de Castro, Karim Lasri, Ibrahim Farouq, Lakshminarayanan Subramanian, Victor Orozco-Olvera, Samuel P. Fraiberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the global issue of online hate, hate speech detection (HSD)
systems are typically developed on datasets from the United States, thereby
failing to generalize to English dialects from the Majority World. Furthermore,
HSD models are often evaluated on non-representative samples, raising concerns
about overestimating model performance in real-world settings. In this work, we
introduce NaijaHate, the first dataset annotated for HSD which contains a
representative sample of Nigerian tweets. We demonstrate that HSD evaluated on
biased datasets traditionally used in the literature consistently overestimates
real-world performance by at least two-fold. We then propose NaijaXLM-T, a
pretrained model tailored to the Nigerian Twitter context, and establish the
key role played by domain-adaptive pretraining and finetuning in maximizing HSD
performance. Finally, owing to the modest performance of HSD systems in
real-world conditions, we find that content moderators would need to review
about ten thousand Nigerian tweets flagged as hateful daily to moderate 60% of
all hateful content, highlighting the challenges of moderating hate speech at
scale as social media usage continues to grow globally. Taken together, these
results pave the way towards robust HSD systems and a better protection of
social media users from hateful content in low-resource settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 main conference. Data and models available at
  https://github.com/worldbank/NaijaHate</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Testing the Limits of Jailbreaking Defenses with the Purple Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyoun Kim, Suhas Kotha, Aditi Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of "jailbreak" attacks on language models has led to a flurry of
defenses aimed at preventing undesirable responses. We critically examine the
two stages of the defense pipeline: (i) defining what constitutes unsafe
outputs, and (ii) enforcing the definition via methods such as input processing
or fine-tuning. To test the efficacy of existing enforcement mechanisms, we
consider a simple and well-specified definition of unsafe outputs--outputs that
contain the word "purple". Surprisingly, existing fine-tuning and input
defenses fail on this simple problem, casting doubt on whether enforcement
algorithms can be robust for more complicated definitions. We find that real
safety benchmarks similarly test enforcement for a fixed definition. We hope
that future research can lead to effective/fast enforcement as well as high
quality definitions used for enforcement and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ha<span class="highlight-title">llm</span>arks of Optimization Trajectories in Neural Networks: Directional
  Exploration and Redundancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidak Pal Singh, Bobby He, Thomas Hofmann, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a fresh take on understanding the mechanisms of neural networks by
analyzing the rich directional structure of optimization trajectories,
represented by their pointwise parameters. Towards this end, we introduce some
natural notions of the complexity of optimization trajectories, both
qualitative and quantitative, which hallmark the directional nature of
optimization in neural networks: when is there redundancy, and when
exploration. We use them to reveal the inherent nuance and interplay involved
between various optimization choices, such as momentum and weight decay.
Further, the trajectory perspective helps us see the effect of scale on
regularizing the directional nature of trajectories, and as a by-product, we
also observe an intriguing heterogeneity of Q,K,V dynamics in the middle
attention layers in LLMs and which is homogenized by scale. Importantly, we put
the significant directional redundancy observed to the test by demonstrating
that training only scalar batchnorm parameters some while into training matches
the performance of training the entire network, which thus exhibits the
potential of hybrid optimization schemes that are geared towards efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 57 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-Assisted Content Conditional Debiasing for Fair Text Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14208v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14208v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Deng, Blair Chen, Beidi Zhao, Chiyu Zhang, Xiaoxiao Li, Christos Thrampoulidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating biases in machine learning models has become an increasing concern
in Natural Language Processing (NLP), particularly in developing fair text
embeddings, which are crucial yet challenging for real-world applications like
search engines. In response, this paper proposes a novel method for learning
fair text embeddings. First, we define a novel content-conditional equal
distance (CCED) fairness for text embeddings, ensuring content-conditional
independence between sensitive attributes and text embeddings. Building on
CCED, we introduce a content-conditional debiasing (CCD) loss to ensure that
embeddings of texts with different sensitive attributes but identical content
maintain the same distance from the embedding of their corresponding neutral
text. Additionally, we tackle the issue of insufficient training data by using
Large Language Models (LLMs) with instructions to fairly augment texts into
different sensitive groups. Our extensive evaluations show that our approach
effectively enhances fairness while maintaining the utility of embeddings.
Furthermore, our augmented dataset, combined with the CCED metric, serves as an
new benchmark for evaluating fairness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafetyBench: Evaluating the Safety of <span class="highlight-title">Large Language Models</span> <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Large Language Models (LLMs), increasing
attention has been paid to their safety concerns. Consequently, evaluating the
safety of LLMs has become an essential task for facilitating the broad
applications of LLMs. Nevertheless, the absence of comprehensive safety
evaluation benchmarks poses a significant impediment to effectively assess and
enhance the safety of LLMs. In this work, we present SafetyBench, a
comprehensive benchmark for evaluating the safety of LLMs, which comprises
11,435 diverse multiple choice questions spanning across 7 distinct categories
of safety concerns. Notably, SafetyBench also incorporates both Chinese and
English data, facilitating the evaluation in both languages. Our extensive
tests over 25 popular Chinese and English LLMs in both zero-shot and few-shot
settings reveal a substantial performance advantage for GPT-4 over its
counterparts, and there is still significant room for improving the safety of
current LLMs. We also demonstrate that the measured safety understanding
abilities in SafetyBench are correlated with safety generation abilities. Data
and evaluation guidelines are available at
\url{https://github.com/thu-coai/SafetyBench}{https://github.com/thu-coai/SafetyBench}.
Submission entrance and leaderboard are available at
\url{https://llmbench.ai/safety}{https://llmbench.ai/safety}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WeatherQA: Can Multimodal Language Models Reason about Severe Weather? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengqian Ma, Zhanxiang Hua, Alexandra Anderson-Frey, Vikram Iyer, Xin Liu, Lianhui Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Severe convective weather events, such as hail, tornadoes, and thunderstorms,
often occur quickly yet cause significant damage, costing billions of dollars
every year. This highlights the importance of forecasting severe weather
threats hours in advance to better prepare meteorologists and residents in
at-risk areas. Can modern large foundation models perform such forecasting?
Existing weather benchmarks typically focus only on predicting time-series
changes in certain weather parameters (e.g., temperature, moisture) with
text-only features. In this work, we introduce WeatherQA, the first multimodal
dataset designed for machines to reason about complex combinations of weather
parameters (a.k.a., ingredients) and predict severe weather in real-world
scenarios. The dataset includes over 8,000 (multi-images, text) pairs for
diverse severe weather events. Each pair contains rich information crucial for
forecasting -- the images describe the ingredients capturing environmental
instability, surface observations, and radar reflectivity, and the text
contains forecast analyses written by human experts. With WeatherQA, we
evaluate state-of-the-art vision language models, including GPT4, Claude3.5,
Gemini-1.5, and a fine-tuned Llama3-based VLM, by designing two challenging
tasks: (1) multi-choice QA for predicting affected area and (2) classification
of the development potential of severe convection. These tasks require deep
understanding of domain knowledge (e.g., atmospheric dynamics) and complex
reasoning over multimodal data (e.g., interactions between weather parameters).
We show a substantial gap between the strongest VLM, GPT4o, and human
reasoning. Our comprehensive case study with meteorologists further reveals the
weaknesses of the models, suggesting that better training and data integration
are necessary to bridge this gap. WeatherQA link:
https://github.com/chengqianma/WeatherQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smurfs: Leve<span class="highlight-title">rag</span>ing Multiple Proficiency Agents with Context-Efficiency
  for Tool Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05955v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05955v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhi Chen, Juhao Liang, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of large language models (LLMs) has opened up unprecedented
possibilities for automating complex tasks that are often comparable to human
performance. Despite their capabilities, LLMs still encounter difficulties in
completing tasks that require high levels of accuracy and complexity due to
their inherent limitations in handling multifaceted problems single-handedly.
This paper introduces `Smurfs', a cutting-edge multi-agent framework designed
to revolutionize the application of LLMs. By seamlessly transforming a
conventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance
the model's ability to solve complex tasks at no additional cost. This is
achieved through innovative prompting strategies that allocate distinct roles
within the model, thereby facilitating collaboration among specialized agents
and forming an intelligent multi-agent system. Our empirical investigation on
both open-ended task of StableToolBench and closed-ended task on HotpotQA
showcases Smurfs' superior capability in intricate tool utilization scenarios.
Notably, Smurfs outmatches all the baseline methods in both experiments,
setting new state-of-the-art performance. Furthermore, through comprehensive
ablation studies, we dissect the contribution of the core components of the
multi-agent framework to its overall efficacy. This not only verifies the
effectiveness of the framework, but also sets a route for future exploration of
multi-agent LLM systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language
  Model Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01054v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01054v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best-of-N (BoN) sampling with a reward model has been shown to be an
effective strategy for aligning Large Language Models (LLMs) to human
preferences at the time of decoding. BoN sampling is susceptible to a problem
known as reward hacking. Because the reward model is an imperfect proxy for the
true objective, over-optimizing its value can compromise its performance on the
true objective. A common solution to prevent reward hacking in preference
learning techniques is to optimize a reward using proximity regularization
(e.g., KL regularization), which ensures that the language model remains close
to the reference model. In this research, we propose Regularized Best-of-N
(RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating
a proximity term in response selection, similar to preference learning
techniques. We evaluate RBoN on the AlpacaFarm and Anthropic's hh-rlhf datasets
and find that it outperforms BoN. As an application of RBoN, we use RBoN to
generate a pairwise preference learning dataset. Experimental results show that
a DPO model trained on a dataset generated with RBoN outperforms a DPO model
generated with vanilla BoN. Our code is available at
https://github.com/CyberAgentAILab/regularized-bon
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language
  Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07269v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07269v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy
issues, which means they are unaware of unseen events or generate text with
incorrect facts owing to outdated/noisy data. To this end, many knowledge
editing approaches for LLMs have emerged -- aiming to subtly inject/edit
updated knowledge or adjust undesired behavior while minimizing the impact on
unrelated inputs. Nevertheless, due to significant differences among various
knowledge editing methods and the variations in task setups, there is no
standard implementation framework available for the community, which hinders
practitioners from applying knowledge editing to applications. To address these
issues, we propose EasyEdit, an easy-to-use knowledge editing framework for
LLMs. It supports various cutting-edge knowledge editing approaches and can be
readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.
Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,
demonstrating that knowledge editing surpasses traditional fine-tuning in terms
of reliability and generalization. We have released the source code on GitHub,
along with Google Colab tutorials and comprehensive documentation for beginners
to get started. Besides, we present an online system for real-time knowledge
editing, and a demo video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 System Demonstrations; Code:
  https://github.com/zjunlp/EasyEdit HF Demo:
  https://huggingface.co/spaces/zjunlp/EasyEdit Video:
  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Privacy Risks in Online Self-Disclosures with Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09538v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09538v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan Ritter, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-disclosure, while being common and rewarding in social media
interaction, also poses privacy risks. In this paper, we take the initiative to
protect the user-side privacy associated with online self-disclosure through
detection and abstraction. We develop a taxonomy of 19 self-disclosure
categories and curate a large corpus consisting of 4.8K annotated disclosure
spans. We then fine-tune a language model for detection, achieving over 65%
partial span F$_1$. We further conduct an HCI user study, with 82% of
participants viewing the model positively, highlighting its real-world
applicability. Motivated by the user feedback, we introduce the task of
self-disclosure abstraction, which is rephrasing disclosures into less specific
terms while preserving their utility, e.g., "Im 16F" to "I'm a teenage girl".
We explore various fine-tuning strategies, and our best model can generate
diverse abstractions that moderately reduce privacy risks while maintaining
high utility according to human evaluation. To help users in deciding which
disclosures to abstract, we present a task of rating their importance for
context understanding. Our fine-tuned model achieves 80% accuracy, on-par with
GPT-3.5. Given safety and privacy considerations, we will only release our
corpus and models to researcher who agree to the ethical guidelines outlined in
Ethics Statement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03049v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03049v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with an online demo app and a demo video for quick-start, calling for
broader research centered on instruction data and synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 System Demonstrations; Project website:
  https://zjunlp.github.io/project/EasyInstruct Code:
  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo
  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClaimVer: Explainable Claim-Level Verification and Evidence Attribution
  of Text Through Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preetam Prabhu Srikar Dammu, Himanshu Naidu, Mouly Dewan, YoungMin Kim, Tanya Roosta, Aman Chadha, Chirag Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the midst of widespread misinformation and disinformation through social
media and the proliferation of AI-generated texts, it has become increasingly
difficult for people to validate and trust information they encounter. Many
fact-checking approaches and tools have been developed, but they often lack
appropriate explainability or granularity to be useful in various contexts. A
text validation method that is easy to use, accessible, and can perform
fine-grained evidence attribution has become crucial. More importantly,
building user trust in such a method requires presenting the rationale behind
each prediction, as research shows this significantly influences people's
belief in automated systems. Localizing and bringing users' attention to the
specific problematic content is also paramount, instead of providing simple
blanket labels. In this paper, we present ClaimVer, a human-centric framework
tailored to meet users' informational and verification needs by generating rich
annotations and thereby reducing cognitive load. Designed to deliver
comprehensive evaluations of texts, it highlights each claim, verifies it
against a trusted knowledge graph (KG), presents the evidence, and provides
succinct, clear explanations for each claim prediction. Finally, our framework
introduces an attribution score, enhancing applicability across a wide range of
downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EFUF: Efficient Fine-grained Unlearning Framework for Mitigating
  Hallucinations in <span class="highlight-title">Multimodal Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, Xinyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have attracted increasing attention
in the past few years, but they may still generate descriptions that include
objects not present in the corresponding images, a phenomenon known as object
hallucination. To eliminate hallucinations, existing methods manually annotate
paired responses with and without hallucinations, and then employ various
alignment algorithms to improve the alignment capability between images and
text. However, they not only demand considerable computation resources during
the finetuning stage but also require expensive human annotation to construct
paired data needed by the alignment algorithms. To address these issues, we
borrow the idea of unlearning and propose an efficient fine-grained unlearning
framework (EFUF), which can eliminate hallucinations without the need for
paired data. Extensive experiments show that our method consistently reduces
hallucinations while preserving the generation quality with modest
computational overhead. Our code and datasets will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VeraCT Scan: <span class="highlight-title">Retrieval-Augmented</span> Fake News Detection with Justifiable
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Niu, Yang Guan, Yuanhao Wu, Juno Zhu, Juntong Song, Randy Zhong, Kaihua Zhu, Siliang Xu, Shizhe Diao, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of fake news poses a significant threat not only by
disseminating misleading information but also by undermining the very
foundations of democracy. The recent advance of generative artificial
intelligence has further exacerbated the challenge of distinguishing genuine
news from fabricated stories. In response to this challenge, we introduce
VeraCT Scan, a novel retrieval-augmented system for fake news detection. This
system operates by extracting the core facts from a given piece of news and
subsequently conducting an internet-wide search to identify corroborating or
conflicting reports. Then sources' credibility is leveraged for information
verification. Besides determining the veracity of news, we also provide
transparent evidence and reasoning to support its conclusions, resulting in the
interpretability and trust in the results. In addition to GPT-4 Turbo, Llama-2
13B is also fine-tuned for news content understanding, information
verification, and reasoning. Both implementations have demonstrated
state-of-the-art accuracy in the realm of fake news detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, Ming-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image retrieval, i.e., finding desired images given a reference image,
inherently encompasses rich, multi-faceted search intents that are difficult to
capture solely using image-based measures. Recent works leverage text
instructions to allow users to more freely express their search intents.
However, they primarily focus on image pairs that are visually similar and/or
can be characterized by a small set of pre-defined relations. The core thesis
of this paper is that text instructions can enable retrieving images with
richer relations beyond visual similarity. To show this, we introduce
MagicLens, a series of self-supervised image retrieval models that support
open-ended instructions. MagicLens is built on a key novel insight: image pairs
that naturally occur on the same web pages contain a wide range of implicit
relations (e.g., inside view of), and we can bring those implicit relations
explicit by synthesizing instructions via foundation models. Trained on 36.7M
(query image, instruction, target image) triplets with rich semantic relations
mined from the web, MagicLens achieves results comparable with or better than
prior best on eight benchmarks of various image retrieval tasks, while
maintaining high parameter efficiency with a significantly smaller model size.
Additional human analyses on a 1.4M-image unseen corpus further demonstrate the
diversity of search intents supported by MagicLens. Code and models are
publicly available at https://open-vision-language.github.io/MagicLens/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024 (Oral); Project Website:
  https://open-vision-language.github.io/MagicLens/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation alignment: Comparing <span class="highlight-title">LLM</span> and human annotations of
  conversational safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06369v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06369v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajiv Movva, Pang Wei Koh, Emma Pierson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To what extent do LLMs align with human perceptions of safety? We study this
question via *annotation alignment*, the extent to which LLMs and humans agree
when annotating the safety of user-chatbot conversations. We leverage the
recent DICES dataset (Aroyo et al., 2023), in which 350 conversations are each
rated for safety by 112 annotators spanning 10 race-gender groups. GPT-4
achieves a Pearson correlation of $r = 0.59$ with the average annotator rating,
higher than the median annotator's correlation with the average ($r=0.51$). We
show that larger datasets are needed to resolve whether GPT-4 exhibits
disparities in how well it correlates with demographic groups. Also, there is
substantial idiosyncratic variation in correlation *within* groups, suggesting
that race & gender do not fully capture differences in alignment. Finally, we
find that GPT-4 cannot predict when one demographic group finds a conversation
more unsafe than another.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working draft, short paper. Main text is 5 pages, 1 figure. (v3
  corrects minor typo)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Care: Assessing the Healthcare Implications of Pre-training Data
  on Language Model Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Chen, Jack Gallifant, Mingye Gao, Pedro Moreira, Nikolaj Munch, Ajay Muthukkumar, Arvind Rajan, Jaya Kolluri, Amelia Fiske, Janna Hastings, Hugo Aerts, Brian Anthony, Leo Anthony Celi, William G. La Cava, Danielle S. Bitterman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly essential in processing natural
languages, yet their application is frequently compromised by biases and
inaccuracies originating in their training data. In this study, we introduce
Cross-Care, the first benchmark framework dedicated to assessing biases and
real world knowledge in LLMs, specifically focusing on the representation of
disease prevalence across diverse demographic groups. We systematically
evaluate how demographic biases embedded in pre-training corpora like $ThePile$
influence the outputs of LLMs. We expose and quantify discrepancies by
juxtaposing these biases against actual disease prevalences in various U.S.
demographic groups. Our results highlight substantial misalignment between LLM
representation of disease prevalence and real disease prevalence rates across
demographic subgroups, indicating a pronounced risk of bias propagation and a
lack of real-world grounding for medical applications of LLMs. Furthermore, we
observe that various alignment methods minimally resolve inconsistencies in the
models' representation of disease prevalence across different languages. For
further exploration and analysis, we make all data and a data visualization
tool available at: www.crosscare.net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for review, data visualization tool available at:
  www.crosscare.net</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting Explicit and Implicit Knowledge for Multi-hop Question
  Answering Based on Human Reading Process <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19350v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19350v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to
simulate human reasoning and inference processes, achieving proficient
performance in multi-hop QA. However, a gap persists between PLMs' reasoning
abilities and those of humans when tackling complex problems. Psychological
studies suggest a vital connection between explicit information in passages and
human prior knowledge during reading. Nevertheless, current research has given
insufficient attention to linking input passages and PLMs' pre-training-based
knowledge from the perspective of human cognition studies. In this study, we
introduce a Prompting Explicit and Implicit knowledge (PEI) framework, which
uses prompts to connect explicit and implicit knowledge, aligning with human
reading process for multi-hop QA. We consider the input passages as explicit
knowledge, employing them to elicit implicit knowledge through unified prompt
reasoning. Furthermore, our model incorporates type-specific reasoning via
prompts, a form of implicit knowledge. Experimental results show that PEI
performs comparably to the state-of-the-art on HotpotQA. Ablation studies
confirm the efficacy of our model in bridging and integrating explicit and
implicit knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Instructions: Compositional Instruction Tuning on Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirley Anugrah Hayati, Taehee Jung, Tristan Bodding-Long, Sudipta Kar, Abhinav Sethy, Joo-Kyung Kim, Dongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) with a collection of large and
diverse instructions has improved the model's generalization to different
tasks, even for unseen tasks. However, most existing instruction datasets
include only single instructions, and they struggle to follow complex
instructions composed of multiple subtasks. In this work, we propose a novel
concept of compositional instructions called chain-of-instructions (CoI), where
the output of one instruction becomes an input for the next like a chain.
Unlike the conventional practice of solving single instruction tasks, our
proposed method encourages a model to solve each subtask step by step until the
final answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions)
improves the model's ability to handle instructions composed of multiple
subtasks as well as unseen composite tasks such as multilingual summarization.
Overall, our study find that simple CoI tuning of existing instruction data can
provide consistent generalization to solve more complex, unseen, and longer
chains of instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16137v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16137v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's large language models (LLMs) typically train on short text segments
(e.g., <4K tokens) due to the quadratic complexity of their Transformer
architectures. As a result, their performance suffers drastically on inputs
longer than those encountered during training, substantially limiting their
applications in real-world tasks involving long contexts such as encoding
scientific articles, code repositories, or long dialogues. Through theoretical
analysis and empirical investigation, this work identifies three major factors
contributing to this length generalization failure. Our theoretical analysis
further reveals that commonly used techniques like truncating the attention
window or relative positional encodings are inadequate to address them.
Answering these challenges, we propose LM-Infinite, a simple and effective
method for enhancing LLMs' capabilities of handling long contexts. LM-Infinite
is highly flexible and can be used with most modern LLMs off-the-shelf. Without
any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments
to generalize to up to 200M length inputs while retaining perplexity. It also
improves performance on downstream tasks such as Passkey Retrieval and Qasper
in the zero-shot setting. LM-Infinite brings substantial efficiency
improvements: it achieves 2.7x decoding speed up and 7.5x memory saving over
the original model. Our codes are released at
\url{https://github.com/Glaciohound/LM-Infinite}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Outstanding paper, 9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinkTransformer: A Unified Package for Record Linkage with Transformer
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Arora, Melissa Dell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linking information across sources is fundamental to a variety of analyses in
social science, business, and government. While large language models (LLMs)
offer enormous promise for improving record linkage in noisy datasets, in many
domains approximate string matching packages in popular softwares such as R and
Stata remain predominant. These packages have clean, simple interfaces and can
be easily extended to a diversity of languages. Our open-source package
LinkTransformer aims to extend the familiarity and ease-of-use of popular
string matching methods to deep learning. It is a general purpose package for
record linkage with transformer LLMs that treats record linkage as a text
retrieval problem. At its core is an off-the-shelf toolkit for applying
transformer models to record linkage with four lines of code. LinkTransformer
contains a rich repository of pre-trained transformer semantic similarity
models for multiple languages and supports easy integration of any transformer
language model from Hugging Face or OpenAI. It supports standard functionality
such as blocking and linking on multiple noisy fields. LinkTransformer APIs
also perform other common text data processing tasks, e.g., aggregation, noisy
de-duplication, and translation-free cross-lingual linkage. Importantly,
LinkTransformer also contains comprehensive tools for efficient model tuning,
to facilitate different levels of customization when off-the-shelf models do
not provide the required accuracy. Finally, to promote reusability,
reproducibility, and extensibility, LinkTransformer makes it easy for users to
contribute their custom-trained models to its model hub. By combining
transformer language models with intuitive APIs that will be familiar to many
users of popular string matching packages, LinkTransformer aims to democratize
the benefits of LLMs among those who may be less familiar with deep learning
frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COFFEE: A Contrastive Oracle-Free Framework for Event Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiru Zhang, Yixuan Su, Zaiqiao Meng, Zihao Fu, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction is a complex information extraction task that involves
extracting events from unstructured text. Prior classification-based methods
require comprehensive entity annotations for joint training, while newer
generation-based methods rely on heuristic templates containing oracle
information such as event type, which is often unavailable in real-world
scenarios. In this study, we consider a more realistic setting of this task,
namely the Oracle-Free Event Extraction (OFEE) task, where only the input
context is given without any oracle information, including event type, event
ontology and trigger word. To solve this task, we propose a new framework,
called COFFEE, which extracts the events solely based on the document context
without referring to any oracle information. In particular, a contrastive
selection model is introduced in COFFEE to rectify the generated triggers and
handle multi-event instances. The proposed COFFEE outperforms state-of-the-art
approaches under the oracle-free setting of the event extraction task, as
evaluated on a public event extraction benchmark ACE05.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOUL: Unlocking the Power of Second-Order Optimization for <span class="highlight-title">LLM</span>
  Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18239v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18239v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, Sijia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have highlighted the necessity of effective
unlearning mechanisms to comply with data regulations and ethical AI practices.
LLM unlearning aims at removing undesired data influences and associated model
capabilities without compromising utility beyond the scope of unlearning. While
interest in studying LLM unlearning is growing, the impact of the optimizer
choice for LLM unlearning remains unexplored. In this work, we shed light on
the significance of optimizer selection in LLM unlearning for the first time,
establishing a clear connection between second-order optimization and influence
unlearning (a classical approach using influence functions to update the model
for data influence removal). This insight propels us to develop a second-order
optimization-based LLM unlearning framework, termed Second-Order UnLearning
(SOUL), which extends the static, one-shot model update using influence
unlearning to a dynamic, iterative unlearning process. Our extensive
experiments show that SOUL consistently outperforms conventional first-order
methods across various unlearning tasks, models, and metrics, indicating that
second-order optimization offers an effective and broadly applicable solution
for LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years
  of German Parliamentary Debates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aida Kostikova, Benjamin Paassen, Dominik Beese, Ole Pütz, Gregor Wiedemann, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solidarity is a crucial concept to understand social relations in societies.
In this paper, we explore fine-grained solidarity frames to study solidarity
towards women and migrants in German parliamentary debates between 1867 and
2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k
Euro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and
GPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation
quality. Using GPT-4, we automatically annotate more than 18k further instances
(with a cost of around 500 Euro) across 155 years and find that solidarity with
migrants outweighs anti-solidarity but that frequencies and solidarity types
shift over time. Most importantly, group-based notions of (anti-)solidarity
fade in favor of compassionate solidarity, focusing on the vulnerability of
migrant groups, and exchange-based anti-solidarity, focusing on the lack of
(economic) contribution. Our study highlights the interplay of historical
events, socio-economic needs, and political ideologies in shaping migration
discourse and social cohesion. We also show that powerful LLMs, if carefully
prompted, can be cost-effective alternatives to human annotation for hard
social scientific tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Note title and author changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligner: Efficient Alignment by Learning to Correct 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02416v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02416v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Tianyi Qiu, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs) and ever-evolving
practical requirements, finding an efficient and effective alignment method has
never been more critical. However, the tension between the complexity of
current alignment methods and the need for rapid iteration in deployment
scenarios necessitates the development of a model-agnostic alignment approach
that can operate under these constraints. In this paper, we introduce Aligner,
a novel and simple alignment paradigm that learns the correctional residuals
between preferred and dispreferred answers using a small model. Designed as a
model-agnostic, plug-and-play module, Aligner can be directly applied to
various open-source and API-based models with only one-off training, making it
suitable for rapid iteration. Notably, Aligner can be applied to any powerful,
large-scale upstream models. Moreover, it can even iteratively bootstrap the
upstream models using corrected responses as synthetic human preference data,
breaking through the model's performance ceiling. Our experiments demonstrate
performance improvements by deploying the same Aligner model across 11
different LLMs, evaluated on the 3H dimensions (helpfulness, harmlessness, and
honesty). Specifically, Aligner-7B has achieved an average improvement of 68.9%
in helpfulness and 23.8% in harmlessness across the tested LLMs while also
effectively reducing hallucination. In the Alpaca-Eval leaderboard, stacking
Aligner-2B on GPT-4 Turbo improved its LC Win Rate from 55.0% to 58.3%,
surpassing GPT-4 Omni's 57.5% Win Rate (community report).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">RAG</span>-RLRC-LaySum at BioLaySumm: Integrating <span class="highlight-title">Retrieval-Augmented</span>
  Generation and Readability Control for Layman Summarization of Biomedical
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13179v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13179v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuelyu Ji, Zhuochun Li, Rui Meng, Sonish Sivarajkumar, Yanshan Wang, Zeshui Yu, Hui Ji, Yushui Han, Hanyu Zeng, Daqing He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the RAG-RLRC-LaySum framework, designed to make complex
biomedical research understandable to laymen through advanced Natural Language
Processing (NLP) techniques. Our Retrieval Augmented Generation (RAG) solution,
enhanced by a reranking method, utilizes multiple knowledge sources to ensure
the precision and pertinence of lay summaries. Additionally, our Reinforcement
Learning for Readability Control (RLRC) strategy improves readability, making
scientific content comprehensible to non-specialists. Evaluations using the
publicly accessible PLOS and eLife datasets show that our methods surpass Plain
Gemini model, demonstrating a 20% increase in readability scores, a 15%
improvement in ROUGE-2 relevance scores, and a 10% enhancement in factual
accuracy. The RAG-RLRC-LaySum framework effectively democratizes scientific
knowledge, enhancing public engagement with biomedical discoveries.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Rag</span>narök: A Reusable <span class="highlight-title">RAG</span> Framework and Baselines for TREC 2024
  <span class="highlight-title">Retrieval-Augmented</span> Generation Track 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronak Pradeep, Nandan Thakur, Sahel Sharifymoghaddam, Eric Zhang, Ryan Nguyen, Daniel Campos, Nick Craswell, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Did you try out the new Bing Search? Or maybe you fiddled around with Google
AI~Overviews? These might sound familiar because the modern-day search stack
has recently evolved to include retrieval-augmented generation (RAG) systems.
They allow searching and incorporating real-time data into large language
models (LLMs) to provide a well-informed, attributed, concise summary in
contrast to the traditional search paradigm that relies on displaying a ranked
list of documents. Therefore, given these recent advancements, it is crucial to
have an arena to build, test, visualize, and systematically evaluate RAG-based
search systems. With this in mind, we propose the TREC 2024 RAG Track to foster
innovation in evaluating RAG systems. In our work, we lay out the steps we've
made towards making this track a reality -- we describe the details of our
reusable framework, Ragnar\"ok, explain the curation of the new MS MARCO V2.1
collection choice, release the development topics for the track, and
standardize the I/O definitions which assist the end user. Next, using
Ragnar\"ok, we identify and provide key industrial baselines such as OpenAI's
GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface
for an interactive arena allowing benchmarking pairwise RAG systems by
crowdsourcing. We open-source our Ragnar\"ok framework and baselines to achieve
a unified standard for future RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-experiments: Improving experimentation through experimentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie J. I. Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A/B testing is widexly used in the industry to optimize customer facing
websites. Many companies employ experimentation specialists to facilitate and
improve the process of A/B testing. Here, we present the application of A/B
testing to this improvement effort itself, by running experiments on the
experimentation process, which we call 'meta-experiments'. We discuss the
challenges of this approach using the example of one of our meta-experiments,
which helped experimenters to run more sufficiently powered A/B tests. We also
point out the benefits of 'dog fooding' for the experimentation specialists
when running their own experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Star+: A New Multi-Domain Model for CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Çağrı Yeşil, Kaya Turgut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Star+, a novel multi-domain model for
click-through rate (CTR) prediction inspired by the Star model. Traditional
single-domain approaches and existing multi-task learning techniques face
challenges in multi-domain environments due to their inability to capture
domain-specific data distributions and complex inter-domain relationships.
Star+ addresses these limitations by enhancing the interaction between shared
and domain-specific information through various fusion strategies, such as add,
adaptive add, concatenation, and gating fusions, to find the optimal balance
between domain-specific and shared information. We also investigate the impact
of different normalization techniques, including layer normalization, batch
normalization, and partition normalization, on the performance of our model.
Our extensive experiments on both industrial and public datasets demonstrate
that Star+ significantly improves prediction accuracy and efficiency. This work
contributes to the advancement of recommendation systems by providing a robust,
scalable, and adaptive solution for multi-domain environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-domain Transfer of Valence Preferences via a Meta-optimization
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang Zhao, Hongke Zhao, Ming He, Xiaomeng Li, Jianping Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-domain recommendation offers a potential avenue for alleviating data
sparsity and cold-start problems. Embedding and mapping, as a classic
cross-domain research genre, aims to identify a common mapping function to
perform representation transformation between two domains. Nevertheless,
previous coarse-grained preference representations, non-personalized mapping
functions, and excessive reliance on overlapping users limit their performance,
especially in scenarios where overlapping users are sparse. To address
aforementioned challenges, we propose a novel cross-domain approach, namely
CVPM. CVPM formalizes cross-domain interest transfer as a hybrid architecture
of parametric meta-learning and self-supervised learning, which not only
transfers user preferences at a finer level, but also enables signal
enhancement with the knowledge of non-overlapping users. Specifically, with
deep insights into user preferences and valence preference theory, we believe
that there exists significant difference between users' positive preferences
and negative behaviors, and thus employ differentiated encoders to learn their
distributions. In particular, we further utilize the pre-trained model and item
popularity to sample pseudo-interaction items to ensure the integrity of both
distributions. To guarantee the personalization of preference transfer, we
treat each user's mapping as two parts, the common transformation and the
personalized bias, where the network used to generate the personalized bias is
output by a meta-learner. Furthermore, in addition to the supervised loss for
overlapping users, we design contrastive tasks for non-overlapping users from
both group and individual-levels to avoid model skew and enhance the semantics
of representations. Exhaustive data analysis and extensive experimental results
demonstrate the effectiveness and advancement of our proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-augmented Retrieval: A Novel Framework for Fast Information
  Retrieval based Response Generation using <span class="highlight-title">Large Language Model</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Ganesh, Anupam Purwar, Gautam B
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating high-quality answers consistently by providing contextual
information embedded in the prompt passed to the Large Language Model (LLM) is
dependent on the quality of information retrieval. As the corpus of contextual
information grows, the answer/inference quality of Retrieval Augmented
Generation (RAG) based Question Answering (QA) systems declines. This work
solves this problem by combining classical text classification with the Large
Language Model (LLM) to enable quick information retrieval from the vector
store and ensure the relevancy of retrieved information. For the same, this
work proposes a new approach Context Augmented retrieval (CAR), where
partitioning of vector database by real-time classification of information
flowing into the corpus is done. CAR demonstrates good quality answer
generation along with significant reduction in information retrieval and answer
generation time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Long-tail Knowledge in <span class="highlight-title">Retrieval Augmented</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Li, Junbing Yan, Taolin Zhang, Chengyu Wang, Xiaofeng He, Longtao Huang, Hui Xue, Jun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation (RAG) exhibits outstanding performance in
promoting the knowledge capabilities of large language models (LLMs) with
retrieved documents related to user queries. However, RAG only focuses on
improving the response quality of LLMs via enhancing queries indiscriminately
with retrieved information, paying little attention to what type of knowledge
LLMs really need to answer original queries more accurately. In this paper, we
suggest that long-tail knowledge is crucial for RAG as LLMs have already
remembered common world knowledge during large-scale pre-training. Based on our
observation, we propose a simple but effective long-tail knowledge detection
method for LLMs. Specifically, the novel Generative Expected Calibration Error
(GECE) metric is derived to measure the ``long-tailness'' of knowledge based on
both statistics and semantics. Hence, we retrieve relevant documents and infuse
them into the model for patching knowledge loopholes only when the input query
relates to long-tail knowledge. Experiments show that, compared to existing RAG
pipelines, our method achieves over 4x speedup in average inference time and
consistent performance improvement in downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Intent-aware Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dietmar Jannach, Markus Zanker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many modern online services feature personalized recommendations. A central
challenge when providing such recommendations is that the reason why an
individual user accesses the service may change from visit to visit or even
during an ongoing usage session. To be effective, a recommender system should
therefore aim to take the users' probable intent of using the service at a
certain point in time into account. In recent years, researchers have thus
started to address this challenge by incorporating intent-awareness into
recommender systems. Correspondingly, a number of technical approaches were put
forward, including diversification techniques, intent prediction models or
latent intent modeling approaches. In this paper, we survey and categorize
existing approaches to building the next generation of Intent-Aware Recommender
Systems (IARS). Based on an analysis of current evaluation practices, we
outline open gaps and possible future directions in this area, which in
particular include the consideration of additional interaction signals and
contextual information to further improve the effectiveness of such systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DemoRank: Selecting Effective Demonstrations for <span class="highlight-title">Large Language Models</span>
  in Ranking Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Liu, Yutao Zhu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been increasing interest in applying large language
models (LLMs) as zero-shot passage rankers. However, few studies have explored
how to select appropriate in-context demonstrations for the passage ranking
task, which is the focus of this paper. Previous studies mainly apply a
demonstration retriever to retrieve demonstrations and use top-$k$
demonstrations for in-context learning (ICL). Although effective, this approach
overlooks the dependencies between demonstrations, leading to inferior
performance of few-shot ICL in the passage ranking task. In this paper, we
formulate the demonstration selection as a \textit{retrieve-then-rerank}
process and introduce the DemoRank framework. In this framework, we first use
LLM feedback to train a demonstration retriever and construct a novel
dependency-aware training samples to train a demonstration reranker to improve
few-shot ICL. The construction of such training samples not only considers
demonstration dependencies but also performs in an efficient way. Extensive
experiments demonstrate DemoRank's effectiveness in in-domain scenarios and
strong generalization to out-of-domain scenarios. Our codes are available
at~\url{https://github.com/8421BCD/DemoRank}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiased Recommendation with Noisy Feedback <span class="chip">KDD 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, Xiao-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ratings of a user to most items in recommender systems are usually missing
not at random (MNAR), largely because users are free to choose which items to
rate. To achieve unbiased learning of the prediction model under MNAR data,
three typical solutions have been proposed, including error-imputation-based
(EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods.
However, these methods ignore an alternative form of bias caused by the
inconsistency between the observed ratings and the users' true preferences,
also known as noisy feedback or outcome measurement errors (OME), e.g., due to
public opinion or low-quality data collection process. In this work, we study
intersectional threats to the unbiased learning of the prediction model from
data MNAR and OME in the collected data. First, we design OME-EIB, OME-IPS, and
OME-DR estimators, which largely extend the existing estimators to combat OME
in real-world recommendation scenarios. Next, we theoretically prove the
unbiasedness and generalization bound of the proposed estimators. We further
propose an alternate denoising training approach to achieve unbiased learning
of the prediction model under MNAR data with OME. Extensive experiments are
conducted on three real-world datasets and one semi-synthetic dataset to show
the effectiveness of our proposed approaches. The code is available at
https://github.com/haoxuanli-pku/KDD24-OME-DR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 24 Research Track Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEXTER: A Benchmark for open-domain Complex Question Answering using
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venktesh V. Deepali Prabhu, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain complex Question Answering (QA) is a difficult task with
challenges in evidence retrieval and reasoning. The complexity of such
questions could stem from questions being compositional, hybrid evidence, or
ambiguity in questions. While retrieval performance for classical QA tasks is
well explored, their capabilities for heterogeneous complex retrieval tasks,
especially in an open-domain setting, and the impact on downstream QA
performance, are relatively unexplored. To address this, in this work, we
propose a benchmark composing diverse complex QA tasks and provide a toolkit to
evaluate state-of-the-art pre-trained dense and sparse retrieval models in an
open-domain setting. We observe that late interaction models and surprisingly
lexical models like BM25 perform well compared to other pre-trained dense
retrieval models. In addition, since context-based reasoning is critical for
solving complex QA tasks, we also evaluate the reasoning capabilities of LLMs
and the impact of retrieval performance on their reasoning capabilities.
Through experiments, we observe that much progress is to be made in retrieval
for complex QA to improve downstream QA performance. Our software and related
data can be accessed at https://github.com/VenkteshV/DEXTER
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under submission, 22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make <span class="highlight-title">Large Language Model</span> a Better Ranker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenshuo Chao, Zhi Zheng, Hengshu Zhu, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate robust capabilities across various
fields, leading to a paradigm shift in LLM-enhanced Recommender System (RS).
Research to date focuses on point-wise and pair-wise recommendation paradigms,
which are inefficient for LLM-based recommenders due to high computational
costs. However, existing list-wise approaches also fall short in ranking tasks
due to misalignment between ranking objectives and next-token prediction.
Moreover, these LLM-based methods struggle to effectively address the order
relation among candidates, particularly given the scale of ratings. To address
these challenges, this paper introduces the large language model framework with
Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap
between the capabilities of LLMs and the nuanced requirements of ranking tasks.
Specifically, ALRO employs explicit feedback in a listwise manner by
introducing soft lambda loss, a customized adaptation of lambda loss designed
for optimizing order relations. This mechanism provides more accurate
optimization goals, enhancing the ranking process. Additionally, ALRO
incorporates a permutation-sensitive learning mechanism that addresses position
bias, a prevalent issue in generative models, without imposing additional
computational burdens during inference. Our evaluative studies reveal that ALRO
outperforms both existing embedding-based recommendation methods and LLM-based
recommendation baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Neural Topic Models: Methods, Applications, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobao Wu, Thong Nguyen, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic models have been prevalent for decades to discover latent topics and
infer topic proportions of documents in an unsupervised fashion. They have been
widely used in various applications like text analysis and context
recommendation. Recently, the rise of neural networks has facilitated the
emergence of a new research field -- Neural Topic Models (NTMs). Different from
conventional topic models, NTMs directly optimize parameters without requiring
model-specific derivations. This endows NTMs with better scalability and
flexibility, resulting in significant research attention and plentiful new
methods and applications. In this paper, we present a comprehensive survey on
neural topic models concerning methods, applications, and challenges.
Specifically, we systematically organize current NTM methods according to their
network structures and introduce the NTMs for various scenarios like short
texts and bilingual documents. We also discuss a wide range of popular
applications built on NTMs. Finally, we highlight the challenges confronted by
NTMs to inspire future research. We accompany this survey with a repository for
easier access to the mentioned paper resources:
https://github.com/bobxwu/Paper-Neural-Topic-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Artificial Intelligence Review. See
  https://doi.org/10.1007/s10462-023-10661-7 and a paper list at
  https://github.com/BobXWu/Paper-Neural-Topic-Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generation of Asset Administration Shell with <span class="highlight-title">Large Language Model</span>
  Agents: Toward Semantic Interoperability in Digital Twins in the Context of
  Industry 4.0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17209v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17209v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Xia, Zhewen Xiao, Nasser Jazdi, Michael Weyrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces a novel approach for achieving semantic
interoperability in digital twins and assisting the creation of Asset
Administration Shell (AAS) as digital twin model within the context of Industry
4.0. The foundational idea of our research is that the communication based on
semantics and the generation of meaningful textual data are directly linked,
and we posit that these processes are equivalent if the exchanged information
can be serialized in text form. Based on this, we construct a "semantic node"
data structure in our research to capture the semantic essence of textual data.
Then, a system powered by large language models is designed and implemented to
process the "semantic node" and generate standardized digital twin models from
raw textual data collected from datasheets describing technical assets. Our
evaluation demonstrates an effective generation rate of 62-79%, indicating a
substantial proportion of the information from the source text can be
translated error-free to the target digital twin instance model with the
generative capability of large language models. This result has a direct
application in the context of Industry 4.0, and the designed system is
implemented as a data model generation tool for reducing the manual effort in
creating AAS model. In our evaluation, a comparative analysis of different LLMs
and an in-depth ablation study of Retrieval-Augmented Generation (RAG)
mechanisms provide insights into the effectiveness of LLM systems for
interpreting technical concepts and translating data. Our findings emphasize
LLMs' capability to automate AAS instance creation and contribute to the
broader field of semantic interoperability for digital twins in industrial
applications. The prototype implementation and evaluation results are presented
on our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClickPrompt: CTR Models are Strong Prompt Generators for Adapting
  Language Models to CTR Prediction <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09234v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09234v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction has become increasingly indispensable for
various Internet applications. Traditional CTR models convert the multi-field
categorical data into ID features via one-hot encoding, and extract the
collaborative signals among features. Such a paradigm suffers from the problem
of semantic information loss. Another line of research explores the potential
of pretrained language models (PLMs) for CTR prediction by converting input
data into textual sentences through hard prompt templates. Although semantic
signals are preserved, they generally fail to capture the collaborative
information (e.g., feature interactions, pure ID features), not to mention the
unacceptable inference overhead brought by the huge model size. In this paper,
we aim to model both the semantic knowledge and collaborative knowledge for
accurate CTR estimation, and meanwhile address the inference inefficiency
issue. To benefit from both worlds and close their gaps, we propose a novel
model-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models
to generate interaction-aware soft prompts for PLMs. We design a
prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM
has to recover the masked tokens based on the language context, as well as the
soft prompts generated by CTR model. The collaborative and semantic knowledge
from ID and textual features would be explicitly aligned and interacted via the
prompt interface. Then, we can either tune the CTR model with PLM for superior
performance, or solely tune the CTR model without PLM for inference efficiency.
Experiments on four real-world datasets validate the effectiveness of
ClickPrompt compared with existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fundus: A Simple-to-Use News Scraper Optimized for High Quality
  Extractions <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Dallabetta, Conrad Dobberstein, Adrian Breiding, Alan Akbik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Fundus, a user-friendly news scraper that enables users
to obtain millions of high-quality news articles with just a few lines of code.
Unlike existing news scrapers, we use manually crafted, bespoke content
extractors that are specifically tailored to the formatting guidelines of each
supported online newspaper. This allows us to optimize our scraping for quality
such that retrieved news articles are textually complete and without HTML
artifacts. Further, our framework combines both crawling (retrieving HTML from
the web or large web archives) and content extraction into a single pipeline.
By providing a unified interface for a predefined collection of newspapers, we
aim to make Fundus broadly usable even for non-technical users. This paper
gives an overview of the framework, discusses our design choices, and presents
a comparative evaluation against other popular news scrapers. Our evaluation
shows that Fundus yields significantly higher quality extractions (complete and
artifact-free news articles) than prior work. The framework is available on
GitHub under https://github.com/flairNLP/fundus and can be simply installed
using pip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, ACL 2024, for a screencast see
  https://www.youtube.com/watch?v=9GJExMelhdI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous-time Autoencoders for Regular and Irregular Time Series
  Imputation <span class="chip">WSDM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16581v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16581v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyowon Wi, Yehjin Shin, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series imputation is one of the most fundamental tasks for time series.
Real-world time series datasets are frequently incomplete (or irregular with
missing observations), in which case imputation is strongly required. Many
different time series imputation methods have been proposed. Recent
self-attention-based methods show the state-of-the-art imputation performance.
However, it has been overlooked for a long time to design an imputation method
based on continuous-time recurrent neural networks (RNNs), i.e., neural
controlled differential equations (NCDEs). To this end, we redesign time series
(variational) autoencoders based on NCDEs. Our method, called continuous-time
autoencoder (CTA), encodes an input time series sample into a continuous hidden
path (rather than a hidden vector) and decodes it to reconstruct and impute the
input. In our experiments with 4 datasets and 19 baselines, our method shows
the best imputation performance in almost all cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a WSDM'24 full paper (oral presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReLLa: Retrieval-enhanced <span class="highlight-title">Large Language Models</span> for Lifelong Sequential
  Behavior Comprehension in Recommendation <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11131v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11131v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With large language models (LLMs) achieving remarkable breakthroughs in
natural language processing (NLP) domains, LLM-enhanced recommender systems
have received much attention and have been actively explored currently. In this
paper, we focus on adapting and empowering a pure large language model for
zero-shot and few-shot recommendation tasks. First and foremost, we identify
and formulate the lifelong sequential behavior incomprehension problem for LLMs
in recommendation domains, i.e., LLMs fail to extract useful information from a
textual context of long user behavior sequence, even if the length of context
is far from reaching the context limitation of LLMs. To address such an issue
and improve the recommendation performance of LLMs, we propose a novel
framework, namely Retrieval-enhanced Large Language models (ReLLa) for
recommendation tasks in both zero-shot and few-shot settings. For zero-shot
recommendation, we perform semantic user behavior retrieval (SUBR) to improve
the data quality of testing samples, which greatly reduces the difficulty for
LLMs to extract the essential knowledge from user behavior sequences. As for
few-shot recommendation, we further design retrieval-enhanced instruction
tuning (ReiT) by adopting SUBR as a data augmentation technique for training
samples. Specifically, we develop a mixed training dataset consisting of both
the original data samples and their retrieval-enhanced counterparts. We conduct
extensive experiments on three real-world public datasets to demonstrate the
superiority of ReLLa compared with existing baseline models, as well as its
capability for lifelong sequential behavior comprehension. To be highlighted,
with only less than 10% training samples, few-shot ReLLa can outperform
traditional CTR models that are trained on the entire training set (e.g.,
DCNv2, DIN, SIM). The code is available
\url{https://github.com/LaVieEnRose365/ReLLa}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2024. Full and More Readable Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language
  Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07269v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07269v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy
issues, which means they are unaware of unseen events or generate text with
incorrect facts owing to outdated/noisy data. To this end, many knowledge
editing approaches for LLMs have emerged -- aiming to subtly inject/edit
updated knowledge or adjust undesired behavior while minimizing the impact on
unrelated inputs. Nevertheless, due to significant differences among various
knowledge editing methods and the variations in task setups, there is no
standard implementation framework available for the community, which hinders
practitioners from applying knowledge editing to applications. To address these
issues, we propose EasyEdit, an easy-to-use knowledge editing framework for
LLMs. It supports various cutting-edge knowledge editing approaches and can be
readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.
Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,
demonstrating that knowledge editing surpasses traditional fine-tuning in terms
of reliability and generalization. We have released the source code on GitHub,
along with Google Colab tutorials and comprehensive documentation for beginners
to get started. Besides, we present an online system for real-time knowledge
editing, and a demo video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 System Demonstrations; Code:
  https://github.com/zjunlp/EasyEdit HF Demo:
  https://huggingface.co/spaces/zjunlp/EasyEdit Video:
  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03049v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03049v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with an online demo app and a demo video for quick-start, calling for
broader research centered on instruction data and synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 System Demonstrations; Project website:
  https://zjunlp.github.io/project/EasyInstruct Code:
  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo
  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VeraCT Scan: <span class="highlight-title">Retrieval-Augmented</span> Fake News Detection with Justifiable
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Niu, Yang Guan, Yuanhao Wu, Juno Zhu, Juntong Song, Randy Zhong, Kaihua Zhu, Siliang Xu, Shizhe Diao, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of fake news poses a significant threat not only by
disseminating misleading information but also by undermining the very
foundations of democracy. The recent advance of generative artificial
intelligence has further exacerbated the challenge of distinguishing genuine
news from fabricated stories. In response to this challenge, we introduce
VeraCT Scan, a novel retrieval-augmented system for fake news detection. This
system operates by extracting the core facts from a given piece of news and
subsequently conducting an internet-wide search to identify corroborating or
conflicting reports. Then sources' credibility is leveraged for information
verification. Besides determining the veracity of news, we also provide
transparent evidence and reasoning to support its conclusions, resulting in the
interpretability and trust in the results. In addition to GPT-4 Turbo, Llama-2
13B is also fine-tuned for news content understanding, information
verification, and reasoning. Both implementations have demonstrated
state-of-the-art accuracy in the realm of fake news detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, Ming-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image retrieval, i.e., finding desired images given a reference image,
inherently encompasses rich, multi-faceted search intents that are difficult to
capture solely using image-based measures. Recent works leverage text
instructions to allow users to more freely express their search intents.
However, they primarily focus on image pairs that are visually similar and/or
can be characterized by a small set of pre-defined relations. The core thesis
of this paper is that text instructions can enable retrieving images with
richer relations beyond visual similarity. To show this, we introduce
MagicLens, a series of self-supervised image retrieval models that support
open-ended instructions. MagicLens is built on a key novel insight: image pairs
that naturally occur on the same web pages contain a wide range of implicit
relations (e.g., inside view of), and we can bring those implicit relations
explicit by synthesizing instructions via foundation models. Trained on 36.7M
(query image, instruction, target image) triplets with rich semantic relations
mined from the web, MagicLens achieves results comparable with or better than
prior best on eight benchmarks of various image retrieval tasks, while
maintaining high parameter efficiency with a significantly smaller model size.
Additional human analyses on a 1.4M-image unseen corpus further demonstrate the
diversity of search intents supported by MagicLens. Code and models are
publicly available at https://open-vision-language.github.io/MagicLens/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024 (Oral); Project Website:
  https://open-vision-language.github.io/MagicLens/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-23T00:00:00Z">2024-06-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference Tuning For Toxicity Mitigation Generalizes Across Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Li, Zheng-Xin Yong, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detoxifying multilingual Large Language Models (LLMs) has become crucial due
to their increasing global use. In this work, we explore zero-shot
cross-lingual generalization of preference tuning in detoxifying LLMs. Unlike
previous studies that show limited cross-lingual generalization for other
safety tasks, we demonstrate that Direct Preference Optimization (DPO) training
with only English data can significantly reduce toxicity in multilingual
open-ended generations. For example, the probability of mGPT-1.3B generating
toxic continuations drops from 46.8% to 3.9% across 17 different languages
after training. Our results also extend to other multilingual LLMs, such as
BLOOM, Llama3, and Aya-23. Using mechanistic interpretability tools like causal
intervention and activation analysis, we identified the dual multilinguality
property of MLP layers in LLMs, which explains the cross-lingual generalization
of DPO. Finally, we show that bilingual sentence retrieval can predict the
cross-lingual transferability of DPO preference tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Linguistic Control of <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dang Nguyen, Jiuhai Chen, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), despite their breakthroughs on many challenging
benchmark tasks, lean to generate verbose responses and lack the
controllability of output complexity, which is usually preferred by human users
in practice. In this paper, we study how to precisely control multiple
linguistic complexities of LLM output by finetuning using off-the-shelf data.
To this end, we propose multi-control tuning (MCTune), which includes multiple
linguistic complexity values of ground-truth responses as controls in the input
for instruction tuning. We finetune LLaMA2-7B on Alpaca-GPT4 and WizardLM
datasets. Evaluations on widely used benchmarks demonstrate that our method
does not only improve LLMs' multi-complexity controllability substantially but
also retains or even enhances the quality of the responses as a side benefit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Output Personality Detection Models via Mixed Strategy
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rong Wang, Kun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traditional personality models only yield binary results. This paper
presents a novel approach for training personality detection models that
produce continuous output values, using mixed strategies. By leveraging the
PANDORA dataset, which includes extensive personality labeling of Reddit
comments, we developed models that predict the Big Five personality traits with
high accuracy. Our approach involves fine-tuning a RoBERTa-base model with
various strategies such as Multi-Layer Perceptron (MLP) integration, and
hyperparameter tuning. The results demonstrate that our models significantly
outperform traditional binary classification methods, offering precise
continuous outputs for personality traits, thus enhancing applications in AI,
psychology, human resources, marketing and health care fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>s' Classification Performance is Overclaimed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzi Xu, Renze Lou, Jiangshu Du, Vahid Mahzoon, Elmira Talebianaraki, Zhuoan Zhou, Elizabeth Garrison, Slobodan Vucetic, Wenpeng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many classification tasks designed for AI or human to solve, gold labels
are typically included within the label space by default, often posed as "which
of the following is correct?" This standard setup has traditionally highlighted
the strong performance of advanced AI, particularly top-performing Large
Language Models (LLMs), in routine classification tasks. However, when the gold
label is intentionally excluded from the label space, it becomes evident that
LLMs still attempt to select from the available label candidates, even when
none are correct. This raises a pivotal question: Do LLMs truly demonstrate
their intelligence in understanding the essence of classification tasks?
  In this study, we evaluate both closed-source and open-source LLMs across
representative classification tasks, arguing that the perceived performance of
LLMs is overstated due to their inability to exhibit the expected comprehension
of the task. This paper makes a threefold contribution: i) To our knowledge,
this is the first work to identify the limitations of LLMs in classification
tasks when gold labels are absent. We define this task as Classify-w/o-Gold and
propose it as a new testbed for LLMs. ii) We introduce a benchmark, Know-No,
comprising two existing classification tasks and one new task, to evaluate
Classify-w/o-Gold. iii) This work defines and advocates for a new evaluation
metric, OmniAccuracy, which assesses LLMs' performance in classification tasks
both when gold labels are present and absent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blind Baselines Beat Membership Inference Attacks for Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debeshee Das, Jie Zhang, Florian Tramèr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference (MI) attacks try to determine if a data sample was used
to train a machine learning model. For foundation models trained on unknown Web
data, MI attacks can be used to detect copyrighted training materials, measure
test set contamination, or audit machine unlearning. Unfortunately, we find
that evaluations of MI attacks for foundation models are flawed, because they
sample members and non-members from different distributions. For 8 published MI
evaluation datasets, we show that blind attacks -- that distinguish the member
and non-member distributions without looking at any trained model -- outperform
state-of-the-art MI attacks. Existing evaluations thus tell us nothing about
membership leakage of a foundation model's training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphEval2000: Benchmarking and Improving <span class="highlight-title">Large Language Models</span> on Graph
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Wu, Zichen Chen, Will Corcoran, Misha Sra, Ambuj K. Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success in natural
language processing (NLP), demonstrating significant capabilities in processing
and understanding text data. However, recent studies have identified
limitations in LLMs' ability to reason about graph-structured data. To address
this gap, we introduce GraphEval2000, the first comprehensive graph dataset,
comprising 40 graph data structure problems along with 2000 test cases.
Additionally, we introduce an evaluation framework based on GraphEval2000,
designed to assess the graph reasoning abilities of LLMs through coding
challenges. Our dataset categorizes test cases into four primary and four
sub-categories, ensuring a comprehensive evaluation. We evaluate eight popular
LLMs on GraphEval2000, revealing that LLMs exhibit a better understanding of
directed graphs compared to undirected ones. While private LLMs consistently
outperform open-source models, the performance gap is narrowing. Furthermore,
to improve the usability of our evaluation framework, we propose Structured
Symbolic Decomposition (SSD), an instruction-based method designed to enhance
LLM performance on GraphEval2000. Results show that SSD improves the
performance of GPT-3.5, GPT-4, and GPT-4o on complex graph problems, with an
increase of 11.11\%, 33.37\%, and 33.37\%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPs 2024 Dataset and Benchmark track, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FS-<span class="highlight-title">RAG</span>: A Frame Semantics Based Approach for Improved Factual Accuracy
  in <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harish Tayyar Madabushi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel extension to Retrieval Augmented Generation with the goal
of mitigating factual inaccuracies in the output of large language models.
Specifically, our method draws on the cognitive linguistic theory of frame
semantics for the indexing and retrieval of factual information relevant to
helping large language models answer queries. We conduct experiments to
demonstrate the effectiveness of this method both in terms of retrieval
effectiveness and in terms of the relevance of the frames and frame relations
automatically generated. Our results show that this novel mechanism of Frame
Semantic-based retrieval, designed to improve Retrieval Augmented Generation
(FS-RAG), is effective and offers potential for providing data-driven insights
into frame semantics theory. We provide open access to our program code and
prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>program code and prompts available at
  https://github.com/H-TayyarMadabushi/A-Frame-Semantics-based-approach-for-Improved-Factual-Accuracy-in-Large-Language-Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Region-aware Bias Evaluation Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angana Borah, Aparna Garimella, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When exposed to human-generated data, language models are known to learn and
amplify societal biases. While previous works introduced benchmarks that can be
used to assess the bias in these models, they rely on assumptions that may not
be universally true. For instance, a gender bias dimension commonly used by
these metrics is that of family--career, but this may not be the only common
bias in certain regions of the world. In this paper, we identify topical
differences in gender bias across different regions and propose a region-aware
bottom-up approach for bias assessment. Our proposed approach uses
gender-aligned topics for a given region and identifies gender bias dimensions
in the form of topic pairs that are likely to capture gender societal biases.
Several of our proposed bias topic pairs are on par with human perception of
gender biases in these regions in comparison to the existing ones, and we also
identify new pairs that are more aligned than the existing ones. In addition,
we use our region-aware bias topic pairs in a Word Embedding Association Test
(WEAT)-based evaluation metric to test for gender biases across different
regions in different data domains. We also find that LLMs have a higher
alignment to bias pairs for highly-represented regions showing the importance
of region-aware bias evaluation metric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research found the issue of Early Answering in large language models
(LLMs), where the models already have an answer before generating the
Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary
dependency between the predicted answer and the reasoning process.
Consequently, two important questions arise: (1) Is CoT still necessary if the
model already has an answer? (2) Can the correctness of the answer serve as
valid evidence for the correctness of CoT? To address these questions, we
propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind
during the model's reasoning. The probing results show that in a significant
number of question-answer cases, CoT appears to be unnecessary, and this
necessity correlates with the simplicity of the task, defined by reasoning
steps required. Furthermore, by analyzing patterns in mind change, we examine
the correctness of the model's reasoning. Our validation reveals that many
responses, although correct in their final answer, contain errors in their
reasoning process. To this end, we propose a strategic approach based on CoP to
prioritize answers with correct reasoning among multiple candidates, thereby
bolstering the reliability of the model's reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crosslingual Capabilities and Knowledge Barriers in Multilingual Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chulin Xie, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are typically multilingual due to pretraining on
diverse multilingual corpora. But can these models relate corresponding
concepts across languages, effectively being crosslingual? This study evaluates
six state-of-the-art LLMs on inherently crosslingual tasks. We observe that
while these models show promising surface-level crosslingual abilities on
machine translation and embedding space analyses, they struggle with deeper
crosslingual knowledge transfer, revealing a crosslingual knowledge barrier in
both general (MMLU benchmark) and domain-specific (Harry Potter quiz) contexts.
We observe that simple inference-time mitigation methods offer only limited
improvement. On the other hand, we propose fine-tuning of LLMs on
mixed-language data, which effectively reduces these gaps, even when using
out-of-domain datasets like WikiText. Our findings suggest the need for
explicit optimization to unlock the full crosslingual potential of LLMs. Our
code is publicly available at
https://github.com/google-research/crosslingual-knowledge-barriers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Use Your INSTINCT: INSTruction optimization for <span class="highlight-title">LLM</span>s usIng Neural
  bandits Coupled with Transformers <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable instruction-following
capabilities and achieved impressive performances in various applications.
However, the performances of LLMs depend heavily on the instructions given to
them, which are typically manually tuned with substantial human efforts. Recent
work has used the query-efficient Bayesian optimization (BO) algorithm to
automatically optimize the instructions given to black-box LLMs. However, BO
usually falls short when optimizing highly sophisticated (e.g.,
high-dimensional) objective functions, such as the functions mapping an
instruction to the performance of an LLM. This is mainly due to the limited
expressive power of the Gaussian process (GP) which is used by BO as a
surrogate to model the objective function. Meanwhile, it has been repeatedly
shown that neural networks (NNs), especially pre-trained transformers, possess
strong expressive power and can model highly complex functions. So, we adopt a
neural bandit algorithm which replaces the GP in BO by an NN surrogate to
optimize instructions for black-box LLMs. More importantly, the neural bandit
algorithm allows us to naturally couple the NN surrogate with the hidden
representation learned by a pre-trained transformer (i.e., an open-source LLM),
which significantly boosts its performance. These motivate us to propose our
INSTruction optimization usIng Neural bandits Coupled with Transformers
(INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use
extensive experiments to show that INSTINCT consistently outperforms baselines
in different tasks, e.g., various instruction induction tasks and the task of
improving zero-shot chain-of-thought instructions. Our code is available at
https://github.com/xqlin98/INSTINCT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I2EDL: Interactive Instruction Error Detection and Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Yiming Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Vision-and-Language Navigation in Continuous Environments (VLN-CE)
task, the human user guides an autonomous agent to reach a target goal via a
series of low-level actions following a textual instruction in natural
language. However, most existing methods do not address the likely case where
users may make mistakes when providing such instruction (e.g. "turn left"
instead of "turn right"). In this work, we address a novel task of Interactive
VLN in Continuous Environments (IVLN-CE), which allows the agent to interact
with the user during the VLN-CE navigation to verify any doubts regarding the
instruction errors. We propose an Interactive Instruction Error Detector and
Localizer (I2EDL) that triggers the user-agent interaction upon the detection
of instruction errors during the navigation. We leverage a pre-trained module
to detect instruction errors and pinpoint them in the instruction by
cross-referencing the textual input and past observations. In such way, the
agent is able to query the user for a timely correction, without demanding the
user's cognitive load, as we locate the probable errors to a precise part of
the instruction. We evaluate the proposed I2EDL on a dataset of instructions
containing errors, and further devise a novel metric, the Success weighted by
Interaction Number (SIN), to reflect both the navigation performance and the
interaction effectiveness. We show how the proposed method can ask focused
requests for corrections to the user, which in turn increases the navigation
success, while minimizing the interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE RO-MAN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Models</span> for Data Annotation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data annotation generally refers to the labeling or generating of raw data
with relevant information, which could be used for improving the efficacy of
machine learning models. The process, however, is labor-intensive and costly.
The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4,
presents an unprecedented opportunity to automate the complicated process of
data annotation. While existing surveys have extensively covered LLM
architecture, training, and general applications, we uniquely focus on their
specific utility for data annotation. This survey contributes to three core
aspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment,
and LLM-Generated Annotations Utilization. Furthermore, this survey includes an
in-depth taxonomy of data types that LLMs can annotate, a comprehensive review
of learning strategies for models utilizing LLM-generated annotations, and a
detailed discussion of the primary challenges and limitations associated with
using LLMs for data annotation. Serving as a key guide, this survey aims to
assist researchers and practitioners in exploring the potential of the latest
LLMs for data annotation, thereby fostering future advancements in this
critical field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open Models, Closed Minds? On Agents Capabilities in Mimicking Human
  Personalities through Open <span class="highlight-title">Large Language Models</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucio La Cava, Andrea Tagarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of unveiling human-like behaviors in Large Language Models
(LLMs) has led to a closer connection between NLP and human psychology.
Scholars have been studying the inherent personalities exhibited by LLMs and
attempting to incorporate human traits and behaviors into them. However, these
efforts have primarily focused on commercially-licensed LLMs, neglecting the
widespread use and notable advancements seen in Open LLMs. This work aims to
address this gap by employing a set of 12 LLM Agents based on the most
representative Open models and subject them to a series of assessments
concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five
Inventory (BFI) test. Our approach involves evaluating the intrinsic
personality traits of Open LLM agents and determining the extent to which these
agents can mimic human personalities when conditioned by specific personalities
and roles. Our findings unveil that $(i)$ each Open LLM agent showcases
distinct human personalities; $(ii)$ personality-conditioned prompting produces
varying effects on the agents, with only few successfully mirroring the imposed
personality, while most of them being ``closed-minded'' (i.e., they retain
their intrinsic traits); and $(iii)$ combining role and personality
conditioning can enhance the agents' ability to mimic human personalities. Our
work represents a step up in understanding the dense relationship between NLP
and human psychology through the lens of Open LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Enhanced methodology and evaluation based on BFI in addition to MBTI,
  with expanded set of LLM agents. Author list changed w.r.t. the previous
  version (v1), see Acknowledgements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Models</span> Can Self-Correct with Minimal Effort 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intrinsic self-correct was a method that instructed large language models
(LLMs) to verify and correct their responses without external feedback.
Unfortunately, the study concluded that the LLMs could not self-correct
reasoning yet. We find that a simple yet effective verification method can
unleash inherent capabilities of the LLMs. That is to mask a key condition in
the question, add the current response to construct a verification question,
and predict the condition to verify the response. The condition can be an
entity in an open-domain question or a numeric value in a math question, which
requires minimal effort (via prompting) to identify. We propose an iterative
verify-then-correct framework to progressively identify and correct (probably)
false responses, named ProCo. We conduct experiments on three reasoning tasks.
On average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields $+6.8$ exact
match on four open-domain question answering datasets, $+14.1$ accuracy on
three arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense
reasoning dataset, compared to Self-Correct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s in the Loop: Leve<span class="highlight-title">rag</span>ing <span class="highlight-title">Large Language Model</span> Annotations for Active
  Learning in Low-Resource Languages <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah Gumus, Michael Granitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource languages face significant barriers in AI development due to
limited linguistic resources and expertise for data labeling, rendering them
rare and costly. The scarcity of data and the absence of preexisting tools
exacerbate these challenges, especially since these languages may not be
adequately represented in various NLP datasets. To address this gap, we propose
leveraging the potential of LLMs in the active learning loop for data
annotation. Initially, we conduct evaluations to assess inter-annotator
agreement and consistency, facilitating the selection of a suitable LLM
annotator. The chosen annotator is then integrated into a training loop for a
classifier using an active learning paradigm, minimizing the amount of queried
data required. Empirical evaluations, notably employing GPT-4-Turbo,
demonstrate near-state-of-the-art performance with significantly reduced data
requirements, as indicated by estimated potential cost savings of at least
42.45 times compared to human annotation. Our proposed solution shows promising
potential to substantially reduce both the monetary and computational costs
associated with automation in low-resource settings. By bridging the gap
between low-resource languages and AI, this approach fosters broader inclusion
and shows the potential to enable automation across diverse linguistic
landscapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 tables. The source code related to this paper is
  available at https://github.com/mkandai/llms-in-the-loop. This paper has been
  accepted for publication at ECML PKDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference-Time Decontamination: Reusing Leaked Benchmarks for Large
  Language Model Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Zhu, Qingyuan Cheng, Runyu Peng, Xiaonan Li, Tengxiao Liu, Ru Peng, Xipeng Qiu, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training process of large language models (LLMs) often involves varying
degrees of test data contamination. Although current LLMs are achieving
increasingly better performance on various benchmarks, their performance in
practical applications does not always match their benchmark results. Leakage
of benchmarks can prevent the accurate assessment of LLMs' true performance.
However, constructing new benchmarks is costly, labor-intensive and still
carries the risk of leakage. Therefore, in this paper, we ask the question, Can
we reuse these leaked benchmarks for LLM evaluation? We propose Inference-Time
Decontamination (ITD) to address this issue by detecting and rewriting leaked
samples without altering their difficulties. ITD can mitigate performance
inflation caused by memorizing leaked benchmarks. Our proof-of-concept
experiments demonstrate that ITD reduces inflated accuracy by 22.9% on GSM8K
and 19.0% on MMLU. On MMLU, using Inference-time Decontamination can lead to a
decrease in the results of Phi3 and Mistral by 6.7% and 3.6% respectively. We
hope that ITD can provide more truthful evaluation results for large language
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMLU-Pro: A More Robust and Challenging Multi-Task Language
  Understanding Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01574v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01574v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of large-scale language models, benchmarks like the Massive
Multitask Language Understanding (MMLU) have been pivotal in pushing the
boundaries of what AI can achieve in language comprehension and reasoning
across diverse domains. However, as models continue to improve, their
performance on these benchmarks has begun to plateau, making it increasingly
difficult to discern differences in model capabilities. This paper introduces
MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven
MMLU benchmark by integrating more challenging, reasoning-focused questions and
expanding the choice set from four to ten options. Additionally, MMLU-Pro
eliminates the trivial and noisy questions in MMLU. Our experimental results
show that MMLU-Pro not only raises the challenge, causing a significant drop in
accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability
under varying prompts. With 24 different prompt styles tested, the sensitivity
of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in
MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)
reasoning achieved better performance on MMLU-Pro compared to direct answering,
which is in stark contrast to the findings on the original MMLU, indicating
that MMLU-Pro includes more complex reasoning questions. Our assessments
confirm that MMLU-Pro is a more discriminative benchmark to better track
progress in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extending Token Computation for <span class="highlight-title">LLM</span> Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14932v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14932v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingli Liao, Danilo Vasconcellos Vargas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are pivotal in advancing natural language
processing but often struggle with complex reasoning tasks due to inefficient
attention distributions. In this paper, we explore the effect of increased
computed tokens on LLM performance and introduce a novel method for extending
computed tokens in the Chain-of-Thought (CoT) process, utilizing attention
mechanism optimization. By fine-tuning an LLM on a domain-specific, highly
structured dataset, we analyze attention patterns across layers, identifying
inefficiencies caused by non-semantic tokens with outlier high attention
scores. To address this, we propose an algorithm that emulates early layer
attention patterns across downstream layers to re-balance skewed attention
distributions and enhance knowledge abstraction. Our findings demonstrate that
our approach not only facilitates a deeper understanding of the internal
dynamics of LLMs but also significantly improves their reasoning capabilities,
particularly in non-STEM domains. Our study lays the groundwork for further
innovations in LLM design, aiming to create more powerful, versatile, and
responsible models capable of tackling a broad range of real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mechanism for Optimizing Media Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian McFadden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A mechanism is described that addresses the fundamental trade off between
media producers who want to increase reach and consumers who provide attention
based on the rate of utility received, and where overreach negatively impacts
that rate. An optimal solution can be achieved when the media source considers
the impact of overreach in a cost function used in determining the optimal
distribution of content to maximize individual consumer utility and
participation. The result is a Nash equilibrium between producer and consumer
that is also Pareto efficient. Comparison with the literature on Recommender
systems highlights the advantages of the mechanism.The review suggests
advancements over that literature including identifying an optimal content
volume for the consumer and improvements for handling multiple objectives A
practical algorithm to generate the optimal distribution for each consumer is
provided.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main Paper: 20 pages, Appendix with proofs and additional material:
  26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimCE: Simplifying Cross-Entropy Loss for Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Yang, Huiyuan Chen, Yuchen Yan, Yuxin Tang, Yuying Zhao, Eric Xu, Yiwei Cai, Hanghang Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The learning objective is integral to collaborative filtering systems, where
the Bayesian Personalized Ranking (BPR) loss is widely used for learning
informative backbones. However, BPR often experiences slow convergence and
suboptimal local optima, partially because it only considers one negative item
for each positive item, neglecting the potential impacts of other unobserved
items. To address this issue, the recently proposed Sampled Softmax
Cross-Entropy (SSM) compares one positive sample with multiple negative
samples, leading to better performance. Our comprehensive experiments confirm
that recommender systems consistently benefit from multiple negative samples
during training. Furthermore, we introduce a \underline{Sim}plified Sampled
Softmax \underline{C}ross-\underline{E}ntropy Loss (SimCE), which simplifies
the SSM using its upper bound. Our validation on 12 benchmark datasets, using
both MF and LightGCN backbones, shows that SimCE significantly outperforms both
BPR and SSM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Ensemble Methods for News Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Gray, Noorhan Abbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  News recommendation is crucial for facilitating individuals' access to
articles, particularly amid the increasingly digital landscape of news
consumption. Consequently, extensive research is dedicated to News Recommender
Systems (NRS) with increasingly sophisticated algorithms. Despite this
sustained scholarly inquiry, there exists a notable research gap regarding the
potential synergy achievable by amalgamating these algorithms to yield superior
outcomes. This paper endeavours to address this gap by demonstrating how
ensemble methods can be used to combine many diverse state-of-the-art
algorithms to achieve superior results on the Microsoft News dataset (MIND).
Additionally, we identify scenarios where ensemble methods fail to improve
results and offer explanations for this occurrence. Our findings demonstrate
that a combination of NRS algorithms can outperform individual algorithms,
provided that the base learners are sufficiently diverse, with improvements of
up to 5\% observed for an ensemble consisting of a content-based BERT approach
and the collaborative filtering LSTUR algorithm. Additionally, our results
demonstrate the absence of any improvement when combining insufficiently
distinct methods. These findings provide insight into successful approaches of
ensemble methods in NRS and advocates for the development of better systems
through appropriate ensemble solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating D-MERIT of Partial-annotation on Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Royi Rassin, Yaron Fairstein, Oren Kalinsky, Guy Kushilevitz, Nachshon Cohen, Alexander Libov, Yoav Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval models are often evaluated on partially-annotated datasets. Each
query is mapped to a few relevant texts and the remaining corpus is assumed to
be irrelevant. As a result, models that successfully retrieve false negatives
are punished in evaluation. Unfortunately, completely annotating all texts for
every query is not resource efficient. In this work, we show that using
partially-annotated datasets in evaluation can paint a distorted picture. We
curate D-MERIT, a passage retrieval evaluation set from Wikipedia, aspiring to
contain all relevant passages for each query. Queries describe a group (e.g.,
``journals about linguistics'') and relevant passages are evidence that
entities belong to the group (e.g., a passage indicating that Language is a
journal about linguistics). We show that evaluating on a dataset containing
annotations for only a subset of the relevant passages might result in
misleading ranking of the retrieval systems and that as more relevant texts are
included in the evaluation set, the rankings converge. We propose our dataset
as a resource for evaluation and our study as a recommendation for balance
between resource-efficiency and reliable evaluation when annotating evaluation
sets for text retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our dataset can be downloaded from https://D-MERIT.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Database-Augmented Query Representation for Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval models that aim to search for the documents relevant to
the given query have shown many successes, which have been applied to diverse
tasks. However, the query provided by the user is oftentimes very short, which
challenges the retrievers to correctly fetch relevant documents. To tackle
this, existing studies have proposed expanding the query with a couple of
additional (user-related) features related to the query. Yet, they may be
suboptimal to effectively augment the query, though there is plenty of
information available to augment it in a relational database. Motivated by
this, we present a novel retrieval framework called Database-Augmented Query
representation (DAQu), which augments the original query with various
(query-related) metadata across multiple tables. In addition, as the number of
features in the metadata can be very large and there is no order among them, we
encode them with our graph-based set encoding strategy, which considers
hierarchies of features in the database without order. We validate DAQu in
diverse retrieval scenarios that can incorporate metadata from the relational
database, demonstrating that ours significantly enhances overall retrieval
performance, compared to existing query augmentation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning k-Determinantal Point Processes for Personalized Ranking <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuli Liu, Christian Walder, Lexing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key to personalized recommendation is to predict a personalized ranking
on a catalog of items by modeling the user's preferences. There are many
personalized ranking approaches for item recommendation from implicit feedback
like Bayesian Personalized Ranking (BPR) and listwise ranking. Despite these
methods have shown performance benefits, there are still limitations affecting
recommendation performance. First, none of them directly optimize ranking of
sets, causing inadequate exploitation of correlations among multiple items.
Second, the diversity aspect of recommendations is insufficiently addressed
compared to relevance.
  In this work, we present a new optimization criterion LkP based on set
probability comparison for personalized ranking that moves beyond traditional
ranking-based methods. It formalizes set-level relevance and diversity ranking
comparisons through a Determinantal Point Process (DPP) kernel decomposition.
To confer ranking interpretability to the DPP set probabilities and prioritize
the practicality of LkP, we condition the standard DPP on the cardinality k of
the DPP-distributed set, known as k-DPP, a less-explored extension of DPP. The
generic stochastic gradient descent based technique can be directly applied to
optimizing models that employ LkP. We implement LkP in the context of both
Matrix Factorization (MF) and neural networks approaches, on three real-world
datasets, obtaining improved relevance and diversity performances. LkP is
broadly applicable, and when applied to existing recommendation models it also
yields strong performance improvements, suggesting that LkP holds significant
value to the field of recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, accepted at ICDE 2024 (40th IEEE International Conference
  on Data Engineering)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Margin Loss: Proposal and Application in Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makbule Gulcin Ozsoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems guide users through vast amounts of information by
suggesting items based on their predicted preferences. Collaborative
filtering-based deep learning techniques have regained popularity due to their
simplicity, using only user-item interactions. Typically, these systems consist
of three main components: an interaction module, a loss function, and a
negative sampling strategy. Initially, researchers focused on enhancing
performance by developing complex interaction modules with techniques like
multi-layer perceptrons, transformers, or graph neural networks. However, there
has been a recent shift toward refining loss functions and negative sampling
strategies. This shift has increased interest in contrastive learning, which
pulls similar pairs closer while pushing dissimilar ones apart. Contrastive
learning involves key practices such as heavy data augmentation, large batch
sizes, and hard-negative sampling, but these also bring challenges like high
memory demands and under-utilization of some negative samples. The proposed
Multi-Margin Loss (MML) addresses these challenges by introducing multiple
margins and varying weights for negative samples. MML efficiently utilizes not
only the hardest negatives but also other non-trivial negatives, offering a
simpler yet effective loss function that outperforms more complex methods,
especially when resources are limited. Experiments on two well-known datasets
showed MML achieved up to a 20\% performance improvement compared to a baseline
contrastive loss function with fewer negative samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s in the Loop: Leve<span class="highlight-title">rag</span>ing <span class="highlight-title">Large Language Model</span> Annotations for Active
  Learning in Low-Resource Languages <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02261v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02261v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah Gumus, Michael Granitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource languages face significant barriers in AI development due to
limited linguistic resources and expertise for data labeling, rendering them
rare and costly. The scarcity of data and the absence of preexisting tools
exacerbate these challenges, especially since these languages may not be
adequately represented in various NLP datasets. To address this gap, we propose
leveraging the potential of LLMs in the active learning loop for data
annotation. Initially, we conduct evaluations to assess inter-annotator
agreement and consistency, facilitating the selection of a suitable LLM
annotator. The chosen annotator is then integrated into a training loop for a
classifier using an active learning paradigm, minimizing the amount of queried
data required. Empirical evaluations, notably employing GPT-4-Turbo,
demonstrate near-state-of-the-art performance with significantly reduced data
requirements, as indicated by estimated potential cost savings of at least
42.45 times compared to human annotation. Our proposed solution shows promising
potential to substantially reduce both the monetary and computational costs
associated with automation in low-resource settings. By bridging the gap
between low-resource languages and AI, this approach fosters broader inclusion
and shows the potential to enable automation across diverse linguistic
landscapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 tables. The source code related to this paper is
  available at https://github.com/mkandai/llms-in-the-loop. This paper has been
  accepted for publication at ECML PKDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiral of Silence: How is <span class="highlight-title">Large Language Model</span> Killing Information
  Retrieval? -- A Case Study on Open Domain Question Answering <span class="chip">ACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10496v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10496v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Chen, Ben He, Hongyu Lin, Xianpei Han, Tianshu Wang, Boxi Cao, Le Sun, Yingfei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The practice of Retrieval-Augmented Generation (RAG), which integrates Large
Language Models (LLMs) with retrieval systems, has become increasingly
prevalent. However, the repercussions of LLM-derived content infiltrating the
web and influencing the retrieval-generation feedback loop are largely
uncharted territories. In this study, we construct and iteratively run a
simulation pipeline to deeply investigate the short-term and long-term effects
of LLM text on RAG systems. Taking the trending Open Domain Question Answering
(ODQA) task as a point of entry, our findings reveal a potential digital
"Spiral of Silence" effect, with LLM-generated text consistently outperforming
human-authored content in search rankings, thereby diminishing the presence and
impact of human contributions online. This trend risks creating an imbalanced
information ecosystem, where the unchecked proliferation of erroneous
LLM-generated content may result in the marginalization of accurate
information. We urge the academic community to take heed of this potential
issue, ensuring a diverse and authentic digital information landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Async Learned User Embeddings for Ads Delivery Optimization <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05898v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05898v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingwei Tang, Meng Liu, Hong Li, Junjie Yang, Chenglin Wei, Boyang Li, Dai Li, Rengan Xu, Yifan Xu, Zehua Zhang, Xiangyu Wang, Linfeng Liu, Yuelei Xie, Chengye Liu, Labib Fawaz, Li Li, Hongnan Wang, Bill Zhu, Sri Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, high-quality user embeddings can capture subtle
preferences, enable precise similarity calculations, and adapt to changing
preferences over time to maintain relevance. The effectiveness of
recommendation systems depends on the quality of user embedding. We propose to
asynchronously learn high fidelity user embeddings for billions of users each
day from sequence based multimodal user activities through a Transformer-like
large scale feature learning module. The async learned user representations
embeddings (ALURE) are further converted to user similarity graphs through
graph learning and then combined with user realtime activities to retrieval
highly related ads candidates for the ads delivery system. Our method shows
significant gains in both offline and online experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by workshop on Multimodal Representation and Retrieval at
  SIGIR 2024, Washington DC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Collaborative Filtering: A Relook at Task Formulation in
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RecSys) have become indispensable in numerous
applications, profoundly influencing our everyday experiences. Despite their
practical significance, academic research in RecSys often abstracts the
formulation of research tasks from real-world contexts, aiming for a clean
problem formulation and more generalizable findings. However, it is observed
that there is a lack of collective understanding in RecSys academic research.
The root of this issue may lie in the simplification of research task
definitions, and an overemphasis on modeling the decision outcomes rather than
the decision-making process. That is, we often conceptualize RecSys as the task
of predicting missing values in a static user-item interaction matrix, rather
than predicting a user's decision on the next interaction within a dynamic,
changing, and application-specific context. There exists a mismatch between the
inputs accessible to a model and the information available to users during
their decision-making process, yet the model is tasked to predict users'
decisions. While collaborative filtering is effective in learning general
preferences from historical records, it is crucial to also consider the dynamic
contextual factors in practical settings. Defining research tasks based on
application scenarios using domain-specific datasets may lead to more
insightful findings. Accordingly, viable solutions and effective evaluations
can emerge for different application scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ACM SIGWEB Newsletter, Spring 2024:
  https://dl.acm.org/doi/10.1145/3663752.3663756</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Group-aware Search Success 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolun Wu, Bhaskar Mitra, Nick Craswell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional measures of search success often overlook the varying information
needs of different demographic groups. To address this gap, we introduce a
novel metric, named Group-aware Search Success (GA-SS). GA-SS redefines search
success to ensure that all demographic groups achieve satisfaction from search
outcomes. We introduce a comprehensive mathematical framework to calculate
GA-SS, incorporating both static and stochastic ranking policies and
integrating user browsing models for a more accurate assessment. In addition,
we have proposed Group-aware Most Popular Completion (gMPC) ranking model to
account for demographic variances in user intent, aligning more closely with
the diverse needs of all user groups. We empirically validate our metric and
approach with two real-world datasets: one focusing on query auto-completion
and the other on movie recommendations, where the results highlight the impact
of stochasticity and the complex interplay among various search success
metrics. Our findings advocate for a more inclusive approach in measuring
search success, as well as inspiring future investigations into the quality of
service of search.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-07-01T05:27:21.352638898Z">
            2024-07-01 05:27:21 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
